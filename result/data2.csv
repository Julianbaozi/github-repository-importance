full_name,size,stars,watches,forks,owner_type,if_fork,commits,branches,releases,contributors,license,description,website,topics,labels,milestones,open_issues,closed_issues,open_issues_recent,closed_issues_recent,open_prs,closed_prs,open_prs_recent,closed_prs_recent,age,recent_contributors,recent_commits,recent_added,recent_deleted,dependent_repositories,dependent_packages,repositories,people,followers,info,readme
memcached/memcached,8496,10155,737,2796,Organization,False,1697,5,96,156,False,memcached development tree,https://memcached.org,0,9,1,36,277,13,32,10,362,7,43,6228,11,88,2864,1405,0,0,5,5,,,"# Memcached

Memcached is a high performance multithreaded event-based key/value cache
store intended to be used in a distributed system.

See: https://memcached.org/about

A fun story explaining usage: https://memcached.org/tutorial

If you're having trouble, try the wiki: https://memcached.org/wiki

If you're trying to troubleshoot odd behavior or timeouts, see:
https://memcached.org/timeouts

https://memcached.org/ is a good resource in general. Please use the mailing
list to ask questions, github issues aren't seen by everyone!

## Dependencies

* libevent, https://www.monkey.org/~provos/libevent/ (libevent-dev)
* libseccomp, (optional, experimental, linux) - enables process restrictions for
  better security. Tested only on x86-64 architectures.
* openssl, (optional) - enables TLS support. need relatively up to date
  version.

## Environment

Be warned that the -k (mlockall) option to memcached might be
dangerous when using a large cache.  Just make sure the memcached machines
don't swap.  memcached does non-blocking network I/O, but not disk.  (it
should never go to disk, or you've lost the whole point of it)

## Build status

See https://build.memcached.org/ for multi-platform regression testing status.

## Bug reports

Feel free to use the issue tracker on github.

**If you are reporting a security bug** please contact a maintainer privately.
We follow responsible disclosure: we handle reports privately, prepare a
patch, allow notifications to vendor lists. Then we push a fix release and your
bug can be posted publicly with credit in our release notes and commit
history.

## Website

* https://www.memcached.org

## Contributing

See https://github.com/memcached/memcached/wiki/DevelopmentRepos"
bilibili/ijkplayer,8034,26693,1218,6964,Organization,False,2584,6,78,40,False,"Android/iOS video player based on FFmpeg n3.4, with MediaCodec, VideoToolbox support.",,6,27,0,2436,2418,97,20,49,140,2,0,2567,0,0,0,0,0,0,58,25,,,"# ijkplayer

 Platform | Build Status
 -------- | ------------
 Android | [![Build Status](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-android.svg?branch=master)](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-android)
 iOS | [![Build Status](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-ios.svg?branch=master)](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-ios)

Video player based on [ffplay](http://ffmpeg.org)

### Download

- Android:
 - Gradle
```
# required
allprojects {
    repositories {
        jcenter()
    }
}

dependencies {
    # required, enough for most devices.
    compile 'tv.danmaku.ijk.media:ijkplayer-java:0.8.8'
    compile 'tv.danmaku.ijk.media:ijkplayer-armv7a:0.8.8'

    # Other ABIs: optional
    compile 'tv.danmaku.ijk.media:ijkplayer-armv5:0.8.8'
    compile 'tv.danmaku.ijk.media:ijkplayer-arm64:0.8.8'
    compile 'tv.danmaku.ijk.media:ijkplayer-x86:0.8.8'
    compile 'tv.danmaku.ijk.media:ijkplayer-x86_64:0.8.8'

    # ExoPlayer as IMediaPlayer: optional, experimental
    compile 'tv.danmaku.ijk.media:ijkplayer-exo:0.8.8'
}
```
- iOS
 - in coming...

### My Build Environment
- Common
 - Mac OS X 10.11.5
- Android
 - [NDK r10e](http://developer.android.com/tools/sdk/ndk/index.html)
 - Android Studio 2.1.3
 - Gradle 2.14.1
- iOS
 - Xcode 7.3 (7D175)
- [HomeBrew](http://brew.sh)
 - ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""
 - brew install git

### Latest Changes
- [NEWS.md](NEWS.md)

### Features
- Common
 - remove rarely used ffmpeg components to reduce binary size [config/module-lite.sh](config/module-lite.sh)
 - workaround for some buggy online video.
- Android
 - platform: API 9~23
 - cpu: ARMv7a, ARM64v8a, x86 (ARMv5 is not tested on real devices)
 - api: [MediaPlayer-like](android/ijkplayer/ijkplayer-java/src/main/java/tv/danmaku/ijk/media/player/IMediaPlayer.java)
 - video-output: NativeWindow, OpenGL ES 2.0
 - audio-output: AudioTrack, OpenSL ES
 - hw-decoder: MediaCodec (API 16+, Android 4.1+)
 - alternative-backend: android.media.MediaPlayer, ExoPlayer
- iOS
 - platform: iOS 7.0~10.2.x
 - cpu: armv7, arm64, i386, x86_64, (armv7s is obselete)
 - api: [MediaPlayer.framework-like](ios/IJKMediaPlayer/IJKMediaPlayer/IJKMediaPlayback.h)
 - video-output: OpenGL ES 2.0
 - audio-output: AudioQueue, AudioUnit
 - hw-decoder: VideoToolbox (iOS 8+)
 - alternative-backend: AVFoundation.Framework.AVPlayer, MediaPlayer.Framework.MPMoviePlayerControlelr (obselete since iOS 8)

### NOT-ON-PLAN
- obsolete platforms (Android: API-8 and below; iOS: pre-6.0)
- obsolete cpu: ARMv5, ARMv6, MIPS (I don't even have these types of devices…)
- native subtitle render
- avfilter support

### Before Build
```
# install homebrew, git, yasm
ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""
brew install git
brew install yasm

# add these lines to your ~/.bash_profile or ~/.profile
# export ANDROID_SDK=<your sdk path>
# export ANDROID_NDK=<your ndk path>

# on Cygwin (unmaintained)
# install git, make, yasm
```

- If you prefer more codec/format
```
cd config
rm module.sh
ln -s module-default.sh module.sh
cd android/contrib
# cd ios
sh compile-ffmpeg.sh clean
```

- If you prefer less codec/format for smaller binary size (include hevc function)
```
cd config
rm module.sh
ln -s module-lite-hevc.sh module.sh
cd android/contrib
# cd ios
sh compile-ffmpeg.sh clean
```

- If you prefer less codec/format for smaller binary size (by default)
```
cd config
rm module.sh
ln -s module-lite.sh module.sh
cd android/contrib
# cd ios
sh compile-ffmpeg.sh clean
```

- For Ubuntu/Debian users.
```
# choose [No] to use bash
sudo dpkg-reconfigure dash
```

- If you'd like to share your config, pull request is welcome.

### Build Android
```
git clone https://github.com/Bilibili/ijkplayer.git ijkplayer-android
cd ijkplayer-android
git checkout -B latest k0.8.8

./init-android.sh

cd android/contrib
./compile-ffmpeg.sh clean
./compile-ffmpeg.sh all

cd ..
./compile-ijk.sh all

# Android Studio:
#     Open an existing Android Studio project
#     Select android/ijkplayer/ and import
#
#     define ext block in your root build.gradle
#     ext {
#       compileSdkVersion = 23       // depending on your sdk version
#       buildToolsVersion = ""23.0.0"" // depending on your build tools version
#
#       targetSdkVersion = 23        // depending on your sdk version
#     }
#
# If you want to enable debugging ijkplayer(native modules) on Android Studio 2.2+: (experimental)
#     sh android/patch-debugging-with-lldb.sh armv7a
#     Install Android Studio 2.2(+)
#     Preference -> Android SDK -> SDK Tools
#     Select (LLDB, NDK, Android SDK Build-tools,Cmake) and install
#     Open an existing Android Studio project
#     Select android/ijkplayer
#     Sync Project with Gradle Files
#     Run -> Edit Configurations -> Debugger -> Symbol Directories
#     Add ""ijkplayer-armv7a/.externalNativeBuild/ndkBuild/release/obj/local/armeabi-v7a"" to Symbol Directories
#     Run -> Debug 'ijkplayer-example'
#     if you want to reverse patches:
#     sh patch-debugging-with-lldb.sh reverse armv7a
#
# Eclipse: (obselete)
#     File -> New -> Project -> Android Project from Existing Code
#     Select android/ and import all project
#     Import appcompat-v7
#     Import preference-v7
#
# Gradle
#     cd ijkplayer
#     gradle

```


### Build iOS
```
git clone https://github.com/Bilibili/ijkplayer.git ijkplayer-ios
cd ijkplayer-ios
git checkout -B latest k0.8.8

./init-ios.sh

cd ios
./compile-ffmpeg.sh clean
./compile-ffmpeg.sh all

# Demo
#     open ios/IJKMediaDemo/IJKMediaDemo.xcodeproj with Xcode
# 
# Import into Your own Application
#     Select your project in Xcode.
#     File -> Add Files to ... -> Select ios/IJKMediaPlayer/IJKMediaPlayer.xcodeproj
#     Select your Application's target.
#     Build Phases -> Target Dependencies -> Select IJKMediaFramework
#     Build Phases -> Link Binary with Libraries -> Add:
#         IJKMediaFramework.framework
#
#         AudioToolbox.framework
#         AVFoundation.framework
#         CoreGraphics.framework
#         CoreMedia.framework
#         CoreVideo.framework
#         libbz2.tbd
#         libz.tbd
#         MediaPlayer.framework
#         MobileCoreServices.framework
#         OpenGLES.framework
#         QuartzCore.framework
#         UIKit.framework
#         VideoToolbox.framework
#
#         ... (Maybe something else, if you get any link error)
# 
```


### Support (支持) ###
- Please do not send e-mail to me. Public technical discussion on github is preferred.
- 请尽量在 github 上公开讨论[技术问题](https://github.com/bilibili/ijkplayer/issues)，不要以邮件方式私下询问，恕不一一回复。


### License

```
Copyright (c) 2017 Bilibili
Licensed under LGPLv2.1 or later
```

ijkplayer required features are based on or derives from projects below:
- LGPL
  - [FFmpeg](http://git.videolan.org/?p=ffmpeg.git)
  - [libVLC](http://git.videolan.org/?p=vlc.git)
  - [kxmovie](https://github.com/kolyvan/kxmovie)
  - [soundtouch](http://www.surina.net/soundtouch/sourcecode.html)
- zlib license
  - [SDL](http://www.libsdl.org)
- BSD-style license
  - [libyuv](https://code.google.com/p/libyuv/)
- ISC license
  - [libyuv/source/x86inc.asm](https://code.google.com/p/libyuv/source/browse/trunk/source/x86inc.asm)

android/ijkplayer-exo is based on or derives from projects below:
- Apache License 2.0
  - [ExoPlayer](https://github.com/google/ExoPlayer)

android/example is based on or derives from projects below:
- GPL
  - [android-ndk-profiler](https://github.com/richq/android-ndk-profiler) (not included by default)

ios/IJKMediaDemo is based on or derives from projects below:
- Unknown license
  - [iOS7-BarcodeScanner](https://github.com/jpwiddy/iOS7-BarcodeScanner)

ijkplayer's build scripts are based on or derives from projects below:
- [gas-preprocessor](http://git.libav.org/?p=gas-preprocessor.git)
- [VideoLAN](http://git.videolan.org)
- [yixia/FFmpeg-Android](https://github.com/yixia/FFmpeg-Android)
- [kewlbear/FFmpeg-iOS-build-script](https://github.com/kewlbear/FFmpeg-iOS-build-script) 

### Commercial Use
ijkplayer is licensed under LGPLv2.1 or later, so itself is free for commercial use under LGPLv2.1 or later

But ijkplayer is also based on other different projects under various licenses, which I have no idea whether they are compatible to each other or to your product.

[IANAL](https://en.wikipedia.org/wiki/IANAL), you should always ask your lawyer for these stuffs before use it in your product."
vurtun/nuklear,12127,13220,576,1081,User,False,1712,1,0,100,False,A single-header ANSI C gui library,,6,9,0,208,451,0,0,31,249,0,0,1930,0,0,0,0,0,0,4,,510,,
ggreer/the_silver_searcher,2396,20377,430,1193,User,False,2021,55,49,189,False,"A code-searching tool similar to ack, but faster.",http://geoff.greer.fm/ag/,6,2,1,354,390,18,8,112,531,12,2,3120,3,3,28,4,0,0,66,,1,,"# The Silver Searcher

A code searching tool similar to `ack`, with a focus on speed.

[![Build Status](https://travis-ci.org/ggreer/the_silver_searcher.svg?branch=master)](https://travis-ci.org/ggreer/the_silver_searcher)

[![Floobits Status](https://floobits.com/ggreer/ag.svg)](https://floobits.com/ggreer/ag/redirect)

[![#ag on Freenode](https://img.shields.io/badge/Freenode-%23ag-brightgreen.svg)](https://webchat.freenode.net/?channels=ag)

Do you know C? Want to improve ag? [I invite you to pair with me](http://geoff.greer.fm/2014/10/13/help-me-get-to-ag-10/).


## What's so great about Ag?

* It is an order of magnitude faster than `ack`.
* It ignores file patterns from your `.gitignore` and `.hgignore`.
* If there are files in your source repo you don't want to search, just add their patterns to a `.ignore` file. (\*cough\* `*.min.js` \*cough\*)
* The command name is 33% shorter than `ack`, and all keys are on the home row!

Ag is quite stable now. Most changes are new features, minor bug fixes, or performance improvements. It's much faster than Ack in my benchmarks:

    ack test_blah ~/code/  104.66s user 4.82s system 99% cpu 1:50.03 total

    ag test_blah ~/code/  4.67s user 4.58s system 286% cpu 3.227 total

Ack and Ag found the same results, but Ag was 34x faster (3.2 seconds vs 110 seconds). My `~/code` directory is about 8GB. Thanks to git/hg/ignore, Ag only searched 700MB of that.

There are also [graphs of performance across releases](http://geoff.greer.fm/ag/speed/).

## How is it so fast?

* Ag uses [Pthreads](https://en.wikipedia.org/wiki/POSIX_Threads) to take advantage of multiple CPU cores and search files in parallel.
* Files are `mmap()`ed instead of read into a buffer.
* Literal string searching uses [Boyer-Moore strstr](https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm).
* Regex searching uses [PCRE's JIT compiler](http://sljit.sourceforge.net/pcre.html) (if Ag is built with PCRE >=8.21).
* Ag calls `pcre_study()` before executing the same regex on every file.
* Instead of calling `fnmatch()` on every pattern in your ignore files, non-regex patterns are loaded into arrays and binary searched.

I've written several blog posts showing how I've improved performance. These include how I [added pthreads](http://geoff.greer.fm/2012/09/07/the-silver-searcher-adding-pthreads/), [wrote my own `scandir()`](http://geoff.greer.fm/2012/09/03/profiling-ag-writing-my-own-scandir/), [benchmarked every revision to find performance regressions](http://geoff.greer.fm/2012/08/25/the-silver-searcher-benchmarking-revisions/), and profiled with [gprof](http://geoff.greer.fm/2012/02/08/profiling-with-gprof/) and [Valgrind](http://geoff.greer.fm/2012/01/23/making-programs-faster-profiling/).


## Installing

### macOS

    brew install the_silver_searcher

or

    port install the_silver_searcher


### Linux

* Ubuntu >= 13.10 (Saucy) or Debian >= 8 (Jessie)

        apt-get install silversearcher-ag
* Fedora 21 and lower

        yum install the_silver_searcher
* Fedora 22+

        dnf install the_silver_searcher
* RHEL7+

        yum install epel-release.noarch the_silver_searcher
* Gentoo

        emerge -a sys-apps/the_silver_searcher
* Arch

        pacman -S the_silver_searcher

* Slackware

        sbopkg -i the_silver_searcher

* openSUSE:

        zypper install the_silver_searcher

* CentOS:
        
        yum install the_silver_searcher

* SUSE Linux Enterprise: Follow [these simple instructions](https://software.opensuse.org/download.html?project=utilities&package=the_silver_searcher).


### BSD

* FreeBSD

        pkg install the_silver_searcher
* OpenBSD/NetBSD

        pkg_add the_silver_searcher

### Windows

* Win32/64

  Unofficial daily builds are [available](https://github.com/k-takata/the_silver_searcher-win32).
  
* Chocolatey

        choco install ag
* MSYS2

        pacman -S mingw-w64-{i686,x86_64}-ag
* Cygwin

  Run the relevant [`setup-*.exe`](https://cygwin.com/install.html), and select ""the\_silver\_searcher"" in the ""Utils"" category.

## Building from source

### Building master

1. Install dependencies (Automake, pkg-config, PCRE, LZMA):
    * macOS:

            brew install automake pkg-config pcre xz
        or

            port install automake pkgconfig pcre xz
    * Ubuntu/Debian:

            apt-get install -y automake pkg-config libpcre3-dev zlib1g-dev liblzma-dev
    * Fedora:

            yum -y install pkgconfig automake gcc zlib-devel pcre-devel xz-devel
    * CentOS:

            yum -y groupinstall ""Development Tools""
            yum -y install pcre-devel xz-devel zlib-devel
    * openSUSE:

            zypper source-install --build-deps-only the_silver_searcher

    * Windows: It's complicated. See [this wiki page](https://github.com/ggreer/the_silver_searcher/wiki/Windows).
2. Run the build script (which just runs aclocal, automake, etc):

        ./build.sh

   On Windows (inside an msys/MinGW shell):

        make -f Makefile.w32
3. Make install:

        sudo make install


### Building a release tarball

GPG-signed releases are available [here](http://geoff.greer.fm/ag).

Building release tarballs requires the same dependencies, except for automake and pkg-config. Once you've installed the dependencies, just run:

    ./configure
    make
    make install

You may need to use `sudo` or run as root for the make install.


## Editor Integration

### Vim

You can use Ag with [ack.vim](https://github.com/mileszs/ack.vim) by adding the following line to your `.vimrc`:

    let g:ackprg = 'ag --nogroup --nocolor --column'

or:

    let g:ackprg = 'ag --vimgrep'

Which has the same effect but will report every match on the line.

### Emacs

You can use [ag.el][] as an Emacs front-end to Ag. See also: [helm-ag].

[ag.el]: https://github.com/Wilfred/ag.el
[helm-ag]: https://github.com/syohex/emacs-helm-ag

### TextMate

TextMate users can use Ag with [my fork](https://github.com/ggreer/AckMate) of the popular AckMate plugin, which lets you use both Ack and Ag for searching. If you already have AckMate you just want to replace Ack with Ag, move or delete `""~/Library/Application Support/TextMate/PlugIns/AckMate.tmplugin/Contents/Resources/ackmate_ack""` and run `ln -s /usr/local/bin/ag ""~/Library/Application Support/TextMate/PlugIns/AckMate.tmplugin/Contents/Resources/ackmate_ack""`

## Other stuff you might like

* [Ack](https://github.com/petdance/ack2) - Better than grep. Without Ack, Ag would not exist.
* [ack.vim](https://github.com/mileszs/ack.vim)
* [Exuberant Ctags](http://ctags.sourceforge.net/) - Faster than Ag, but it builds an index beforehand. Good for *really* big codebases.
* [Git-grep](http://git-scm.com/docs/git-grep) - As fast as Ag but only works on git repos.
* [ripgrep](https://github.com/BurntSushi/ripgrep)
* [Sack](https://github.com/sampson-chen/sack) - A utility that wraps Ack and Ag. It removes a lot of repetition from searching and opening matching files."
git/git,141126,32929,2190,19259,Organization,False,59611,5,772,1373,False,Git Source Code Mirror - This is a publish-only repository and all pull requests are ignored. Please follow Documentation/SubmittingPatches procedure for any of your improvements.,,2,4,0,,,,,32,681,25,81,5542,33,853,84103,63199,0,0,7,17,,,"[![Build status](https://github.com/git/git/workflows/CI/PR/badge.svg)](https://github.com/git/git/actions?query=branch%3Amaster+event%3Apush)

Git - fast, scalable, distributed revision control system
=========================================================

Git is a fast, scalable, distributed revision control system with an
unusually rich command set that provides both high-level operations
and full access to internals.

Git is an Open Source project covered by the GNU General Public
License version 2 (some parts of it are under different licenses,
compatible with the GPLv2). It was originally written by Linus
Torvalds with help of a group of hackers around the net.

Please read the file [INSTALL][] for installation instructions.

Many Git online resources are accessible from <https://git-scm.com/>
including full documentation and Git related tools.

See [Documentation/gittutorial.txt][] to get started, then see
[Documentation/giteveryday.txt][] for a useful minimum set of commands, and
`Documentation/git-<commandname>.txt` for documentation of each command.
If git has been correctly installed, then the tutorial can also be
read with `man gittutorial` or `git help tutorial`, and the
documentation of each command with `man git-<commandname>` or `git help
<commandname>`.

CVS users may also want to read [Documentation/gitcvs-migration.txt][]
(`man gitcvs-migration` or `git help cvs-migration` if git is
installed).

The user discussion and development of Git take place on the Git
mailing list -- everyone is welcome to post bug reports, feature
requests, comments and patches to git@vger.kernel.org (read
[Documentation/SubmittingPatches][] for instructions on patch submission).
To subscribe to the list, send an email with just ""subscribe git"" in
the body to majordomo@vger.kernel.org. The mailing list archives are
available at <https://lore.kernel.org/git/>,
<http://marc.info/?l=git> and other archival sites.

Issues which are security relevant should be disclosed privately to
the Git Security mailing list <git-security@googlegroups.com>.

The maintainer frequently sends the ""What's cooking"" reports that
list the current status of various development topics to the mailing
list.  The discussion following them give a good reference for
project status, development direction and remaining tasks.

The name ""git"" was given by Linus Torvalds when he wrote the very
first version. He described the tool as ""the stupid content tracker""
and the name as (depending on your mood):

 - random three-letter combination that is pronounceable, and not
   actually used by any common UNIX command.  The fact that it is a
   mispronunciation of ""get"" may or may not be relevant.
 - stupid. contemptible and despicable. simple. Take your pick from the
   dictionary of slang.
 - ""global information tracker"": you're in a good mood, and it actually
   works for you. Angels sing, and a light suddenly fills the room.
 - ""goddamn idiotic truckload of sh*t"": when it breaks

[INSTALL]: INSTALL
[Documentation/gittutorial.txt]: Documentation/gittutorial.txt
[Documentation/giteveryday.txt]: Documentation/giteveryday.txt
[Documentation/gitcvs-migration.txt]: Documentation/gitcvs-migration.txt
[Documentation/SubmittingPatches]: Documentation/SubmittingPatches"
irungentoo/toxcore,10353,8601,598,1246,User,False,3771,2,1,155,False,The future of online communications.,https://tox.chat/,2,25,2,94,551,0,0,7,931,0,1,2546,0,0,0,0,0,0,13,,438,,"![Project Tox](https://raw.github.com/irungentoo/toxcore/master/other/tox.png ""Project Tox"")
***

With the rise of government surveillance programs, Tox, a FOSS initiative, aims to be an easy to use, all-in-one communication platform that ensures full privacy and secure message delivery.<br /> <br />

[**Website**](https://tox.chat) **|** [**Wiki**](https://wiki.tox.chat/) **|** [**Blog**](https://blog.tox.chat/) **|** [**FAQ**](https://wiki.tox.chat/doku.php?id=users:faq) **|** [**Binaries/Downloads**](https://wiki.tox.chat/Binaries) **|** [**Clients**](https://wiki.tox.chat/doku.php?id=clients) **|** [**Compiling**](/INSTALL.md)

**IRC Channels:** [#tox@freenode](https://webchat.freenode.net/?channels=tox), [#tox-dev@freenode](https://webchat.freenode.net/?channels=tox-dev)


## The Complex Stuff:
### UDP vs. TCP
Tox must use UDP simply because [hole punching](https://en.wikipedia.org/wiki/UDP_hole_punching) with TCP is not as reliable.
However, Tox does use [TCP relays](/docs/TCP_Network.txt) as a fallback if it encounters a firewall that prevents UDP hole punching.

### Connecting & Communicating
Every peer is represented as a [byte string](https://en.wikipedia.org/wiki/String_(computer_science)) (the public key [Tox ID] of the peer). By using torrent-style DHT, peers can find the IP of other peers by using their Tox ID. Once the IP is obtained, peers can initiate a [secure](/docs/updates/Crypto.md) connection with each other. Once the connection is made, peers can exchange messages, send files, start video chats, etc. using encrypted communications.


**Current build status:** [![Build Status](https://travis-ci.org/irungentoo/toxcore.png?branch=master)](https://travis-ci.org/irungentoo/toxcore)


## Q&A:

### What are your goals with Tox?

We want Tox to be as simple as possible while remaining as secure as possible.

### Why are you doing this? There are already a bunch of free Skype alternatives.
The goal of this project is to create a configuration-free P2P Skype replacement. “Configuration-free” means that the user will simply have to open the program and will be capable of adding people and communicating with them without having to set up an account. There are many so-called Skype replacements, but all of them are either hard to configure for the normal user or suffer from being way too centralized.

## TODO:
- [TODO](/docs/TODO.md)


## Documentation:

- [Compiling](/INSTALL.md)
- [DHT Protocol](/docs/updates/DHT.md)<br />
- [Crypto](/docs/updates/Crypto.md)<br />"
git-up/GitUp,50051,9058,175,617,Organization,False,378,5,48,40,False,The Git interface you've been missing all your life has finally arrived.,http://gitup.co,0,8,0,245,293,34,17,3,139,2,10,1769,7,19,2603,3730,0,0,12,0,,,"[![Build Status](https://travis-ci.org/git-up/GitUp.svg?branch=master)](https://travis-ci.org/git-up/GitUp)

GitUp
=====

**Work quickly, safely, and without headaches. The Git interface you've been missing all your life has finally arrived.**

<p align=""center"">
<img src=""https://i.imgur.com/JuQIxJu.png"" width=""50%"" height=""50%""><img src=""https://i.imgur.com/9rgXktz.png"" width=""50%"" height=""50%"">
</p>

Git recently celebrated its 10 years anniversary, but most engineers are still confused by its intricacy (3 of the [top 5 questions of all time](http://stackoverflow.com/questions?sort=votes) on Stack Overflow are Git related). Since Git turns even simple actions into mystifying commands (“git add” to stage versus “git reset HEAD” to unstage anyone?), it’s no surprise users waste time, get frustrated, distract the rest of their team for help, or worse, screw up their repo!

GitUp is a bet to invent a new Git interaction model that lets engineers of all levels work quickly, safely, and without headaches. It's unlike any other Git client out there from the way it’s built (it interacts directly with the Git database on disk), to the way it works (you manipulate the repository graph instead of manipulating commits).

With GitUp, you get a truly efficient Git client for Mac:
- A **live and interactive repo graph** (edit, reorder, fixup, merge commits…),
- **Unlimited undo / redo** of almost all operations (even rebases and merges),
- Time Machine like **snapshots for 1-click rollbacks** to previous repo states,
- Features that don’t even exist natively in Git like a **visual commit splitter** or a **unified reflog browser**,
- **Instant search across the entire repo** including diff contents, 
- A **ridiculously fast UI**, often faster than the command line.

*GitUp was created by [@swisspol](https://github.com/swisspol) in late 2014 as a bet to reinvent the way developers interact with Git. After several months of work, it was made available in pre-release early 2015 and reached the [top of Hacker News](https://news.ycombinator.com/item?id=9653978) along with being [featured by Product Hunt](http://www.producthunt.com/tech/gitup-1) and [Daring Fireball](http://daringfireball.net/linked/2015/06/04/gitup). 30,000 lines of code later, GitUp reached 1.0 mid-August 2015 and was released open source as a gift to the developer community.*

Getting Started
===============

**Learn all about GitUp and download the latest release from http://gitup.co.**

**Read the [docs](https://github.com/git-up/GitUp/wiki) and use [GitHub Issues](https://github.com/git-up/GitUp/issues) for support & feedback.**

Releases notes are available at https://github.com/git-up/GitUp/releases. Builds tagged with a `v` (e.g. `v1.2.3`) are released on the ""Stable"" channel, while builds tagged with a `b` (e.g. `b1234`) are only released on the ""Continuous"" channel. You can change the update channel used by GitUp in the app preferences.

To build GitUp yourself, simply run the command `git clone --recursive https://github.com/git-up/GitUp.git` in Terminal, then open the `GitUp/GitUp.xcodeproj` Xcode project and hit Run.

**IMPORTANT:** If you do not have an Apple ID with a developer account for code signing Mac apps, the build  will fail with a code signing error. Simply delete the ""Code Signing Identity"" build setting of the ""Application"" target to work around the issue:

<p align=""center"">
<img src=""http://i.imgur.com/dWpJExk.png"">
</p>

**Alternatively**, if you do have a developer account, you can create the file ""Xcode-Configurations/DEVELOPMENT_TEAM.xcconfig"" with the following build setting as its content:
> DEVELOPMENT_TEAM = [Your TeamID]

For a more detailed description of this, you can have a look at the comments at the end of the file ""Xcode-Configurations/Base.xcconfig"". 

GitUpKit
========

**GitUp is built as a thin layer on top of a reusable generic Git toolkit called ""GitUpKit"". This means that you can use that same GitUpKit framework to build your very own Git UI!**

*GitUpKit has a very different goal than [ObjectiveGit](https://github.com/libgit2/objective-git). Instead of offering extensive raw bindings to [libgit2](https://github.com/libgit2/libgit2), GitUpKit only uses a minimal subset of libgit2 and reimplements everything else on top of it (it has its own ""rebase engine"" for instance).
This allows it to expose a very tight and consistent API, that completely follows Obj-C conventions and hides away the libgit2 complexity and sometimes inconsistencies. GitUpKit adds on top of that a number of exclusive and powerful features, from undo/redo and Time Machine like snapshots, to entire drop-in UI components.*

Architecture
------------

The GitUpKit source code is organized as 2 independent layers communicating only through the use of public APIs:

**Base Layer (depends on Foundation only and is compatible with OS X and iOS)**
- `Core/`: wrapper around the required minimal functionality of [libgit2](https://github.com/libgit2/libgit2), on top of which is then implemented all the Git functionality required by GitUp (note that GitUp uses a [slightly customized fork](https://github.com/git-up/libgit2/tree/gitup) of libgit2)
- `Extensions/`: categories on the `Core` classes to add convenience features implemented only using the public APIs

**UI Layer (depends on AppKit and is compatible with OS X only)**
- `Interface/`: low-level view classes e.g. `GIGraphView` to render the GitUp Map view
- `Utilities/`: interface utility classes e.g. the base view controller class `GIViewController`
- `Components/`: reusable single-view view controllers e.g. `GIDiffContentsViewController` to render a diff
- `Views/`: high-level reusable multi-views view controllers e.g. `GIAdvancedCommitViewController` to implement the entire GitUp Advanced Commit view

**IMPORTANT**: If the preprocessor constant `DEBUG` is defined to a non-zero value when building GitUpKit (this is the default when building in ""Debug"" configuration), a number of extra consistency checks are enabled at run time as well as extra logging. Be aware that this overhead can significantly affect performance.

GitUpKit API
------------

Using the GitUpKit API should be pretty straightforward since it is organized by functionality (e.g. repository, branches, commits, interface components, etc...) and a best effort has been made to name functions clearly.

Regarding the ""Core"" APIs, the best way to learn them is to peruse the associated unit tests - for instance see [the branch tests](GitUpKit/Core/GCBranch-Tests.m) for the branch API.

Here is some sample code to get you started (error handling is left as an exercise to the reader):

**Opening and browsing a repository:**
```objc
// Open repo
GCRepository* repo = [[GCRepository alloc] initWithExistingLocalRepository:<PATH> error:NULL];

// Make sure repo is clean
assert([repo checkClean:kGCCleanCheckOption_IgnoreUntrackedFiles error:NULL]);

// List all branches
NSArray* branches = [repo listAllBranches:NULL];
NSLog(@""%@"", branches);

// Lookup HEAD
GCLocalBranch* headBranch;  // This would be nil if the HEAD is detached
GCCommit* headCommit;
[repo lookupHEADCurrentCommit:&headCommit branch:&headBranch error:NULL];
NSLog(@""%@ = %@"", headBranch, headCommit);

// Load the *entire* repo history in memory for fast access, including all commits, branches and tags
GCHistory* history = [repo loadHistoryUsingSorting:kGCHistorySorting_ReverseChronological error:NULL];
assert(history);
NSLog(@""%lu commits total"", history.allCommits.count);
NSLog(@""%@\n%@"", history.rootCommits, history.leafCommits);
```

**Modifying a repository:**
```objc
// Take a snapshot of the repo
GCSnapshot* snapshot = [repo takeSnapshot:NULL];

// Create a new branch and check it out
GCLocalBranch* newBranch = [repo createLocalBranchFromCommit:headCommit withName:@""temp"" force:NO error:NULL];
NSLog(@""%@"", newBranch);
assert([repo checkoutLocalBranch:newBranch options:0 error:NULL]);

// Add a file to the index
[[NSData data] writeToFile:[repo.workingDirectoryPath stringByAppendingPathComponent:@""empty.data""] atomically:YES];
assert([repo addFileToIndex:@""empty.data"" error:NULL]);

// Check index status
GCDiff* diff = [repo diffRepositoryIndexWithHEAD:nil options:0 maxInterHunkLines:0 maxContextLines:0 error:NULL];
assert(diff.deltas.count == 1);
NSLog(@""%@"", diff);

// Create a commit
GCCommit* newCommit = [repo createCommitFromHEADWithMessage:@""Added file"" error:NULL];
assert(newCommit);
NSLog(@""%@"", newCommit);

// Restore repo to saved snapshot before topic branch and commit were created
BOOL success = [repo restoreSnapshot:snapshot withOptions:kGCSnapshotOption_IncludeAll reflogMessage:@""Rolled back"" didUpdateReferences:NULL error:NULL];
assert(success);
  
// Make sure topic branch is gone
assert([repo findLocalBranchWithName:@""temp"" error:NULL] == nil);
  
// Update workdir and index to match HEAD
assert([repo resetToHEAD:kGCResetMode_Hard error:NULL]);
```

Complete Example #1: GitDown
----------------------------

[GitDown](Examples/GitDown) is a very basic app that prompts the user for a repo and displays an interactive and live-updating list of its stashes (all with ~20 lines of code in `-[AppDelegate applicationDidFinishLaunching:]`):

<p align=""center"">
<img src=""http://i.imgur.com/ZfxM7su.png"">
</p>

Through GitUpKit, this basic app also gets for free unlimited undo/redo, unified and side-by-side diffs, text selection and copy, keyboard shortcuts, etc...

This source code also demonstrates how to use some other GitUpKit view controllers as well as building a customized one.

Complete Example #2: GitDiff
----------------------------

[GitDiff](Examples/GitDiff) demonstrates how to create a view controller that displays a live updating diff between `HEAD` and the workdir à la `git diff HEAD`:

<p align=""center"">
<img src=""http://i.imgur.com/29hxDcJ.png"">
</p>

Complete Example #3: GitY
-------------------------

[GitY](Examples/GitY) is a [GitX](http://gitx.frim.nl/) clone built using GitUpKit and less than 200 lines of code:

<p align=""center"">
<img src=""http://i.imgur.com/6cuPcT4.png"">
</p>

Complete Example #4: iGit
-------------------------

[iGit](Examples/iGit) is a test iOS app that simply uses GitUpKit to clone a GitHub repo and perform a commit.

Contributing
============

See [CONTRIBUTING.md](CONTRIBUTING.md).

Credits
=======

- [@swisspol](https://github.com/swisspol): concept and code
- [@wwayneee](https://github.com/wwayneee): UI design
- [@jayeb](https://github.com/jayeb): website

*Also a big thanks to the fine [libgit2](https://libgit2.github.com/) contributors without whom GitUp would have never existed!*

License
=======

GitUp is copyright 2015-2018 Pierre-Olivier Latour and available under [GPL v3 license](http://www.gnu.org/licenses/gpl-3.0.txt). See the [LICENSE](LICENSE) file in the project for more information.

**IMPORTANT:** GitUp includes some other open-source projects and such projects remain under their own license."
libgit2/libgit2,53375,7462,412,1935,Organization,False,13256,101,83,395,False,"A cross-platform, linkable library implementation of Git that you can use in your application.",https://libgit2.org/,3,16,2,236,1422,45,39,89,3793,29,101,4247,9,239,7059,5864,0,0,21,37,,,"libgit2 - the Git linkable library
==================================

| Build Status | |
| ------------ | - |
| **master** branch CI builds | [![Azure Pipelines Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/libgit2?branchName=master)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=7&branchName=master)   |
| **v1.0 branch** CI builds | [![Azure Pipelines Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/libgit2?branchName=maint/v1.0)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=7&branchName=maint/v1.0) |
| **v0.28 branch** CI builds | [![Azure Pipelines Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/libgit2?branchName=maint/v0.28)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=7&branchName=maint/v0.28) |
| **Nightly** builds | [![Azure Pipelines Build Status](https://libgit2.visualstudio.com/libgit2/_apis/build/status/nightly?branchName=master&label=Full+Build)](https://libgit2.visualstudio.com/libgit2/_build/latest?definitionId=9&branchName=master) [![Coverity Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/coverity?branchName=master&label=Coverity+Build)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=21?branchName=master) [![Coverity Scan Build Status](https://scan.coverity.com/projects/639/badge.svg)](https://scan.coverity.com/projects/639) |

`libgit2` is a portable, pure C implementation of the Git core methods
provided as a linkable library with a solid API, allowing to build Git
functionality into your application.  Language bindings like
[Rugged](https://github.com/libgit2/rugged) (Ruby),
[LibGit2Sharp](https://github.com/libgit2/libgit2sharp) (.NET),
[pygit2](http://www.pygit2.org/) (Python) and
[NodeGit](http://nodegit.org) (Node) allow you to build Git tooling
in your favorite language.

`libgit2` is used to power Git GUI clients like
[GitKraken](https://gitkraken.com/) and [gmaster](https://gmaster.io/)
and on Git hosting providers like [GitHub](https://github.com/),
[GitLab](https://gitlab.com/) and
[Azure DevOps](https://azure.com/devops).
We perform the merge every time you click ""merge pull request"".

`libgit2` is licensed under a **very permissive license** (GPLv2 with a special
Linking Exception).  This basically means that you can link it (unmodified)
with any kind of software without having to release its source code.
Additionally, the example code has been released to the public domain (see the
[separate license](examples/COPYING) for more information).

Table of Contents
=================

* [Quick Start](#quick-start)
* [Getting Help](#getting-help)
* [What It Can Do](#what-it-can-do)
* [Optional dependencies](#optional-dependencies)
* [Initialization](#initialization)
* [Threading](#threading)
* [Conventions](#conventions)
* [Building libgit2 - Using CMake](#building-libgit2---using-cmake)
    * [Building](#building)
    * [Installation](#installation)
    * [Advanced Usage](#advanced-usage)
    * [Compiler and linker options](#compiler-and-linker-options)
    * [MacOS X](#macos-x)
    * [Android](#android)
    * [MinGW](#mingw)
* [Language Bindings](#language-bindings)
* [How Can I Contribute?](#how-can-i-contribute)
* [License](#license)

Quick Start
===========

**Prerequisites** for building libgit2:

1. [CMake](https://cmake.org/), and is recommended to be installed into
   your `PATH`.
2. [Python](https://www.python.org) is used by our test framework, and
   should be installed into your `PATH`.
3. C compiler: libgit2 is C90 and should compile on most compilers.
   * Windows: Visual Studio is recommended
   * Mac: Xcode is recommended
   * Unix: gcc or clang is recommended.

**Build**

1. Create a build directory beneath the libgit2 source directory, and change
   into it: `mkdir build && cd build`
2. Create the cmake build environment: `cmake ..`
3. Build libgit2: `cmake --build .`

Trouble with these steps?  Read our [troubleshooting guide](docs/troubleshooting.md).
More detailed build guidance is available below.

Getting Help
============

**Chat with us**

- via IRC: join [#libgit2](https://webchat.freenode.net/#libgit2) on Freenode
- via Slack: visit [slack.libgit2.org](http://slack.libgit2.org/) to sign up,
  then join us in `#libgit2`

**Getting Help**

If you have questions about the library, please be sure to check out the
[API documentation](http://libgit2.github.com/libgit2/).  If you still have
questions, reach out to us on Slack or post a question on 
[StackOverflow](http://stackoverflow.com/questions/tagged/libgit2) (with the `libgit2` tag).

**Reporting Bugs**

Please open a [GitHub Issue](https://github.com/libgit2/libgit2/issues) and
include as much information as possible.  If possible, provide sample code
that illustrates the problem you're seeing.  If you're seeing a bug only
on a specific repository, please provide a link to it if possible.

We ask that you not open a GitHub Issue for help, only for bug reports.

**Reporting Security Issues**

Please have a look at SECURITY.md.

What It Can Do
==============

libgit2 provides you with the ability to manage Git repositories in the
programming language of your choice.  It's used in production to power many
applications including GitHub.com, Plastic SCM and Azure DevOps.

It does not aim to replace the git tool or its user-facing commands. Some APIs
resemble the plumbing commands as those align closely with the concepts of the
Git system, but most commands a user would type are out of scope for this
library to implement directly.

The library provides:

* SHA conversions, formatting and shortening
* abstracted ODB backend system
* commit, tag, tree and blob parsing, editing, and write-back
* tree traversal
* revision walking
* index file (staging area) manipulation
* reference management (including packed references)
* config file management
* high level repository management
* thread safety and reentrancy
* descriptive and detailed error messages
* ...and more (over 175 different API calls)

As libgit2 is purely a consumer of the Git system, we have to
adjust to changes made upstream. This has two major consequences:

* Some changes may require us to change provided interfaces. While we try to
  implement functions in a generic way so that no future changes are required,
  we cannot promise a completely stable API.
* As we have to keep up with changes in behavior made upstream, we may lag
  behind in some areas. We usually to document these incompatibilities in our
  issue tracker with the label ""git change"".

Optional dependencies
=====================

While the library provides git functionality without the need for
dependencies, it can make use of a few libraries to add to it:

- pthreads (non-Windows) to enable threadsafe access as well as multi-threaded pack generation
- OpenSSL (non-Windows) to talk over HTTPS and provide the SHA-1 functions
- LibSSH2 to enable the SSH transport
- iconv (OSX) to handle the HFS+ path encoding peculiarities

Initialization
===============

The library needs to keep track of some global state. Call

    git_libgit2_init();

before calling any other libgit2 functions. You can call this function many times. A matching number of calls to

    git_libgit2_shutdown();

will free the resources.  Note that if you have worker threads, you should
call `git_libgit2_shutdown` *after* those threads have exited.  If you
require assistance coordinating this, simply have the worker threads call
`git_libgit2_init` at startup and `git_libgit2_shutdown` at shutdown.

Threading
=========

See [threading](docs/threading.md) for information

Conventions
===========

See [conventions](docs/conventions.md) for an overview of the external
and internal API/coding conventions we use.

Building libgit2 - Using CMake
==============================

Building
--------

`libgit2` builds cleanly on most platforms without any external dependencies.
Under Unix-like systems, like Linux, \*BSD and Mac OS X, libgit2 expects `pthreads` to be available;
they should be installed by default on all systems. Under Windows, libgit2 uses the native Windows API
for threading.

The `libgit2` library is built using [CMake](<https://cmake.org/>) (version 2.8 or newer) on all platforms.

On most systems you can build the library using the following commands

 $ mkdir build && cd build
 $ cmake ..
 $ cmake --build .

Alternatively you can point the CMake GUI tool to the CMakeLists.txt file and generate platform specific build project or IDE workspace.

Running Tests
-------------

Once built, you can run the tests from the `build` directory with the command

 $ ctest -V

Alternatively you can run the test suite directly using,

 $ ./libgit2_clar

Invoking the test suite directly is useful because it allows you to execute
individual tests, or groups of tests using the `-s` flag.  For example, to
run the index tests:

    $ ./libgit2_clar -sindex

To run a single test named `index::racy::diff`, which corresponds to the test
function [`test_index_racy__diff`](https://github.com/libgit2/libgit2/blob/master/tests/index/racy.c#L23):

    $ ./libgit2_clar -sindex::racy::diff

The test suite will print a `.` for every passing test, and an `F` for any
failing test.  An `S` indicates that a test was skipped because it is not
applicable to your platform or is particularly expensive.

**Note:** There should be _no_ failing tests when you build an unmodified
source tree from a [release](https://github.com/libgit2/libgit2/releases),
or from the [master branch](https://github.com/libgit2/libgit2/tree/master).
Please contact us or [open an issue](https://github.com/libgit2/libgit2/issues)
if you see test failures.

Installation
------------

To install the library you can specify the install prefix by setting:

 $ cmake .. -DCMAKE_INSTALL_PREFIX=/install/prefix
 $ cmake --build . --target install

Advanced Usage
--------------

For more advanced use or questions about CMake please read <https://cmake.org/Wiki/CMake_FAQ>.

The following CMake variables are declared:

- `CMAKE_INSTALL_BINDIR`: Where to install binaries to.
- `CMAKE_INSTALL_LIBDIR`: Where to install libraries to.
- `CMAKE_INSTALL_INCLUDEDIR`: Where to install headers to.
- `BUILD_SHARED_LIBS`: Build libgit2 as a Shared Library (defaults to ON)
- `BUILD_CLAR`: Build [Clar](https://github.com/vmg/clar)-based test suite (defaults to ON)
- `THREADSAFE`: Build libgit2 with threading support (defaults to ON)

To list all build options and their current value, you can do the
following:

 # Create and set up a build directory
 $ mkdir build
 $ cmake ..
 # List all build options and their values
 $ cmake -L

Compiler and linker options
---------------------------

CMake lets you specify a few variables to control the behavior of the
compiler and linker. These flags are rarely used but can be useful for
64-bit to 32-bit cross-compilation.

- `CMAKE_C_FLAGS`: Set your own compiler flags
- `CMAKE_FIND_ROOT_PATH`: Override the search path for libraries
- `ZLIB_LIBRARY`, `OPENSSL_SSL_LIBRARY` AND `OPENSSL_CRYPTO_LIBRARY`:
Tell CMake where to find those specific libraries

MacOS X
-------

If you want to build a universal binary for Mac OS X, CMake sets it
all up for you if you use `-DCMAKE_OSX_ARCHITECTURES=""i386;x86_64""`
when configuring.

Android
-------

Extract toolchain from NDK using, `make-standalone-toolchain.sh` script.
Optionally, crosscompile and install OpenSSL inside of it. Then create CMake
toolchain file that configures paths to your crosscompiler (substitute `{PATH}`
with full path to the toolchain):

 SET(CMAKE_SYSTEM_NAME Linux)
 SET(CMAKE_SYSTEM_VERSION Android)

 SET(CMAKE_C_COMPILER   {PATH}/bin/arm-linux-androideabi-gcc)
 SET(CMAKE_CXX_COMPILER {PATH}/bin/arm-linux-androideabi-g++)
 SET(CMAKE_FIND_ROOT_PATH {PATH}/sysroot/)

 SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
 SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
 SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

Add `-DCMAKE_TOOLCHAIN_FILE={pathToToolchainFile}` to cmake command
when configuring.

MinGW
-----

If you want to build the library in MinGW environment with SSH support enabled,
you may need to pass `-DCMAKE_LIBRARY_PATH=""${MINGW_PREFIX}/${MINGW_CHOST}/lib/""` flag
to CMake when configuring. This is because CMake cannot find the Win32 libraries in
MinGW folders by default and you might see an error message stating that CMake
could not resolve `ws2_32` library during configuration.

Another option would be to install `msys2-w32api-runtime` package before configuring.
This package installs the Win32 libraries into `/usr/lib` folder which is by default
recognized as the library path by CMake. Please note though that this package is meant
for MSYS subsystem which is different from MinGW.

Language Bindings
==================================

Here are the bindings to libgit2 that are currently available:

* C++
    * libqgit2, Qt bindings <https://projects.kde.org/projects/playground/libs/libqgit2/repository/>
* Chicken Scheme
    * chicken-git <https://wiki.call-cc.org/egg/git>
* D
    * dlibgit <https://github.com/s-ludwig/dlibgit>
* Delphi
    * GitForDelphi <https://github.com/libgit2/GitForDelphi>
* Erlang
    * Geef <https://github.com/carlosmn/geef>
* Go
    * git2go <https://github.com/libgit2/git2go>
* GObject
    * libgit2-glib <https://wiki.gnome.org/Projects/Libgit2-glib>
* Guile
 * Guile-Git <https://gitlab.com/guile-git/guile-git>
* Haskell
    * hgit2 <https://github.com/jwiegley/gitlib>
* Java
    * Jagged <https://github.com/ethomson/jagged>
* Javascript / WebAssembly ( browser and nodejs )
    * WASM-git <https://github.com/petersalomonsen/wasm-git>
* Julia
    * LibGit2.jl <https://github.com/JuliaLang/julia/tree/master/stdlib/LibGit2>
* Lua
    * luagit2 <https://github.com/libgit2/luagit2>
* .NET
    * libgit2sharp <https://github.com/libgit2/libgit2sharp>
* Node.js
    * nodegit <https://github.com/nodegit/nodegit>
* Objective-C
    * objective-git <https://github.com/libgit2/objective-git>
* OCaml
    * ocaml-libgit2 <https://github.com/fxfactorial/ocaml-libgit2>
* Parrot Virtual Machine
    * parrot-libgit2 <https://github.com/letolabs/parrot-libgit2>
* Perl
    * Git-Raw <https://github.com/jacquesg/p5-Git-Raw>
* PHP
    * php-git <https://github.com/libgit2/php-git>
* PowerShell
    * PSGit <https://github.com/PoshCode/PSGit>
* Python
    * pygit2 <https://github.com/libgit2/pygit2>
* R
    * git2r <https://github.com/ropensci/git2r>
* Ruby
    * Rugged <https://github.com/libgit2/rugged>
* Rust
    * git2-rs <https://github.com/rust-lang/git2-rs>
* Swift
    * SwiftGit2 <https://github.com/SwiftGit2/SwiftGit2>
* Vala
    * libgit2.vapi <https://github.com/apmasell/vapis/blob/master/libgit2.vapi>

If you start another language binding to libgit2, please let us know so
we can add it to the list.

How Can I Contribute?
==================================

We welcome new contributors!  We have a number of issues marked as
[""up for grabs""](https://github.com/libgit2/libgit2/issues?q=is%3Aissue+is%3Aopen+label%3A%22up+for+grabs%22)
and
[""easy fix""](https://github.com/libgit2/libgit2/issues?utf8=✓&q=is%3Aissue+is%3Aopen+label%3A%22easy+fix%22)
that are good places to jump in and get started.  There's much more detailed
information in our list of [outstanding projects](docs/projects.md).

Please be sure to check the [contribution guidelines](docs/contributing.md) to
understand our workflow, and the libgit2 [coding conventions](docs/conventions.md).

License
==================================

`libgit2` is under GPL2 **with linking exception**. This means you can link to
and use the library from any program, proprietary or open source; paid or
gratis.  However, if you modify libgit2 itself, you must distribute the
source to your modified version of libgit2.

See the [COPYING file](COPYING) for the full license text."
facebook/zstd,23803,12241,392,1143,Organization,False,7718,39,59,152,False,Zstandard - Fast real-time compression algorithm,http://www.zstd.net,0,25,0,41,673,28,72,10,1471,8,166,1972,17,291,18972,14362,0,0,125,168,,,"<p align=""center""><img src=""https://raw.githubusercontent.com/facebook/zstd/dev/doc/images/zstd_logo86.png"" alt=""Zstandard""></p>

__Zstandard__, or `zstd` as short version, is a fast lossless compression algorithm,
targeting real-time compression scenarios at zlib-level and better compression ratios.
It's backed by a very fast entropy stage, provided by [Huff0 and FSE library](https://github.com/Cyan4973/FiniteStateEntropy).

The project is provided as an open-source dual [BSD](LICENSE) and [GPLv2](COPYING) licensed **C** library,
and a command line utility producing and decoding `.zst`, `.gz`, `.xz` and `.lz4` files.
Should your project require another programming language,
a list of known ports and bindings is provided on [Zstandard homepage](http://www.zstd.net/#other-languages).

**Development branch status:**

[![Build Status][travisDevBadge]][travisLink]
[![Build status][AppveyorDevBadge]][AppveyorLink]
[![Build status][CircleDevBadge]][CircleLink]
[![Build status][CirrusDevBadge]][CirrusLink]
[![Fuzzing Status][OSSFuzzBadge]][OSSFuzzLink]

[travisDevBadge]: https://travis-ci.org/facebook/zstd.svg?branch=dev ""Continuous Integration test suite""
[travisLink]: https://travis-ci.org/facebook/zstd
[AppveyorDevBadge]: https://ci.appveyor.com/api/projects/status/xt38wbdxjk5mrbem/branch/dev?svg=true ""Windows test suite""
[AppveyorLink]: https://ci.appveyor.com/project/YannCollet/zstd-p0yf0
[CircleDevBadge]: https://circleci.com/gh/facebook/zstd/tree/dev.svg?style=shield ""Short test suite""
[CircleLink]: https://circleci.com/gh/facebook/zstd
[CirrusDevBadge]: https://api.cirrus-ci.com/github/facebook/zstd.svg?branch=dev
[CirrusLink]: https://cirrus-ci.com/github/facebook/zstd
[OSSFuzzBadge]: https://oss-fuzz-build-logs.storage.googleapis.com/badges/zstd.svg
[OSSFuzzLink]: https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:zstd

## Benchmarks

For reference, several fast compression algorithms were tested and compared
on a server running Arch Linux (`Linux version 5.5.11-arch1-1`),
with a Core i9-9900K CPU @ 5.0GHz,
using [lzbench], an open-source in-memory benchmark by @inikep
compiled with [gcc] 9.3.0,
on the [Silesia compression corpus].

[lzbench]: https://github.com/inikep/lzbench
[Silesia compression corpus]: http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia
[gcc]: https://gcc.gnu.org/

| Compressor name         | Ratio | Compression| Decompress.|
| ---------------         | ------| -----------| ---------- |
| **zstd 1.4.5 -1**       | 2.884 |   500 MB/s |  1660 MB/s |
| zlib 1.2.11 -1          | 2.743 |    90 MB/s |   400 MB/s |
| brotli 1.0.7 -0         | 2.703 |   400 MB/s |   450 MB/s |
| **zstd 1.4.5 --fast=1** | 2.434 |   570 MB/s |  2200 MB/s |
| **zstd 1.4.5 --fast=3** | 2.312 |   640 MB/s |  2300 MB/s |
| quicklz 1.5.0 -1        | 2.238 |   560 MB/s |   710 MB/s |
| **zstd 1.4.5 --fast=5** | 2.178 |   700 MB/s |  2420 MB/s |
| lzo1x 2.10 -1           | 2.106 |   690 MB/s |   820 MB/s |
| lz4 1.9.2               | 2.101 |   740 MB/s |  4530 MB/s |
| **zstd 1.4.5 --fast=7** | 2.096 |   750 MB/s |  2480 MB/s |
| lzf 3.6 -1              | 2.077 |   410 MB/s |   860 MB/s |
| snappy 1.1.8            | 2.073 |   560 MB/s |  1790 MB/s |

[zlib]: http://www.zlib.net/
[LZ4]: http://www.lz4.org/

The negative compression levels, specified with `--fast=#`,
offer faster compression and decompression speed in exchange for some loss in
compression ratio compared to level 1, as seen in the table above.

Zstd can also offer stronger compression ratios at the cost of compression speed.
Speed vs Compression trade-off is configurable by small increments.
Decompression speed is preserved and remains roughly the same at all settings,
a property shared by most LZ compression algorithms, such as [zlib] or lzma.

The following tests were run
on a server running Linux Debian (`Linux version 4.14.0-3-amd64`)
with a Core i7-6700K CPU @ 4.0GHz,
using [lzbench], an open-source in-memory benchmark by @inikep
compiled with [gcc] 7.3.0,
on the [Silesia compression corpus].

Compression Speed vs Ratio | Decompression Speed
---------------------------|--------------------
![Compression Speed vs Ratio](doc/images/CSpeed2.png ""Compression Speed vs Ratio"") | ![Decompression Speed](doc/images/DSpeed3.png ""Decompression Speed"")

A few other algorithms can produce higher compression ratios at slower speeds, falling outside of the graph.
For a larger picture including slow modes, [click on this link](doc/images/DCspeed5.png).


## The case for Small Data compression

Previous charts provide results applicable to typical file and stream scenarios (several MB). Small data comes with different perspectives.

The smaller the amount of data to compress, the more difficult it is to compress. This problem is common to all compression algorithms, and reason is, compression algorithms learn from past data how to compress future data. But at the beginning of a new data set, there is no ""past"" to build upon.

To solve this situation, Zstd offers a __training mode__, which can be used to tune the algorithm for a selected type of data.
Training Zstandard is achieved by providing it with a few samples (one file per sample). The result of this training is stored in a file called ""dictionary"", which must be loaded before compression and decompression.
Using this dictionary, the compression ratio achievable on small data improves dramatically.

The following example uses the `github-users` [sample set](https://github.com/facebook/zstd/releases/tag/v1.1.3), created from [github public API](https://developer.github.com/v3/users/#get-all-users).
It consists of roughly 10K records weighing about 1KB each.

Compression Ratio | Compression Speed | Decompression Speed
------------------|-------------------|--------------------
![Compression Ratio](doc/images/dict-cr.png ""Compression Ratio"") | ![Compression Speed](doc/images/dict-cs.png ""Compression Speed"") | ![Decompression Speed](doc/images/dict-ds.png ""Decompression Speed"")


These compression gains are achieved while simultaneously providing _faster_ compression and decompression speeds.

Training works if there is some correlation in a family of small data samples. The more data-specific a dictionary is, the more efficient it is (there is no _universal dictionary_).
Hence, deploying one dictionary per type of data will provide the greatest benefits.
Dictionary gains are mostly effective in the first few KB. Then, the compression algorithm will gradually use previously decoded content to better compress the rest of the file.

### Dictionary compression How To:

1. Create the dictionary

   `zstd --train FullPathToTrainingSet/* -o dictionaryName`

2. Compress with dictionary

   `zstd -D dictionaryName FILE`

3. Decompress with dictionary

   `zstd -D dictionaryName --decompress FILE.zst`


## Build instructions

### Makefile

If your system is compatible with standard `make` (or `gmake`),
invoking `make` in root directory will generate `zstd` cli in root directory.

Other available options include:
- `make install` : create and install zstd cli, library and man pages
- `make check` : create and run `zstd`, tests its behavior on local platform

### cmake

A `cmake` project generator is provided within `build/cmake`.
It can generate Makefiles or other build scripts
to create `zstd` binary, and `libzstd` dynamic and static libraries.

By default, `CMAKE_BUILD_TYPE` is set to `Release`.

### Meson

A Meson project is provided within [`build/meson`](build/meson). Follow
build instructions in that directory.

You can also take a look at [`.travis.yml`](.travis.yml) file for an
example about how Meson is used to build this project.

Note that default build type is **release**.

### VCPKG
You can build and install zstd [vcpkg](https://github.com/Microsoft/vcpkg/) dependency manager:

    git clone https://github.com/Microsoft/vcpkg.git
    cd vcpkg
    ./bootstrap-vcpkg.sh
    ./vcpkg integrate install
    ./vcpkg install zstd

The zstd port in vcpkg is kept up to date by Microsoft team members and community contributors.
If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.

### Visual Studio (Windows)

Going into `build` directory, you will find additional possibilities:
- Projects for Visual Studio 2005, 2008 and 2010.
  + VS2010 project is compatible with VS2012, VS2013, VS2015 and VS2017.
- Automated build scripts for Visual compiler by [@KrzysFR](https://github.com/KrzysFR), in `build/VS_scripts`,
  which will build `zstd` cli and `libzstd` library without any need to open Visual Studio solution.

### Buck

You can build the zstd binary via buck by executing: `buck build programs:zstd` from the root of the repo.
The output binary will be in `buck-out/gen/programs/`.

## Status

Zstandard is currently deployed within Facebook. It is used continuously to compress large amounts of data in multiple formats and use cases.
Zstandard is considered safe for production environments.

## License

Zstandard is dual-licensed under [BSD](LICENSE) and [GPLv2](COPYING).

## Contributing

The ""dev"" branch is the one where all contributions are merged before reaching ""master"".
If you plan to propose a patch, please commit into the ""dev"" branch, or its own feature branch.
Direct commit to ""master"" are not permitted.
For more information, please read [CONTRIBUTING](CONTRIBUTING.md)."
numpy/numpy,80836,13969,513,4611,Organization,False,23203,23,172,931,False,The fundamental package for scientific computing with Python.,https://www.numpy.org/,2,82,3,1929,6670,322,353,257,7713,100,699,6753,33,668,36593,21682,394099,23590,19,28,,,"# <img alt=""NumPy"" src=""https://cdn.rawgit.com/numpy/numpy/master/branding/icons/numpylogo.svg"" height=""60"">

[![Travis](https://img.shields.io/travis/numpy/numpy/master.svg?label=Travis%20CI)](
    https://travis-ci.org/numpy/numpy)
[![Azure](https://dev.azure.com/numpy/numpy/_apis/build/status/azure-pipeline%20numpy.numpy)](
    https://dev.azure.com/numpy/numpy/_build/latest?definitionId=5)
[![codecov](https://codecov.io/gh/numpy/numpy/branch/master/graph/badge.svg)](
    https://codecov.io/gh/numpy/numpy)

NumPy is the fundamental package needed for scientific computing with Python.

- **Website:** https://www.numpy.org
- **Documentation:** https://numpy.org/doc
- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion
- **Source code:** https://github.com/numpy/numpy
- **Contributing:** https://www.numpy.org/devdocs/dev/index.html
- **Bug reports:** https://github.com/numpy/numpy/issues
- **Report a security vulnerability:** https://tidelift.com/docs/security

It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities

Testing:

- NumPy versions &ge; 1.15 require `pytest`
- NumPy versions &lt; 1.15 require `nose`

Tests can then be run after installation with:

    python -c 'import numpy; numpy.test()'


Call for Contributions
----------------------

NumPy appreciates help from a wide range of different backgrounds.
Work such as high level documentation or website improvements are valuable
and we would like to grow our team with people filling these roles.
Small improvements or fixes are always appreciated and issues labeled as easy
may be a good starting point.
If you are considering larger contributions outside the traditional coding work,
please contact us through the mailing list.


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)"
stedolan/jq,6848,17128,305,1020,User,False,1323,16,11,112,False,Command-line JSON processor,http://stedolan.github.io/jq/,0,34,5,499,1169,60,22,89,385,19,12,2889,7,13,69,57,0,0,45,,575,,"jq
==

jq is a lightweight and flexible command-line JSON processor.

[![Coverage Status](https://coveralls.io/repos/stedolan/jq/badge.svg?branch=master&service=github)](https://coveralls.io/github/stedolan/jq?branch=master),
Unix: [![Build Status](https://travis-ci.org/stedolan/jq.svg?branch=master)](https://travis-ci.org/stedolan/jq),
Windows: [![Windows build status](https://ci.appveyor.com/api/projects/status/mi816811c9e9mx29?svg=true)](https://ci.appveyor.com/project/stedolan/jq)


If you want to learn to use jq, read the documentation at
[https://stedolan.github.io/jq](https://stedolan.github.io/jq).  This
documentation is generated from the docs/ folder of this repository.
You can also try it online at [jqplay.org](https://jqplay.org).

If you want to hack on jq, feel free, but be warned that its internals
are not well-documented at the moment. Bring a hard hat and a
shovel.  Also, read the wiki: https://github.com/stedolan/jq/wiki, where
you will find cookbooks, discussion of advanced topics, internals,
release engineering, and more.

Source tarball and built executable releases can be found on the
homepage and on the github release page, https://github.com/stedolan/jq/releases

If you're building directly from the latest git, you'll need flex,
bison (3.0 or newer), libtool, make, automake, and autoconf installed.
To get regexp support you'll also need to install Oniguruma or clone it as a
git submodule as per the instructions below.
(note that jq's tests require regexp support to pass).  To build, run:

    git submodule update --init # if building from git to get oniguruma
    autoreconf -fi              # if building from git
    ./configure --with-oniguruma=builtin
    make -j8
    make check

To build without bison or flex, add `--disable-maintainer-mode` to the
./configure invocation:

    ./configure --with-oniguruma=builtin --disable-maintainer-mode

(Developers must not use `--disable-maintainer-mode`, not when making
changes to the jq parser and/or lexer.)

To build a statically linked version of jq, run:

    make LDFLAGS=-all-static

After make finishes, you'll be able to use `./jq`.  You can also
install it using:

    sudo make install

If you're not using the latest git version but instead building a
released tarball (available on the website), then you won't need to
run `autoreconf` (and shouldn't), and you won't need flex or bison.

To cross-compile for OS X and Windows, see docs/Rakefile's build task
and scripts/crosscompile.  You'll need a cross-compilation environment,
such as Mingw for cross-compiling for Windows.

Cross-compilation requires a clean workspace, then:

    # git clean ...
    autoreconf -i
    ./configure
    make distclean
    scripts/crosscompile <name-of-build> <configure-options>

Use the `--host=` and `--target=` ./configure options to select a
cross-compilation environment.  See also 
[""Cross compilation""](https://github.com/stedolan/jq/wiki/Cross-compilation) on
the wiki.

Send questions to https://stackoverflow.com/questions/tagged/jq or to the #jq channel (http://irc.lc/freenode/%23jq/) on Freenode (https://webchat.freenode.net/)."
raspberrypi/linux,2405550,6807,745,3324,Organization,False,798503,66,53,9730,False,Kernel source tree for Raspberry Pi Foundation-provided kernel builds. Issues unrelated to the linux kernel should be posted on the community forum at https://www.raspberrypi.org/forum,,0,11,0,311,2133,106,79,27,1193,9,103,,0,0,0,0,0,0,25,1,,,
julycoding/The-Art-Of-Programming-By-July,27451,19188,1906,7002,User,False,3633,1,0,99,False,本项目曾冲到全球第一，干货集锦见本页面最底部，另完整精致的纸质版《编程之法：面试和算法心得》已在京东/当当上销售,,0,6,0,45,82,0,0,22,320,2,0,2378,0,0,0,0,0,0,4,,4,,
nodemcu/nodemcu-firmware,111218,6083,563,2760,Organization,False,2251,5,27,152,False,"Lua based interactive firmware for ESP8266, ESP8285 and ESP32",https://nodemcu.readthedocs.io,8,20,1,142,1744,57,51,33,1239,21,50,2049,18,61,75195,13360,0,0,5,6,,,"# NodeMCU 3.0.0

[![Join the chat at https://gitter.im/nodemcu/nodemcu-firmware](https://img.shields.io/gitter/room/badges/shields.svg)](https://gitter.im/nodemcu/nodemcu-firmware?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Build Status](https://travis-ci.org/nodemcu/nodemcu-firmware.svg)](https://travis-ci.org/nodemcu/nodemcu-firmware)
[![Documentation Status](https://img.shields.io/badge/docs-master-yellow.svg?style=flat)](http://nodemcu.readthedocs.io/en/master/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE)

### A Lua based firmware for ESP8266 WiFi SOC

NodeMCU is an open source [Lua](https://www.lua.org/) based firmware for the [ESP8266 WiFi SOC from Espressif](http://espressif.com/en/products/esp8266/) and uses an on-module flash-based [SPIFFS](https://github.com/pellepl/spiffs) file system. NodeMCU is implemented in C and is layered on the [Espressif NON-OS SDK](https://github.com/espressif/ESP8266_NONOS_SDK).

The firmware was initially developed as is a companion project to the popular ESP8266-based [NodeMCU development modules]((https://github.com/nodemcu/nodemcu-devkit-v1.0)), but the project is now community-supported, and the firmware can now be run on _any_ ESP module.

# Summary

- Easy to program wireless node and/or access point
- Based on Lua 5.1.4 but without `debug`, `io`, `os` and (most of the) `math` modules
- Asynchronous event-driven programming model
- more than **65 built-in modules**
- Firmware available with or without floating point support (integer-only uses less memory)
- Up-to-date documentation at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io)

### LFS support
In July 2018 support for a Lua Flash Store (LFS) was introduced. LFS  allows Lua code and its associated constant data to be executed directly out of flash-memory; just as the firmware itself is executed. This now enables NodeMCU developers to create **Lua applications with up to 256Kb** Lua code and read-only constants executing out of flash. All of the RAM is available for read-write data!

# Programming Model

The NodeMCU programming model is similar to that of [Node.js](https://en.wikipedia.org/wiki/Node.js), only in Lua. It is asynchronous and event-driven. Many functions, therefore, have parameters for callback functions. To give you an idea what a NodeMCU program looks like study the short snippets below. For more extensive examples have a look at the [`/lua_examples`](lua_examples) folder in the repository on GitHub.

```lua
-- a simple HTTP server
srv = net.createServer(net.TCP)
srv:listen(80, function(conn)
 conn:on(""receive"", function(sck, payload)
  print(payload)
  sck:send(""HTTP/1.0 200 OK\r\nContent-Type: text/html\r\n\r\n<h1> Hello, NodeMCU.</h1>"")
 end)
 conn:on(""sent"", function(sck) sck:close() end)
end)
```
```lua
-- connect to WiFi access point
wifi.setmode(wifi.STATION)
wifi.sta.config{ssid=""SSID"", pwd=""password""}
```

# Documentation

The entire [NodeMCU documentation](https://nodemcu.readthedocs.io) is maintained right in this repository at [/docs](docs). The fact that the API documentation is maintained in the same repository as the code that *provides* the API ensures consistency between the two. With every commit the documentation is rebuilt by Read the Docs and thus transformed from terse Markdown into a nicely browsable HTML site at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io).

- How to [build the firmware](https://nodemcu.readthedocs.io/en/master/en/build/)
- How to [flash the firmware](https://nodemcu.readthedocs.io/en/master/en/flash/)
- How to [upload code and NodeMCU IDEs](https://nodemcu.readthedocs.io/en/master/en/upload/)
- API documentation for every module

# Releases

Due to the ever-growing number of modules available within NodeMCU, pre-built binaries are no longer made available. Use the automated [custom firmware build service](http://nodemcu-build.com/) to get the specific firmware configuration you need, or consult the [documentation](http://nodemcu.readthedocs.io/en/master/en/build/) for other options to build your own firmware.

This project uses two main branches, `master` and `dev`. `dev` is actively worked on and it's also where PRs should be created against. `master` thus can be considered ""stable"" even though there are no automated regression tests. The goal is to merge back to `master` roughly every 2 months. Depending on the current ""heat"" (issues, PRs) we accept changes to `dev` for 5-6 weeks and then hold back for 2-3 weeks before the next snap is completed.

A new tag is created every time `dev` is merged back to `master`. They are listed in the [releases section here on GitHub](https://github.com/nodemcu/nodemcu-firmware/releases). Tag names follow the \<SDK-version\>-master_yyyymmdd pattern.

# Support

See [https://nodemcu.readthedocs.io/en/master/en/support/](https://nodemcu.readthedocs.io/en/master/en/support/).

# License

[MIT](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE) © [zeroday](https://github.com/NodeMCU)/[nodemcu.com](http://nodemcu.com/index_en.html)"
postgres/postgres,488792,6824,441,2281,Organization,False,49354,33,509,33,False,"Mirror of the official PostgreSQL GIT repository. Note that this is just a *mirror* - we don't work with pull requests on github. To contribute, please see https://wiki.postgresql.org/wiki/Submitting_a_Patch",https://www.postgresql.org/,0,0,0,,,,,0,52,0,8,8741,21,1052,342000,238238,0,0,9,5,,,
jonas/tig,6511,9101,173,519,User,False,2616,10,47,141,False,Text-mode interface for git,https://jonas.github.io/tig/,4,21,1,147,481,16,16,18,365,3,4,5178,5,23,6000,5688,0,0,110,,276,,
contiki-os/contiki,73109,3220,457,2465,Organization,False,12329,4,16,152,False,"The official git repository for Contiki, the open source OS for the Internet of Things",http://www.contiki-os.org/,0,30,2,430,521,15,3,168,1528,1,1,4093,0,0,0,0,0,0,4,9,,,"The Contiki Operating System
============================

[![Build Status](https://travis-ci.org/contiki-os/contiki.svg?branch=master)](https://travis-ci.org/contiki-os/contiki/branches)

Contiki is an open source operating system that runs on tiny low-power
microcontrollers and makes it possible to develop applications that
make efficient use of the hardware while providing standardized
low-power wireless communication for a range of hardware platforms.

Contiki is used in numerous commercial and non-commercial systems,
such as city sound monitoring, street lights, networked electrical
power meters, industrial monitoring, radiation monitoring,
construction site monitoring, alarm systems, remote house monitoring,
and so on.

For more information, see the Contiki website:

[http://contiki-os.org](http://contiki-os.org)"
videolan/vlc,379481,6226,516,2331,Organization,False,85467,1,50,540,False,"VLC media player - All pull requests are ignored, please follow https://wiki.videolan.org/Sending_Patches_VLC/",http://www.videolan.org/vlc,7,0,0,,,,,1,98,1,5,6809,20,1351,42093,27455,0,0,18,6,,,
lpereira/lwan,4896,5196,309,542,User,False,2775,5,3,38,False,"Experimental, scalable, high performance HTTP server",https://lwan.ws,5,11,0,48,145,1,0,3,84,1,4,3064,4,173,3774,2245,0,0,70,,532,,"Lwan Web Server
===============

Lwan is a **high-performance** & **scalable** web server.

The [project web site](https://lwan.ws/) contains more details.

Build status
------------

| OS          | Arch   | Release | Debug | Static Analysis | Tests |
|-------------|--------|---------|-------|-----------------|------------|
| Linux       | x86_64 | ![release](https://shield.lwan.ws/img/gycKbr/release ""Release"")  | ![debug](https://shield.lwan.ws/img/gycKbr/debug ""Debug"")     | ![static-analysis](https://shield.lwan.ws/img/gycKbr/clang-analyze ""Static Analysis"") ![coverity](https://scan.coverity.com/projects/375/badge.svg) [Report history](https://buildbot.lwan.ws/sa/) | ![tests](https://shield.lwan.ws/img/gycKbr/unit-tests ""Test"") [![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/lwan.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:lwan)           |
| Linux       | armv7  | ![release-arm](https://shield.lwan.ws/img/gycKbr/release-arm ""Release"")  | ![debug-arm](https://shield.lwan.ws/img/gycKbr/debug-arm ""Debug"")     |        |           |
| FreeBSD     | x86_64 | ![freebsd-release](https://shield.lwan.ws/img/gycKbr/release-freebsd ""Release FreeBSD"") | ![freebsd-debug](https://shield.lwan.ws/img/gycKbr/debug-freebsd ""Debug FreeBSD"")     |                |           |
| macOS       | x86_64 | ![osx-release](https://shield.lwan.ws/img/gycKbr/release-sierra ""Release macOS"")       | ![osx-debug](https://shield.lwan.ws/img/gycKbr/debug-sierra ""Debug macOS"")     |               |          |
| OpenBSD 6.6 | x86_64 | ![openbsd-release](https://shield.lwan.ws/img/gycKbr/release-openbsd ""Release OpenBSD"")       | ![openbsd-debug](https://shield.lwan.ws/img/gycKbr/debug-openbsd ""Debug OpenBSD"")     |               | ![openbsd-tests](https://shield.lwan.ws/img/gycKbr/openbsd-unit-tests ""OpenBSD Tests"")         |

Building
--------

Before installing Lwan, ensure all dependencies are installed. All of them
are common dependencies found in any GNU/Linux distribution; package names
will be different, but it shouldn't be difficult to search using whatever
package management tool that's used by your distribution.

### Required dependencies

 - [CMake](https://cmake.org/), at least version 2.8
 - [ZLib](http://zlib.net)

### Optional dependencies

The build system will look for these libraries and enable/link if available.

 - [Lua 5.1](http://www.lua.org) or [LuaJIT 2.0](http://luajit.org)
 - [Valgrind](http://valgrind.org)
 - [Brotli](https://github.com/google/brotli)
 - [ZSTD](https://github.com/facebook/zstd)
 - Alternative memory allocators can be used by passing `-DUSE_ALTERNATIVE_MALLOC` to CMake with the following values:
    - [""mimalloc""](https://github.com/microsoft/mimalloc)
    - [""jemalloc""](http://jemalloc.net/)
    - [""tcmalloc""](https://github.com/gperftools/gperftools)
    - ""auto"": Autodetect from the list above, falling back to libc malloc if none found
 - To run test suite:
    - [Python](https://www.python.org/) (2.6+) with Requests
    - [Lua 5.1](http://www.lua.org)
 - To run benchmark:
    - Special version of [Weighttp](https://github.com/lpereira/weighttp)
    - [Matplotlib](https://github.com/matplotlib/matplotlib)
 - To build TechEmpower benchmark suite:
    - Client libraries for either [MySQL](https://dev.mysql.com) or [MariaDB](https://mariadb.org)
    - [SQLite 3](http://sqlite.org)


### Common operating system package names

#### Minimum to build
 - ArchLinux: `pacman -S cmake zlib`
 - FreeBSD: `pkg install cmake pkgconf`
 - Ubuntu 14+: `apt-get update && apt-get install git cmake zlib1g-dev pkg-config`
 - macOS: `brew install cmake`

#### Build all examples
 - ArchLinux: `pacman -S cmake zlib sqlite luajit libmariadbclient gperftools valgrind`
 - FreeBSD: `pkg install cmake pkgconf sqlite3 lua51`
 - Ubuntu 14+: `apt-get update && apt-get install git cmake zlib1g-dev pkg-config lua5.1-dev libsqlite3-dev libmysqlclient-dev`
 - macOS: `brew install cmake mysql-connector-c sqlite lua@5.1 pkg-config`

### Build commands

#### Clone the repository

    ~$ git clone git://github.com/lpereira/lwan
    ~$ cd lwan

#### Create the build directory

    ~/lwan$ mkdir build
    ~/lwan$ cd build

#### Select build type

Selecting a *release* version (no debugging symbols, messages, enable some
optimizations, etc):

    ~/lwan/build$ cmake .. -DCMAKE_BUILD_TYPE=Release

If you'd like to enable optimizations but still use a debugger, use this instead:

    ~/lwan/build$ cmake .. -DCMAKE_BUILD_TYPE=RelWithDebInfo

To disable optimizations and build a more debugging-friendly version:

    ~/lwan/build$ cmake .. -DCMAKE_BUILD_TYPE=Debug

#### Build Lwan

    ~/lwan/build$ make

This will generate a few binaries:

 - `src/bin/lwan/lwan`: The main Lwan executable. May be executed with `--help` for guidance.
 - `src/bin/testrunner/testrunner`: Contains code to execute the test suite.
 - `src/samples/freegeoip/freegeoip`: [FreeGeoIP sample implementation](https://freegeoip.lwan.ws). Requires SQLite.
 - `src/samples/techempower/techempower`: Code for the TechEmpower Web Framework benchmark. Requires SQLite and MySQL libraries.
 - `src/samples/clock/clock`: [Clock sample](https://time.lwan.ws). Generates a GIF file that always shows the local time.
 - `src/bin/tools/mimegen`: Builds the extension-MIME type table. Used during build process.
 - `src/bin/tools/bin2hex`: Generates a C file from a binary file, suitable for use with #include.
 - `src/bin/tools/configdump`: Dumps a configuration file using the configuration reader API.

#### Remarks

Passing `-DCMAKE_BUILD_TYPE=Release` will enable some compiler
optimizations (such as [LTO](http://gcc.gnu.org/wiki/LinkTimeOptimization))
and tune the code for current architecture. *Please use this version
when benchmarking*, as the default is the Debug build, which not only
logs all requests to the standard output, but does so while holding a
mutex.

The default build (i.e. not passing `-DCMAKE_BUILD_TYPE=Release`) will build
a version suitable for debugging purposes.  This version can be used under
Valgrind *(if its headers are present)* and includes debugging messages that
are stripped in the release version.  Debugging messages are printed for
each and every request.

On debug builds, sanitizers can be enabled.  To select which one to build Lwan
with, specify one of the following options to the CMake invocation line:

 - `-DSANITIZER=ubsan` selects the Undefined Behavior Sanitizer.
 - `-DSANITIZER=address` selects the Address Sanitizer.
 - `-DSANITIZER=thread` selects the Thread Sanitizer.

Alternative memory allocators can be selected as well.  Lwan currently
supports [TCMalloc](https://github.com/gperftools/gperftools),
[mimalloc](https://github.com/microsoft/mimalloc), and
[jemalloc](http://jemalloc.net/) out of the box.  To use either one of them,
pass `-DALTERNATIVE_MALLOC=ON` to the CMake invocation line.

### Tests

    ~/lwan/build$ make testsuite

This will compile the `testrunner` program and execute regression test suite
in `src/scripts/testsuite.py`.

### Benchmark

    ~/lwan/build$ make benchmark

This will compile `testrunner` and execute benchmark script
`src/scripts/benchmark.py`.

### Coverage

Lwan can also be built with the Coverage build type by specifying
`-DCMAKE_BUILD_TYPE=Coverage`.  This enables the `generate-coverage` make
target, which will run `testrunner` to prepare a test coverage report with
[lcov](http://ltp.sourceforge.net/coverage/lcov.php).

Every commit in this repository triggers the generation of this report,
and results are [publicly available](https://buildbot.lwan.ws/lcov/).

Running
-------

Set up the server by editing the provided `lwan.conf`; the format is
explained in details below.  (Lwan will try to find a configuration file
based in the executable name in the current directory; `testrunner.conf`
will be used for the `testrunner` binary, `lwan.conf` for the `lwan` binary,
and so on.)

Configuration files are loaded from the current directory. If no changes
are made to this file, running Lwan will serve static files located in
the `./wwwroot` directory. Lwan will listen on port 8080 on all interfaces.

Lwan will detect the number of CPUs, will increase the maximum number of
open file descriptors and generally try its best to autodetect reasonable
settings for the environment it's running on.  Many of these settings can
be tweaked in the configuration file, but it's usually a good idea to not
mess with them.

Optionally, the `lwan` binary can be used for one-shot static file serving
without any configuration file. Run it with `--help` for help on that.

Configuration File
----------------

### Format

Lwan uses a familiar `key = value` configuration file syntax.  Comments are
supported with the `#` character (similar to e.g.  shell scripts, Python,
and Perl).  Nested sections can be created with curly brackets.  Sections
can be empty; in this case, curly brackets are optional.

`some_key_name` is equivalent to `some key name` in configuration files (as
an implementation detail, code reading configuration options will only be
given the version with underscores).

Values can contain environment variables. Use the syntax `${VARIABLE_NAME}`.
Default values can be specified with a colon (e.g.  `${VARIABLE_NAME:foo}`,
which evaluates to `${VARIABLE_NAME}` if it's set, or `foo` otherwise).

```
sound volume = 11 # This one is 1 louder

playlist metal {
   files = '''
 /multi/line/strings/are/supported.mp3
 /anything/inside/these/are/stored/verbatim.mp3
   '''
}

playlist chiptune {
   files = """"""
 /if/it/starts/with/single/quotes/it/ends/with/single/quotes.mod
 /but/it/can/use/double/quotes.s3m
   """"""
}
```

Some examples can be found in `lwan.conf` and `techempower.conf`.

#### Value types

| Type   | Description |
|--------|-------------|
| `str`  | Any kind of free-form text, usually application specific |
| `int`  | Integer number. Range is application specific |
| `time` | Time interval.  See table below for units |
| `bool` | Boolean value. See table below for valid values |

#### Time Intervals

Time fields can be specified using multipliers. Multiple can be specified, they're
just added together; for instance, ""1M 1w"" specifies ""1 month and 1 week"".  The following
table lists all known multipliers:

| Multiplier | Description |
|------------|-------------|
| `s`        | Seconds |
| `m`        | Minutes |
| `h`        | Hours |
| `d`        | Days |
| `w`        | Weeks |
| `M`        | Months |
| `y`        | Years |

A number with a multiplier not in this table is ignored; a warning is issued while
reading the configuration file.  No spaces must exist between the number and its
multiplier.

#### Boolean Values

| True Values | False Values |
|-------------|--------------|
| Any integer number different than 0 | 0 |
| `on` | `off` |
| `true` | `false` |
| `yes` | `no` |

### Global Settings

It's generally a good idea to let Lwan decide the best settings for your
environment.  However, not every environment is the same, and not all uses
can be decided automatically, so some configuration options are provided.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `keep_alive_timeout` | `time`  | `15` | Timeout to keep a connection alive |
| `quiet` | `bool` | `false` | Set to true to not print any debugging messages. Only effective in release builds. |
| `reuse_port` | `bool` | `false` | Sets `SO_REUSEPORT` to `1` in the master socket |
| `expires` | `time` | `1M 1w` | Value of the ""Expires"" header. Default is 1 month and 1 week |
| `threads` | `int` | `0` | Number of I/O threads. Default (0) is the number of online CPUs |
| `proxy_protocol` | `bool` | `false` | Enables the [PROXY protocol](https://www.haproxy.com/blog/haproxy/proxy-protocol/). Versions 1 and 2 are supported. Only enable this setting if using Lwan behind a proxy, and the proxy supports this protocol; otherwise, this allows anybody to spoof origin IP addresses |
| `max_post_data_size` | `int` | `40960` | Sets the maximum number of data size for POST requests, in bytes |

### Straitjacket

Lwan can drop its privileges to a user in the system, and limit its
filesystem view with a chroot.  While not bulletproof, this provides a
first layer of security in the case there's a bug in Lwan.

In order to use this feature, declare a `straitjacket` section, and set
some options.  This requires Lwan to be executed as `root`.

Although this section can be written anywhere in the file (as long as
it is a top level declaration), if any directories are open, due to
e.g.  instantiating the `serve_files` module, Lwan will refuse to
start.  (This check is only performed on Linux as a safeguard for
malconfiguration.)

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `user` | `str`  | `NULL` | Drop privileges to this user name |
| `chroot` | `str` | `NULL` | Path to `chroot()` |
| `drop_capabilities` | `bool` | `true` | Drop all capabilities with capset(2) (under Linux), or pledge(2) (under OpenBSD). |

### Listeners

In order to specify which interfaces Lwan should listen on, a `listener` section
must be specified.  Only one listener per Lwan process is accepted at the moment.
The only parameter to a listener block is the interface address and the port to
listen on; anything inside a listener section are instances of modules.

The syntax for the listener parameter is `${ADDRESS}:${PORT}`, where `${ADDRESS}`
can either be `*` (binding to all interfaces), an IPv6 address (if surrounded by
square brackets), an IPv4 address, or a hostname.  If systemd's socket activation
is used, `systemd` can be specified as a parameter.

### Routing URLs Using Modules or Handlers

In order to route URLs, Lwan matches the largest common prefix from the request
URI with a set of prefixes specified in the listener section.  How a request to
a particular prefix will be handled depends on which handler or module has been
declared in the listener section.  Handlers and modules are similar internally;
handlers are merely functions and hold no state, and modules holds state (named
instance).  Multiple instances of a module can appear in a listener section.

There is no special syntax to attach a prefix to a handler or module; all the
configuration parser rules apply here.  Use `${NAME} ${PREFIX}` to link the
`${PREFIX}` prefix path to either a handler named `${NAME}` (if `${NAME}`
begins with `&`, as with C's ""address of"" operator), or a module named
`${NAME}`.  Empty sections can be used here.

Each module will have its specific set of options, and they're listed in the
next sections.  In addition to configuration options, a special `authorization`
section can be present in the declaration of a module instance.  Handlers do
not take any configuration options, but may include the `authorization`
section.

A list of built-in modules can be obtained by executing Lwan with the `-m`
command-line argument.  The following is some basic documentation for the
modules shipped with Lwan.

#### File Serving

The `serve_files` module will serve static files, and automatically create
directory indices or serve pre-compressed files.  It'll generally try its
best to serve files in the fastest way possible according to some heuristics.


| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `path`                     | `str`  | `NULL`       | Path to a directory containing files to be served |
| `index_path`               | `str`  | `index.html` | File name to serve as an index for a directory |
| `serve_precompressed_path` | `bool` | `true`       | If $FILE.gz exists, is smaller and newer than $FILE, and the client accepts `gzip` encoding, transfer it |
| `auto_index`               | `bool` | `true`       | Generate a directory list automatically if no `index_path` file present.  Otherwise, yields 404 |
| `auto_index_readme`        | `bool` | `true`       | Includes the contents of README files as part of the automatically generated directory index |
| `directory_list_template`  | `str`  | `NULL`       | Path to a Mustache template for the directory list; by default, use an internal template |
| `read_ahead`               | `int`  | `131702`     | Maximum amount of bytes to read ahead when caching open files.  A value of `0` disables readahead.  Readahead is performed by a low priority thread to not block the I/O threads while file extents are being read from the filesystem. |
| `cache_for`                | `time` | `5s`         | Time to keep file metadata (size, compressed contents, open file descriptor, etc.) in cache |

#### Lua

The `lua` module will allow requests to be serviced by scripts written in
the [Lua](https://www.lua.org/) programming language.  Although the
functionality provided by this module is quite spartan, it's able to run
frameworks such as [Sailor](https://github.com/lpereira/sailor-hello-lwan).

Scripts can be served from files or embedded in the configuration file, and
the results of loading them, the standard Lua modules, and (optionally, if
using LuaJIT) optimizing the code will be cached for a while.  Each I/O
thread in Lwan will create an instance of a Lua VM (i.e.  one `lua_State`
struct for every I/O thread), and each Lwan coroutine will spawn a Lua
thread (with `lua_newthread()`) per request.  Because of this, Lua scripts
can't use global variables, as they may be not only serviced by different
threads, but the state will be available only for the amount of time
specified in the `cache_period` configuration option.

There's no need to have one instance of the Lua module for each endpoint; a
single script, embedded in the configuration file or otherwise, can service
many different endpoints.  Scripts are supposed to implement functions with
the following signature: `handle_${METHOD}_${ENDPOINT}(req)`, where
`${METHOD}` can be a HTTP method (i.e.  `get`, `post`, `head`, etc.), and
`${ENDPOINT}` is the desired endpoint to be handled by that function.  The
special `${ENDPOINT}` `root` can be specified to act as a catchall.  The
`req` parameter points to a metatable that contains methods to obtain
information from the request, or to set the response, as seen below:

   - `req:query_param(param)` returns the query parameter (from the query string) with the key `param`, or `nil` if not found
   - `req:post_param(param)` returns the post parameter (only for `${POST}` handlers) with the key `param`, or `nil` if not found
   - `req:set_response(str)` sets the response to the string `str`
   - `req:say(str)` sends a response chunk (using chunked encoding in HTTP)
   - `req:send_event(event, str)` sends an event (using server-sent events)
   - `req:cookie(param)` returns the cookie named `param`, or `nil` is not found
   - `req:set_headers(tbl)` sets the response headers from the table `tbl`; a header may be specified multiple times by using a table, rather than a string, in the table value (`{'foo'={'bar', 'baz'}}`); must be called before sending any response with `say()` or `send_event()`
   - `req:sleep(ms)` pauses the current handler for the specified amount of milliseconds
   - `req:ws_upgrade()` returns `1` if the connection could be upgraded to a WebSocket; `0` otherwise
   - `req:ws_write(str)` sends `str` through the WebSocket-upgraded connection
   - `req:ws_read()` returns a string obtained from the WebSocket, or `nil` on error

Handler functions may return either `nil` (in which case, a `200 OK` response
is generated), or a number matching an HTTP status code.  Attempting to return
an invalid HTTP status code or anything other than a number or `nil` will result
in a `500 Internal Server Error` response being thrown.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `default_type` | `str` | `text/plain` | Default MIME-Type for responses |
| `script_file` | `str` | `NULL` | Path to Lua script|
| `cache_period` | `time` | `15s` | Time to keep Lua state loaded in memory |
| `script` | `str` | `NULL` | Inline lua script |

#### Rewrite

The `rewrite` module will match
[patterns](https://man.openbsd.org/patterns.7) in URLs and give the option
to either redirect to another URL, or rewrite the request in a way that Lwan
will handle the request as if it were made in that way originally.  The
patterns are a special kind of regular expressions, forked from Lua 5.3.1,
that do not contain backreferences and other features that could create
denial-of-service issues in Lwan.  The new URL can be specified using a
simple text substitution syntax, or use Lua scripts; Lua scripts will
contain the same metamethods available in the `req` metatable provided by
the Lua module, so it can be quite powerful.

Each instance of the rewrite module will require a `pattern` and the action
to execute when such pattern is matched.  Patterns are evaluated in the
order they appear in the configuration file, and are specified using nested
sections in the configuration file.  For instance, consider the following
example, where two patterns are specified:

```
rewrite /some/base/endpoint {
    pattern posts/(%d+) {
        # Matches /some/base/endpointposts/2600 and /some/base/endpoint/posts/2600
        rewrite_as = /cms/view-post?id=%1
    }
    pattern imgur/(%a+)/(%g+) {
        # Matches /some/base/endpointimgur/gif/mpT94Ld and /some/base/endpoint/imgur/gif/mpT94Ld
        redirect_to = https://i.imgur.com/%2.%1
    }
}
```

This example defines two patterns, one providing a nicer URL that's hidden
from the user, and another providing a different way to obtain a direct link
to an image hosted on a popular image hosting service (i.e.  requesting
`/some/base/endpoint/imgur/mp4/4kOZNYX` will redirect directly to a resource
in the Imgur service).

The value of `rewrite_as` or `redirect_to` can be Lua scripts as well; in
which case, the option `expand_with_lua` must be set to `true`, and, instead
of using the simple text substitution syntax as the example above, a
function named `handle_rewrite(req, captures)` has to be defined instead.
The `req` parameter is documented in the Lua module section; the `captures`
parameter is a table containing all the captures, in order.  This function
returns the new URL to redirect to.

This module has no options by itself.  Options are specified in each and
every pattern.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `rewrite_as` | `str` | `NULL` | Rewrite the URL following this pattern |
| `redirect_to` | `str` | `NULL` | Redirect to a new URL following this pattern |
| `expand_with_lua` | `bool` | `false` | Use Lua scripts to redirect to or rewrite a request |

`redirect_to` and `rewrite_as` options are mutually exclusive, and one of
them must be specified at least.

#### Redirect

The `redirect` module will, as it says in the tin, generate a `301
Moved permanently` (by default; the code can be changed, see below)
response, according to the options specified in its configuration.
Generally, the `rewrite` module should be used instead as it packs more
features; however, this module serves also as an example of how to
write Lwan modules (less than 100 lines of code).

If the `to` option is not specified, it always generates a `500
Internal Server Error` response.  Specifying an invalid HTTP code, or a
code that Lwan doesn't know about (see `enum lwan_http_status`), will
produce a `301 Moved Permanently` response.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `to` | `str` | `NULL` | The location to redirect to |
| `code` | `int` | `301` | The HTTP code to perform a redirect |

#### Response

The `response` module will generate an artificial response of any HTTP code.
In addition to also serving as an example of how to write a Lwan module,
it can be used to carve out voids from other modules (e.g. generating a
`405 Not Allowed` response for files in `/.git`, if `/` is served with
the `serve_files` module).

If the supplied `code` falls outside the response codes known by Lwan,
a `404 Not Found` error will be sent instead.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `code` | `int` | `999` | A HTTP response code |

### Authorization Section

Authorization sections can be declared in any module instance or handler,
and provides a way to authorize the fulfillment of that request through
the standard HTTP authorization mechanism.  In order to require authorization
to access a certain module instance or handler, declare an `authorization`
section with a `basic` parameter, and set one of its options.

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `realm` | `str` | `Lwan` | Realm for authorization. This is usually shown in the user/password UI in browsers |
| `password_file` | `str` | `NULL` | Path for a file containing username and passwords (in clear text).  The file format is the same as the configuration file format used by Lwan |

Hacking
-------

Please read this section (and follow it) if you're planning on contributing
to Lwan.  There's nothing unexpected here; this mostly follows the rules and
expectations of many other FOSS projects, but every one expects things a
little bit different from one another.

### Coding Style

Lwan tries to follow a consistent coding style throughout the project.  If you're
considering contributing a patch to the project, please respect this style by trying
to match the style of the surrounding code.  In general:

 - `global_variables_are_named_like_this`, even though they tend to be rare and should be marked as `static` (with rare exceptions)
 - Local variables are usually shorter, e.g. `local_var`, `i`, `conn`
 - Struct names are often as short as they're descriptive.  `typedef` for structs are rarely used in Lwan
 - Header files should use `#pragma once` instead of the usual include guard hackery
 - Functions that are used between .c files but are not APIs to be exposed to liblwan should have their prototype added to `lwan-private.h`
 - Functions should be short and sweet.  Exceptions may apply
 - Public functions should be prefixed with `lwan_`
 - Public types should be prefixed with `lwan_`
 - Private functions must be static, and can be named without the `lwan_` prefix
 - Code is indented with 4 spaces; don't use tabs
 - There's a suggested line break at column 80, but it's not enforced
 - `/* Old C-style comments are preferred */`
 - `clang-format` can be used to format the source code in an acceptable way; a `.clang-format` file is provided

### Tests

If modifying well-tested areas of the code (e.g. the event loop, HTTP parser,
etc.), please add a new integration test and make sure that, before you send a
pull request, all tests (including the new ones you've sent) are working.
Tests can be added by modifying `src/scripts/testsuite.py`, and executed by
either invoking that script directly from the source root, or executing the
`testsuite` build target.

Some tests will only work on Linux, and won't be executed on other platforms.

### Fuzz-testing

Lwan is automatically fuzz-tested by
[OSS-Fuzz](https://github.com/google/oss-fuzz/).  To fuzz-test locally,
though, one can [follow the instructions to test
locally](https://github.com/google/oss-fuzz/blob/master/docs/new_project_guide.md#testing-locally).

This fuzzes only the request parsing code.  There are plans to add fuzzing
drivers for other parts of the code, including the rewriting engine,
configuration file reader, template parser, and URL routing.

### Exporting APIs

The shared object version of `liblwan` on ELF targets (e.g. Linux) will use
a symbol filter script to hide symbols that are considered private to the
library.  Please edit `src/lib/liblwan.sym` to add new symbols that should
be exported to `liblwan.so`.

### Using Git and Pull Requests

Lwan tries to maintain a source history that's as flat as possible, devoid of
merge commits.  This means that pull requests should be rebased on top of the
current master before they can be merged; sometimes this can be done
automatically by the GitHub interface, sometimes they need some manual work to
fix conflicts.  It is appreciated if the contributor fixes these conflicts when
asked.

It is advisable to push your changes to your fork on a branch-per-pull request,
rather than pushing to the `master` branch; the reason is explained below.

Please ensure that Git is configured properly with your name (it doesn't really
matter if it is your legal name or a nickname, but it should be enough to credit
you) and a valid email address.  There's no need to add `Signed-off-by` lines,
even though it's fine to send commits with them.

If a change is requested in a pull request, you have two choices:

 - *Reply asking for clarification.*  Maybe the intentions were not clear enough,
and whoever asked for changes didn't fully understand what you were trying to
achieve
 - *Fix the issue.*  When fixing issues found in pull requests, *please* use
[interactive rebases](https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History) to
squash or fixup commits; don't add your fixes on top of your tree.  Do not create
another pull request just to accomodate the changes. After rewriting
the history locally, force-push to your PR branch; the PR will update automatically
with your changes.  Rewriting the history of development branches is fine, and
force-pushing them is normal and expected

It is not enforced, but it is recommended to create smaller commits. How
commits are split in Lwan is pretty much arbitrary, so please take a look at
the commit history to get an idea on how the division should be made.  Git
offers a plethora of commands to achieve this result: the already mentioned
interactive rebase, the `-p` option to `git add`, and `git commit --amend`
are good examples.

Commit messages should have one line of summary (~72 chars), followed by an
empty line, followed by paragraphs of 80-char lines explaining the change.  The
paragraphs explaining the changes are usually not necessary if the summary
is good enough.  Try to [write good commit messages](https://chris.beams.io/posts/git-commit/).

### Licensing

Lwan is licensed under the GNU General Public License, version 2, or (at your option),
any later version.  Therefore:

 - Code must be either LGPLv2.1, GPLv2, a permissive ""copyfree"" license that is compatible
with GPLv2 (e.g. MIT, BSD 3-clause), or public domain code (e.g. CC0)
 - Although the program can be distributed and used as if it were licensed as GPLv3,
its code must be compatible with GPLv2 as well; no new code can be licensed under versions
of GPL newer than 2
 - Likewise, code licensed under licenses compatible with GPLv3 but
incompatible with GPLv2 (e.g.  Apache 2) are not suitable for inclusion in
Lwan
 - Even if the license does not specify that credit should be given (e.g. CC0-licensed code),
please give credit to the original author for that piece of code
 - Contrary to popular belief, it is possible to use a GPL'd piece of code on a server without
having to share the code for your application.  It is only when the binary of that server is
shared that source must be available to whoever has that binary.  Merely accessing a Lwan
server through HTTP does not qualify as having access to the binary program that's running
on the server
 - When in doubt, don't take legal advice from a README file: please consult
a lawyer that understands free software licensing

Portability
-----------

While Lwan was written originally for Linux, it has been ported to BSD
systems as well.  The build system will detect the supported features
and build support library functions as appropriate.

For instance, [epoll](https://en.wikipedia.org/wiki/Epoll) has been
implemented on top of [kqueue](https://en.wikipedia.org/wiki/Kqueue), and
Linux-only syscalls and GNU extensions have been implemented for the
supported systems.  [This blog post](https://tia.mat.br/posts/2018/06/28/include_next_and_portability.html)
explains the details and how `#include_next` is used.

Performance
-----------

It can achieve good performance, yielding about **320000 requests/second**
on a Core i7 laptop for requests without disk access, and without pipelining.

When disk I/O is required, for files up to 16KiB, it yields about
**290000 requests/second**; for larger files, this drops to **185000
requests/second**, which isn't too shabby either.

These results, of course, with keep-alive connections, and with weighttp
running on the same machine (and thus using resources that could be used
for the webserver itself).

Without keep-alive, these numbers drop around 6-fold.

IRC Channel
-----------

There is an IRC channel (`#lwan`) on [Freenode](http://freenode.net). A
standard IRC client can be used.  A [web IRC gateway](http://webchat.freenode.net?channels=%23lwan&uio=d4)
is also available.

Lwan in the wild
----------------

Here's a non-definitive list of third-party stuff that uses Lwan and have
been seen in the wild.  *Help build this list!*

* [This project uses Cython and Lwan](https://www.erp5.com/NXD-Blog.Multicore.Python.HTTP.Server) to make it possible to write handlers in Python.
* [An experimental version of Node.js using Lwan](https://github.com/raadad/node-lwan) as its HTTP server is maintained by [@raadad](https://github.com/raadad).
* The beginnings of a C++11 [web framework](https://github.com/vileda/wfpp) based on Lwan written by [@vileda](https://github.com/vileda).
* A more complete C++14 [web framework](https://github.com/matt-42/silicon) by [@matt-42](https://github.com/matt-42) offers Lwan as one of its backends.
* A [word ladder sample program](https://github.com/sjnam/lwan-sgb-ladders) by [@sjnam](https://github.com/sjnam). [Demo](http://tbcoe.ddns.net/sgb/ladders?start=chaos&goal=order).
* A [Shodan search](https://www.shodan.io/search?query=server%3A+lwan) listing some brave souls that expose Lwan to the public internet.

Some other distribution channels were made available as well:

* A `Dockerfile` is maintained by [@jaxgeller](https://github.com/jaxgeller), and is [available from the Docker registry](https://hub.docker.com/r/jaxgeller/lwan/).
* A buildpack for Heroku is maintained by [@bherrera](https://github.com/bherrera), and is [available from its repo](https://github.com/bherrera/heroku-buildpack-lwan).
* Lwan is also available as a package in [Biicode](http://docs.biicode.com/c++/examples/lwan.html).
* It's also available in some GNU/Linux distributions:
    * [Arch Linux](https://aur.archlinux.org/packages/lwan-git/)
    * [Ubuntu](https://launchpad.net/lwan-unofficial)
    * [Alpine Linux](https://pkgs.alpinelinux.org/package/edge/testing/x86_64/lwan)
    * [NixOS](https://nixos.org/nixos/packages.html#lwan)
* It's also available as a package for the [Nanos unikernel](https://github.com/nanovms/nanos).

Lwan has been also used as a benchmark:

* [Raphael Javaux's master thesis](https://github.com/RaphaelJ/master-thesis) cites Lwan in chapter 5 (""Performance Analysis"").
* Lwan is used as a benchmark by the [PyParallel](http://pyparallel.org/) [author](https://www.reddit.com/r/programming/comments/3jhv80/pyparallel_an_experimental_proofofconcept_fork_of/cur4tut).
* [Kong](https://getkong.org/about/benchmark/) uses Lwan as the [backend API](https://gist.github.com/montanaflynn/01376991f0a3ad07059c) in its benchmark.
* [TechEmpower Framework benchmarks](https://www.techempower.com/benchmarks/#section=data-r10&hw=peak&test=json) feature Lwan since round 10.
* [KrakenD](http://www.krakend.io) used Lwan for the REST API in all official [benchmarks](http://www.krakend.io/docs/benchmarks/aws/)

Mentions in academic journals:

* [A dynamic predictive race detector for C/C++ programs](https://link.springer.com/article/10.1007/s11227-017-1996-8) uses Lwan as a ""real world example"".

Some talks mentioning Lwan:

* [Talk about Lwan](https://www.youtube.com/watch?v=cttY9FdCzUE) at Polyconf16, given by [@lpereira](https://github.com/lpereira).
* This [talk about Iron](https://michaelsproul.github.io/iron-talk/), a framework for Rust, mentions Lwan as an *insane C thing*.
* [University seminar presentation](https://github.com/cu-data-engineering-s15/syllabus/blob/master/student_lectures/LWAN.pdf) about Lwan.
* This [presentation about Sailor web framework](http://www.slideshare.net/EtieneDalcol/web-development-with-lua-bulgaria-web-summit) mentions Lwan.
* [Performance and Scale @ Istio Service Mesh](https://www.youtube.com/watch?v=G4F5aRFEXnU), at around 7:30min, presented at KubeCon Europe 2018, mentions that Lwan is used on the server side for testing due to its performance and robustness.
* [A multi-core Python HTTP server (much) faster than Go (spoiler: Cython)](https://www.youtube.com/watch?v=mZ9cXOH6NYk) presented at PyConFR 2018 by J.-P. Smets mentions [Nexedi's work](https://www.nexedi.com/NXD-Blog.Multicore.Python.HTTP.Server) on using Lwan as a backend for Python services with Cython.

Not really third-party, but alas:

* The [author's blog](http://tia.mat.br).
* The [project's webpage](http://lwan.ws).

Lwan quotes
-----------

These are some of the quotes found in the wild about Lwan.  They're presented
in no particular order.  Contributions are appreciated:

> ""I read lwan's source code. Especially, the part of using coroutine was
> very impressive and it was more interesting than a good novel.  Thank you
> for that."" --
> [@patagonia](https://twitter.com/hakman314/status/996617563470680064)

> ""For the server side, we're using Lwan, which can handle 100k+ reqs/s.
> It's supposed to be super robust and it's working well for us."" --
> [@fawadkhaliq](https://twitter.com/fawadkhaliq)

> ""Insane C thing"" -- [Michael
> Sproul](https://michaelsproul.github.io/iron-talk/)

> ""I've never had a chance to thank you for Lwan.  It inspired me a lot to
> develop [Zewo](https://github.com/Zewo/Zero)"" --
> [@paulofariarl](https://twitter.com/paulofariarl/status/707926806373003265)

> ""Let me say that lwan is a thing of beauty.  I got sucked into reading the
> source code for pure entertainment, it's so good.  *high five*"" --
> [@kwilczynski](https://twitter.com/kwilczynski/status/692881117003644929)

> ""Nice work with Lwan! I haven't looked _that_ carefully yet but so far I
> like what I saw.  You definitely have the right ideas."" --
> [@thinkingfish](https://twitter.com/thinkingfish/status/521574267612196864)

> ""Lwan is a work of art. Every time I read through it, I am almost always
> awe-struck."" --
> [@neurodrone](https://twitter.com/neurodrone/status/359296080283840513)

> ""For Round 10, Lwan has taken the crown"" --
> [TechEmpower](https://www.techempower.com/blog/2015/04/21/framework-benchmarks-round-10/)

> ""Jeez this is amazing. Just end to end, rock solid engineering. (...) But that sells this work short.""
> [kjeetgill](https://news.ycombinator.com/item?id=17548983)

> ""I am only a spare time C coder myself and was surprised that I can follow the code. Nice!""
> [cntlzw](https://news.ycombinator.com/item?id=17550319)

> ""Impressive all and all, even more for being written in (grokkable!) C. Nice work.""
> [tpaschalis](https://news.ycombinator.com/item?id=17550961)"
mruby/mruby,14048,4609,268,692,Organization,False,11446,4,13,246,False,Lightweight Ruby,,0,7,0,117,1353,7,30,58,3495,5,105,2980,10,235,19286,5481,6,0,3,7,,,"[![Build Status][build-status-img]][travis-ci]

## What is mruby

mruby is the lightweight implementation of the Ruby language complying to (part
of) the [ISO standard][ISO-standard]. Its syntax is Ruby 2.x compatible.

mruby can be linked and embedded within your application.  We provide the
interpreter program ""mruby"" and the interactive mruby shell ""mirb"" as examples.
You can also compile Ruby programs into compiled byte code using the mruby
compiler ""mrbc"".  All those tools reside in the ""bin"" directory.  ""mrbc"" is
also able to generate compiled byte code in a C source file, see the ""mrbtest""
program under the ""test"" directory for an example.

This achievement was sponsored by the Regional Innovation Creation R&D Programs
of the Ministry of Economy, Trade and Industry of Japan.

## How to get mruby

The stable version 2.1.1 of mruby can be downloaded via the following URL: [https://github.com/mruby/mruby/archive/2.1.1.zip](https://github.com/mruby/mruby/archive/2.1.1.zip)

The latest development version of mruby can be downloaded via the following URL: [https://github.com/mruby/mruby/zipball/master](https://github.com/mruby/mruby/zipball/master)

The trunk of the mruby source tree can be checked out with the
following command:

    $ git clone https://github.com/mruby/mruby.git

You can also install and compile mruby using [ruby-install](https://github.com/postmodern/ruby-install), [ruby-build](https://github.com/rbenv/ruby-build) or [rvm](https://github.com/rvm/rvm).

## mruby home-page

The URL of the mruby home-page is: https://mruby.org.

## Mailing list

We don't have a mailing list, but you can use [GitHub issues](https://github.com/mruby/mruby/issues).

## How to compile and install (mruby and gems)

See the [compile.md](https://github.com/mruby/mruby/blob/master/doc/guides/compile.md) file.

## Running Tests

To run the tests, execute the following from the project's root directory.

    $ rake test

Note: `bison` bundled with MacOS is too old to compile `mruby`.
Try `brew install bison` and follow the instuction shown to update
the `$PATH` to compile `mruby`.

## Building documentation

There are two sets of documentation in mruby: the mruby API (generated by yard) and C API (Doxygen)

To build both of them, simply go

    rake doc

You can also view them in your browser

    rake view_api
    rake view_capi

## How to customize mruby (mrbgems)

mruby contains a package manager called *mrbgems*. To create extensions
in C and/or Ruby you should create a *GEM*. For a documentation of how to
use mrbgems consult the file [mrbgems.md](https://github.com/mruby/mruby/blob/master/doc/guides/mrbgems.md).
For example code of how to use mrbgems look into the folder *examples/mrbgems/*.

## License

mruby is released under the [MIT License](https://github.com/mruby/mruby/blob/master/LICENSE).

## Note for License

mruby has chosen a MIT License due to its permissive license allowing
developers to target various environments such as embedded systems.
However, the license requires the display of the copyright notice and license
information in manuals for instance. Doing so for big projects can be
complicated or troublesome.  This is why mruby has decided to display ""mruby
developers"" as the copyright name to make it simple conventionally.
In the future, mruby might ask you to distribute your new code
(that you will commit,) under the MIT License as a member of
""mruby developers"" but contributors will keep their copyright.
(We did not intend for contributors to transfer or waive their copyrights,
Actual copyright holder name (contributors) will be listed in the AUTHORS
file.)

Please ask us if you want to distribute your code under another license.

## How to Contribute

See the [contribution guidelines][contribution-guidelines], and then send a pull
request to <http://github.com/mruby/mruby>.  We consider you have granted
non-exclusive right to your contributed code under MIT license.  If you want to
be named as one of mruby developers, please include an update to the AUTHORS
file in your pull request.

[ISO-standard]: http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=59579
[build-status-img]: https://travis-ci.org/mruby/mruby.svg?branch=master
[contribution-guidelines]: https://github.com/mruby/mruby/blob/master/CONTRIBUTING.md
[travis-ci]: https://travis-ci.org/mruby/mruby"
nanomsg/nanomsg,8221,4945,457,839,Organization,False,1643,11,19,87,False,nanomsg library,,0,6,0,6,623,1,5,1,415,1,2,2805,1,1,11,0,0,0,16,2,,,"Welcome to nanomsg
==================

[![Release](https://img.shields.io/github/release/nanomsg/nanomsg.svg)](https://github.com/nanomsg/nanomsg/releases/latest)
[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/nanomsg/nanomsg/blob/master/COPYING)
[![Linux Status](https://img.shields.io/circleci/project/github/nanomsg/nanomsg/master.svg?label=linux)](https://circleci.com/gh/nanomsg/nanomsg)
[![Windows Status](https://img.shields.io/appveyor/ci/nanomsg/nanomsg/master.svg?label=windows)](https://ci.appveyor.com/project/nanomsg/nanomsg)
[![Coverage](https://codecov.io/gh/nanomsg/nanomsg/branch/master/graph/badge.svg?label=coverage)](https://codecov.io/gh/nanomsg/nanomsg)
[![Gitter](https://img.shields.io/badge/gitter-join-brightgreen.svg)](https://gitter.im/nanomsg/nanomsg)

The nanomsg library is a simple high-performance implementation of several
""scalability protocols"". These scalability protocols are light-weight messaging
protocols which can be used to solve a number of very common messaging
patterns, such as request/reply, publish/subscribe, surveyor/respondent,
and so forth.  These protocols can run over a variety of transports such
as TCP, UNIX sockets, and even WebSocket.

For more information check the [website](http://nanomsg.org).

Prerequisites
-------------

1. Windows.
   * Windows Vista or newer (Windows XP and 2003 are *NOT* supported)
   * Microsoft Visual Studio 2010 (including C++) or newer, or mingw-w64.
     (Specifically mingw and older Microsoft compilers are *NOT supported,
     and we do not test mingw-w64 at all, so YMMV.)
   * CMake 2.8.7 or newer, available in $PATH as `cmake`

2. POSIX (Linux, MacOS X, UNIX)
   * ANSI C compiler supporting C89
   * POSIX pthreads (should be present on all modern POSIX systems)
   * BSD sockets support for both TCP and UNIX domain sockets
   * CMake (http://cmake.org) 2.8.7 or newer, available in $PATH as `cmake`

3. Documentation (optional)
   * asciidoctor (http://asciidoctor.org/) available as `asciidoctor`
   * If not present, docs are not formatted, but left in readable ASCII
   * Also available on-line at http://nanomsg.org/documentation

Quick Build Instructions
------------------------

These steps here are the minimum steps to get a default Debug
build.  Using CMake you can do many other things, including
setting additional variables, setting up for static builds, or
generation project or solution files for different development
environments.  Please check the CMake website for all the various
options that CMake supports.

## POSIX

This assumes you have a shell in the project directory, and have
the cmake and suitable compilers (and any required supporting tools
like linkers or archivers) on your path.

1.  `% mkdir build`
2.  `% cd build`
3.  `% cmake ..`
4.  `% cmake --build .`
5.  `% ctest .`
6.  `% sudo cmake --build . --target install`
7.  `% sudo ldconfig` (if on Linux)

## Windows

This assumes you are in a command or powershell window and have
the appropriate variables setup to support Visual Studio, typically
by running `vcvarsall.bat` or similar with the appropriate argument(s).
It also assumes you are in the project directory.

1.  `md build`
2.  `cd build`
3.  `cmake ..`
4.  `cmake --build . --config Debug`
5.  `ctest -C Debug .`
6.  `cmake --build . --config Debug --target install`
    *NB:* This may have to be done using an Administrator account.

Alternatively, you can build and install nanomsg using [vcpkg](https://github.com/microsoft/vcpkg/) dependency manager:

1.  `git clone https://github.com/Microsoft/vcpkg.git`
2.  `cd vcpkg`
3.  `./bootstrap-vcpkg.bat`
4.  `./vcpkg integrate install`
5.  `./vcpkg install nanomsg`

The nanomsg port in vcpkg is kept up to date by microsoft team members and community contributors.
If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.

Static Library
--------------

We normally build a dynamic library (.so or .DLL) by default.

If you want a static library (.a or .LIB), configure by passing
`-DNN_STATIC_LIB=ON` to the first `cmake` command.

### POSIX

POSIX systems will need to link with the libraries normally used when building
network applications.  For some systems this might mean -lnsl or -lsocket.

### Windows

You will also need to define `NN_STATIC_LIB` in your compilation environment
when building programs that use this library.  This is required because of
the way Windows changes symbol names depending on whether the symbols should
be exported in a DLL or not.

When using the .LIB on Windows, you will also need to link with the
ws2_32, mswsock, and advapi32 libraries, as nanomsg depends on them.

Support
-------

This library is considered to be in ""sustaining"" mode, which means that new
feature development has ended, and bug fixes are made only when strictly
necessary for severe issues.

New development is now occurring in the [NNG](https://github.com/nanomsg/nng)
project, which offers both protocol and API compatibility with this project.
Please consider using NNG for new projects.

Please see the file SUPPORT for more details.

Resources
---------

Website: [http://nanomsg.org](http://nanomsg.org)

Source code: [https://github.com/nanomsg/nanomsg](http://github.com/nanomsg/nanomsg)

Documentation: [http://nanomsg.org/documentation.html](http://nanomsg.org/documentation.html)

Bug tracker: [https://github.com/nanomsg/nanomsg/issues](http://github.com/nanomsg/nanomsg/issues)

Mailing list: [nanomsg@freelists.org](http://www.freelists.org/list/nanomsg)

Gitter Chat: [https://gitter.im/nanomsg/nanomsg](https://gitter.im/nanomsg/nanomsg)

IRC chatroom: [#nanomsg at irc.freenode.net/8001](http://webchat.freenode.net?channels=%23nanomsg)"
bartobri/no-more-secrets,185,4662,123,231,User,False,252,1,7,14,False,A command line tool that recreates the famous data decryption effect seen in the 1992 movie Sneakers.,,6,7,0,2,38,0,1,0,34,0,0,1531,0,0,0,0,0,0,12,,175,,"![Version](https://img.shields.io/badge/Version-0.3.3-green.svg)

No More Secrets
===============

This project provides a command line tool called `nms` that recreates the
famous data decryption effect seen on screen in the 1992 hacker movie Sneakers.
For reference, you can see this effect at 0:35 in [this movie clip](https://www.youtube.com/watch?v=F5bAa6gFvLs&t=35).

This command works on piped data. Pipe any ASCII or UTF-8 text to `nms`,
and it will apply the Hollywood effect, initially showing encrypted data,
then starting a decryption sequence to reveal the original plain-text characters.

![Screenshot](http://www.brianbarto.info/extern/images/nms/nms.gif)

Also included in this project is a program called `sneakers` that recreates
what we see in the above movie clip. Note that this program requires the
user to select one of the menu options before it terminates.

![Screenshot](http://www.brianbarto.info/extern/images/nms/sneakers.gif)

By default, this project has no dependencies, but it does rely on ANSI/VT100
terminal escape sequences to recreate the effect. Most modern terminal
programs support these sequences so this should not be an issue for most
users. If yours does not, this project also provides a ncurses implementation
which supports non-ANSI terminals, but at the expense of losing the inline
functionality (ncurses will always clear the screen prior to displaying output).

Table of Contents
-----------------

1. [Download and Install](#download-and-install)
2. [Usage](#usage)
3. [The NMS Library](#the-nms-library)
4. [License](#license)
5. [Tips](#tips)

Download and Install
--------------------

More and more Unix/Linux platforms are including this project in their
package manager. You may wish to search your package manager to see if it
is an installation option. If you install form a package manager, please
check that you have the latest version (`nms -v`). If not, I suggest
installing from source by following the instructions below.

To install this project from source, you will need to have the tools `git`,
`gcc`, and `make` to download and build it. Install them from your package
manager if they are not already installed.

Once you have the necessary tools installed, follow these instructions:

#### Install:
```
$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms
$ make sneakers             ## Optional
$ sudo make install
```

#### Uninstall:

```
$ sudo make uninstall
```

#### Install with Ncurses Support

If your terminal does not support ANSI/VT100 escape sequences, the effect
may not render properly. This project provides a ncurses implementation
for such cases. You will need the ncurses library installed. [Install this
library from your package manager](NCURSES.md). Next, follow these instructions:

```
$ git clone https://github.com/bartobri/no-more-secrets.git
$ cd ./no-more-secrets
$ make nms-ncurses
$ make sneakers-ncurses     ## Optional
$ sudo make install
```

Usage
-----

`nms` works on piped data. Pipe any ASCII or UTF-8 characters to it and
enjoy the magic. In the below examples, I use a simple directory listing.

```
$ ls -l | nms
$ ls -l | nms -a           // Set auto-decrypt flag
$ ls -l | nms -s           // Set flag to mask space characters
$ ls -l | nms -f green     // Set foreground color to green
$ ls -l | nms -c           // Clear screen
$ nms -v                   // Display version
```

Note that by default, after the initial encrypted characters are displayed,
`nms` will wait for the user to press a key before initiating the decryption
sequence. This is how the it is depicted in the movie.

#### Command Line Options

`-a`

Set the auto-decrypt flag. This will automatically start the
decryption sequence without requiring a key press.

`-s`

Set a flag to mask space characters. This will only mask single blank space
characters. Other space characters such as tabs and newlines will not be masked.

`-f <color>`

Set the foreground color of the decrypted text to the color
specified. Valid options are white, yellow, black, magenta, blue, green,
or red. This is blue by default.

`-c`

Clear the screen prior to printing any output. Specifically,
it saves the state of the terminal (all current output), and restores it
once the effect is completed. Note that when using this option, `nms` requires
the user to press a key before restoring the terminal.

`-v`

Display version info.

The NMS Library
---------------

For those who would like to use this effect in their own projects, I have
created a C library that provides simple interface and can easily be used
for any program that runs from the command line.

See [LibNMS](https://github.com/bartobri/libnms) for more info.

License
-------

This program is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License. See [LICENSE](LICENSE) for
more details.

Tips
----

[Tips are always appreciated!](https://github.com/bartobri/tips)"
borgbackup/borg,18678,6278,169,472,Organization,False,5853,7,83,185,False,Deduplicating archiver with compression and authenticated encryption.,https://www.borgbackup.org/,11,36,7,443,2303,79,106,15,2455,11,151,3771,18,98,13827,8446,19,6,7,3,,,
jonls/redshift,2116,4608,109,331,User,False,687,8,19,43,False,Redshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night.,http://jonls.dk/redshift,6,11,1,167,367,6,3,31,190,4,0,3876,0,0,0,0,0,0,43,,242,,"Redshift
========

Redshift adjusts the color temperature of your screen according to
your surroundings. This may help your eyes hurt less if you are
working in front of the screen at night.

![Redshift logo](http://jonls.dk/assets/redshift-icon-256.png)

Run `redshift -h` for help on command line options. You can run the program
as `redshift-gtk` instead of `redshift` for a graphical status icon.

* Website: http://jonls.dk/redshift/
* Project page: https://github.com/jonls/redshift

Build status
------------

[![Build Status](https://travis-ci.org/jonls/redshift.svg?branch=master)](https://travis-ci.org/jonls/redshift)
[![Build Status](https://ci.appveyor.com/api/projects/status/github/jonls/redshift?branch=master&svg=true)](https://ci.appveyor.com/project/jonls/redshift)

FAQ
---

**How do I install Redshift?**

Use the packages provided by your distribution, e.g. for Ubuntu:
`apt-get install redshift` or `apt-get install redshift-gtk`. For developers,
please see _Building from source_ and _Latest builds from master branch_ below.

**How do I setup a configuration file?**

A configuration file is not required but is useful for saving custom
configurations and manually defining the location in case of issues with the
automatic location provider. An example configuration can be found in
[redshift.conf.sample](redshift.conf.sample).

The configuration file should be saved in the following location depending on
the platform:

- Linux/macOS: `~/.config/redshift/redshift.conf` (if the environment variable `XDG_CONFIG_HOME` is undefined) or `${XDG_CONFIG_HOME}/redshift/redshift.conf` (if `XDG_CONFIG_HOME` is defined).
- Windows: Put `redshift.conf` in `%USERPROFILE%\AppData\Local\`
    (aka `%localappdata%`).

**Where can I find my coordinates to put in the configuration file?**

There are multiple web sites that provide coordinates for map locations, for
example clicking anywhere on Google Maps will bring up a box with the
coordinates. Remember that longitudes in the western hemisphere (e.g. the
Americas) must be provided to Redshift as negative numbers.

**Why does GeoClue fail with access denied error?**

It is possible that the location services have been disabled completely. The
check for this case varies by desktop environment. For example, in GNOME the
location services can be toggled in Settings > Privacy > Location Services.

If this is not the case, it is possible that Redshift has been improperly
installed or not been given the required permissions to obtain location
updates from a system administrator. See
https://github.com/jonls/redshift/issues/318 for further discussion on this
issue.

**Why doesn't Redshift work on my Chromebook/Raspberry Pi?**

Certain video drivers do not support adjustable gamma ramps. In some cases
Redshift will fail with an error message, but other drivers silently ignore
adjustments to the gamma ramp.

**Why doesn't Redshift change the backlight when I use the brightness option?**

Redshift has a brightness adjustment setting but it does not work the way most
people might expect. In fact it is a fake brightness adjustment obtained by
manipulating the gamma ramps which means that it does not reduce the backlight
of the screen. Preferably only use it if your normal backlight adjustment is
too coarse-grained.

**Why doesn't Redshift work on Wayland (e.g. Fedora 25)?**

The Wayland protocol does not support Redshift. There is currently no way for
Redshift to adjust the color temperature in Wayland.

**Why doesn't Redshift work on Ubuntu with Mir enabled?**

Mir does not support Redshift.

**The redness effect is applied during the day instead of at night. Why?**

This usually happens to users in America when the longitude has been set in the
configuration file to a positive number. Longitudes in the western hemisphere
should be provided as negative numbers (e.g. New York City is at approximately
latitude/longitude 41, -74).

**Why does the redness effect occasionally switch off for a few seconds?**

Redshift uses the gamma ramps of the graphics driver to apply the redness
effect but Redshift cannot block other applications from also changing the
gamma ramps. Some applications (particularly games and video players) will
reset the gamma ramps. After a few seconds Redshift will kick in again. There
is no way for Redshift to prevent this from happening.

**Why does the redness effect continuously flicker?**

You may have multiple instances of Redshift running simultaneously. Make sure
that only one instance is running for the display where you are seeing the
flicker.

**Why doesn't Redshift change the color of the mouse cursor?**

Mouse cursors are usually handled separately by the graphics hardware and is
not affected by gamma ramps. Some graphics drivers can be configured to use
software cursors instead.

**I have an issue with Redshift but it was not mentioned in this FAQ. What
do I do?**

Please go to [the issue tracker](https://github.com/jonls/redshift/issues) and
check if your issue has already been reported. If not, please open a new issue
describing you problem.

Latest builds from master branch
--------------------------------

- [Ubuntu PPA](https://launchpad.net/~dobey/+archive/ubuntu/redshift-daily/+packages) (`sudo add-apt-repository ppa:dobey/redshift-daily`)
- [Windows x86_64](https://ci.appveyor.com/api/projects/jonls/redshift/artifacts/redshift-windows-x86_64.zip?branch=master&job=Environment%3A+arch%3Dx86_64&pr=false)
- [Windows x86](https://ci.appveyor.com/api/projects/jonls/redshift/artifacts/redshift-windows-i686.zip?branch=master&job=Environment%3A+arch%3Di686&pr=false)

Contributing / Building from source
-----------------------------------

See the file [CONTRIBUTING](CONTRIBUTING.md) for more details."
sqlcipher/sqlcipher,40344,4082,243,960,Organization,False,735,13,40,18,False,SQLCipher is an SQLite extension that provides 256 bit AES encryption of database files.,https://www.zetetic.net/sqlcipher/,0,0,0,9,294,3,13,6,50,0,1,4338,1,11,11886,2993,0,0,10,4,,,"## SQLCipher

SQLCipher extends the [SQLite](https://www.sqlite.org) database library to add security enhancements that make it more suitable for encrypted local data storage like:

- on-the-fly encryption
- tamper detection
- memory sanitization
- strong key derivation

SQLCipher is based on SQLite and stable upstream release features are periodically integrated. 

SQLCipher is maintained by Zetetic, LLC, and additional information and documentation is available on the official [SQLCipher site](https://www.zetetic.net/sqlcipher/).

## Features

- Fast performance with as little as 5-15% overhead for encryption on many operations
- 100% of data in the database file is encrypted
- Good security practices (CBC mode, HMAC, key derivation)
- Zero-configuration and application level cryptography
- Algorithms provided by the peer reviewed OpenSSL crypto library.
- Configurable crypto providers

## Compatibility

SQLCipher maintains database format compatibility within the same major version number so an application on any platform can open databases created by any other application provided the major version of SQLCipher is the same between them. However, major version updates (e.g. from 3.x to 4.x) often include changes to default settings. This means that newer major versions of SQLCipher will not open databases created by older versions without using special settings. For example, SQLCipher 4 introduces many new performance and security enhancements. The new default algorithms, increased KDF iterations, and larger page size mean that SQLCipher 4 will not open databases created by SQLCipher 1.x, 2.x, or 3.x by default. Instead, an application would either need to migrate the older databases to use the new format or enable a special backwards-compatibility mode. The available options are described in SQLCipher's [upgrade documentation](https://discuss.zetetic.net/t/upgrading-to-sqlcipher-4/3283). 

SQLCipher is also compatible with standard SQLite databases. When a key is not provided, SQLCipher will behave just like the standard SQLite library. It is also possible to convert from a plaintext database (standard SQLite) to an encrypted SQLCipher database using [ATTACH and the sqlcipher_export() convenience function](https://discuss.zetetic.net/t/how-to-encrypt-a-plaintext-sqlite-database-to-use-sqlcipher-and-avoid-file-is-encrypted-or-is-not-a-database-errors/868).

## Contributions

The SQLCipher team welcomes contributions to the core library. All contributions including pull requests and patches should be based on the `prerelease` branch, and must be accompanied by a [contributor agreement](https://www.zetetic.net/contributions/). We strongly encourage [discussion](https://discuss.zetetic.net/c/sqlcipher) of the proposed change prior to development and submission.

## Compiling

Building SQLCipher is similar to compiling a regular version of SQLite from source a couple small exceptions:  

 1. You *must* define `SQLITE_HAS_CODEC` and either `SQLITE_TEMP_STORE=2` or SQLITE_TEMP_STORE=3` 
 2. You will need to link against a support cryptograpic provider (OpenSSL, LibTomCrypt, CommonCrypto/Security.framework, or NSS)
 
The following examples demonstrate linking against OpenSSL, which is a readily available provider on most Unix-like systems. 

Example 1. Static linking (replace /opt/local/lib with the path to libcrypto.a). Note in this 
example, `--enable-tempstore=yes` is setting `SQLITE_TEMP_STORE=2` for the build.

```
 $ ./configure --enable-tempstore=yes CFLAGS=""-DSQLITE_HAS_CODEC"" \
  LDFLAGS=""/opt/local/lib/libcrypto.a""
 $ make
```

Example 2. Dynamic linking

```
 $ ./configure --enable-tempstore=yes CFLAGS=""-DSQLITE_HAS_CODEC"" \
  LDFLAGS=""-lcrypto""
 $ make
```

## Testing

The full SQLite test suite will not complete successfully when using SQLCipher. In some cases encryption interferes with low-level tests that require access to database file data or features which are unsupported by SQLCipher. Those tests that are intended to support encryption are intended for non-SQLCipher implementations. In addition, because SQLite tests are not always isolated, if one test fails it can trigger a domino effect with other failures in later steps.

As a result, the SQLCipher package includes it's own independent tests that exercise and verify the core functionality of the SQLCipher extensions. This test suite is intended to provide an abbreviated verification of SQLCipher's internal logic; it does not perform an exhaustive test of the SQLite database system as a whole or verify functionality on specific platforms. Because SQLCipher is based on stable upstream builds of SQLite, it is consider a basic assumption that the core SQLite library code is operating properly (the SQLite core is almost untouched in SQLCipher). Thus, the additional SQLCipher-specific test provide the requisite verification that the library is operating as expected with SQLCipher's security features enabled.

To run SQLCipher specific tests, configure as described above and run the following to execute the tests and recieve a report of the results:

```
  $ make testfixture
  $ ./testfixture test/sqlcipher.test
```

## Encrypting a database

To specify an encryption passphrase for the database via the SQL interface you 
use a PRAGMA. The passphrase you enter is passed through PBKDF2 key derivation to
obtain the encryption key for the database 

 PRAGMA key = 'passphrase';

Alternately, you can specify an exact byte sequence using a blob literal. If you
use this method it is your responsibility to ensure that the data you provide is a
64 character hex string, which will be converted directly to 32 bytes (256 bits) of 
key data without key derivation.

 PRAGMA key = ""x'2DD29CA851E7B56E4697B0E1F08507293D761A05CE4D1B628663F411A8086D99'"";

To encrypt a database programmatically you can use the `sqlite3_key` function. 
The data provided in `pKey` is converted to an encryption key according to the 
same rules as `PRAGMA key`. 

 int sqlite3_key(sqlite3 *db, const void *pKey, int nKey);

`PRAGMA key` or `sqlite3_key` should be called as the first operation when a database is open.

## Changing a database key

To change the encryption passphrase for an existing database you may use the rekey PRAGMA
after you've supplied the correct database password;

 PRAGMA key = 'passphrase'; -- start with the existing database passphrase
 PRAGMA rekey = 'new-passphrase'; -- rekey will reencrypt with the new passphrase

The hex rekey pragma may be used to rekey to a specific binary value

 PRAGMA rekey = ""x'2DD29CA851E7B56E4697B0E1F08507293D761A05CE4D1B628663F411A8086D99'"";

This can be accomplished programmatically by using sqlite3_rekey;
  
 sqlite3_rekey(sqlite3 *db, const void *pKey, int nKey)

## Support

The primary source for complete documentation (desing, API, platforms, usage) is the SQLCipher website:

https://www.zetetic.net/sqlcipher/documentation

The primary avenue for support and discussions is the SQLCipher discuss site:

https://discuss.zetetic.net/c/sqlcipher

Issues or support questions on using SQLCipher should be entered into the 
GitHub Issue tracker:

https://github.com/sqlcipher/sqlcipher/issues

Please DO NOT post issues, support questions, or other problems to blog 
posts about SQLCipher as we do not monitor them frequently.

If you are using SQLCipher in your own software please let us know at 
support@zetetic.net!

## Community Edition Open Source License

Copyright (c) 2020, ZETETIC LLC
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:
    * Redistributions of source code must retain the above copyright
      notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above copyright
      notice, this list of conditions and the following disclaimer in the
      documentation and/or other materials provided with the distribution.
    * Neither the name of the ZETETIC LLC nor the
      names of its contributors may be used to endorse or promote products
      derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY ZETETIC LLC ''AS IS'' AND ANY
EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL ZETETIC LLC BE LIABLE FOR ANY
DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

# Begin SQLite README.md

<h1 align=""center"">SQLite Source Repository</h1>

This repository contains the complete source code for the 
[SQLite database engine](https://sqlite.org/).  Some test scripts 
are also included.  However, many other test scripts
and most of the documentation are managed separately.

## Version Control

SQLite sources are managed using the
[Fossil](https://www.fossil-scm.org/), a distributed version control system
that was specifically designed and written to support SQLite development.
The [Fossil repository](https://sqlite.org/src/timeline) contains the urtext.

If you are reading this on GitHub or some other Git repository or service,
then you are looking at a mirror.  The names of check-ins and
other artifacts in a Git mirror are different from the official
names for those objects.  The offical names for check-ins are
found in a footer on the check-in comment for authorized mirrors.
The official check-in name can also be seen in the `manifest.uuid` file
in the root of the tree.  Always use the official name, not  the
Git-name, when communicating about an SQLite check-in.

If you pulled your SQLite source code from a secondary source and want to
verify its integrity, there are hints on how to do that in the
[Verifying Code Authenticity](#vauth) section below.

## Obtaining The Code

If you do not want to use Fossil, you can download tarballs or ZIP
archives or [SQLite archives](https://sqlite.org/cli.html#sqlar) as follows:

  *  Lastest trunk check-in as
     [Tarball](https://www.sqlite.org/src/tarball/sqlite.tar.gz),
     [ZIP-archive](https://www.sqlite.org/src/zip/sqlite.zip), or
     [SQLite-archive](https://www.sqlite.org/src/sqlar/sqlite.sqlar).

  *  Latest release as
     [Tarball](https://www.sqlite.org/src/tarball/sqlite.tar.gz?r=release),
     [ZIP-archive](https://www.sqlite.org/src/zip/sqlite.zip?r=release), or
     [SQLite-archive](https://www.sqlite.org/src/sqlar/sqlite.sqlar?r=release).

  *  For other check-ins, substitute an appropriate branch name or
     tag or hash prefix in place of ""release"" in the URLs of the previous
     bullet.  Or browse the [timeline](https://www.sqlite.org/src/timeline)
     to locate the check-in desired, click on its information page link,
     then click on the ""Tarball"" or ""ZIP Archive"" links on the information
     page.

If you do want to use Fossil to check out the source tree, 
first install Fossil version 2.0 or later.
(Source tarballs and precompiled binaries available
[here](https://www.fossil-scm.org/fossil/uv/download.html).  Fossil is
a stand-alone program.  To install, simply download or build the single 
executable file and put that file someplace on your $PATH.)
Then run commands like this:

        mkdir ~/sqlite
        cd ~/sqlite
        fossil clone https://www.sqlite.org/src sqlite.fossil
        fossil open sqlite.fossil
    
After setting up a repository using the steps above, you can always
update to the lastest version using:

        fossil update trunk   ;# latest trunk check-in
        fossil update release ;# latest official release

Or type ""fossil ui"" to get a web-based user interface.

## Compiling

First create a directory in which to place
the build products.  It is recommended, but not required, that the
build directory be separate from the source directory.  Cd into the
build directory and then from the build directory run the configure
script found at the root of the source tree.  Then run ""make"".

For example:

        tar xzf sqlite.tar.gz    ;#  Unpack the source tree into ""sqlite""
        mkdir bld                ;#  Build will occur in a sibling directory
        cd bld                   ;#  Change to the build directory
        ../sqlite/configure      ;#  Run the configure script
        make                     ;#  Run the makefile.
        make sqlite3.c           ;#  Build the ""amalgamation"" source file
        make test                ;#  Run some tests (requires Tcl)

See the makefile for additional targets.

The configure script uses autoconf 2.61 and libtool.  If the configure
script does not work out for you, there is a generic makefile named
""Makefile.linux-gcc"" in the top directory of the source tree that you
can copy and edit to suit your needs.  Comments on the generic makefile
show what changes are needed.

## Using MSVC

On Windows, all applicable build products can be compiled with MSVC.
First open the command prompt window associated with the desired compiler
version (e.g. ""Developer Command Prompt for VS2013"").  Next, use NMAKE
with the provided ""Makefile.msc"" to build one of the supported targets.

For example:

        mkdir bld
        cd bld
        nmake /f Makefile.msc TOP=..\sqlite
        nmake /f Makefile.msc sqlite3.c TOP=..\sqlite
        nmake /f Makefile.msc sqlite3.dll TOP=..\sqlite
        nmake /f Makefile.msc sqlite3.exe TOP=..\sqlite
        nmake /f Makefile.msc test TOP=..\sqlite

There are several build options that can be set via the NMAKE command
line.  For example, to build for WinRT, simply add ""FOR_WINRT=1"" argument
to the ""sqlite3.dll"" command line above.  When debugging into the SQLite
code, adding the ""DEBUG=1"" argument to one of the above command lines is
recommended.

SQLite does not require [Tcl](http://www.tcl.tk/) to run, but a Tcl installation
is required by the makefiles (including those for MSVC).  SQLite contains
a lot of generated code and Tcl is used to do much of that code generation.

## Source Code Tour

Most of the core source files are in the **src/** subdirectory.  The
**src/** folder also contains files used to build the ""testfixture"" test
harness. The names of the source files used by ""testfixture"" all begin
with ""test"".
The **src/** also contains the ""shell.c"" file
which is the main program for the ""sqlite3.exe""
[command-line shell](https://sqlite.org/cli.html) and
the ""tclsqlite.c"" file which implements the
[Tcl bindings](https://sqlite.org/tclsqlite.html) for SQLite.
(Historical note:  SQLite began as a Tcl
extension and only later escaped to the wild as an independent library.)

Test scripts and programs are found in the **test/** subdirectory.
Addtional test code is found in other source repositories.
See [How SQLite Is Tested](http://www.sqlite.org/testing.html) for
additional information.

The **ext/** subdirectory contains code for extensions.  The
Full-text search engine is in **ext/fts3**.  The R-Tree engine is in
**ext/rtree**.  The **ext/misc** subdirectory contains a number of
smaller, single-file extensions, such as a REGEXP operator.

The **tool/** subdirectory contains various scripts and programs used
for building generated source code files or for testing or for generating
accessory programs such as ""sqlite3_analyzer(.exe)"".

### Generated Source Code Files

Several of the C-language source files used by SQLite are generated from
other sources rather than being typed in manually by a programmer.  This
section will summarize those automatically-generated files.  To create all
of the automatically-generated files, simply run ""make target&#95;source"".
The ""target&#95;source"" make target will create a subdirectory ""tsrc/"" and
fill it with all the source files needed to build SQLite, both
manually-edited files and automatically-generated files.

The SQLite interface is defined by the **sqlite3.h** header file, which is
generated from src/sqlite.h.in, ./manifest.uuid, and ./VERSION.  The
[Tcl script](http://www.tcl.tk) at tool/mksqlite3h.tcl does the conversion.
The manifest.uuid file contains the SHA3 hash of the particular check-in
and is used to generate the SQLITE\_SOURCE\_ID macro.  The VERSION file
contains the current SQLite version number.  The sqlite3.h header is really
just a copy of src/sqlite.h.in with the source-id and version number inserted
at just the right spots. Note that comment text in the sqlite3.h file is
used to generate much of the SQLite API documentation.  The Tcl scripts
used to generate that documentation are in a separate source repository.

The SQL language parser is **parse.c** which is generate from a grammar in
the src/parse.y file.  The conversion of ""parse.y"" into ""parse.c"" is done
by the [lemon](./doc/lemon.html) LALR(1) parser generator.  The source code
for lemon is at tool/lemon.c.  Lemon uses the tool/lempar.c file as a
template for generating its parser.
Lemon also generates the **parse.h** header file, at the same time it
generates parse.c.

The **opcodes.h** header file contains macros that define the numbers
corresponding to opcodes in the ""VDBE"" virtual machine.  The opcodes.h
file is generated by the scanning the src/vdbe.c source file.  The
Tcl script at ./mkopcodeh.tcl does this scan and generates opcodes.h.
A second Tcl script, ./mkopcodec.tcl, then scans opcodes.h to generate
the **opcodes.c** source file, which contains a reverse mapping from
opcode-number to opcode-name that is used for EXPLAIN output.

The **keywordhash.h** header file contains the definition of a hash table
that maps SQL language keywords (ex: ""CREATE"", ""SELECT"", ""INDEX"", etc.) into
the numeric codes used by the parse.c parser.  The keywordhash.h file is
generated by a C-language program at tool mkkeywordhash.c.

The **pragma.h** header file contains various definitions used to parse
and implement the PRAGMA statements.  The header is generated by a
script **tool/mkpragmatab.tcl**. If you want to add a new PRAGMA, edit
the **tool/mkpragmatab.tcl** file to insert the information needed by the
parser for your new PRAGMA, then run the script to regenerate the
**pragma.h** header file.

### The Amalgamation

All of the individual C source code and header files (both manually-edited
and automatically-generated) can be combined into a single big source file
**sqlite3.c** called ""the amalgamation"".  The amalgamation is the recommended
way of using SQLite in a larger application.  Combining all individual
source code files into a single big source code file allows the C compiler
to perform more cross-procedure analysis and generate better code.  SQLite
runs about 5% faster when compiled from the amalgamation versus when compiled
from individual source files.

The amalgamation is generated from the tool/mksqlite3c.tcl Tcl script.
First, all of the individual source files must be gathered into the tsrc/
subdirectory (using the equivalent of ""make target_source"") then the
tool/mksqlite3c.tcl script is run to copy them all together in just the
right order while resolving internal ""#include"" references.

The amalgamation source file is more than 200K lines long.  Some symbolic
debuggers (most notably MSVC) are unable to deal with files longer than 64K
lines.  To work around this, a separate Tcl script, tool/split-sqlite3c.tcl,
can be run on the amalgamation to break it up into a single small C file
called **sqlite3-all.c** that does #include on about seven other files
named **sqlite3-1.c**, **sqlite3-2.c**, ..., **sqlite3-7.c**.  In this way,
all of the source code is contained within a single translation unit so
that the compiler can do extra cross-procedure optimization, but no
individual source file exceeds 32K lines in length.

## How It All Fits Together

SQLite is modular in design.
See the [architectural description](http://www.sqlite.org/arch.html)
for details. Other documents that are useful in
(helping to understand how SQLite works include the
[file format](http://www.sqlite.org/fileformat2.html) description,
the [virtual machine](http://www.sqlite.org/opcode.html) that runs
prepared statements, the description of
[how transactions work](http://www.sqlite.org/atomiccommit.html), and
the [overview of the query planner](http://www.sqlite.org/optoverview.html).

Years of effort have gone into optimizating SQLite, both
for small size and high performance.  And optimizations tend to result in
complex code.  So there is a lot of complexity in the current SQLite
implementation.  It will not be the easiest library in the world to hack.

Key files:

  *  **sqlite.h.in** - This file defines the public interface to the SQLite
     library.  Readers will need to be familiar with this interface before
     trying to understand how the library works internally.

  *  **sqliteInt.h** - this header file defines many of the data objects
     used internally by SQLite.  In addition to ""sqliteInt.h"", some
     subsystems have their own header files.

  *  **parse.y** - This file describes the LALR(1) grammar that SQLite uses
     to parse SQL statements, and the actions that are taken at each step
     in the parsing process.

  *  **vdbe.c** - This file implements the virtual machine that runs
     prepared statements.  There are various helper files whose names
     begin with ""vdbe"".  The VDBE has access to the vdbeInt.h header file
     which defines internal data objects.  The rest of SQLite interacts
     with the VDBE through an interface defined by vdbe.h.

  *  **where.c** - This file (together with its helper files named
     by ""where*.c"") analyzes the WHERE clause and generates
     virtual machine code to run queries efficiently.  This file is
     sometimes called the ""query optimizer"".  It has its own private
     header file, whereInt.h, that defines data objects used internally.

  *  **btree.c** - This file contains the implementation of the B-Tree
     storage engine used by SQLite.  The interface to the rest of the system
     is defined by ""btree.h"".  The ""btreeInt.h"" header defines objects
     used internally by btree.c and not published to the rest of the system.

  *  **pager.c** - This file contains the ""pager"" implementation, the
     module that implements transactions.  The ""pager.h"" header file
     defines the interface between pager.c and the rest of the system.

  *  **os_unix.c** and **os_win.c** - These two files implement the interface
     between SQLite and the underlying operating system using the run-time
     pluggable VFS interface.

  *  **shell.c.in** - This file is not part of the core SQLite library.  This
     is the file that, when linked against sqlite3.a, generates the
     ""sqlite3.exe"" command-line shell.  The ""shell.c.in"" file is transformed
     into ""shell.c"" as part of the build process.

  *  **tclsqlite.c** - This file implements the Tcl bindings for SQLite.  It
     is not part of the core SQLite library.  But as most of the tests in this
     repository are written in Tcl, the Tcl language bindings are important.

  *  **test*.c** - Files in the src/ folder that begin with ""test"" go into
     building the ""testfixture.exe"" program.  The testfixture.exe program is
     an enhanced Tcl shell.  The testfixture.exe program runs scripts in the
     test/ folder to validate the core SQLite code.  The testfixture program
     (and some other test programs too) is build and run when you type
     ""make test"".

  *  **ext/misc/json1.c** - This file implements the various JSON functions
     that are build into SQLite.

There are many other source files.  Each has a succinct header comment that
describes its purpose and role within the larger system.

<a name=""vauth""></a>
## Verifying Code Authenticity

The `manifest` file at the root directory of the source tree
contains either a SHA3-256 hash (for newer files) or a SHA1 hash (for 
older files) for every source file in the repository.
The SHA3-256 hash of the `manifest`
file itself is the official name of the version of the source tree that you
have. The `manifest.uuid` file should contain the SHA3-256 hash of the
`manifest` file. If all of the above hash comparisons are correct, then
you can be confident that your source tree is authentic and unadulterated.

The format of the `manifest` file should be mostly self-explanatory, but
if you want details, they are available
[here](https://fossil-scm.org/fossil/doc/trunk/www/fileformat.wiki#manifest).

## Contacts

The main SQLite website is [http://www.sqlite.org/](http://www.sqlite.org/)
with geographically distributed backups at
[http://www2.sqlite.org/](http://www2.sqlite.org) and
[http://www3.sqlite.org/](http://www3.sqlite.org)."
FreeRDP/FreeRDP,41435,4384,333,1561,Organization,False,14365,6,32,262,False,FreeRDP is a free remote desktop protocol library and clients,http://www.freerdp.com/,3,31,2,334,2688,70,174,30,3222,12,241,3274,15,463,25994,18500,0,0,8,17,,,"# FreeRDP: A Remote Desktop Protocol Implementation

FreeRDP is a free implementation of the Remote Desktop Protocol (RDP), released under the Apache license.
Enjoy the freedom of using your software wherever you want, the way you want it, in a world where
interoperability can finally liberate your computing experience.

## Resources

Project website: https://www.freerdp.com/  
Issue tracker: https://github.com/FreeRDP/FreeRDP/issues  
Sources: https://github.com/FreeRDP/FreeRDP/  
Downloads: https://pub.freerdp.com/releases/  
Wiki: https://github.com/FreeRDP/FreeRDP/wiki  
API documentation: https://pub.freerdp.com/api/  

IRC channel: #freerdp @ irc.freenode.net  
Mailing list: https://lists.sourceforge.net/lists/listinfo/freerdp-devel

## Microsoft Open Specifications

Information regarding the Microsoft Open Specifications can be found at:
http://www.microsoft.com/openspecifications/

A list of reference documentation is maintained here:
https://github.com/FreeRDP/FreeRDP/wiki/Reference-Documentation

## Compilation

Instructions on how to get started compiling FreeRDP can be found on the wiki:
https://github.com/FreeRDP/FreeRDP/wiki/Compilation"
tmk/tmk_keyboard,24764,3019,210,1528,User,False,1586,10,4,60,False,Keyboard firmwares for Atmel AVR and Cortex-M,,0,16,0,124,322,4,6,42,163,2,3,3596,2,97,32694,416405,0,0,37,,407,,"TMK Keyboard Firmware Collection
================================
This repository includes keyboard and converter firmware projects built with [`tmk_core`][tmk_core] keyboard library.

The latest source code is available here: <http://github.com/tmk/tmk_keyboard>


Updates
-------
#### 2017/01/11
Changed action code for `ACTION_LAYER_MODS` and this may cause incompatibility with existent shared URL and downloaded firmwware of keymap editor. If you are using the action you just have to redefine it on keymap editor. Existent keymap code should not suffer.

#### 2016/06/26
Keymap framework was updated. `fn_actions[]` should be defined as `action_t` instead of `uint16_t`. And default code for keymap handling is now included in core you just need define `uint8_t keymaps[][MATRIX_ROWS][MATRIX_COLS]` and `action_t fn_actions[]`.


#### 2016/06/22
Some projects were moved from `converter` and `keyboard` to `orphan` directory. Those might be removed in some future but you will be able to access them with `orphans` tag. See <https://github.com/tmk/tmk_keyboard/issues/173>

#### 2016/02/10
core: flabbergast's Chibios protocol was merged from <https://github.com/flabbergast/tmk_keyboard/tree/chibios> (@72b1668). See [tmk_core/protocol/chibios/README.md](tmk_core/protocol/chibios/README.md). Chibios protocol supports Cortex-M such as STM32 and Kinetis.

#### 2015/04/22
Core library was separated to other branch `core`. <https://github.com/tmk/tmk_keyboard/tree/core>

In `Makefile` you need to set `TMK_DIR` to indicate core library location now.

    TMK_DIR = ../../tmk_core



Projects
--------
You can find some keyboard specific projects under `converter` and `keyboard` directory.

### converter
* [ps2_usb](converter/ps2_usb/)             - [PS/2 keyboard to USB][GH_ps2]
* [adb_usb](converter/adb_usb/)             - [ADB keyboard to USB][GH_adb]
* [m0110_usb](converter/m0110_usb)          - [Macintosh 128K/512K/Plus keyboard to USB][GH_m0110]
* [terminal_usb](converter/terminal_usb/)   - [IBM Model M terminal keyboard(PS/2 scancode set3) to USB][GH_terminal]
* [news_usb](converter/news_usb/)           - [Sony NEWS keyboard to USB][GH_news]
* [x68k_usb](converter/x68k_usb/)           - [Sharp X68000 keyboard to USB][GH_x68k]
* [sun_usb](converter/sun_usb/)             - [Sun] to USB(type4, 5 and 3?)
* [pc98_usb](converter/pc98_usb/)           - [PC98] to USB
* [usb_usb](converter/usb_usb/)             - [USB to USB][GH_usb]
* [ibm4704_usb](converter/ibm4704_usb)      - [IBM 4704 keyboard to USB][GH_ibm4704]
* [next_usb](converter/next_usb)            - NeXT(Non-ADB) to USB, contributed by [BCG](https://github.com/bgould) and based on [Adafruit's work](https://learn.adafruit.com/usb-next-keyboard-with-arduino-micro/overview)

### keyboard
* [hhkb](keyboard/hhkb/)                    - [Happy Hacking Keyboard pro][GH_hhkb] **my main board**
* [alps64](keyboard/alps64/)                - [Alps64 PCB](https://geekhack.org/index.php?topic=69740.0)
* [hbkb](keyboard/hbkb/)                    - [Happy Buckling spring keyboard][GH_hbkb](IBM Model M 60% mod)
* [Infinity](keyboard/infinity/)            - Massdrop [Infinity keyboard][Infinity]
* [gh60](keyboard/gh60/)                    - [GH60] DIY 60% keyboard [prototype][GH60_proto] **my second board**
* [onekey](keyboard/onekey/)                - Simple one key keyboard example


### Projects based tmk_keyboard or tmk_core
https://github.com/tmk/tmk_keyboard/wiki/TMK-Based-Projects


[GH_hhkb]:      http://geekhack.org/showwiki.php?title=Island:12047
[GH_ps2]:       http://geekhack.org/showwiki.php?title=Island:14618
[GH_adb]:       http://geekhack.org/showwiki.php?title=Island:14290
[GH_hhkb_bt]:   http://geekhack.org/showwiki.php?title=Island:20851
[GH_m0110]:     http://geekhack.org/showwiki.php?title=Island:24965
[GH_news]:      http://geekhack.org/showwiki.php?title=Island:25759
[GH_terminal]:  http://geekhack.org/showwiki.php?title=Island:27272
[GH_x68k]:      http://geekhack.org/showwiki.php?title=Island:29060
[GH_hbkb]:      http://geekhack.org/showwiki.php?title=Island:29483
[GH_ibm4704]:   http://geekhack.org/index.php?topic=54706.0
[GH60]:         http://geekhack.org/index.php?topic=34959
[GH60_proto]:   http://geekhack.org/index.php?topic=37570.0
[PC98]:         http://en.wikipedia.org/wiki/NEC_PC-9801
[Sun]:          http://en.wikipedia.org/wiki/Sun-3
[Infinity]:     https://www.massdrop.com/buy/infinity-keyboard-kit
[tmk_core]:     https://github.com/tmk/tmk_core



License
-------
**GPLv2** or later. Some protocol files are under **Modified BSD License**.

Third party libraries like LUFA, PJRC and V-USB have their own license respectively.



Build Firmware and Program Controller
-------------------------------------
See [tmk_core/doc/build.md](tmk_core/doc/build.md).



Change your keymap
------------------
See [tmk_core/doc/keymap.md](tmk_core/doc/keymap.md).



Magic Commands
--------------
To see help press `Magic` + `H`.

`Magic` key combination is `LShift` + `RShift` in many projects, but `Power` key on ADB converter.
`Magic` keybind can be vary on each project, check `config.h` in project directory.

Following commands can be also executed with `Magic` + key. In console mode `Magic` keybind is not needed.

    ----- Command Help -----
    c:      enter console mode
    d:      toggle debug enable
    x:      toggle matrix debug
    k:      toggle keyboard debug
    m:      toggle mouse debug
    v:      print device version & info
    t:      print timer count
    s:      print status
    e:     print eeprom config
    n:     toggle NKRO
    0/F10:  switch to Layer0
    1/F1:   switch to Layer1
    2/F2:   switch to Layer2
    3/F3:   switch to Layer3
    4/F4:   switch to Layer4
    PScr:   power down/remote wake-up
    Caps:   Lock Keyboard(Child Proof)
    Paus:   jump to bootloader



Boot Magic Configuration - Virtual DIP Switch
---------------------------------------------
Boot Magic are executed during boot up time. Press Magic key below then plug in keyboard cable.
Note that you must use keys of **Layer 0** as Magic keys. These settings are stored in EEPROM so that retain your configure over power cycles.

To avoid configuring accidentally additive salt key `KC_SPACE` also needs to be pressed along with the following configuration keys. The salt key is configurable in `config.h`. See [tmk_core/common/bootmagic.h](tmk_core/common/bootmagic.h).

#### General
- Skip reading EEPROM to start with default configuration(`ESC`)
- Clear configuration stored in EEPROM to reset configuration(`Backspace`)

#### Bootloader
- Kick up Bootloader(`B`)

#### Debug
- Debug enable(`D`)
- Debug matrix enable(`D`+`X`)
- Debug keyboard enable(`D`+`K`)
- Debug mouse enable(`D`+`M`)

#### Keymap
- Swap Control and CapsLock(`Left Control`)
- Change CapsLock to Control(`Caps Lock`)
- Swap LeftAlt and Gui(`Left Alt`)
- Swap RightAlt and Gui(`Right Alt`)
- Disable Gui(`Left Gui`)
- Swap Grave and Escape(`Grave`)
- Swap BackSlash and BackSpace(`Back Slash`)
- Enable NKRO on boot(`N`)

#### Default Layer
- Set Default Layer to 0(`0`)
- Set Default Layer to 1(`1`)
- Set Default Layer to 2(`2`)
- Set Default Layer to 3(`3`)
- Set Default Layer to 4(`4`)
- Set Default Layer to 5(`5`)
- Set Default Layer to 6(`6`)
- Set Default Layer to 7(`7`)



Mechanical Locking support
--------------------------
This feature makes it possible for you to use mechanical locking switch for `CapsLock`, `NumLock`
or `ScrollLock`. To enable this feature define these macros in `config.h` and use `KC_LCAP`, `KC_LN
UM` or `KC_LSCR` in keymap for locking key instead of normal `KC_CAPS`, `KC_NLCK` or `KC_SLCK`. Res
ync option tries to keep switch state consistent with keyboard LED state.

    #define LOCKING_SUPPORT_ENABLE
    #define LOCKING_RESYNC_ENABLE



Start Your Own Project
-----------------------
1. Add `tmk_core` into your repository using `git submodule` or `git subtree`.
2. Copy files from `tmk_keybaord` or other project similar to yours
3. Edit those files to support your keyboard.

See these as examples.
- https://github.com/tmk/infinity_ergodox
- https://github.com/tmk/whitefox



Debugging
--------
Use PJRC's `hid_listen` to see debug messages. You can use xprintf() to display debug info, see `tmk_core/common/xprintf.h`.

- https://www.pjrc.com/teensy/hid_listen.html



Files and Directories
-------------------
### Top
* keyboard/     - keyboard projects
* converter/    - protocol converter projects
* tmk_core/     - core library
* tmk_core/doc/ - documents



Contribution
------------
- Report bugs in github **[Issues](https://github.com/tmk/tmk_keyboard/issues)**.
- Pull requets are also welcomed.



Coding Style
-------------
- Doesn't use Tab to indent, use 4-spaces instead.



Other Keyboard Firmware Projects
------------------
You can learn a lot about keyboard firmware from these. See [Other Projects](https://github.com/tmk/tmk_keyboard/wiki/Other-Projects) other than TMK."
cloudius-systems/osv,20445,3062,302,554,Organization,False,8016,11,45,100,False,"OSv, a new operating system for the cloud.",http://osv.io,0,35,5,310,703,7,12,0,67,0,3,2756,6,148,11591,4588,0,0,35,6,,,"***OSv was originally designed and implemented by Cloudius Systems (now ScyllaDB) however
 currently it is being maintained and enhanced by a small community of volunteers.
 If you are into systems programming or want to learn and help us improve OSv, then please
 contact us on [OSv Google Group forum](https://groups.google.com/forum/#!forum/osv-dev).
 For details on how to format and send patches, please read
 [this wiki](https://github.com/cloudius-systems/osv/wiki/Formatting-and-sending-patches)
 (__we do NOT accept pull requests__).***

# OSv

OSv is an open-source versatile modular **unikernel** designed to run single **unmodified
Linux application** securely as microVM on top of a hypervisor, when compared to traditional
operating systems which were designed for a vast range of physical machines. Built from
the ground up for effortless deployment and management of microservices
and serverless apps, with superior performance.

OSv has been designed to run unmodified x86-64 Linux
binaries **as is**, which effectively makes it a **Linux binary compatible unikernel**
(for more details about Linux ABI compatibility please read
[this doc](https://github.com/cloudius-systems/osv/wiki/OSv-Linux-ABI-Compatibility)).
In particular OSv can run many managed language runtimes including
[**JVM**](https://github.com/cloudius-systems/osv-apps/tree/master/java-example),
**Python** [**2**](https://github.com/cloudius-systems/osv-apps/tree/master/python2x) and
[**3**](https://github.com/cloudius-systems/osv-apps/tree/master/python3x),
[**Node.JS**](https://github.com/cloudius-systems/osv-apps/tree/master/node-from-host),
[**Ruby**](https://github.com/cloudius-systems/osv-apps/tree/master/ruby-example), **Erlang**,
and applications built on top of those runtimes.
It can also run applications written in languages compiling directly to native machine code like
**C**, **C++**,
[**Golang**](https://github.com/cloudius-systems/osv-apps/tree/master/golang-httpserver)
and [**Rust**](https://github.com/cloudius-systems/osv-apps/tree/master/rust-httpserver)
as well as native images produced
by [**GraalVM**](https://github.com/cloudius-systems/osv-apps/tree/master/graalvm-example)
and [WebAssembly/Wasmer](https://github.com/cloudius-systems/osv-apps/tree/master/webassembly).

OSv can boot as fast as **~5 ms** on Firecracker using as low as 15 MB of memory.
OSv can run on many hypervisors including QEMU/KVM,
[Firecracker](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-Firecracker),
Xen, [VMWare](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-VMware-ESXi),
[VirtualBox](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-VirtualBox) and
Hyperkit as well as open clouds like AWS EC2, GCE and OpenStack.

For more information about OSv, see the [main wiki page](https://github.com/cloudius-systems/osv/wiki)
and http://osv.io/.

## Building and Running Apps on OSv

In order to run an application on OSv, one needs to build an image by fusing OSv kernel, and
the application files together. This, in high level can be achieved in two ways, either:
- by using the shell script located at `./scripts/build`
 that builds the kernel from sources and fuses it with application files, or
- by using the [capstan tool](https://github.com/cloudius-systems/capstan) that uses *pre-built
 kernel* and combines it with application files to produce a final image.

If your intention is to try to run your app on OSv with the least effort possible, you should pursue the *capstan*
route. For introduction please read this 
[crash course](https://github.com/cloudius-systems/osv/wiki/Build-and-run-apps-on-OSv-using-Capstan).
For more details about *capstan* please read 
this more detailed [documentation](https://github.com/cloudius-systems/capstan#documentation). Pre-built OSv kernel files
(`ovs-loader.qemu`) can be automatically downloaded by *capstan* from 
the [OSv regular releases page](https://github.com/cloudius-systems/osv/releases) or manually from 
the [nightly releases repo](https://github.com/osvunikernel/osv-nightly-releases/releases/tag/ci-master-latest).

If you are comfortable with make and GCC toolchain and want to try the latest OSv code, then you should
read this [part of the readme](#setting-up-development-environment) to guide you how to set up your
 development environment and build OSv kernel and application images.

## Releases

We aim to release OSv 2-3 times a year. You can find the [latest one on github](https://github.com/cloudius-systems/osv/releases)
along with number of published artifacts including kernel and some modules.

In addition, we have set up [Travis-based CI/CD pipeline](https://travis-ci.org/github/cloudius-systems/osv) where each
commit to the master and ipv6 branches triggers full build of the latest kernel and publishes some artifacts to 
the [nightly releases repo](https://github.com/osvunikernel/osv-nightly-releases/releases). Each commit also
triggers publishing of new Docker ""build tool chain"" images to the [Docker hub](https://hub.docker.com/u/osvunikernel).

## Design

Good bit of the design of OSv is pretty well explained in 
the [Components of OSv](https://github.com/cloudius-systems/osv/wiki/Components-of-OSv) wiki page. You 
can find even more information in the original 
[USENIX paper and its presentation](https://www.usenix.org/conference/atc14/technical-sessions/presentation/kivity).

In addition, you can find a lot of good information about design of specific OSv components on
the [main wiki page](https://github.com/cloudius-systems/osv/wiki) and http://osv.io/ and http://blog.osv.io/.
Unfortunately, some of that information may be outdated (especially on http://osv.io/), so it is always
best to ask on the [mailing list](https://groups.google.com/forum/#!forum/osv-dev) if in doubt.

## Metrics and Performance

There are no official **up-to date** performance metrics comparing OSv to other unikernels or Linux.
In general OSv lags behind Linux in disk-I/O-intensive workloads partially due to coarse-grained locking 
in VFS around read/write operations as described in this [issue](https://github.com/cloudius-systems/osv/issues/450).
In network-I/O-intensive workloads, OSv should fare better (or at least used to as Linux has advanced a lot since)
as shown with performance tests of Redis and [Memcached](https://github.com/cloudius-systems/osv/wiki/OSv-Case-Study:-Memcached).
You can find some old ""numbers"" on the main wiki, http://osv.io/benchmarks and some papers listed at the bottom of this readme.

So OSv is probably not best suited to run MySQL or ElasticSearch, but should deliver pretty solid performance for general
 stateless applications like microservices or serverless (at least as some papers show).

### Kernel Size

At this moment (as of May 2020) the size of the uncompressed OSv kernel (`kernel.elf` artifact) is around
6.7 MB (the compressed is ~ 2.7 MB). This is not that small comparing to Linux kernel and quite large comparing
to other unikernels. However, bear in mind that OSv kernel (being unikernel) provides **subset** of functionality
 of the following Linux libraries (see their approximate size on Linux host):
- `libresolv.so.2` (_100 K_)
- `libc.so.6` (_2 MB_)
- `libm.so.6` (_1.4 MB_)
- `ld-linux-x86-64.so.2` (_184 K_)
- `libpthread.so.0` (_156 K_)
- `libdl.so.2` (_20 K_)
- `librt.so.1` (_40 K_)
- `libstdc++.so.6` (_2 MB_)
- `libaio.so.1` (_16 K_)
- `libxenstore.so.3.0` (_32 K_)
- `libcrypt.so.1` (_44 K_)

The equivalent static version of `libstdc++.so.6` is actually linked `--whole-archive` so that
any C++ apps can run without having to add `libstdc++.so.6` to the image (whether it needs it or not).
Finally, OSv kernel comes with ZFS implementation which in theory later can be extracted as a 
[separate library](https://github.com/cloudius-systems/osv/issues/1009). The
point of this is to illustrate that comparing OSv kernel size to Linux kernel size does not
quite make sense.

### Boot Time

OSv with _Read-Only FS with networking off_ can boot as fast as **~5 ms** on Firecracker 
and even faster around **~3 ms** on QEMU with the microvm machine. However, in general the boot time
will depend on many factors like hypervisor including settings of individual para-virtual devices, 
filesystem (ZFS, ROFS or RAMFS) and some boot parameters. Please note that by default OSv images
get built with ZFS filesystem.

For example, the boot time of ZFS image on Firecracker is around ~40 ms and regular QEMU around 200 ms these days. Also,
newer versions of QEMU (>=4.0) are typically faster to boot. Booting on QEMU in PVH/HVM mode (aka direct kernel boot, enabled 
by `-k` option of `run.py`) should always be faster as OSv is directly invoked in 64-bit long mode. Please see
[this Wiki](https://github.com/cloudius-systems/osv/wiki/OSv-boot-methods-overview) for the brief review of the boot
methods OSv supports.

Finally, some boot parameters passed to the kernel may affect the boot time:
- `--console serial` - this disables VGA console that is [slow to initialize](https://github.com/cloudius-systems/osv/issues/987) and can shave off 60-70 ms on QEMU
- `--nopci` - this disables enumeration of PCI devices especially if we know none are present (QEMU with microvm or Firecracker) and can shave off 10-20 ms 
- `--redirect=/tmp/out` - writing to the console can impact the performance quite severely (30-40%) if application logs 
a lot, so redirecting standard output and error to a file might speed up performance quite a lot

You can always see boot time breakdown by adding `--bootchart` parameter:
```
./scripts/run.py -e '--bootchart /hello'
OSv v0.54.0-197-g1f0df4e4
eth0: 192.168.122.15
 disk read (real mode): 25.85ms, (+25.85ms)
 uncompress lzloader.elf: 45.11ms, (+19.26ms)
 TLS initialization: 45.72ms, (+0.61ms)
 .init functions: 47.61ms, (+1.89ms)
 SMP launched: 48.08ms, (+0.47ms)
 VFS initialized: 50.99ms, (+2.91ms)
 Network initialized: 51.12ms, (+0.14ms)
 pvpanic done: 51.25ms, (+0.13ms)
 pci enumerated: 61.55ms, (+10.29ms)
 drivers probe: 61.55ms, (+0.00ms)
 drivers loaded: 135.91ms, (+74.36ms)
 ROFS mounted: 136.98ms, (+1.07ms)
 Total time: 138.16ms, (+1.18ms)
Cmdline: /hello
Hello from C code
```

### Memory Utilization

OSv needs at least 15 M of memory to run a _hello world_ app. Even though it is half of
what it was 2 years ago, it is still quite a lot comparing to other unikernels. We are planning to further lower
this number by reducing size of the kernel, adding [self-tuning logic to L1/L2 memory pools](https://github.com/cloudius-systems/osv/issues/1013) and
making application threads use [lazily allocated stacks](https://github.com/cloudius-systems/osv/issues/143).

## Testing

OSv comes with around 130 unit tests that get executed upon every commit and run on ScyllaDB servers. There are also number of extra
tests located under `tests/` sub-tree that are not automated at this point.

You can run unit tests in number of ways:
```
./scripts/build check                  # Create ZFS test image and run all tests on QEMU

./scripts/build check fs=rofs          # Create ROFS test image and run all tests on QEMU

./scripts/build image=tests && \       # Create ZFS test image and run all tests on Firecracker
./scripts/test.py -p firecracker

./scripts/build image=tests && \       # Create ZFS test image and run all tests on QEMU
./scripts/test.py -p qemu_microvm      # with microvm machine
```

In addition, there is an [Automated Testing Framework](https://github.com/cloudius-systems/osv/wiki/Automated-Testing-Framework)
that can be used to run around 30 real apps, some of them
under stress using `ab` or `wrk` tools. The intention is to catch any regressions that might be missed
by unit tests.

Finally, one can use [Docker files](https://github.com/cloudius-systems/osv/tree/master/docker#docker-osv-builder) to
test OSv on different Linux distribution.

## Setting up Development Environment

OSv can only be built on a 64-bit x86 Linux distribution. Please note that
this means the ""x86_64"" or ""amd64"" version, not the 32-bit ""i386"" version.

In order to build OSv kernel you need a physical or virtual machine with Linux distribution on it and GCC toolchain and
all necessary packages and libraries OSv build process depends on. The fastest way to set it up is to use the
[Docker files](https://github.com/cloudius-systems/osv/tree/master/docker#docker-osv-builder) that OSv comes with.
You can use them to build your own Docker image and then start it in order to build OSv kernel or run an app on OSv inside of it.
Please note that the main docker file depends on pre-built base **Docker images** for 
[Ubuntu](https://hub.docker.com/repository/docker/osvunikernel/osv-ubuntu-19.10-builder-base) 
or [Fedora](https://hub.docker.com/repository/docker/osvunikernel/osv-fedora-31-builder-base) 
that get published to DockerHub upon every commit. This should speed up building the final images
as all necessary packages should already be part of the base images.

Alternatively, you can manually clone OSv repo and use [setup.py](https://github.com/cloudius-systems/osv/blob/master/scripts/setup.py)
to install all required packages and libraries, as long as it supports your Linux distribution, and you have both git 
and python 3 installed on your machine:
```bash
git clone https://github.com/cloudius-systems/osv.git
cd osv && git submodule update --init --recursive
./scripts/setup.py
```

The `setup.py` recognizes and installs packages for number of Linux distributions including Fedora, Ubuntu, 
[Debian](https://github.com/cloudius-systems/osv/wiki/Building-OSv-on-Debian-stable), LinuxMint and RedHat ones 
(Scientific Linux, NauLinux, CentOS Linux, Red Hat Enterprise Linux, Oracle Linux). Please note that we actively
maintain and test only Ubuntu and Fedora, so your mileage with other distributions may vary. The `setup.py`
is actually used by Docker files internally to achieve the same result. 

### IDEs

If you like working in IDEs, we recommend either [Eclipse CDT](https://www.eclipse.org/cdt/) which can be setup
as described in this [wiki page](https://github.com/cloudius-systems/osv/wiki/Working-With-Eclipse-CDT) or 
[CLion from JetBrains](https://www.jetbrains.com/clion/) which can be setup to work with OSv makefile using
so called compilation DB as described in this [guide](https://www.jetbrains.com/help/clion/managing-makefile-projects.html).

## Building OSv Kernel and Creating Images

Building OSv is as easy as using the shell script `./scripts/build`
that orchestrates the build process by delegating to the main [makefile](https://github.com/cloudius-systems/osv/blob/master/Makefile)
to build the kernel and by using number of Python scripts like `./scripts/module.py` 
to build application and *fuse* it together with the kernel
into a final image placed at `./build/release/usr.img` (or `./build/$(arch)/usr.img` in general).
Please note that *building an application* does not necessarily mean building from sources as in many 
cases the application binaries would be located on and copied from the Linux build machine
using the shell script `./scripts/manifest_from_host.sh`
(see [this Wiki page](https://github.com/cloudius-systems/osv/wiki/Running-unmodified-Linux-executables-on-OSv) for details).

The shell script `build` can be used as the examples below illustrate:
```bash
# Create default image that comes with command line and REST API server
./scripts/build

# Create image with native-example app
./scripts/build -j4 fs=rofs image=native-example

# Create image with spring boot app with Java 10 JRE
./scripts/build JAVA_VERSION=10 image=openjdk-zulu-9-and-above,spring-boot-example

 # Create image with 'ls' executable taken from the host
./scripts/manifest_from_host.sh -w ls && ./scripts/build --append-manifest

# Create test image and run all tests in it
./scripts/build check

# Clean the build tree
./scripts/build clean
```

Command nproc will calculate the number of jobs/threads for make and `./scripts/build` automatically.
Alternatively, the environment variable MAKEFLAGS can be exported as follows:

```
export MAKEFLAGS=-j$(nproc)
```

In that case, make and scripts/build do not need the parameter -j.

For details on how to use the build script, please run `./scripts/build --help`.

The `./scripts/build` creates the image `build/last/usr.img` in qcow2 format.
To convert this image to other formats, use the `./scripts/convert`
tool, which can convert an image to the vmdk, vdi or raw formats.
For example:

```
./scripts/convert raw
```

### Aarch64

By default, OSv kernel gets built for x86_64 architecture, but it is also possible
 to build one for ARM by adding **arch** parameter like so:
```bash
./scripts/build arch=aarch64
```
At this point cross-compiling the **aarch64** version of OSv is only supported
on Fedora and relevant aarch64 gcc and libraries' binaries can be downloaded using
the `./scripts/download_fedora_aarch64_packages.py` script.
Please note that simple ""hello world"" app should work just fine, but overall the ARM part of OSv has not been
 as well maintained and tested as x86_64 due to the lack of volunteers. In addition,
 the same simple example can successfully run on QEMU on Raspberry PI 4 with KVM acceleration enabled.
 For more information about the aarch64 port please read [this Wiki page](https://github.com/cloudius-systems/osv/wiki/AArch64).

### Filesystems

At the end of the boot process, OSv dynamic linker loads an application ELF and any related libraries
 from the filesystem on a disk that is part of the image. By default, the images built by `./scripts/build`
 contain a disk formatted as ZFS, which you can read more about [here](https://github.com/cloudius-systems/osv/wiki/ZFS).
 ZFS is a great read-write file system and may be a perfect fit if you want to run MySQL on OSv. However, it may be an overkill
 if you want to run stateless apps in which case you may consider 
 [Read-Only FS](https://github.com/cloudius-systems/osv/commit/cd449667b7f86721095ddf4f9f3f8b87c1c414c9). Finally,
 you can also have OSv read the application binary from RAMFS, in which case the filesystem get embedded as part of
 the kernel ELF. You can specify which filesystem to build image disk as
  by setting parameter `fs` of `./scripts/build` to one of the three values -`zfs`, `rofs` or `ramfs`.

In addtion, one can mount NFS filesystem, which had been recently transformed to be a shared library pluggable as a [module](https://github.com/cloudius-systems/osv/tree/master/modules/nfs), and newly implemented [Virtio-FS filesystem](https://stefanha.github.io/virtio/virtio-fs.html#x1-41500011). The NFS and Virtio-FS mounts can be setup by adding proper entry `/etc/fstab` or by passing a boot parameter as explained in this [commit comments](https://github.com/cloudius-systems/osv/commit/47c7e9268ff96f67f4649bb6c63685a5c2d74f00).

## Running OSv

Running an OSv image, built by `scripts/build`, is as easy as:
```bash
./scripts/run.py
```

By default, the `run.py` runs OSv under KVM, with 4 vCPUs and 2 GB of memory. 
You can control these and tens of other ones by passing relevant parameters to 
the `run.py`. For details, on how to use the script, please run `./scripts/run.py --help`.

The `run.py` can run OSv image on QEMU/KVM, Xen and VMware. If running under KVM you can terminate by hitting Ctrl+A X.

Alternatively, you can use `./scripts/firecracker.py` to run OSv on [Firecracker](https://firecracker-microvm.github.io/). 
This script automatically downloads firecracker binary if missing, and accepts number of parameters like number ot vCPUs, memory
named exactly like `run.py` does. You can learn more about running OSv on Firecracker 
from this [wiki](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-Firecracker). 

Please note that in order to run OSv with the best performance on Linux under QEMU or Firecracker you need KVM enabled 
(this is only possible on *physical* Linux machines, EC2 ""bare metal"" (i3) instances or VMs that support nested virtualization with KVM on). 
The easiest way to verify if KVM is enabled is to check if `/dev/kvm` is present, and your user account can read from and write to it. 
Adding your user to the kvm group may be necessary like so:
```bash
usermod -aG kvm <user name>
```

For more information about building and running JVM, Node.JS, Python and other managed runtimes as well as Rust, Golang or C/C++ apps
 on OSv, please read this [wiki page](https://github.com/cloudius-systems/osv/wiki#running-your-application-on-osv). 
 For more information about various example apps you can build and run on OSv, please read 
 [the osv-apps repo README](https://github.com/cloudius-systems/osv-apps#osv-applications).

### Networking

By default, the `run.py`  starts OSv with
 [user networking/SLIRP](https://wiki.qemu.org/Documentation/Networking#User_Networking_.28SLIRP.29) on. 
To start OSv with more performant external networking, you need to enable `-n` and `-v` options like so:

```
sudo ./scripts/run.py -nv
```

The -v is for KVM's vhost that provides better performance
and its setup requires tap device and thus we use sudo.

By default, OSv spawns a `dhcpd`-like thread that automatically configures virtual NICs.
A static configuration can be done within OSv by configuring networking like so:

```
ifconfig virtio-net0 192.168.122.100 netmask 255.255.255.0 up
route add default gw 192.168.122.1
```

To enable networking on Firecracker, you have to explicitly enable `-n` option
to `firecracker.py`.

Finally, please note that the master branch of OSv only implements IPV4 subset of networking stack.
If you need IPV6, please build from [ipv6 branch](https://github.com/cloudius-systems/osv/tree/ipv6)
 or use IPV6 kernel published to [nightly releases repo](https://github.com/osvunikernel/osv-nightly-releases/releases/tag/ci-ipv6-latest). 

## Debugging, Monitoring, Profiling OSv

- OSv can be debugged with gdb; for more details please read this
 [wiki](https://github.com/cloudius-systems/osv/wiki/Debugging-OSv)
- OSv kernel and application can be traced and profiled; for more details please read 
this [wiki](https://github.com/cloudius-systems/osv/wiki/Trace-analysis-using-trace.py)
- OSv comes with the admin/monitoring REST API server; for more details please read 
[this](https://github.com/cloudius-systems/osv/wiki/Command-Line-Interface-(CLI)) and
 [that wiki page](https://github.com/cloudius-systems/osv/wiki/Using-OSv-REST-API). There is also
 lighter [monitoring REST API module](https://github.com/cloudius-systems/osv/commit/aa32614221254ce300f401bb99c506b528b85682) 
 that is effectively a read-only subset of the former one. 
 
## FAQ and Contact

If you want to learn more about OSv or ask questions, 
please contact us on [OSv Google Group forum](https://groups.google.com/forum/#!forum/osv-dev).
You can also follow us on [Twitter](https://twitter.com/osv_unikernel).

## Papers and Articles about OSv

List of somewhat newer articles about OSv found on the Web:
* [Unikernels vs Containers: An In-Depth Benchmarking Study in the context of Microservice Applications](https://biblio.ugent.be/publication/8582433/file/8582438)
* [Towards a Practical Ecosystem of Specialized OS Kernels](http://cs.iit.edu/~khale/docs/diver-ross19.pdf)
* [A Performance Evaluation of Unikernels](https://pdfs.semanticscholar.org/d956/f72dbc65301578dc95e0f751f4ae7c09d831.pdf)
* [Security Perspective on Unikernels](https://arxiv.org/pdf/1911.06260.pdf)
* [Performance Evaluation of OSv for Server Applications](http://www.cs.utah.edu/~peterm/prelim-osv-performance.pdf)
* [Time provisioning Evaluation of KVM, Docker and Unikernels in a Cloud Platform](https://tiagoferreto.github.io/pubs/2016ccgrid_xavier.pdf)
* [Unikernels - Beyond Containers to the Next Generation of the Cloud](https://theswissbay.ch/pdf/_to_sort/O'Reilly/unikernels.pdf)

You can find some older articles and presentations at http://osv.io/resources and http://blog.osv.io/."
DaveDavenport/rofi,17272,5936,95,323,Organization,False,3182,29,29,91,False,"Rofi: A window switcher, application launcher and dmenu replacement",,7,25,3,64,842,17,60,14,222,6,19,2910,15,100,8117,6301,0,0,9,0,,davatorium/rofi,"[![Codacy Badge](https://api.codacy.com/project/badge/Grade/ca0310962a7c4b829d0c57f1ab023531)](https://app.codacy.com/app/davatorium/rofi?utm_source=github.com&utm_medium=referral&utm_content=davatorium/rofi&utm_campaign=Badge_Grade_Settings)
[![Build Status](https://travis-ci.org/davatorium/rofi.svg?branch=master)](https://travis-ci.org/davatorium/rofi)
[![codecov.io](https://codecov.io/github/davatorium/rofi/coverage.svg?branch=master)](https://codecov.io/github/davatorium/rofi?branch=master)
[![Issues](https://img.shields.io/github/issues/davatorium/rofi.svg)](https://github.com/davatorium/rofi/issues)
[![Forks](https://img.shields.io/github/forks/davatorium/rofi.svg)](https://github.com/davatorium/rofi/network)
[![Stars](https://img.shields.io/github/stars/davatorium/rofi.svg)](https://github.com/davatorium/rofi/stargazers)
[![Downloads](https://img.shields.io/github/downloads/davatorium/rofi/total.svg)](https://github.com/davatorium/rofi/releases)
[![Coverity](https://scan.coverity.com/projects/3850/badge.svg)](https://scan.coverity.com/projects/davedavenport-rofi)
[![Forum](https://img.shields.io/badge/forum-online-green.svg)](https://reddit.com/r/qtools/)

# A window switcher, Application launcher and dmenu replacement

**Rofi** started as a clone of simpleswitcher, written by [Sean Pringle](http://github.com/seanpringle/simpleswitcher) - a
popup window switcher roughly based on [superswitcher](http://code.google.com/p/superswitcher/).
Simpleswitcher laid the foundations, and therefore Sean Pringle deserves most of the credit for this tool. **Rofi**
(renamed, as it lost the *simple* property) has been extended with extra features, like an application launcher and 
ssh-launcher, and can act as a drop-in dmenu replacement, making it a very versatile tool.

**Rofi**, like dmenu, will provide the user with a textual list of options where one or more can be selected.
This can either be running an application, selecting a window, or options provided by an external script.

Its main features are:

 * Fully configurable keyboard navigation
 * Type to filter
    - Tokenized: type any word in any order to filter
    - Case insensitive (togglable)
    - Support for fuzzy-, regex-, and glob matching
 * UTF-8 enabled
    - UTF-8-aware string collating
    - International keyboard support (`e -> è)
 * RTL language support
 * Cairo drawing and Pango font rendering
 * Built-in modes:
    - Window switcher mode
        - EWMH compatible WM
    - Application launcher
    - Desktop file application launcher
    - SSH launcher mode
    - Combi mode, allowing several modes to be merged into one list
 * History-based ordering — last 25 choices are ordered on top based on use (optional)
 * Levenshtein distance ordering of matches (optional)
 * Drop-in dmenu replacement
    - Many added improvements
 * Easily extensible using scripts
 * Theming

**Rofi** has several built-in modes implementing common use cases and can be extended by scripts (either called from
**Rofi** or calling **Rofi**).

Below is a list of the different modes:

## Window Switcher

![Window List](https://davatorium.github.io/rofi/images/rofi/window-list.png)

The window switcher shows the following informations in columns (can be customized):

1. Desktop name
2. Window class
3. Window title

Window mode features:

 * Closing applications with `Shift-Delete`
 * Custom command with `Shift-Return`


## Application launcher

![run mode](https://davatorium.github.io/rofi/images/rofi/run-dialog.png)

The run mode allows users to quickly search for and launch a program.

Run mode features:

 * `Shift-Return` to run the selected program in a terminal
 * Favorites list, with frequently used programs sorted on top
 * Custom entries, like aliases, added by executing a command


## Desktop File Application launcher

The desktop run mode allows users to quickly search and launch an application from the *freedesktop.org* Desktop
Entries. These are used by most Desktop Environments to populate launchers and menus.
Drun mode features:

 * Favorites list, with frequently used programs sorted on top
 * Auto starting terminal applications in a terminal

## SSH launcher

![SSH Launcher](https://davatorium.github.io/rofi/images/rofi/ssh-dialog.png)

Quickly `ssh` into remote machines. Parses `~/.ssh/config` to find hosts.

## Script mode

Loads external scripts to add modes to **Rofi**, for example a file-browser.

```
rofi  -show fb -modi fb:../Examples/rofi-file-browser.sh
```

## COMBI mode

Combine multiple modes in one view. This is especially useful when merging the window and run mode into one view.
Allowing to quickly switch to an application, either by switching to it when it is already running or starting it.

Example to combine Desktop run and the window switcher:

```
rofi -combi-modi window,drun -show combi -modi combi
```

## dmenu replacement

![DMENU replacement (running teiler)](https://davatorium.github.io/rofi/images/rofi/dmenu-replacement.png)

Drop in dmenu replacement. (Screenshot shows rofi used by
[teiler](https://github.com/carnager/teiler) ).

**Rofi** features several improvements over dmenu to improve usability. There is the option to add
an extra message bar (`-mesg`), pre-entering of text (`-filter`), or selecting entries based on a
pattern (`-select`). Also highlighting (`-u` and `-a`) options and modi to force user to select one
provided option (`-only-match`). In addition to this, rofi's dmenu mode can select multiple lines and
write them to stdout.

# Usage

If used with `-show [mode]`, rofi will immediately open in the specified [mode].

If used with `-dmenu`, rofi will use data from STDIN to let the user select an option.

For example, to show a run dialog:

  `rofi -show run`

To show a ssh dialog:

  `rofi -show ssh`

## dmenu

If rofi is passed the `-dmenu` option, or run as `dmenu` (ie, /usr/bin/dmenu is symlinked to /usr/bin/rofi),
it will use the data passed from STDIN.

```
~/scripts/my_script.sh | rofi -dmenu
echo -e ""Option #1\nOption #2\nOption #3"" | rofi -dmenu
```

In both cases, rofi will output the user's selection to STDOUT.

## Switching Between Modi

Type `Shift-/Left/Right` to switch between active modi.


## Key bindings

| Key                                  | Action                                                             |
|:-------------------------------------|:-------------------------------------------------------------------|
|`Ctrl-v, Insert`                      | Paste from clipboard |
|`Ctrl-Shift-v, Shift-Insert`          | Paste primary selection |
|`Ctrl-w`                              | Clear the line |
|`Ctrl-u`                              | Delete till the start of line |
|`Ctrl-a`                              | Move to beginning of line |
|`Ctrl-e`                              | Move to end of line |
|`Ctrl-f, Right`                       | Move forward one character |
|`Alt-f, Ctrl-Right`                   | Move forward one word |
|`Ctrl-b, Left`                        | Move back one character |
|`Alt-b, Ctrl-Left`                    | Move back one word |
|`Ctrl-d, Delete`                      | Delete character |
|`Ctrl-Alt-d`                          | Delete word |
|`Ctrl-h, Backspace, Shift-Backspace`  | Backspace (delete previous character) |
|`Ctrl-Alt-h`                          | Delete previous word |
|`Ctrl-j,Ctrl-m,Enter`                 | Accept entry |
|`Ctrl-n,Down`                         | Select next entry |
|`Ctrl-p,Up`                           | Select previous entry |
|`Page Up`                             | Go to the previous page |
|`Page Down`                           | Go to the next page |
|`Ctrl-Page Up`                        | Go to the previous column |
|`Ctrl-Page Down`                      | Go to the next column |
|`Ctrl-Enter`                          | Use entered text as a command (in `ssh/run modi`) |
|`Shift-Enter`                         | Launch the application in a terminal (in run mode) |
|`Shift-Enter`                         | Return the selected entry and move to the next item while keeping Rofi open. (in dmenu) |
|`Shift-Right`                         | Switch to the next modi. The list can be customized with the -modi option. |
|`Shift-Left`                          | Switch to the previous modi. The list can be customized with the -modi option. |
|`Ctrl-Tab`                            | Switch to the next modi. The list can be customized with the -modi option. |
|`Ctrl-Shift-Tab`                      | Switch to the previous modi. The list can be customized with the -modi option. |
|`Ctrl-space`                          | Set selected item as input text. |
|`Shift-Del`                           | Delete entry from history. |
|`grave`                               | Toggle case sensitivity. |
|`Alt-grave`                           | Toggle levenshtein sort. |
|`Alt-Shift-S`                         | Take a screenshot and store it in the Pictures directory. |

For the full list of key bindings, see: `rofi -show keys` or `rofi -help`.

# Configuration

There are currently three methods of setting configuration options:

 * Local configuration. Normally, depending on XDG, in `~/.config/rofi/config`. This uses the Xresources format.
 * Xresources: A method of storing key values in the Xserver. See
   [here](https://en.wikipedia.org/wiki/X_resources) for more information.
 * Command line options: Arguments are passed to **Rofi**.

A distribution can ship defaults in `/etc/rofi.conf`.

The Xresources options and the command line options are aliased. To define option X set:

    `rofi.X: value`

In the Xresources file. To set/override this from command line pass the same key
prefixed with '-':

    `rofi -X value`

To get a list of available options formatted as Xresources entries, run:

    `rofi -dump-Xresources`

or in a more readable format:

    `rofi -help`

The configuration system supports the following types:

 * String
 * Integer (signed and unsigned)
 * Char
 * Boolean

The Boolean option has a non-default command line syntax, to enable option X you do:

    `rofi -X`

to disable it:

    `rofi -no-X`

# Manpage

For more detailed information, please see the [manpage](doc/rofi.1.markdown), the [wiki](https://github.com/davatorium/rofi/wiki), or the [forum](https://reddit.com/r/qtools/).

# Installation

Please see the [installation guide](https://github.com/davatorium/rofi/blob/next/INSTALL.md) for instructions on how to
install **Rofi**.

# What is rofi not?

Rofi is not:

 * A preview application. In other words, it will not show a (small) preview of images, movies or other files.
 * A UI toolkit.
 * A library to be used in other applications.
 * An application that can support every possible use-case. It tries to be generic enough to be usable by everybody.
   Specific functionality can be added using scripts.
 * Just a dmenu replacement. The dmenu functionality is a nice 'extra' to **rofi**, not its main purpose."
cmus/cmus,5836,3786,117,412,Organization,False,2159,6,56,111,False,"Small, fast and powerful console music player for Unix-like operating systems.",https://cmus.github.io/,0,27,1,203,490,24,8,18,276,8,0,5395,2,2,19,14,0,0,2,2,,,"*Warning: cmus is not actively maintained. For details, please see [#856](https://github.com/cmus/cmus/issues/856)*

cmus — C\* Music Player
=======================

https://cmus.github.io/

[![Build Status](https://travis-ci.org/cmus/cmus.svg?branch=master)](https://travis-ci.org/cmus/cmus)

Copyright © 2004-2008 Timo Hirvonen <tihirvon@gmail.com>

Copyright © 2008-2017 Various Authors


Configuration
-------------

List available optional features

    $ ./configure --help

Auto-detect everything

    $ ./configure

To disable some feature, arts for example, and install to `$HOME` run

    $ ./configure prefix=$HOME CONFIG_ARTS=n

After running configure you can see from the generated `config.mk` file
what features have been configured in (see the `CONFIG_*` options).

*Note*: For some distributions you need to install development versions
of the dependencies.  For example if you want to use 'mad' input plugin
(mp3) you need to install `libmad0-dev` (Debian) or `libmad-devel` (RPM)
package. After installing dependencies you need to run `./configure`
again, of course.

If you want to use the Tremor library as alternative for decoding
Ogg/Vorbis files you have to pass `CONFIG_TREMOR=y` to the configure
script:

    $ ./configure CONFIG_VORBIS=y CONFIG_TREMOR=y

The Tremor library is supposed to be used on hardware that has no FPU.


Building
--------

    $ make

Or on some BSD systems you need to explicitly use GNU make:

    $ gmake


Installation
------------

    $ make install

Or to install to a temporary directory:

    $ make install DESTDIR=~/tmp/cmus

This is useful when creating binary packages.

Remember to replace `make` with `gmake` if needed.


Manuals
-------

    $ man cmus-tutorial

And

    $ man cmus


Mailing List
------------

To subscribe to cmus-devel@lists.sourceforge.net or view the archive visit
http://lists.sourceforge.net/lists/listinfo/cmus-devel.

The mailing list now serves as an archive for old releases and issues.
Please use the github [issues](https://github.com/cmus/cmus/issues)
page for any problems, suggestions, or bug reports.


Reporting Bugs
--------------

Bugs should be reported using the Github [issue tracker](https://github.com/cmus/cmus/issues).
When creating a new issue, a template will be shown containing instructions on how to collect
the necessary information.

Additional debug information can be found in `~/cmus-debug.txt` if you configured cmus with
maximum debug level (`./configure DEBUG=2`). In case of a crash the last lines may be helpful.


Git Repository
--------------

https://github.com/cmus/cmus

    $ git clone https://github.com/cmus/cmus.git


Hacking
-------

cmus uses the [Linux kernel coding style](https://www.kernel.org/doc/html/latest/process/coding-style.html).
Use hard tabs.  Tabs are _always_ 8 characters wide.  Keep the style consistent with rest of the
code.

Bug fixes and implementations of new features should be suggested as a
[pull request](https://github.com/cmus/cmus/pulls) directly on Github."
robotmedia/RMStore,11667,2351,92,443,Organization,False,354,1,13,16,False,A lightweight iOS library for In-App Purchases,,0,8,1,68,105,1,0,24,35,0,0,2511,0,0,0,0,0,0,44,1,,,"#RMStore

[![CocoaPods Version](https://cocoapod-badges.herokuapp.com/v/RMStore/badge.png)](http://cocoadocs.org/docsets/RMStore) [![Platform](https://cocoapod-badges.herokuapp.com/p/RMStore/badge.png)](http://cocoadocs.org/docsets/RMStore)
[![Build Status](https://travis-ci.org/robotmedia/RMStore.png)](https://travis-ci.org/robotmedia/RMStore)
[![Join the chat at https://gitter.im/robotmedia/RMStore](https://badges.gitter.im/robotmedia/RMStore.svg)](https://gitter.im/robotmedia/RMStore?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

A lightweight iOS library for In-App Purchases.

RMStore adds [blocks](#storekit-with-blocks) and [notifications](#notifications) to StoreKit, plus [receipt verification](#receipt-verification), [content downloads](#downloading-content) and [transaction persistence](#transaction-persistence). All in one class without external dependencies. Purchasing a product is as simple as:

```objective-c
[[RMStore defaultStore] addPayment:productID success:^(SKPaymentTransaction *transaction) {
    NSLog(@""Purchased!"");
} failure:^(SKPaymentTransaction *transaction, NSError *error) {
    NSLog(@""Something went wrong"");
}];
```

##Installation

Using [CocoaPods](http://cocoapods.org/):

```ruby
pod 'RMStore', '~> 0.7'
```

Or add the files from the [RMStore](https://github.com/robotmedia/RMStore/tree/master/RMStore) directory if you're doing it manually.

Check out the [wiki](https://github.com/robotmedia/RMStore/wiki/Installation) for more options.

##StoreKit with blocks

RMStore adds blocks to all asynchronous StoreKit operations.

###Requesting products

```objective-c
NSSet *products = [NSSet setWithArray:@[@""fabulousIdol"", @""rootBeer"", @""rubberChicken""]];
[[RMStore defaultStore] requestProducts:products success:^(NSArray *products, NSArray *invalidProductIdentifiers) {
    NSLog(@""Products loaded"");
} failure:^(NSError *error) {
    NSLog(@""Something went wrong"");
}];
```

###Add payment

```objective-c
[[RMStore defaultStore] addPayment:@""waxLips"" success:^(SKPaymentTransaction *transaction) {
    NSLog(@""Product purchased"");
} failure:^(SKPaymentTransaction *transaction, NSError *error) {
    NSLog(@""Something went wrong"");
}];
```

###Restore transactions

```objective-c
[[RMStore defaultStore] restoreTransactionsOnSuccess:^(NSArray *transactions){
    NSLog(@""Transactions restored"");
} failure:^(NSError *error) {
    NSLog(@""Something went wrong"");
}];
```

###Refresh receipt (iOS 7+ only)

```objective-c
[[RMStore defaultStore] refreshReceiptOnSuccess:^{
    NSLog(@""Receipt refreshed"");
} failure:^(NSError *error) {
    NSLog(@""Something went wrong"");
}];
```

##Notifications

RMStore sends notifications of StoreKit related events and extends `NSNotification` to provide relevant information. To receive them, implement the desired methods of the `RMStoreObserver` protocol and add the observer to `RMStore`.

###Adding and removing the observer

```objective-c
[[RMStore defaultStore] addStoreObserver:self];
...
[[RMStore defaultStore] removeStoreObserver:self];
```

###Products request notifications

```objective-c
- (void)storeProductsRequestFailed:(NSNotification*)notification
{
    NSError *error = notification.rm_storeError;
}

- (void)storeProductsRequestFinished:(NSNotification*)notification
{
    NSArray *products = notification.rm_products;
    NSArray *invalidProductIdentifiers = notification.rm_invalidProductIdentififers;
}
```

###Payment transaction notifications

Payment transaction notifications are sent after a payment has been requested or for each restored transaction.

```objective-c
- (void)storePaymentTransactionFinished:(NSNotification*)notification
{
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
}

- (void)storePaymentTransactionFailed:(NSNotification*)notification
{
    NSError *error = notification.rm_storeError;
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
}

// iOS 8+ only

- (void)storePaymentTransactionDeferred:(NSNotification*)notification
{
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
}
```

###Restore transactions notifications

```objective-c
- (void)storeRestoreTransactionsFailed:(NSNotification*)notification;
{
    NSError *error = notification.rm_storeError;
}

- (void)storeRestoreTransactionsFinished:(NSNotification*)notification
{
 NSArray *transactions = notification.rm_transactions;
}
```

###Download notifications (iOS 6+ only)

For Apple-hosted and self-hosted downloads:

```objective-c
- (void)storeDownloadFailed:(NSNotification*)notification
{
    SKDownload *download = notification.rm_storeDownload; // Apple-hosted only
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
    NSError *error = notification.rm_storeError;
}

- (void)storeDownloadFinished:(NSNotification*)notification;
{
    SKDownload *download = notification.rm_storeDownload; // Apple-hosted only
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
}

- (void)storeDownloadUpdated:(NSNotification*)notification
{
    SKDownload *download = notification.rm_storeDownload; // Apple-hosted only
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
    float progress = notification.rm_downloadProgress;
}
```

Only for Apple-hosted downloads:

```objective-c
- (void)storeDownloadCanceled:(NSNotification*)notification
{
 SKDownload *download = notification.rm_storeDownload;
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
}

- (void)storeDownloadPaused:(NSNotification*)notification
{
 SKDownload *download = notification.rm_storeDownload;
    NSString *productIdentifier = notification.rm_productIdentifier;
    SKPaymentTransaction *transaction = notification.rm_transaction;
}
```

###Refresh receipt notifications (iOS 7+ only)

```objective-c
- (void)storeRefreshReceiptFailed:(NSNotification*)notification;
{
    NSError *error = notification.rm_storeError;
}

- (void)storeRefreshReceiptFinished:(NSNotification*)notification { }
```

##Receipt verification

RMStore doesn't perform receipt verification by default but provides reference implementations. You can implement your own custom verification or use the reference verifiers provided by the library.

Both options are outlined below. For more info, check out the [wiki](https://github.com/robotmedia/RMStore/wiki/Receipt-verification).

###Reference verifiers

RMStore provides receipt verification via `RMStoreAppReceiptVerifier` (for iOS 7 or higher) and `RMStoreTransactionReceiptVerifier` (for iOS 6 or lower). To use any of them, add the corresponding files from [RMStore/Optional](https://github.com/robotmedia/RMStore/tree/master/RMStore/Optional) into your project and set the verifier delegate (`receiptVerifier`) at startup. For example:

```objective-c
- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions
{
    const BOOL iOS7OrHigher = floor(NSFoundationVersionNumber) > NSFoundationVersionNumber_iOS_6_1;
    _receiptVerifier = iOS7OrHigher ? [[RMStoreAppReceiptVerifier alloc] init] : [[RMStoreTransactionReceiptVerifier alloc] init];
    [RMStore defaultStore].receiptVerifier = _receiptVerifier;
    // Your code
    return YES;
}
```

If security is a concern you might want to avoid using an open source verification logic, and provide your own custom verifier instead.

###Custom verifier

RMStore delegates receipt verification, enabling you to provide your own implementation using  the `RMStoreReceiptVerifier` protocol:

```objective-c
- (void)verifyTransaction:(SKPaymentTransaction*)transaction
                           success:(void (^)())successBlock
                           failure:(void (^)(NSError *error))failureBlock;
```

Call `successBlock` if the receipt passes verification, and `failureBlock` if it doesn't. If verification could not be completed (e.g., due to connection issues), then `error` must be of code `RMStoreErrorCodeUnableToCompleteVerification` to prevent RMStore to finish the transaction.

You will also need to set the `receiptVerifier` delegate at startup, as indicated above.

##Downloading content

RMStore automatically downloads Apple-hosted content and provides a delegate for a self-hosted content.

###Apple-hosted content

Downloadable content hosted by Apple (`SKDownload`) will be automatically downloaded when purchasing o restoring a product. RMStore will notify observers of the download progress by calling `storeDownloadUpdate:` and finally `storeDownloadFinished:`. Additionally, RMStore notifies when downloads are paused, cancelled or have failed.

RMStore will notify that a transaction finished or failed only after all of its downloads have been processed. If you use blocks, they will called afterwards as well. The same applies to restoring transactions.

###Self-hosted content

RMStore delegates the downloading of self-hosted content via the optional `contentDownloader` delegate. You can provide your own implementation using the `RMStoreContentDownloader` protocol:

```objective-c
- (void)downloadContentForTransaction:(SKPaymentTransaction*)transaction
                              success:(void (^)())successBlock
                             progress:(void (^)(float progress))progressBlock
                              failure:(void (^)(NSError *error))failureBlock;
```

Call `successBlock` if the download is successful, `failureBlock` if it isn't and `progressBlock` to notify the download progress. RMStore will consider that a transaction has finished or failed only after the content downloader delegate has successfully or unsuccessfully downloaded its content.

##Transaction persistence

RMStore delegates transaction persistence and provides two optional reference implementations for storing transactions in the Keychain or in `NSUserDefaults`. You can implement your transaction, use the reference implementations provided by the library or, in the case of non-consumables and auto-renewable subscriptions, get the transactions directly from the receipt.

For more info, check out the [wiki](https://github.com/robotmedia/RMStore/wiki/Transaction-persistence).


##Requirements

RMStore requires iOS 5.0 or above and ARC.

##Roadmap

RMStore is in initial development and its public API should not be considered stable. Future enhancements will include:

* [Better OS X support](https://github.com/robotmedia/RMStore/issues/4)

##License

 Copyright 2013-2014 [Robot Media SL](http://www.robotmedia.net)

 Licensed under the Apache License, Version 2.0 (the ""License"");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at

 http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an ""AS IS"" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License."
vanhoefm/krackattacks-scripts,17604,2817,225,733,User,False,13280,1,0,212,False,,,0,8,0,25,46,2,1,0,6,0,0,4492,1,1,79,27,0,0,54,,906,,
ms-iot/samples,295111,1210,298,1442,Organization,False,1657,7,4,58,False,Windows 10 IoT Core Samples,,0,7,0,23,128,0,0,5,390,0,0,1874,0,0,0,0,0,0,191,5,,,"Windows 10 IoT Core Samples
==============

##Welcome to the Windows 10 IoT Core Samples

Please download, build, deploy, and contribute!!  For more information and descriptions about the samples found here, see the samples tab [here](http://ms-iot.github.io/content/en-US/win10/StartCoding.htm)

For more information about Windows 10 IoT Core, see our online documentation [here](http://windowsondevices.com)

We are working hard to improve Windows 10 IoT Core and deeply value any feedback we get."
P-H-C/phc-winner-argon2,5865,3157,123,257,Organization,False,640,4,6,61,False,"The password hash Argon2, winner of PHC",,0,7,0,30,86,1,2,8,162,2,1,1713,1,1,1,1,0,0,2,3,,,"# Argon2

[![Build Status](https://travis-ci.org/P-H-C/phc-winner-argon2.svg?branch=master)](https://travis-ci.org/P-H-C/phc-winner-argon2)
[![Build status](https://ci.appveyor.com/api/projects/status/8nfwuwq55sgfkele?svg=true)](https://ci.appveyor.com/project/P-H-C/phc-winner-argon2)
[![codecov.io](https://codecov.io/github/P-H-C/phc-winner-argon2/coverage.svg?branch=master)](https://codecov.io/github/P-H-C/phc-winner-argon2?branch=master)

This is the reference C implementation of Argon2, the password-hashing
function that won the [Password Hashing Competition
(PHC)](https://password-hashing.net).

Argon2 is a password-hashing function that summarizes the state of the
art in the design of memory-hard functions and can be used to hash
passwords for credential storage, key derivation, or other applications.

It has a simple design aimed at the highest memory filling rate and
effective use of multiple computing units, while still providing defense
against tradeoff attacks (by exploiting the cache and memory organization
of the recent processors).

Argon2 has three variants: Argon2i, Argon2d, and Argon2id. Argon2d is faster
and uses data-depending memory access, which makes it highly resistant
against GPU cracking attacks and suitable for applications with no threats
from side-channel timing attacks (eg. cryptocurrencies). Argon2i instead
uses data-independent memory access, which is preferred for password
hashing and password-based key derivation, but it is slower as it makes
more passes over the memory to protect from tradeoff attacks. Argon2id is a
hybrid of Argon2i and Argon2d, using a combination of data-depending and
data-independent memory accesses, which gives some of Argon2i's resistance to
side-channel cache timing attacks and much of Argon2d's resistance to GPU
cracking attacks.

Argon2i, Argon2d, and Argon2id are parametrized by:

* A **time** cost, which defines the amount of computation realized and
  therefore the execution time, given in number of iterations
* A **memory** cost, which defines the memory usage, given in kibibytes
* A **parallelism** degree, which defines the number of parallel threads

The [Argon2 document](argon2-specs.pdf) gives detailed specs and design
rationale.

Please report bugs as issues on this repository.

## Usage

`make` builds the executable `argon2`, the static library `libargon2.a`,
and the shared library `libargon2.so` (or `libargon2.dylib` on OSX).
Make sure to run `make test` to verify that your build produces valid
results. `make install PREFIX=/usr` installs it to your system.

### Command-line utility

`argon2` is a command-line utility to test specific Argon2 instances
on your system. To show usage instructions, run
`./argon2 -h` as
```
Usage:  ./argon2 [-h] salt [-i|-d|-id] [-t iterations] [-m memory] [-p parallelism] [-l hash length] [-e|-r] [-v (10|13)]
        Password is read from stdin
Parameters:
        salt            The salt to use, at least 8 characters
        -i              Use Argon2i (this is the default)
        -d              Use Argon2d instead of Argon2i
        -id             Use Argon2id instead of Argon2i
        -t N            Sets the number of iterations to N (default = 3)
        -m N            Sets the memory usage of 2^N KiB (default 12)
        -p N            Sets parallelism to N threads (default 1)
        -l N            Sets hash output length to N bytes (default 32)
        -e              Output only encoded hash
        -r              Output only the raw bytes of the hash
        -v (10|13)      Argon2 version (defaults to the most recent version, currently 13)
        -h              Print argon2 usage
```
For example, to hash ""password"" using ""somesalt"" as a salt and doing 2
iterations, consuming 64 MiB, using four parallel threads and an output hash
of 24 bytes
```
$ echo -n ""password"" | ./argon2 somesalt -t 2 -m 16 -p 4 -l 24
Type:           Argon2i
Iterations:     2
Memory:         65536 KiB
Parallelism:    4
Hash:           45d7ac72e76f242b20b77b9bf9bf9d5915894e669a24e6c6
Encoded:        $argon2i$v=19$m=65536,t=2,p=4$c29tZXNhbHQ$RdescudvJCsgt3ub+b+dWRWJTmaaJObG
0.188 seconds
Verification ok
```

### Library

`libargon2` provides an API to both low-level and high-level functions
for using Argon2.

The example program below hashes the string ""password"" with Argon2i
using the high-level API and then using the low-level API. While the
high-level API takes the three cost parameters (time, memory, and
parallelism), the password input buffer, the salt input buffer, and the
output buffers, the low-level API takes in these and additional parameters
, as defined in [`include/argon2.h`](include/argon2.h).

There are many additional parameters, but we will highlight three of them here.

1. The `secret` parameter, which is used for [keyed hashing](
   https://en.wikipedia.org/wiki/Hash-based_message_authentication_code).
   This allows a secret key to be input at hashing time (from some external
   location) and be folded into the value of the hash. This means that even if
   your salts and hashes are compromized, an attacker cannot brute-force to find
   the password without the key.

2. The `ad` parameter, which is used to fold any additional data into the hash
   value. Functionally, this behaves almost exactly like the `secret` or `salt`
   parameters; the `ad` parameter is folding into the value of the hash.
   However, this parameter is used for different data. The `salt` should be a
   random string stored alongside your password. The `secret` should be a random
   key only usable at hashing time. The `ad` is for any other data.

3. The `flags` parameter, which determines which memory should be securely
   erased. This is useful if you want to securly delete the `pwd` or `secret`
   fields right after they are used. To do this set `flags` to either
   `ARGON2_FLAG_CLEAR_PASSWORD` or `ARGON2_FLAG_CLEAR_SECRET`. To change how
   internal memory is cleared, change the global flag
   `FLAG_clear_internal_memory` (defaults to clearing internal memory).

Here the time cost `t_cost` is set to 2 iterations, the
memory cost `m_cost` is set to 2<sup>16</sup> kibibytes (64 mebibytes),
and parallelism is set to 1 (single-thread).

Compile for example as `gcc test.c libargon2.a -Isrc -o test`, if the program
below is named `test.c` and placed in the project's root directory.

```c
#include ""argon2.h""
#include <stdio.h>
#include <string.h>
#include <stdlib.h>

#define HASHLEN 32
#define SALTLEN 16
#define PWD ""password""

int main(void)
{
    uint8_t hash1[HASHLEN];
    uint8_t hash2[HASHLEN];

    uint8_t salt[SALTLEN];
    memset( salt, 0x00, SALTLEN );

    uint8_t *pwd = (uint8_t *)strdup(PWD);
    uint32_t pwdlen = strlen((char *)pwd);

    uint32_t t_cost = 2;            // 1-pass computation
    uint32_t m_cost = (1<<16);      // 64 mebibytes memory usage
    uint32_t parallelism = 1;       // number of threads and lanes

    // high-level API
    argon2i_hash_raw(t_cost, m_cost, parallelism, pwd, pwdlen, salt, SALTLEN, hash1, HASHLEN);

    // low-level API
    argon2_context context = {
        hash2,  /* output array, at least HASHLEN in size */
        HASHLEN, /* digest length */
        pwd, /* password array */
        pwdlen, /* password length */
        salt,  /* salt array */
        SALTLEN, /* salt length */
        NULL, 0, /* optional secret data */
        NULL, 0, /* optional associated data */
        t_cost, m_cost, parallelism, parallelism,
        ARGON2_VERSION_13, /* algorithm version */
        NULL, NULL, /* custom memory allocation / deallocation functions */
        /* by default only internal memory is cleared (pwd is not wiped) */
        ARGON2_DEFAULT_FLAGS
    };

    int rc = argon2i_ctx( &context );
    if(ARGON2_OK != rc) {
        printf(""Error: %s\n"", argon2_error_message(rc));
        exit(1);
    }
    free(pwd);

    for( int i=0; i<HASHLEN; ++i ) printf( ""%02x"", hash1[i] ); printf( ""\n"" );
    if (memcmp(hash1, hash2, HASHLEN)) {
        for( int i=0; i<HASHLEN; ++i ) {
            printf( ""%02x"", hash2[i] );
        }
        printf(""\nfail\n"");
    }
    else printf(""ok\n"");
    return 0;
}
```

To use Argon2d instead of Argon2i call `argon2d_hash_raw` instead of
`argon2i_hash_raw` using the high-level API, and `argon2d` instead of
`argon2i` using the low-level API. Similarly for Argon2id, call `argon2id_hash_raw`
and `argon2id`.

To produce the crypt-like encoding rather than the raw hash, call
`argon2i_hash_encoded` for Argon2i, `argon2d_hash_encoded` for Argon2d, and
`argon2id_hash_encoded` for Argon2id

See [`include/argon2.h`](include/argon2.h) for API details.

*Note: in this example the salt is set to the all-`0x00` string for the
sake of simplicity, but in your application you should use a random salt.*


### Benchmarks

`make bench` creates the executable `bench`, which measures the execution
time of various Argon2 instances:

```
$ ./bench
Argon2d 1 iterations  1 MiB 1 threads:  5.91 cpb 5.91 Mcycles
Argon2i 1 iterations  1 MiB 1 threads:  4.64 cpb 4.64 Mcycles
0.0041 seconds

Argon2d 1 iterations  1 MiB 2 threads:  2.76 cpb 2.76 Mcycles
Argon2i 1 iterations  1 MiB 2 threads:  2.87 cpb 2.87 Mcycles
0.0038 seconds

Argon2d 1 iterations  1 MiB 4 threads:  3.25 cpb 3.25 Mcycles
Argon2i 1 iterations  1 MiB 4 threads:  3.57 cpb 3.57 Mcycles
0.0048 seconds

(...)

Argon2d 1 iterations  4096 MiB 2 threads:  2.15 cpb 8788.08 Mcycles
Argon2i 1 iterations  4096 MiB 2 threads:  2.15 cpb 8821.59 Mcycles
13.0112 seconds

Argon2d 1 iterations  4096 MiB 4 threads:  1.79 cpb 7343.72 Mcycles
Argon2i 1 iterations  4096 MiB 4 threads:  2.72 cpb 11124.86 Mcycles
19.3974 seconds

(...)
```

## Bindings

Bindings are available for the following languages (make sure to read
their documentation):

* [Android (Java/Kotlin)](https://github.com/lambdapioneer/argon2kt) by [@lambdapioneer](https://github.com/lambdapioneer)
* [Elixir](https://github.com/riverrun/argon2_elixir) by [@riverrun](https://github.com/riverrun)
* [Erlang](https://github.com/ergenius/eargon2) by [@ergenius](https://github.com/ergenius)
* [Go](https://github.com/tvdburgt/go-argon2) by [@tvdburgt](https://github.com/tvdburgt)
* [Haskell](https://hackage.haskell.org/package/argon2) by [@hvr](https://github.com/hvr)
* [JavaScript (native)](https://github.com/ranisalt/node-argon2), by [@ranisalt](https://github.com/ranisalt)
* [JavaScript (native)](https://github.com/jdconley/argon2themax), by [@jdconley](https://github.com/jdconley)
* [JavaScript (ffi)](https://github.com/cjlarose/argon2-ffi), by [@cjlarose](https://github.com/cjlarose)
* [JavaScript (browser)](https://github.com/antelle/argon2-browser), by [@antelle](https://github.com/antelle)
* [JVM](https://github.com/phxql/argon2-jvm) by [@phXql](https://github.com/phxql)
* [JVM (with keyed hashing)](https://github.com/kosprov/jargon2-api) by [@kosprov](https://github.com/kosprov)
* [Lua (native)](https://github.com/thibaultCha/lua-argon2) by [@thibaultCha](https://github.com/thibaultCha)
* [Lua (ffi)](https://github.com/thibaultCha/lua-argon2-ffi) by [@thibaultCha](https://github.com/thibaultCha)
* [OCaml](https://github.com/Khady/ocaml-argon2) by [@Khady](https://github.com/Khady)
* [Python (native)](https://pypi.python.org/pypi/argon2), by [@flamewow](https://github.com/flamewow)
* [Python (ffi)](https://pypi.python.org/pypi/argon2_cffi), by [@hynek](https://github.com/hynek)
* [Python (ffi, with keyed hashing)](https://github.com/thusoy/porridge), by [@thusoy](https://github.com/thusoy)
* [R](https://cran.r-project.org/package=argon2) by [@wrathematics](https://github.com/wrathematics)
* [Ruby](https://github.com/technion/ruby-argon2) by [@technion](https://github.com/technion)
* [Rust](https://github.com/quininer/argon2-rs) by [@quininer](https://github.com/quininer)
* [Rust](https://docs.rs/argonautica/) by [@bcmyers](https://github.com/bcmyers/)
* [C#/.NET CoreCLR](https://github.com/kmaragon/Konscious.Security.Cryptography) by [@kmaragon](https://github.com/kmaragon)
* [Perl](https://github.com/Leont/crypt-argon2) by [@leont](https://github.com/Leont)
* [mruby](https://github.com/Asmod4n/mruby-argon2) by [@Asmod4n](https://github.com/Asmod4n)
* [Swift](https://github.com/ImKcat/CatCrypto) by [@ImKcat](https://github.com/ImKcat)


## Test suite

There are two sets of test suites. One is a low level test for the hash
function, the other tests the higher level API. Both of these are built and
executed by running:

`make test`

## Intellectual property

Except for the components listed below, the Argon2 code in this
repository is copyright (c) 2015 Daniel Dinu, Dmitry Khovratovich (main
authors), Jean-Philippe Aumasson and Samuel Neves, and dual licensed under the
[CC0 License](https://creativecommons.org/about/cc0) and the
[Apache 2.0 License](http://www.apache.org/licenses/LICENSE-2.0). For more info
see the LICENSE file.

The string encoding routines in [`src/encoding.c`](src/encoding.c) are
copyright (c) 2015 Thomas Pornin, and under
[CC0 License](https://creativecommons.org/about/cc0).

The BLAKE2 code in [`src/blake2/`](src/blake2) is copyright (c) Samuel
Neves, 2013-2015, and under
[CC0 License](https://creativecommons.org/about/cc0).

All licenses are therefore GPL-compatible."
msysgit/msysgit,225005,1866,226,634,Organization,False,1245,27,71,58,False,msysGit has been superseded by Git for Windows 2.x,https://git-for-windows.github.io/,0,9,0,0,209,0,0,0,123,0,0,4695,0,0,0,0,0,0,5,9,,,"# Please note!

Git for Windows 1.x was retired on August 18th, 2015, superseded by [Git for Windows 2.x](https://git-for-windows.github.io/). The development environment of Git for Windows 2.x is no longer maintained in a monolithic Git repository but rather as the [Git SDK](https://git-for-windows.github.io/#download-sdk), a friendly fork of [MSys2](https://msys2.github.io/) pre-configured to ease the development of Git for Windows.

# Build environment for Git for Windows 1.x

This is the build environment -- also known as msysGit -- for [Git for Windows](http://msysgit.github.io/).

The easiest way is to install it via the [net installer](https://github.com/msysgit/msysgit/releases). This installer will clone our [two](http://github.com/msysgit/msysgit) [repositories](http://github.com/msysgit/git), including all the necessary components to build Git for Windows, and perform an initial build.

# The build environment

msysGit brings a few components that are required to build Git:

- Bash, a Unix-type command-line shell. Quite a few components of Git itself are still shell scripts. Therefore, Bash is required to execute Git commands (see the output of `cd /git && git ls-files \*.sh` for a full list).
- the GNU C Compiler. Since we try to rely only on free software (apart from the Operating System, of course), we think it makes more sense to rely on GCC than on Visual Studio express. Also, it makes the maintenance burden lighter, as [upstream Git](http://github.com/gitster/git) also targets mainly GCC.
- GNU Make.
- Perl. Still required for a couple of Git components (see the output of `cd /git && git ls-files \*.perl`), most notably `git svn`.
- Tcl/Tk, a scripting language making it easy to implement cross-platform graphical user interfaces. We need this for `gitk` and `git gui`.
- [cURL](http://curl.haxx.se), a library implementing HTTP and FTP transport.
- many more libraries.
- some Unix programs required by the shell scripts in Git.

# The relationship between _msysGit_ and _Git for Windows_

[Git for Windows](https://github.com/msysgit/msysgit/releases) is the software package that installs a minimal environment to run Git on Windows. It comes with a Bash (a Unix-type shell), with a Perl interpreter and with the Git executable and its dependencies.

On the other hand, msysGit is the software package installing the _build environment_ that can build Git for Windows.  The easiest way is to install it via the [net installer](https://github.com/msysgit/msysgit/releases).

# The difference between MSys and MinGW

The [MinGW project](http://mingw.org/)'s goal is to provide a way to compile native Windows binaries with no POSIX layer using the GNU C Compiler.

However, at least the Bash needs a POSIX layer (most notably due to the absence of the `fork()` call on Windows). Therefore, MSys (the _minimal system_) is thrown in, offering the minimal system necessary to offer Bash (and Perl) functionality on Windows.

Consequently, MSys ships with a POSIX layer (based on an old version of Cygwin) that is only used by the Bash and Perl, but not by anything compiled within that environment.

# Further information

For more information and documentation, please have a look and enhance our [Wiki](https://github.com/msysgit/msysgit/wiki).

For code contributions and discussions, please see our [mailing list](http://groups.google.com/group/msysgit)."
universal-ctags/ctags,15215,3715,103,378,Organization,False,7838,18,0,126,False,A maintained ctags implementation,https://ctags.io,8,17,6,136,663,21,45,33,1732,7,122,5017,8,385,37112,253606,0,0,11,8,,,"# Universal Ctags

[![Build Status](https://travis-ci.org/universal-ctags/ctags.svg?branch=master)](https://travis-ci.org/universal-ctags/ctags)
[![Coverity Scan Build Status](https://scan.coverity.com/projects/4355/badge.svg)](https://scan.coverity.com/projects/4355)
[![Coverage Status](https://coveralls.io/repos/universal-ctags/ctags/badge.svg?branch=master&service=github)](https://coveralls.io/github/universal-ctags/ctags?branch=master)
[![Build status](https://ci.appveyor.com/api/projects/status/6hk2p5lv6jsrd9o7/branch/master?svg=true)](https://ci.appveyor.com/project/universalctags/ctags/branch/master)
[![RTD build status](https://readthedocs.org/projects/ctags/badge)](https://docs.ctags.io)
[![CircleCI Build Status](https://circleci.com/gh/universal-ctags/ctags.svg?style=shield&circle-token=2e582261da84ebc6d21725b05381f410bc5de29d)](https://circleci.com/gh/universal-ctags)

Universal Ctags generates an index (or tag) file of language objects found in source files for many popular programming languages. This index makes it easy for text editors and other tools to locate the indexed items. Universal Ctags improves on traditional ctags because of its multilanguage support, its ability for the user to define new languages searched by regular expressions, and its ability to generate emacs-style TAGS files.

universal-ctags has the objective of continuing the development from
what existed in the Sourceforge area. Github exuberant-ctags
repository was started by Reza Jelveh and was later moved to the
universal-ctags organization.

The goal of the project is preparing and maintaining common/unified working
space where people interested in making ctags better can work
together.

## Getting PACKCC compiler-compiler ##

Packcc is a compiler-compiler; it translates .peg grammar file to .c
file.  packcc was originally written by Arihiro Yoshida. Its source
repository is at sourceforge. It seems that packcc at sourceforge is
not actively maintained. Some derived repositories are at
github. Currently, our choice is
https://github.com/enechaev/packcc. It is the most active one in the
derived repositories.

The source tree of packcc is grafted at misc/packcc directory.
Building packcc and ctags are integrated in the build-scripts of
Universal-ctags.

## The latest build and package ##

If you want to try the latest universal-ctags without building it yourself...

### Windows
Daily builds are available at the [ctags-win32](https://github.com/universal-ctags/ctags-win32) project.
Go to the [releases](https://github.com/universal-ctags/ctags-win32/releases) page to download zip packages.

### Mac
See [Homebrew Tap for Universal Ctags](https://github.com/universal-ctags/homebrew-universal-ctags)

### Snap
Go to [ctags-snap](https://github.com/universal-ctags/ctags-snap) and
clone the `ctags-snap` repo. Then, follow instructions to build the
snap package of ctags. Snapcraft will automatically fetch the source
code from GitHub.

## How to build and install ##

To build with Autotools, see `docs/autotools.rst` for more information.
(To build on GNU/Linux, Autotools is your choice.)
To build on Windows, see `docs/windows.rst` for more information.
To build on OSX, see `docs/osx.rst` for more information.

## Manual ##
Man page (ctags.1) is generated only in Autotools based building process.
In addition rst2man command is needed.

rst2man is part of the python-docutils package on Ubuntu.

## Differences ##

You may be interested in how universal-ctags is different from
exuberant-ctags. The critical and attractive changes are explained
in docs/\*.rst. The preformatted version is available on line,
https://docs.ctags.io/.

The most significant incompatible changes:

* Universal-ctags doesn't load
`~/.ctags` and `./.ctags` at starting up time. Instead, it loads
`~/.ctags.d/*.ctags` and `./.ctags.d/*.ctags`. See the above web
site and man pages
(man/ctags.1.rst.in and man/ctags-incompatibilities.7.in in the
source tree).

* Universal-ctags is more strict about characters that can be
  used in kind letters and kind names than Exuberant-ctags.

  - The letter must be an alphabetical character (`[a-zA-EG-Z]`).
    `F` is reserved for `file` kind.

  - The first character of the name must be alphabetic, and
    the rest characters must be alphanumeric (`[a-zA-Z][a-zA-Z0-9]*`).

  See the web site and man pages. The detailed background is explained
  in [#1737](https://github.com/universal-ctags/ctags/pull/1737).

  If you want to reuse your .ctags written for Exuberant-ctags,
  you must review kind letters and names defined with `--regex-<LANG>=...`
  options. When updating the definitions, using `--kind-<LANG>` option
  is appreciated.

Pull-requests are welcome!"
fontforge/fontforge,96368,3163,153,487,Organization,False,19266,27,27,139,False,"Free (libre) font editor for Windows, Mac OS X and GNU+Linux",http://fontforge.github.io/,0,44,3,827,1613,80,103,10,1921,9,108,5948,13,114,122449,164423,0,0,18,19,,,"# FontForge [![Build Status](https://travis-ci.org/fontforge/fontforge.svg?branch=master)](https://travis-ci.org/fontforge/fontforge) [![Build status](https://ci.appveyor.com/api/projects/status/y5x0fd1xj23n9l2o?svg=true)](https://ci.appveyor.com/project/fontforge/fontforge) [![Coverity Scan Build Status](https://scan.coverity.com/projects/792/badge.svg?flat=1)](https://scan.coverity.com/projects/792)

![FontForge Logo](http://fontforge.github.io/assets/img/logo-transparent.png)

FontForge is a free (libre) font editor for Windows, Mac OS X and GNU+Linux. 
Use it to create, edit and convert fonts in OpenType, TrueType, UFO, CID-keyed, Multiple Master, and many other formats.

[fontforge.org](http://fontforge.org) &mdash; homepage

[designwithfontforge.com](http://designwithfontforge.com) &mdash; font creation manual

# Getting help

The bug tracker is for _reporting bugs_, not for asking questions. Please direct questions to one of the following:

* [Mailing list](https://sourceforge.net/p/fontforge/mailman/fontforge-users/)
* [Live chat](https://webchat.freenode.net/?channel=#fontforge) &mdash; #fontforge on [Freenode](https://freenode.net/)

# Installation & contributing

[`INSTALL.md`](INSTALL.md) &mdash; developer instructions to build from source

[`.travis.yml`](.travis.yml) &mdash; a list of installation dependencies

[`CONTRIBUTING.md`](CONTRIBUTING.md) &mdash; contributing guidelines"
madler/zlib,3753,2461,161,1378,User,False,419,2,72,17,False,A massively spiffy yet delicately unobtrusive compression library.,http://zlib.net/,0,0,0,96,122,21,6,147,137,16,2,3631,0,0,0,0,0,0,8,,350,,
alibaba/tsar,2457,2138,280,699,Organization,False,404,2,0,22,False,Taobao System Activity Reporter,,0,6,2,10,47,2,1,0,47,0,3,2707,3,3,115,92,0,0,318,151,,,"Introduction
------------
Tsar (Taobao System Activity Reporter) is a monitoring tool, which can be used to gather and summarize system information, e.g. CPU, load, IO, and application information, e.g. nginx, HAProxy, Squid, etc. The results can be stored at local disk or sent to Nagios.

Tsar can be easily extended by writing modules, which makes it a powerful and versatile reporting tool.

Module introduction: [info](https://github.com/alibaba/tsar/blob/master/info.md)

Installation
-------------
Tsar is available on GitHub, you can clone and install it as follows:

    $ git clone https://github.com/alibaba/tsar.git
    $ cd tsar
    $ make
    # make install

Or you can download the zip file and install it:

    $ wget -O tsar.zip https://github.com/alibaba/tsar/archive/master.zip --no-check-certificate
    $ unzip tsar.zip
    $ cd tsar
    $ make
    # make install

After installation, you may see these files:

* `/etc/tsar/tsar.conf`, which is tsar's main configuration file;
* `/etc/cron.d/tsar`, is used to run tsar to collect information every minute;
* `/etc/logrotate.d/tsar` will rotate tsar's log files every month;
* `/usr/local/tsar/modules` is the directory where all module libraries (*.so) are located;

Configuration
-------------
There is no output displayed after installation by default. Just run `tsar -l` to see if the real-time monitoring works, for instance:

    [kongjian@tsar]$ tsar -l -i 1
    Time              ---cpu-- ---mem-- ---tcp-- -----traffic---- --xvda-- -xvda1-- -xvda2-- -xvda3-- -xvda4-- -xvda5--  ---load-
    Time                util     util   retran    pktin  pktout     util     util     util     util     util     util     load1
    11/04/13-14:09:10   0.20    11.57     0.00     9.00    2.00     0.00     0.00     0.00     0.00     0.00     0.00      0.00
    11/04/13-14:09:11   0.20    11.57     0.00     4.00    2.00     0.00     0.00     0.00     0.00     0.00     0.00      0.00

Usually, we configure Tsar by simply editing `/etc/tsar/tsar.conf`:

* To add a module, add a line like `mod_<yourmodname> on`
* To enable or disable a module, use `mod_<yourmodname> on/off`
* To specify parameters for a module, use `mod_<yourmodname> on parameter`
* `output_stdio_mod` is to set modules output to standard I/O
* `output_file_path` is to set history data file, (you should modify the logrotate script `/etc/logrotate.d/tsar` too)
* `output_interface` specifies tsar data output destination, which by default is a local file. See the Advanced section for more information.

Usage
------
* null          :see default mods history data, `tsar`
* --modname     :specify module to show, `tsar --cpu`
* -L/--list     :list available module, `tsar -L`
* -l/--live     :show real-time info, `tsar -l --cpu`
* -i/--interval :set interval for report, `tsar -i 1 --cpu`
* -s/--spec     :specify module detail field, `tsar --cpu -s sys,util`
* -D/--detail   :do not conver data to K/M/G, `tsar --mem -D`
* -m/--merge    :merge multiply item to one, `tsar --io -m`
* -I/--item     :show spec item data, `tsar --io -I sda`
* -d/--date     :specify data, YYYYMMDD, or n means n days ago
* -C/--check    :show the last collect data
* -h/--help     :show help, `tsar -h`

Advanced
--------
* Output to Nagios

To turn it on, just set output type `output_interface file,nagios` in the main configuration file.

You should also specify Nagios' IP address, port, and sending interval, e.g.:

    ####The IP address or the hostname running the NSCA daemon
    server_addr nagios.server.com
    ####The port on which the daemon is listening - by default it is 5667
    server_port 8086
    ####The cycle (interval) of sending alerts to Nagios
    cycle_time 300

As tsar uses Nagios' passive mode, so you should specify the nsca binary and its configuration file, e.g.:

    ####nsca client program
    send_nsca_cmd /usr/bin/send_nsca
    send_nsca_conf /home/a/conf/amon/send_nsca.conf

Then specify the module and fields to be checked. There are 4 threshold levels.

    ####tsar mod alert config file
    ####threshold servicename.key;w-min;w-max;c-min;cmax;
    threshold cpu.util;50;60;70;80;

* Output to MySQL

To use this feature, just add output type `output_interface file,db` in tsar's configuration file.

Then specify which module(s) will be enabled:

    output_db_mod mod_cpu,mod_mem,mod_traffic,mod_load,mod_tcp,mod_udpmod_io

Note that you should set the IP address (or hostname) and port where tsar2db listens, e.g.:

    output_db_addr console2:56677

Tsar2db receives sql data and flush it to MySQL. You can find more information about tsar2db at https://github.com/alibaba/tsar2db.


Module development
------------------
Tsar is easily extended. Whenever you want information that is not collected by tsar yet, you can write a module with `C` or `Lua`.

C Module
--------
First, install the tsardevel tool (`make tsardevel` will do this for you):

Then run `tsardevel <yourmodname>`, and you will get a directory named yourmodname, e.g.:

````bash
[kongjian@tsar]$ tsardevel test
build:make
install:make install
uninstall:make uninstall

[kongjian@tsar]$ ls test
Makefile  mod_test.c  mod_test.conf
````

You can modify the read_test_stats() and set_test_record() functions in mod_test.c as you need.
Then run `make;make install` to install your module and run `tsar --yourmodname` to see the output.

Lua Module
----------
First, install the tsarluadevel tool (`make tsarluadevel` will do this for you):

Then run `tsarluadevel <yourmodname>`, and you will get a directory named yourmodname, e.g.:

````bash
[kongjian@tsar]$ tsarluadevel test
install:make install
uninstall:make uninstall
test:tsar --list or tsar --lua_test --live -i 1

[kongjian@tsar]$ ls test
Makefile  mod_lua_test.conf  mod_lua_test.lua
````

You can modify the register()、read() and set() functions in mod_lua_test.lua as you need.
Then run `make install` to install your module and run `tsar --lua_yourmodname` to see the output.

More
----
Homepage http://tsar.taobao.org

Any question, please feel free to contact me by kongjian@taobao.com"
ohler55/oj,3802,2574,36,181,User,False,1247,5,186,73,False,Optimized JSON,http://www.ohler.com/oj,8,0,0,13,434,7,16,1,154,1,6,3015,4,21,249,133,10841,844,30,,151,,"# [![{}j](http://www.ohler.com/dev/images/oj_comet_64.svg)](http://www.ohler.com/oj) gem

[![Build Status](https://img.shields.io/travis/ohler55/oj/master.svg?logo=travis)](http://travis-ci.org/ohler55/oj?branch=master) [![AppVeyor](https://img.shields.io/appveyor/ci/ohler55/oj/master.svg?logo=appveyor)](https://ci.appveyor.com/project/ohler55/oj) ![Gem](https://img.shields.io/gem/v/oj.svg) ![Gem](https://img.shields.io/gem/dt/oj.svg) [![SemVer compatibility](https://api.dependabot.com/badges/compatibility_score?dependency-name=oj&package-manager=bundler&version-scheme=semver)](https://dependabot.com/compatibility-score.html?dependency-name=oj&package-manager=bundler&version-scheme=semver) [![TideLift](https://tidelift.com/badges/github/ohler55/oj)](https://tidelift.com/subscription/pkg/rubygems-oj?utm_source=rubygems-oj&utm_medium=referral&utm_campaign=readme)

A *fast* JSON parser and Object marshaller as a Ruby gem.

Version 3.0 is out! 3.0 provides better json gem and Rails compatibility. It
also provides additional optimization options.

## Using

```ruby
require 'oj'

h = { 'one' => 1, 'array' => [ true, false ] }
json = Oj.dump(h)

# json =
# {
#   ""one"":1,
#   ""array"":[
#     true,
#     false
#   ]
# }

h2 = Oj.load(json)
puts ""Same? #{h == h2}""
# true
```

## Installation
```
gem install oj
```

or in Bundler:

```
gem 'oj'
```

## Support

[Get supported Oj with a Tidelift Subscription.](https://tidelift.com/subscription/pkg/rubygems-oj?utm_source=rubygems-oj&utm_medium=referral&utm_campaign=readme) Security updates are [supported](https://tidelift.com/security).

## Further Reading

For more details on options, modes, advanced features, and more follow these
links.

 - [{file:Options.md}](pages/Options.md) for parse and dump options.
 - [{file:Modes.md}](pages/Modes.md) for details on modes for strict JSON compliance, mimicing the JSON gem, and mimicing Rails and ActiveSupport behavior.
 - [{file:JsonGem.md}](pages/JsonGem.md) includes more details on json gem compatibility and use.
 - [{file:Rails.md}](pages/Rails.md) includes more details on Rails and ActiveSupport compatibility and use.
 - [{file:Custom.md}](pages/Custom.md) includes more details on Custom mode.
 - [{file:Encoding.md}](pages/Encoding.md) describes the :object encoding format.
 - [{file:Compatibility.md}](pages/Compatibility.md) lists current compatibility with Rubys and Rails.
 - [{file:Advanced.md}](pages/Advanced.md) for fast parser and marshalling features.
 - [{file:Security.md}](pages/Security.md) for security considerations.

## Releases

See [{file:CHANGELOG.md}](CHANGELOG.md)

## Links

- *Documentation*: http://www.ohler.com/oj/doc, http://rubydoc.info/gems/oj

- *GitHub* *repo*: https://github.com/ohler55/oj

- *RubyGems* *repo*: https://rubygems.org/gems/oj

Follow [@peterohler on Twitter](http://twitter.com/peterohler) for announcements and news about the Oj gem.

#### Performance Comparisons

 - [Oj Strict Mode Performance](http://www.ohler.com/dev/oj_misc/performance_strict.html) compares Oj strict mode parser performance to other JSON parsers.

 - [Oj Compat Mode Performance](http://www.ohler.com/dev/oj_misc/performance_compat.html) compares Oj compat mode parser performance to other JSON parsers.

 - [Oj Object Mode Performance](http://www.ohler.com/dev/oj_misc/performance_object.html) compares Oj object mode parser performance to other marshallers.

 - [Oj Callback Performance](http://www.ohler.com/dev/oj_misc/performance_callback.html) compares Oj callback parser performance to other JSON parsers.

#### Links of Interest

 - *Fast XML parser and marshaller on RubyGems*: https://rubygems.org/gems/ox

 - *Fast XML parser and marshaller on GitHub*: https://github.com/ohler55/ox

 - [Need for Speed](http://www.ohler.com/dev/need_for_speed/need_for_speed.html) for an overview of how Oj::Doc was designed.

 - *OjC, a C JSON parser*: https://www.ohler.com/ojc also at https://github.com/ohler55/ojc

 - *Agoo, a high performance Ruby web server supporting GraphQL on GitHub*: https://github.com/ohler55/agoo

 - *Agoo-C, a high performance C web server supporting GraphQL on GitHub*: https://github.com/ohler55/agoo-c

#### Contributing

+ Provide a Pull Request off the `develop` branch.
+ Report a bug
+ Suggest an idea"
lloyd/yajl,2095,1945,94,390,User,False,275,9,20,20,False,A fast streaming JSON parsing library in C.,http://lloyd.github.com/yajl,0,0,0,72,43,2,0,76,37,2,0,4198,0,0,0,0,0,0,120,,568,,
IoLanguage/io,26133,2171,145,268,Organization,False,2633,1,10,121,False,"Io programming language. Inspired by Self, Smalltalk and LISP.",http://iolanguage.org,0,0,0,54,189,0,0,3,184,0,0,4835,1,1,41,0,0,0,82,1,,,"# The Io Language

_Note: This document is intended to be used as a reference for setting up and configuring Io. For a guide on how to use the language itself, please visit the website at <http://iolanguage.org/guide/guide.html>._

# Table of Contents

* [Table of Contents](#table-of-contents)
* [What is Io?](#what-is-io)
 * [Example Code](#example-code)
 * [Quick Links](#quick-links)
* [Installing](#installing)
 * [From a Package Manager](#from-a-package-manager)
 * [From Source](#from-source)
  * [Linux Build Instructions](#linux-build-instructions)
   * [Note About Building Eerie](#note-about-building-eerie)
  * [OS X Build Instructions](#os-x-build-instructions)
  * [Windows Build Instructions](#windows-build-instructions)
   * [Building with MSVC](#building-with-msvc)
   * [Building with MinGW](#building-with-mingw)
   * [Building with MinGW-W64](#building-with-mingw-w64)
   * [Building with Cygwin](#building-with-cygwin)
* [Running Tests](#running-tests)
* [Installing Addons](#installing-addons)

What is Io?
=====

Io is a dynamic prototype-based programming language in the same realm as Smalltalk and Self. It revolves around the idea of message passing from object to object.

For further information, the programming guide and reference manual can be found in the docs folder.


Example Code
---
Basic Math

```
Io> 1 + 1
==> 2

Io> 2 sqrt
==> 1.4142135623730951
```

Lists

```
Io> d := List clone append(30, 10, 5, 20)
==> list(30, 10, 5, 20)

Io> d := d sort
==> list(5, 10, 20, 30)

Io> d select (>10)
==> list(20, 30)
```

Objects

```
Io> Contact := Object clone
==>  Contact_0x7fbc3bc8a6d0:
  type = ""Contact""

Io> Contact name ::= nil
==> nil

Io> Contact address ::= nil
==> nil

Io> Contact city ::= nil
==> nil

Io> holmes := Contact clone setName(""Holmes"") setAddress(""221B Baker St"") setCity(""London"")
==>  Contact_0x7fbc3be2b470:
  address          = ""221B Baker St""
  city             = ""London""
  name             = ""Holmes""

Io> Contact fullAddress := method(list(name, address, city) join(""\n""))
==> method(
    list(name, address, city) join(""\n"")
)

Io> holmes fullAddress
==> Holmes
221B Baker St
London
```




Quick Links
---
* The Wikipedia page for Io has a good overview and shows a few interesting examples of the language: <https://en.wikipedia.org/wiki/Io_(programming_language)>.
* The entry on the c2 wiki has good discussion about the merits of the language: <http://wiki.c2.com/?IoLanguage>.


Installing
==========

From a Package Manager
---

Io is currently only packaged for OS X. To install it, open a terminal and type:

```
brew install io
```

Note that this package may not be as updated as the version from the source repository.

From Source
---

### Linux Build Instructions

First, make sure that this repo and all of its submodules have been cloned to your computer by running `git clone` with the `--recursive` flag:

```
git clone --recursive https://github.com/IoLanguage/io.git
```

Io uses the [CMake build system](https://cmake.org/) and supports all of the normal flags and features provided by CMake. To prepare the project for building, run the following commands:

```
cd io/           # To get into the cloned folder
mkdir build      # To contain the CMake data
cd build/
cmake ..         # This populates the build folder with a Makefile and all of the related things necessary to begin building
```

In a production environment, pass the flag `-DCMAKE_BUILD_TYPE=release` to the `cmake` command to ensure that the C compiler does the proper optimizations. Without this flag, Io is built in debug mode without standard C optimizations.

To install to a different folder than `/usr/local/bin/`, pass the flag `-DCMAKE_INSTALL_PREFIX=/path/to/your/folder/` to the `cmake` command.

To build without Eerie, the Io package manager, pass the flag `-DWITHOUT_EERIE=1` to the `cmake` command.

Once CMake has finished preparing the build environment, ensure you are inside the build folder, and run:

```
make
sudo make install
```

This should build and install the Io language and Eerie, the Io package manager. Io can then be run with the `io` command and Eerie can be run with the `eerie` command.

#### Note About Building Eerie

Running `eerie` after installing with `sudo make install` may shoot back an error such as this one:

```
Exception: unable to open file path '/home/<user>/.eerie/config.json': Permission denied
---------
openForUpdating                     Eerie.io 77
Object Eerie                         eerie 3
CLI doFile                           Z_CLI.io 140
CLI run                              IoState_runCLI() 1
```

If this occurs, this is because the `~/.eerie/` folder isn't accessible due to your user permissions. To fix this, go to your home folder and run:

```
sudo chown -R <your username>:<your username> .eerie/
```


### OS X Build Instructions

See the [Linux build instructions](#linux-build-instructions).

### Windows Build Instructions

For all the different methods explained here, some of the addons won't compile as they depend on libraries not provided by Io.

For methods A and B you must download and install CMake (at least v2.8) from here: <http://www.cmake.org/cmake/resources/software.html>

For method C you must install the CMake Cygwin package (at least v2.8) using the Cygwin package installer.

For the `make install` command, if you are on Windows 7/Vista you will need to run your command prompts as Administrator: right-click on the command prompt launcher->""Run as administrator"" or something similar)

You will also need to add `<install_drive>:\<install_directory>\bin` and `<install_drive>:\<install_directory>\lib` to your `PATH` environment variable.

#### Building with MSVC

1. Install Microsoft Visual C++ 2008 Express (should work with other versions).
2. Install Microsoft Windows SDK 7.0 (or newer).
3. Install CMake (v2.8 at least)
4. Run ""Visual Studio 2008 Command Prompt"" from the ""Microsoft Visual Studio 2008"" start menu.
5. `cd` to `<install_drive>:\Microsoft SDKs\Windows\v7.0\Setup` then run: `WindowsSdkVer.exe -version:v7.0`
6. Close the command prompt window and run step 4 again
7. Ensure CMake bin path is in the `PATH` environment variable (eg: `echo %PATH%` and see that the folder is there) if not you will have to add it to your `PATH`.
8. `cd` to your Io root folder
9. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`
10. a) `cmake ..`

 or

 b) `cmake -DCMAKE_INSTALL_PREFIX=<install_drive>:\<install_directory> ..` (eg: `cmake -DCMAKE_INSTALL_PREFIX=C:\Io ..`)
11. `nmake`
12. `nmake install`


#### Building with MinGW

For automatic MinGW install: <http://sourceforge.net/projects/mingw/files/Automated%20MinGW%20Installer>

For non-automatic MinGW install and detailed instructions refer to: <http://www.mingw.org/wiki/InstallationHOWTOforMinGW>

1. `cd` to your Io root folder
2. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`
3. a) `cmake -G""MSYS Makefiles"" ..`

 or

 b) `cmake -G""MSYS Makefiles"" -DCMAKE_INSTALL_PREFIX=<install_drive>:/<install_directory> ..` (eg: `cmake -G""MSYS Makefiles"" -DCMAKE_INSTALL_PREFIX=C:/Io ..`)
4. `make`
5. `make install`

#### Building with MinGW-W64

1. `cd` to your Io root folder
2. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`
3. a) `cmake -G""MinGW Makefiles"" ..`

 or

 b) `cmake -G""MinGW Makefiles"" -DCMAKE_INSTALL_PREFIX=<install_drive>:/<install_directory> ..` (eg: `cmake -G""MinGW Makefiles"" -DCMAKE_INSTALL_PREFIX=C:/Io ..`)
4. `mingw32-make install`


#### Building with Cygwin

Install Cygwin from: <http://www.cygwin.com/>

1. `cd` to your Io root folder
2. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`
3. a) `cmake ..`

 or

 b) `cmake -DCMAKE_INSTALL_PREFIX=<install_drive>:/<install_directory> ..` (eg: `cmake -DCMAKE_INSTALL_PREFIX=C:/Io ..`)
4. `make`
5. `make install`

Note: If you also have CMake 2.8 for Windows installed (apart from CMake for Cygwin) check your `PATH` environment variable so you won't be running CMake for Windows instead of Cygwin version.


Running Tests
===

You should be inside your out-of-source build dir. The vm tests can be run with the command:

 io ../libs/iovm/tests/correctness/run.io

Installing Addons
===

Many of the common features provided by the Io language aren't prepackaged in the Io core. Instead, these features are contained in addons that get loaded when launching the Io VM. In the past, these addons were automatically installed by the build process, but now they must be installed through [Eerie](https://github.com/IoLanguage/eerie), the Io package manager.

Most of these addons are housed under the IoLanguage group on GitHub: https://github.com/IoLanguage.

To install an addon, ensure both Io and Eerie are installed correctly, then run:

```
eerie install <link to the git repository>
```

For example, to build and install the `Range` addon, run the command:

```
eerie install https://github.com/IoLanguage/Range.git
```

To ensure that an addon installed correctly, pull up an Io interpreter and type the name of the object provided by the addon. It should load dynamically and automatically into the interpreter session, populating a slot in `Lobby Protos Addons`."
raspberrypi/userland,35818,1650,256,942,Organization,False,799,3,0,85,False,Source code for ARM side libraries for interfacing to Raspberry Pi GPU.,,0,9,0,55,273,13,8,7,299,2,10,2791,8,36,1696,1358,0,0,25,1,,,"This repository contains the source code for the ARM side libraries used on Raspberry Pi.
These typically are installed in /opt/vc/lib and includes source for the ARM side code to interface to:
EGL, mmal, GLESv2, vcos, openmaxil, vchiq_arm, bcm_host, WFC, OpenVG.

Use buildme to build. It requires cmake to be installed and an ARM cross compiler. For 32-bit cross compilation it is set up to use this one:
https://github.com/raspberrypi/tools/tree/master/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbian

Whilst 64-bit userspace is not officially supported, some of the libraries will work for it. To cross compile, install gcc-aarch64-linux-gnu and g++-aarch64-linux-gnu first. For both native and cross compiles, add the option ```--aarch64``` to the buildme command.

Note that this repository does not contain the source for the edidparser and vcdbg binaries due to licensing restrictions."
irssi/irssi,7581,2176,108,291,Organization,False,6144,5,70,91,False,The client of the future,https://irssi.org,4,30,1,119,348,13,8,14,709,7,25,5780,4,30,864,397,0,0,4,14,,,"# [Irssi](https://irssi.org/)

[![Build Status](https://travis-ci.org/irssi/irssi.svg?branch=master)](https://travis-ci.org/irssi/irssi)

Irssi is a modular chat client that is most commonly known for its
text mode user interface, but 80% of the code isn't text mode
specific. Irssi comes with IRC support built in, and there are
third party [ICB](https://github.com/jperkin/irssi-icb),
[SILC](http://www.silcnet.org/),
[XMPP](http://cybione.org/~irssi-xmpp/) (Jabber),
[PSYC](http://about.psyc.eu/Irssyc) and
[Quassel](https://github.com/phhusson/quassel-irssi) protocol modules
available.

![irssi](https://user-images.githubusercontent.com/5665186/32180643-cf127f60-bd92-11e7-8aa2-882313ce1d8e.png)

## [Download information](https://irssi.org/download/)

#### Development source installation

[Ninja](https://ninja-build.org/) 1.5 and [Meson](https://mesonbuild.com/) 0.49

```
git clone https://github.com/irssi/irssi
cd irssi
meson Build
ninja -C Build && sudo ninja -C Build install
```

#### Release source installation

* Download [release](https://github.com/irssi/irssi/releases)
* [Verify](https://irssi.org/download/#release-sources) signature
```
tar xJf irssi-*.tar.xz
cd irssi-*
./configure
make && sudo make install
```

### Requirements

- [glib-2.28](https://wiki.gnome.org/Projects/GLib) or greater
- [openssl](https://www.openssl.org/)
- [perl-5.6](https://www.perl.org/) or greater (for perl support)
- terminfo or ncurses (for text frontend)

#### See the [INSTALL](INSTALL) file for details

## [Documentation](https://irssi.org/documentation/)

* [Frequently Asked Questions](https://irssi.org/documentation/faq)
* [Startup How-To](https://irssi.org/documentation/startup)
* Check the built-in `/HELP`, it has all the details on command syntax

## [Themes](https://irssi-import.github.io/themes/)

## [Scripts](https://scripts.irssi.org/)

## [Modules](https://irssi.org/modules/)

## [Security information](https://irssi.org/security/)

Please report security issues to staff@irssi.org. Thanks!

## [Bugs](https://github.com/irssi/irssi/issues) / Suggestions / [Contributing](https://irssi.org/development/)

Check the GitHub issues if it is already listed in there; if not, open
an issue on GitHub or send a mail to [staff@irssi.org](mailto:staff@irssi.org).

Irssi is always looking for developers. Feel free to submit patches through
GitHub pull requests.

You can also contact the Irssi developers in
[#irssi](https://irssi.org/support/irc/) on freenode."
libusb/libusb,4599,2588,262,1178,Organization,False,1522,1,340,144,False,A cross-platform library to access USB devices,https://libusb.info,3,20,2,90,406,15,33,10,234,7,23,,0,0,0,0,0,0,4,4,,,"# libusb

[![Build Status](https://travis-ci.org/libusb/libusb.svg?branch=master)](https://travis-ci.org/libusb/libusb)
[![Build Status](https://ci.appveyor.com/api/projects/status/xvrfam94jii4a6lw?svg=true)](https://ci.appveyor.com/project/LudovicRousseau/libusb)
[![Coverity Scan Build Status](https://scan.coverity.com/projects/2180/badge.svg)](https://scan.coverity.com/projects/libusb-libusb)

libusb is a library for USB device access from Linux, macOS,
Windows, OpenBSD/NetBSD, Haiku and Solaris userspace.
It is written in C (Haiku backend in C++) and licensed under the GNU
Lesser General Public License version 2.1 or, at your option, any later
version (see [COPYING](COPYING)).

libusb is abstracted internally in such a way that it can hopefully
be ported to other operating systems. Please see the [PORTING](PORTING)
file for more information.

libusb homepage:
http://libusb.info/

Developers will wish to consult the API documentation:
http://api.libusb.info

Use the mailing list for questions, comments, etc:
http://mailing-list.libusb.info

- Hans de Goede <hdegoede@redhat.com>
- Xiaofan Chen <xiaofanc@gmail.com>
- Ludovic Rousseau <ludovic.rousseau@gmail.com>
- Nathan Hjelm <hjelmn@cs.unm.edu>
- Chris Dickens <christopher.a.dickens@gmail.com>

(Please use the mailing list rather than mailing developers directly)"
tvheadend/tvheadend,59516,1757,256,745,Organization,False,10811,26,47,231,False,"Tvheadend is a TV streaming server for Linux supporting DVB-S, DVB-S2, DVB-C, DVB-T, ATSC, IPTV,SAT>IP and other formats through the unix pipe as input sources.",https://tvheadend.org,0,0,0,,,,,20,1325,5,18,4695,6,24,82,91,0,0,8,3,,,"Tvheadend
========================================
(c) 2006 - 2020 Tvheadend Foundation CIC

Status
------

[![Build Status](https://travis-ci.org/tvheadend/tvheadend.svg?branch=master)](https://travis-ci.org/tvheadend/tvheadend)

[![Download](https://api.bintray.com/packages/tvheadend/deb/tvheadend/images/download.svg)](https://bintray.com/tvheadend/deb/tvheadend/)

[![Coverity Scan](https://scan.coverity.com/projects/2114/badge.svg)](https://scan.coverity.com/projects/2114)

What it is
----------

Tvheadend is a TV streaming server and digital video recorder.

It supports the following inputs:

  * DVB-C(2)
  * DVB-T(2)
  * DVB-S(2)
  * ATSC
  * SAT>IP
  * HDHomeRun
  * IPTV
    * UDP
    * HTTP

It supports the following outputs:

  * HTTP
  * HTSP (own protocol)
  * SAT>IP

How to build for Linux
----------------------

First you need to configure:

 $ ./configure

If any dependencies are missing the configure script will complain or attempt
to disable optional features.

Build the binary:

 $ make

After build, the binary resides in `build.linux` directory.

Thus, to start it, just type:

 $ ./build.linux/tvheadend

Settings are stored in `$HOME/.hts/tvheadend`.

How to build for OS X
---------------------

Same build procedure applies to OS X.
After build, the binary resides in `build.darwin` directory.

Only network sources (IPTV, SAT>IP) are supported on OS X.
There is no support for DVB USB sticks and PCI cards.
Transcoding is currently not supported.

Packages
--------

Install instructions for various distributions can be found at the [Wiki](https://tvheadend.org/projects/tvheadend/wiki/Download).

Further information
-------------------

For more information about building, including generating packages, please visit:
* https://tvheadend.org/projects/tvheadend/wiki/Building
* https://tvheadend.org/projects/tvheadend/wiki/Packaging
* https://tvheadend.org/projects/tvheadend/wiki/Git
* https://tvheadend.org/projects/tvheadend/wiki/Internationalization"
Qihoo360/phptrace,1704,1620,158,403,Organization,False,502,1,7,10,False,A tracing and troubleshooting tool for PHP scripts.,,0,11,0,26,63,3,0,0,17,0,0,2021,0,0,0,0,0,0,58,7,,,"# phptrace

> We have a new trace tool **[Molten](https://github.com/chuan-yun/Molten)**,
> It's an OpenTracing supported tracer, for Distributed Tracing System.

> 我们开发了新的Trace工具 **[Molten](https://github.com/chuan-yun/Molten)**，
> 它支持OpenTracing，用于分布式追踪系统。


[![Build Status](https://travis-ci.org/Qihoo360/phptrace.svg)](https://travis-ci.org/Qihoo360/phptrace)

> Readme in [Chinese 中文](https://github.com/Qihoo360/phptrace/blob/master/README_ZH.md)

phptrace is a low-overhead tracing tool for PHP.

It can trace all PHP executing, function calls, request information during
run-time. And provides features like Filter, Statistics, Current Status and so
on.

It is very useful to locate blocking, heavy-load problems and debug in all
environments, especially in production environments.

Features:
* low-overhead, when extension loaded and trace is off
* stable, running on [Qihoo 360](http://www.360safe.com/) and tested on mainstream frameworks
* ease of use, view PHP run-time status without extension installation

Misc:
- [PECL Download](https://pecl.php.net/package/trace)

> News
> We have build another interesting project [pika](https://github.com/Qihoo360/pika).
> It's a NoSQL compatible with Redis protocol with huge storage space.


## Install from source

1. Extracting tarball
    ```
    tar -xf phptrace-{version}.tar.gz
    cd phptrace-{version}/extension
    ```

2. Build

    PHP Extension
    ```
    {php_bin_dir}/phpize
    ./configure --with-php-config={php_bin_dir}/php-config
    make
    ```

    CLI Binary
    ```
    make cli
    ```

3. Install & Configure

    Install PHP Extension, CLI Binary into PHP path
    ```
    make install-all
    ```

    Edit `php.ini`, add the following line. A reload is needed if PHP running
    on php-fpm mode.
    ```
    extension=trace.so
    ```

4. Verify
    ```
    php -r 'for ($i = 0; $i < 20; $i++) usleep(50000);' &
    phptrace -p $!
    ```

    You should see something below if it works fine
    ```
    process attached
    [pid 3600]> cli php -
    [pid 3600]> {main}() called at [Command line code:1]
    [pid 3600]    > usleep(50000) called at [Command line code:1]
    [pid 3600]    < usleep(50000) = NULL called at [Command line code:1] ~ 0.051s 0.051s
    [pid 3600]    > usleep(50000) called at [Command line code:1]
    [pid 3600]    < usleep(50000) = NULL called at [Command line code:1] ~ 0.051s 0.051s
    [pid 3600]    > usleep(50000) called at [Command line code:1]
    [pid 3600]    < usleep(50000) = NULL called at [Command line code:1] ~ 0.051s 0.051s
    [pid 3600]    > usleep(50000) called at [Command line code:1]
    ...
    ```


## Usage

Just try `php example.php`.

### Command line options

* trace     trace running php process(default)
* status    display php process status
* version   show version
* -p        specify php process id ('all' to trace all processes)
* -h        show helper
* -v        same as version
* -f        filter data by type(url,function,class) and content
* -l        limit output count
* --ptrace  in status mode fetch data using ptrace

### Trace executing

```
$ phptrace -p 3600

[pid 3600]    > Me->run() called at [example.php:57]
[pid 3600]        > Me->say(""good night"") called at [example.php:33]
[pid 3600]        < Me->say(""good night"") = NULL called at [example.php:33] ~ 0.000s 0.000s
[pid 3600]        > Me->sleep() called at [example.php:34]
[pid 3600]            > Me->say(""sleeping..."") called at [example.php:27]
[pid 3600]            < Me->say(""sleeping..."") = NULL called at [example.php:27] ~ 0.000s 0.000s
[pid 3600]            > sleep(2) called at [example.php:28]
[pid 3600]            < sleep(2) = 0 called at [example.php:28] ~ 2.000s 2.000s
[pid 3600]        < Me->sleep() = NULL called at [example.php:34] ~ 2.000s 0.000s
[pid 3600]        > Me->say(""wake up"") called at [example.php:35]
[pid 3600]        < Me->say(""wake up"") = NULL called at [example.php:35] ~ 0.000s 0.000s
[pid 3600]    < Me->run() = NULL called at [example.php:57] ~ 2.000s 0.000s
```

### Print current status

```
$ phptrace status -p 3600

------------------------------- Status --------------------------------
PHP Version:       7.0.16
SAPI:              cli
script:            example.php
elapse:            26.958s
------------------------------ Arguments ------------------------------
$0
------------------------------ Backtrace ------------------------------
#0  fgets() called at [example.php:53]
#1  {main}() called at [example.php:53]
```

### Tracing with filter of url/class/function

```
$ phptrace -p 3600 -f type=class,content=Me

[pid 3600]> Me->run() called at [example.php:57]
[pid 3600]> Me->say(""good night"") called at [example.php:33]
[pid 3600]< Me->say(""good night"") = NULL called at [example.php:33] ~ 0.000s 0.000s
[pid 3600]> Me->sleep() called at [example.php:34]
[pid 3600]> Me->say(""sleeping..."") called at [example.php:27]
[pid 3600]< Me->say(""sleeping..."") = NULL called at [example.php:27] ~ 0.000s 0.000s
[pid 3600]< Me->sleep() = NULL called at [example.php:34] ~ 2.000s 2.000s
[pid 3600]> Me->say(""wake up"") called at [example.php:35]
[pid 3600]< Me->say(""wake up"") = NULL called at [example.php:35] ~ 0.000s 0.000s
[pid 3600]< Me->run() = NULL called at [example.php:57] ~ 2.001s 0.000s
```

### Limit frame/URL display times

```
$ phptrace -p 3600 -l 2

[pid 3600]    > Me->run() called at [example.php:57]
[pid 3600]        > Me->say(""good night"") called at [example.php:33]
[pid 3600]        < Me->say(""good night"") = NULL called at [example.php:33] ~ 0.000s 0.000s
[pid 3600]        > Me->sleep() called at [example.php:34]
[pid 3600]            > Me->say(""sleeping..."") called at [example.php:27]
[pid 3600]            < Me->say(""sleeping..."") = NULL called at [example.php:27] ~ 0.000s 0.000s
```


## Contributing

Welcome developers who willing to make PHP environment better.

If you are interested but have no idea about how to starting, please try these below:

- Use it on your system, [feedback](https://github.com/monque/phptrace/issues) problem, feature request.
- Here is [roadmap](https://github.com/monque/phptrace/projects), try to develop one.
- Any other? Contact phobosw@gmail.com.


## License

This project is released under the [Apache 2.0 License](https://raw.githubusercontent.com/Qihoo360/phptrace/master/LICENSE)."
paparazzi/paparazzi,77100,1093,170,933,Organization,False,15595,31,58,107,False,Paparazzi is a free and open-source hardware and software project for unmanned (air) vehicles. This is the main software repository.,http://paparazziuav.org,7,42,1,74,632,4,3,11,1821,8,46,5619,15,103,21662,3443,0,0,29,10,,,"# MAIN README

Paparazzi UAS
=============

[![Build Status](https://travis-ci.org/paparazzi/paparazzi.png?branch=master)](https://travis-ci.org/paparazzi/paparazzi) [![Gitter chat](https://badges.gitter.im/paparazzi/discuss.svg)](https://gitter.im/paparazzi/discuss)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/811c4398588f435fa8bc926f53d40e9f)](https://app.codacy.com/app/gautierhattenberger/paparazzi?utm_source=github.com&utm_medium=referral&utm_content=paparazzi/paparazzi&utm_campaign=Badge_Grade_Dashboard)
<a href=""https://scan.coverity.com/projects/paparazzi-paparazzi"">
  <img alt=""Coverity Scan Build Status""
       src=""https://scan.coverity.com/projects/4928/badge.svg""/>
</a>

Paparazzi is a free open source software package for Unmanned (Air) Vehicle Systems.
For many years, the system has been used successfuly by hobbyists, universities and companies all over the world, on vehicles of various sizes (11.9g to 25kg).
Paparazzi supports fixed wing, rotorcraft, hybrids, flapping vehicles and it is even possible to use it for boats and surface vehicles.

Up to date information is available on the wiki http://wiki.paparazziuav.org

To get in touch, subscribe to the mailing list [paparazzi-devel@nongnu.org] (http://savannah.nongnu.org/mail/?group=paparazzi), the IRC channel (freenode, #paparazzi) and Gitter (https://gitter.im/paparazzi/discuss).

Required software
-----------------

Instructions for installation can be found on the wiki (http://wiki.paparazziuav.org/wiki/Installation).

For Ubuntu users, required packages are available in the [paparazzi-uav PPA] (https://launchpad.net/~paparazzi-uav/+archive/ppa),
Debian users can use the [OpenSUSE Build Service repository] (http://download.opensuse.org/repositories/home:/flixr:/paparazzi-uav/Debian_7.0/)

Debian/Ubuntu packages:
- **paparazzi-dev** is the meta-package on which the Paparazzi software depends to compile and run the ground segment and simulator.
- **paparazzi-jsbsim** is needed for using JSBSim as flight dynamics model for the simulator.

Recommended cross compiling toolchain: https://launchpad.net/gcc-arm-embedded


Directories quick and dirty description:
----------------------------------------

_conf_: the configuration directory (airframe, radio, ... descriptions).

_data_: where to put read-only data (e.g. maps, terrain elevation files, icons)

_doc_: documentation (diagrams, manual source files, ...)

_sw_: software (onboard, ground station, simulation, ...)

_var_: products of compilation, cache for the map tiles, ...


Compilation and demo simulation
-------------------------------

1. type ""make"" in the top directory to compile all the libraries and tools.

2. ""./paparazzi"" to run the Paparazzi Center

3. Select the ""Microjet"" aircraft in the upper-left A/C combo box.
  Select ""sim"" from upper-middle ""target"" combo box. Click ""Build"".
  When the compilation is finished, select ""Simulation"" from
  the upper-right session combo box and click ""Execute"".

4. In the GCS, wait about 10s for the aircraft to be in the ""Holding point"" navigation block.
  Switch to the ""Takeoff"" block (lower-left blue airway button in the strip).
  Takeoff with the green launch button.

Uploading the embedded software
----------------------------------

1. Power the flight controller board while it is connected to the PC with the USB cable.

2. From the Paparazzi center, select the ""ap"" target, and click ""Upload"".


Flight
------

1.  From the Paparazzi Center, select the flight session and ... do the same as in simulation !"
audacity/audacity,197499,2953,191,723,Organization,False,12094,3,18,105,False,Audio Editor,https://wiki.audacityteam.org/wiki/Fo…,5,19,3,56,58,51,36,11,443,2,77,,0,0,0,0,0,0,5,4,,,"[![Audacity](https://forum.audacityteam.org/styles/prosilver/theme/images/Audacity-logo_75px_trans_forum.png)](https://www.audacityteam.org) 
=========================

[**Audacity**](https://www.audacityteam.org) is an easy-to-use, multi-track audio editor and recorder for Windows, Mac OS X, GNU/Linux and other operating systems. Developed by a group of volunteers as open source.

- **Recording** from any real, or virtual audio device that is available to the host system.
- **Export / Import** a wide range of audio formats, extendible with FFmpeg.
- **High quality** using 32-bit float audio processing.
- **Plug-ins** Support for multiple audio plug-in formats, including VST, LV2, AU.
- **Macros** for chaining commands and batch processing.
- **Scripting** in Python, Perl, or any language that supports named pipes.
- **Nyquist** Very powerful built-in scripting language that may also be used to create plug-ins.
- **Editing** multi-track editing with sample accuracy and arbitrary sample rates.
- **Accessibility** for VI users.
- **Analysis and visualization** tools to analyze audio, or other signal data.

## Getting Started

For end users, the latest Windows and macOS release version of Audacity is available from the [Audacity website](https://www.audacityteam.org/download/).
Help with using Audacity is available from the [Audacity Forum](https://forum.audacityteam.org/).
Information for developers is available from the [Audacity Wiki](https://wiki.audacityteam.org/wiki/For_Developers)."
veracrypt/VeraCrypt,177056,2502,179,441,Organization,False,1616,2,38,43,False,Disk encryption with strong security based on TrueCrypt,https://www.veracrypt.fr,6,7,0,327,193,51,8,7,104,1,8,,0,0,0,0,24,2,8,0,,,"This archive contains the source code of VeraCrypt.
It is based on original TrueCrypt 7.1a with security enhancements and modifications.


Important
=========

You may use the source code contained in this archive only if you accept and
agree to the license terms contained in the file 'License.txt', which is
included in this archive.

Note that the license specifies, for example, that a derived work must not be
called 'TrueCrypt' or 'VeraCrypt'



Contents
========

I. Windows
   Requirements for Building VeraCrypt for Windows.
   Instructions for Building VeraCrypt for Windows.
 Instructions for Signing and Packaging VeraCrypt for Windows.

II. Linux and Mac OS X
    Requirements for Building VeraCrypt for Linux and Mac OS X.
    Instructions for Building VeraCrypt for Linux and Mac OS X.
 Mac OS X specifics

III. FreeBSD

IV. Third-Party Developers (Contributors)

V. Legal Information

VI. Further Information



I. Windows
==========

Requirements for Building VeraCrypt for Windows:
------------------------------------------------

- Microsoft Visual C++ 2010 SP1 (Professional Edition or compatible)
- Microsoft Visual C++ 1.52 (available from MSDN Subscriber Downloads)
- Microsoft Windows SDK for Windows 7.1 (configured for Visual C++ 2010)
- Microsoft Windows SDK for Windows 8.1 (needed for SHA-256 code signing)
- Microsoft Windows Driver Kit 7.1.0 (build 7600.16385.1)
- NASM assembler 2.08 or compatible
- YASM 1.3.0 or newer.
- gzip compressor
- upx packer (available at https://upx.github.io/)

IMPORTANT:

The 64-bit editions of Windows Vista and later versions of Windows, and in
some cases (e.g. playback of HD DVD content) also the 32-bit editions, do not
allow the VeraCrypt driver to run without an appropriate digital signature.
Therefore, all .sys files in official VeraCrypt binary packages are digitally
signed with the digital certificate of the IDRIX, which was
issued by Thawte certification authority. At the end of each official .exe and
.sys file, there are embedded digital signatures and all related certificates
(i.e. all certificates in the relevant certification chain, such as the
certification authority certificates, CA-MS cross-certificate, and the
IDRIX certificate).
Keep this in mind if you compile VeraCrypt
and compare your binaries with the official binaries. If your binaries are
unsigned, the sizes of the official binaries will usually be approximately
10 KiB greater than sizes of your binaries (there may be further differences
if you use a different version of the compiler, or if you install a different
or no service pack for Visual Studio, or different hotfixes for it, or if you
use different versions of the required SDKs).


Instructions for Building VeraCrypt for Windows:
------------------------------------------------

1) Create an environment variable 'MSVC16_ROOT' pointing to the folder 'MSVC15'
   extracted from the Visual C++ 1.52 self-extracting package.

   Note: The 16-bit installer MSVC15\SETUP.EXE cannot be run on 64-bit Windows,
   but it is actually not necessary to run it. You only need to extract the
   folder 'MSVC15', which contains the 32-bit binaries required to build the
   VeraCrypt Boot Loader.

2) If you have installed the Windows Driver Development Kit in another
   directory than '%SYSTEMDRIVE%\WinDDK', create an environment variable
   'WINDDK_ROOT' pointing to the DDK installation directory.

3) Open the solution file 'VeraCrypt.sln' in Microsoft Visual Studio 2010.

4) Select 'All' as the active solution configuration.

5) Build the solution.

6) If successful, there should be newly built VeraCrypt binaries in the
   'Release' folder.

Instructions for Signing and Packaging VeraCrypt for Windows:
-------------------------------------------------------------

First, create an environment variable 'WSDK81' pointing to the Windows SDK
for Windows 8.1 installation directory.
The folder ""Signing"" contains a batch file (sign.bat) that will sign all
VeraCrypt components using a code signing certificate present on the
certificate store and also build the final installation setup.
The batch file suppose that the code signing certificate is issued by Thawt.
This is the case for IDRIX's certificate. If yours is issued by another CA,
then you should put the Root and Intermediate certificates in the ""Signing""
folder and then modify sign.bat accordingly.

VeraCrypt EFI Boot Loader:
--------------------------

VeraCrypt source code contains pre-built EFI binaries under src\Boot\EFI.
The source code of VeraCrypt EFI Boot Loader is licensed under LGPL and 
it is available at https://github.com/veracrypt/VeraCrypt-DCS.
For build instructions, please refer to the file src\Boot\EFI\Readme.txt.


II. Linux and Mac OS X
======================

Requirements for Building VeraCrypt for Linux and Mac OS X:
-----------------------------------------------------------

- GNU Make
- GNU C++ Compiler 4.0 or compatible
- Apple Xcode (Mac OS X only)
- YASM 1.3.0 or newer (Linux only, x86/x64 architecture only)
- pkg-config
- wxWidgets 3.0 shared library and header files installed or
  wxWidgets 3.0 library source code (available at https://www.wxwidgets.org)
- FUSE library and header files (available at https://github.com/libfuse/libfuse
  and https://osxfuse.github.io/)


Instructions for Building VeraCrypt for Linux and Mac OS X:
-----------------------------------------------------------

1) Change the current directory to the root of the VeraCrypt source code.

2) If you have no wxWidgets shared library installed, run the following
   command to configure the wxWidgets static library for VeraCrypt and to
   build it:

       $ make WXSTATIC=1 WX_ROOT=/usr/src/wxWidgets wxbuild

   The variable WX_ROOT must point to the location of the source code of the
   wxWidgets library. Output files will be placed in the './wxrelease/'
   directory.

3) To build VeraCrypt, run the following command:

       $ make

   or if you have no wxWidgets shared library installed:

       $ make WXSTATIC=1

4) If successful, the VeraCrypt executable should be located in the directory
   'Main'.

By default, a universal executable supporting both graphical and text user
interface (through the switch --text) is built.
On Linux, a console-only executable, which requires no GUI library, can be
built using the 'NOGUI' parameter:

    $ make NOGUI=1 WXSTATIC=1 WX_ROOT=/usr/src/wxWidgets wxbuild
    $ make NOGUI=1 WXSTATIC=1

On MacOSX, building a console-only executable is not supported.

Mac OS X specifics:
-----------------------------------------------------------

Under MacOSX, the SDK for OSX 10.7 is used by default. To use another version
of the SDK (i.e. 10.6), you can export the environment variable VC_OSX_TARGET:

 $ export VC_OSX_TARGET=10.6


Before building under MacOSX, pkg-config must be installed if not yet available.
Get it from https://pkgconfig.freedesktop.org/releases/pkg-config-0.28.tar.gz and
compile using the following commands :

 $ ./configure --with-internal-glib
 $ make
 $ sudo make install

After making sure pkg-config is available, download and install OSXFuse from
https://osxfuse.github.io/ (MacFUSE compatibility layer must selected)

The script build_veracrypt_macosx.sh available under ""src/Build"" performs the
full build of VeraCrypt including the creation of the installer pkg. It expects
to find the wxWidgets 3.0.3 sources at the same level as where you put
VeraCrypt sources (i.e. if ""src"" path is ""/Users/joe/Projects/VeraCrypt/src""
then wxWidgets should be at ""/Users/joe/Projects/wxWidgets-3.0.3"")

The build process uses Code Signing certificates whose ID is specified in
src/Main/Main.make (look for lines containing ""Developer ID Application"" and 
""Developer ID Installer""). You'll have to modify these lines to put the ID of
your Code Signing certificates or comment them if you don't have one.

Because of incompatibility issues with OSXFUSE, the SDK 10.9 generates a
VeraCrypt binary that has issues communicating with the OSXFUSE kernel extension.
Thus, we recommend using a different OSX SDK version for building VeraCrypt.



III. FreeBSD
============================

FreeBSD is supported starting from version 11.
The build requirements and instructions are the same as Linux except that gmake
should be used instead of make.



IV. Third-Party Developers (Contributors)
=========================================

If you intend to implement a feature, please contact us first to make sure:

1) That the feature has not been implemented (we may have already implemented
   it, but haven't released the code yet).
2) That the feature is acceptable.
3) Whether we need help of third-party developers with implementing the feature.

Information on how to contact us can be found at:
https://www.veracrypt.fr/



V. Legal Information
====================

Copyright Information
---------------------

This software as a whole:  
Copyright (c) 2013-2020 IDRIX. All rights reserved.

Portions of this software:  
Copyright (c) 2013-2020 IDRIX. All rights reserved.  
Copyright (c) 2003-2012 TrueCrypt Developers Association. All rights reserved.  
Copyright (c) 1998-2000 Paul Le Roux. All rights reserved.  
Copyright (c) 1998-2008 Brian Gladman, Worcester, UK. All rights reserved.  
Copyright (c) 1995-2017 Jean-loup Gailly and Mark Adler.  
Copyright (c) 2016 Disk Cryptography Services for EFI (DCS), Alex Kolotnikov  
Copyright (c) 1999-2017 Dieter Baron and Thomas Klausner.  
Copyright (c) 2013, Alexey Degtyarev. All rights reserved.  
Copyright (c) 1999-2016 Jack Lloyd. All rights reserved.  
Copyright (c) 2013-2019 Stephan Mueller <smueller@chronox.de>

For more information, please see the legal notices attached to parts of the
source code.

Trademark Information
---------------------

Any trademarks contained in the source code, binaries, and/or in the
documentation, are the sole property of their respective owners.



VI. Further Information
=======================

https://www.veracrypt.fr"
zephyrproject-rtos/zephyr,335351,3332,342,2026,Organization,False,41561,20,114,699,False,"Primary Git Repository for the Zephyr Project. Zephyr is a new generation, scalable, optimized, secure RTOS for multiple hardware architectures.",https://docs.zephyrproject.org,6,177,10,1094,8006,463,845,573,16472,440,3008,2007,60,3971,507194,305777,0,0,69,13,,,
netblue30/firejail,13151,2550,102,344,User,False,6238,8,48,192,False,Linux namespaces and seccomp-bpf sandbox,https://firejail.wordpress.com,0,15,0,204,1976,61,182,6,1263,5,131,1776,29,445,47378,43074,0,0,6,,93,,"# Firejail
[![Test Status](https://travis-ci.org/netblue30/firejail.svg?branch=master)](https://travis-ci.org/netblue30/firejail)
[![Build Status](https://gitlab.com/Firejail/firejail_ci/badges/master/pipeline.svg)](https://gitlab.com/Firejail/firejail_ci/pipelines/)
[![Packaging status](https://repology.org/badge/tiny-repos/firejail.svg)](https://repology.org/project/firejail/versions)

Firejail is a SUID sandbox program that reduces the risk of security breaches by restricting
the running environment of untrusted applications using Linux namespaces, seccomp-bpf
and Linux capabilities. It allows a process and all its descendants to have their own private
view of the globally shared kernel resources, such as the network stack, process table, mount table.
Firejail can work in a SELinux or AppArmor environment, and it is integrated with Linux Control Groups.

Written in C with virtually no dependencies, the software runs on any Linux computer with a 3.x kernel
version or newer. It can sandbox any type of processes: servers, graphical applications, and even
user login sessions. The software includes sandbox profiles for a number of more common Linux programs,
such as Mozilla Firefox, Chromium, VLC, Transmission etc.

The sandbox is lightweight, the overhead is low. There are no complicated configuration files to edit,
no socket connections open, no daemons running in the background. All security features are
implemented directly in Linux kernel and available on any Linux computer.

<table><tr>

<td>
<a href=""http://www.youtube.com/watch?feature=player_embedded&v=7RMz7tePA98
"" target=""_blank""><img src=""http://img.youtube.com/vi/7RMz7tePA98/0.jpg""
alt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Firejail Intro</a>
</td>

<td>
<a href=""http://www.youtube.com/watch?feature=player_embedded&v=J1ZsXrpAgBU
"" target=""_blank""><img src=""http://img.youtube.com/vi/J1ZsXrpAgBU/0.jpg""
alt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Firejail Demo</a>
</td>

<td>
<a href=""http://www.youtube.com/watch?feature=player_embedded&v=EyEz65RYfw4
"" target=""_blank""><img src=""http://img.youtube.com/vi/EyEz65RYfw4/0.jpg""
alt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Debian Install</a>
</td>


</tr><tr>
<td>
<a href=""http://www.youtube.com/watch?feature=player_embedded&v=Uy2ZTHc4s0w
"" target=""_blank""><img src=""http://img.youtube.com/vi/Uy2ZTHc4s0w/0.jpg""
alt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Arch Linux Install</a>

</td>
<td>
<a href=""http://www.youtube.com/watch?feature=player_embedded&v=xuMxRx0zSfQ
"" target=""_blank""><img src=""http://img.youtube.com/vi/xuMxRx0zSfQ/0.jpg""
alt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Disable Network Access</a>

</td>
</tr></table>

Project webpage: https://firejail.wordpress.com/

Download and Installation: https://firejail.wordpress.com/download-2/

Features: https://firejail.wordpress.com/features-3/

Documentation: https://firejail.wordpress.com/documentation-2/

FAQ: https://github.com/netblue30/firejail/wiki/Frequently-Asked-Questions

Wiki: https://github.com/netblue30/firejail/wiki

Travis-CI status: https://travis-ci.org/netblue30/firejail

GitLab-CI status: https://gitlab.com/Firejail/firejail_ci/pipelines/


## Security vulnerabilities

We take security bugs very seriously. If you believe you have found one, please report it by emailing us at netblue30@yahoo.com

## Installing

Try installing Firejail from your system packages first. Firejail is included in Alpine, ALT Linux, Arch, Chakra, Debian, Deepin, Devuan, Fedora, Gentoo, Manjaro, Mint, NixOS, Parabola, Parrot, PCLinuxOS, ROSA, Solus, Slackware/SlackBuilds, Trisquel, Ubuntu, Void and possibly others.

The firejail 0.9.52-LTS version is deprecated. On Ubuntu 18.04 LTS users are advised to use the [PPA](https://launchpad.net/~deki/+archive/ubuntu/firejail). On Debian buster we recommend to use the [backports](https://packages.debian.org/buster-backports/firejail) package.

You can also install one of the [released packages](http://sourceforge.net/projects/firejail/files/firejail), or clone Firejail’s source code from our Git repository and compile manually:

`````
$ git clone https://github.com/netblue30/firejail.git
$ cd firejail
$ ./configure && make && sudo make install-strip
`````
On Debian/Ubuntu you will need to install git and gcc compiler. AppArmor
development libraries and pkg-config are required when using --apparmor
./configure option:
`````
$ sudo apt-get install git build-essential libapparmor-dev pkg-config
`````
For --selinux option, add libselinux1-dev (libselinux-devel for Fedora).

Detailed information on using firejail from git is available on the [wiki](https://github.com/netblue30/firejail/wiki/Using-firejail-from-git).

## Running the sandbox

To start the sandbox, prefix your command with “firejail”:

`````
$ firejail firefox            # starting Mozilla Firefox
$ firejail transmission-gtk   # starting Transmission BitTorrent
$ firejail vlc                # starting VideoLAN Client
$ sudo firejail /etc/init.d/nginx start
`````
Run ""firejail --list"" in a terminal to list all active sandboxes. Example:
`````
$ firejail --list
1617:netblue:/usr/bin/firejail /usr/bin/firefox-esr
7719:netblue:/usr/bin/firejail /usr/bin/transmission-qt
7779:netblue:/usr/bin/firejail /usr/bin/galculator
7874:netblue:/usr/bin/firejail /usr/bin/vlc --started-from-file file:///home/netblue/firejail-whitelist.mp4
7916:netblue:firejail --list
`````

## Desktop integration

Integrate your sandbox into your desktop by running the following two commands:
`````
$ firecfg --fix-sound
$ sudo firecfg
`````

The first command solves some shared memory/PID namespace bugs in PulseAudio software prior to version 9.
The second command integrates Firejail into your desktop. You would need to logout and login back to apply
PulseAudio changes.

Start your programs the way you are used to: desktop manager menus, file manager, desktop launchers.
The integration applies to any program supported by default by Firejail. There are about 250 default applications
in current Firejail version, and the number goes up with every new release.
We keep the application list in [/usr/lib/firejail/firecfg.config](https://github.com/netblue30/firejail/blob/master/src/firecfg/firecfg.config) file.

## Security profiles

Most Firejail command line options can be passed to the sandbox using profile files.
You can find the profiles for all supported applications in [/etc/firejail](https://github.com/netblue30/firejail/tree/master/etc) directory.

If you keep additional Firejail security profiles in a public repository, please give us a link:

* https://github.com/chiraag-nataraj/firejail-profiles

* https://github.com/triceratops1/fe

Use this issue to request new profiles: [#1139](https://github.com/netblue30/firejail/issues/1139)

You can also use this tool to get a list of syscalls needed by a program: [contrib/syscalls.sh](contrib/syscalls.sh).

We also keep a list of profile fixes for previous released versions in [etc-fixes](https://github.com/netblue30/firejail/tree/master/etc-fixes) directory.
`````

`````
## Latest released version: 0.9.62

## Current development version: 0.9.63

### Profile Statistics

A small tool to print profile statistics. Compile as usual and run:
`````
$ make
$ cd etc
$ ./profstats *.profile
    profiles   966
    include local profile 966   (include profile-name.local)
    include globals  966   (include globals.local)
    blacklist ~/.ssh  951   (include disable-common.inc)
    seccomp   908
    capabilities  965
    noexec   830   (include disable-exec.inc)
    memory-deny-write-execute 214
    apparmor   488
    private-bin   483
    private-dev   829
    private-etc   366
    private-tmp   726
    whitelist var  638   (include whitelist-var-common.inc)
    whitelist run/user  282   (include whitelist-runuser-common.inc
     or blacklist ${RUNUSER})
    whitelist usr/share  275   (include whitelist-usr-share-common.inc
    net none   313
`````

Run ./profstats -h for help.

### New profiles:

gfeeds, firefox-x11, tvbrowser, rtv, clipgrab, gnome-passwordsafe, bibtex, gummi, latex, pdflatex, tex, wpp, wpspdf, wps, et,
multimc, gnome-hexgl, com.github.johnfactotum.Foliate, desktopeditors, impressive, mupdf-gl, mupdf-x11, mupdf-x11-curl,
muraster, mutool, planmaker18, planmaker18free, presentations18, presentations18free, textmaker18, textmaker18free, teams, xournal,
gnome-screenshot, ripperX, sound-juicer, iagno, com.github.dahenson.agenda, gnome-pomodoro, gnome-todo, kmplayer,
penguin-command, x2goclient, frogatto, gnome-mines, gnome-nibbles, lightsoff, ts3client_runscript.sh, warmux, ferdi, abiword,
four-in-a-row, gnome-mahjongg, gnome-robots, gnome-sudoku, gnome-taquin, gnome-tetravex, blobwars, gravity-beams-and-evaporating-stars,
hyperrogue, jumpnbump-menu, jumpnbump, magicor, mindless, mirrormagic, mrrescue, scorched3d-wrapper, scorchwentbonkers,
seahorse-adventures, wordwarvi, xbill, gnome-klotski, five-or-more, swell-foop, fdns, jitsi-meet-desktop, nicontine, steam-runtime, apostrophe, quadrapassel, dino-im"
megous/megatools,1,1466,90,182,User,False,2,1,0,1,False,Open-source command line tools for accessing Mega.co.nz cloud storage.,https://megatools.megous.com,2,13,0,33,377,2,2,0,44,0,0,432,0,0,0,0,0,0,3,,75,,"## Repostiory has been moved

If you want to reach me as the maintainer, send an e-mail to megatools@megous.com (What you send there will be pubic, so be careful.) Questions and patches are welcome. I no longer monitor or update my github account.

Official web page is:

https://megatools.megous.com

The main git repository is hosted at:

https://megous.com/git/megatools"
dlundquist/sniproxy,1094,1761,113,323,User,False,741,13,10,25,False,Proxies incoming HTTP and TLS connections based on the hostname contained in the initial request of the TCP session.,,0,11,0,79,179,8,1,11,88,2,2,3463,1,1,2,2,0,0,40,,69,,"SNI Proxy
=========

Proxies incoming HTTP and TLS connections based on the hostname contained in
the initial request of the TCP session. This enables HTTPS name-based virtual
hosting to separate backend servers without installing the private key on the
proxy machine.

Features
--------
+ Name-based proxying of HTTPS without decrypting traffic. No keys or
  certificates required.
+ Supports both TLS and HTTP protocols.
+ Supports IPv4, IPv6 and Unix domain sockets for both back end servers and
  listeners.
+ Supports multiple listening sockets per instance.
+ Supports HAProxy proxy protocol to propagate original source address to
  backend servers.

Usage
-----

    Usage: sniproxy [-c <config>] [-f] [-n <max file descriptor limit>] [-V]
        -c  configuration file, defaults to /etc/sniproxy.conf
        -f  run in foreground, do not drop privileges
        -n  specify file descriptor limit
        -V  print the version of SNIProxy and exit


Installation
------------

For Debian or Fedora based Linux distributions see building packages below.

**Prerequisites**

+ Autotools (autoconf, automake, gettext and libtool)
+ libev4, libpcre and libudns development headers
+ Perl and cURL for test suite

**Install**

    ./autogen.sh && ./configure && make check && sudo make install

**Building Debian/Ubuntu package**

This is the preferred installation method on recent Debian based distributions:

1. Install required packages

        sudo apt-get install autotools-dev cdbs debhelper dh-autoreconf dpkg-dev gettext libev-dev libpcre3-dev libudns-dev pkg-config fakeroot devscripts

2. Build a Debian package

        ./autogen.sh && dpkg-buildpackage

3. Install the resulting package

        sudo dpkg -i ../sniproxy_<version>_<arch>.deb

**Building Fedora/RedHat package**

This is the preferred installation method for modern Fedora based distributions.

1. Install required packages

        sudo yum install autoconf automake curl gettext-devel libev-devel pcre-devel perl pkgconfig rpm-build udns-devel

2. Build a distribution tarball:

        ./autogen.sh && ./configure && make dist

3. Build a RPM package

        rpmbuild --define ""_sourcedir `pwd`"" -ba redhat/sniproxy.spec

4. Install resulting RPM

        sudo yum install ../sniproxy-<version>.<arch>.rpm

I've used Scientific Linux 6 a fair amount, but I prefer Debian based
distributions. RPM builds are tested in Travis-CI on Ubuntu, but not natively.
This build process may not follow the current Fedora packaging standards, and
may not even work.

***Building on OS X with Homebrew***

1. install dependencies.

        brew install libev pcre udns autoconf automake gettext libtool

2. Read the warning about gettext and force link it so autogen.sh works. We need the GNU gettext for the macro `AC_LIB_HAVE_LINKFLAGS` which isn't present in the default OS X package.

        brew link --force gettext

3. Make it so

        ./autogen.sh && ./configure && make

OS X support is a best effort, and isn't a primary target platform.


Configuration Syntax
--------------------

    user daemon

    pidfile /tmp/sniproxy.pid

    error_log {
        syslog daemon
        priority notice
    }

    listener 127.0.0.1:443 {
        protocol tls
        table TableName

        # Specify a server to use if the initial client request doesn't contain
        # a hostname
        fallback 192.0.2.5:443
    }

    table TableName {
        # Match exact request hostnames
        example.com 192.0.2.10:4343
        # If port is not specified the listener port will be used
        example.net [2001:DB8::1:10]
        # Or use regular expression to match
        .*\\.com    [2001:DB8::1:11]:443
        # Combining regular expression and wildcard will resolve the hostname
        # client requested and proxy to it
        .*\\.edu    *:443
    }

DNS Resolution
--------------

Using hostnames or wildcard entries in the configuration requires sniproxy to
be built with [UDNS](http://www.corpit.ru/mjt/udns.html). SNIProxy will still
build without UDNS, but these features will be unavailable.

UDNS uses a single UDP socket for all queries, so it is recommended you use a
local caching DNS resolver (with a single socket each DNS query is protected by
spoofing by a single 16 bit query ID, which makes it relatively easy to spoof)."
vlfeat/vlfeat,10399,1314,158,580,Organization,False,2059,2,26,18,False,An open library of computer vision algorithms,http://vlfeat.org/,0,0,2,103,52,1,1,25,30,1,0,4737,0,0,0,0,0,0,9,0,,,"# VLFeat -- Vision Lab Features Library

> Version 0.9.21

The VLFeat open source library implements popular computer vision
algorithms specialising in image understanding and local featurexs
extraction and matching.  Algorithms incldue Fisher Vector, VLAD,
SIFT, MSER, k-means, hierarchical k-means, agglomerative information
bottleneck, SLIC superpixes, quick shift superpixels, large scale SVM
training, and many others. It is written in C for efficiency and
compatibility, with interfaces in MATLAB for ease of use, and detailed
documentation throughout. It supports Windows, Mac OS X, and Linux.

VLFeat is distributed under the BSD license (see the `COPYING` file).

The documentation is
[available online](http://www.vlfeat.org/index.html) and shipped with
the library as `doc/index.html`. See also:

* [Using with MATLAB](http://www.vlfeat.org/install-matlab.html)
* [Using the command line utilities](http://www.vlfeat.org/install-shell.html)
* [Using the C API](http://www.vlfeat.org/install-c.html)
* [Compiling from source](http://www.vlfeat.org/compiling.html)

## Quick start with MATLAB

To start using VLFeat as a MATLAB toolbox, download the latest VLFeat
[binary package](http://www.vlfeat.org/download/). Note that the
pre-compiled binaries require MATLAB 2009B and later. Unpack it, for
example by using WinZIP (Windows), by double clicking on the archive
(Mac), or by using the command line (Linux and Mac):

    > tar xzf vlfeat-X.Y.Z-bin.tar.gz

Here X.Y.Z denotes the latest version. Start MATLAB and run the
VLFeat setup command:

    > run <VLFEATROOT>/toolbox/vl_setup

Here `<VLFEATROOT>` should be replaced with the path to the VLFeat
directory created by unpacking the archive. All VLFeat demos can now
be run in a row by the command:

    > vl_demo

Check out the individual demos by editing this file: `edit vl_demo`.

## Octave support

The toolbox should be laregly compatible with GNU Octave, an open
source MATLAB equivalent. However, the binary distribution does not
ship with pre-built GNU Octave MEX files. To compile them use

    > cd <vlfeat directory>
    > make MKOCTFILE=<path to the mkoctfile program>

# Changes

- **0.9.21** Maintenance release. Bugfixes.
- **0.9.20** Maintenance release. Bugfixes.
- **0.9.19** Maintenance release. Minor bugfixes and fixes compilation
  with MATLAB 2014a.
- **0.9.18** Several bugfixes. Improved documentation, particularly of
  the covariant detectors. Minor enhancements of the Fisher vectors.
- **0.9.17** Rewritten SVM implementation, adding support for SGD and
  SDCA optimisers and various loss functions (hinge, squared hinge,
  logistic, etc.) and improving the interface. Added infrastructure to
  support multi-core computations using OpenMP (MATLAB 2009B or later
  required). Added OpenMP support to KD-trees and KMeans. Added new
  Gaussian Mixture Models, VLAD encoding, and Fisher Vector encodings
  (also with OpenMP support). Added LIOP feature descriptors. Added
  new object category recognition example code, supporting several
  standard benchmarks off-the-shelf.
- **0.9.16** Added `VL_COVDET`. This function implements the following
  detectors: DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscale
  Hessian, Multiscale Harris. It also implements affine adaptation,
  estiamtion of feature orientation, computation of descriptors on the
  affine patches (including raw patches), and sourcing of custom
  feature frame.
- **0.9.15** Added `VL_HOG` (HOG features). Added `VL_SVMPEGASOS` and
  a vastly improved SVM implementation. Added `VL_IHASHSUM` (hashed
  counting). Improved INTHIST (integral histogram). Added
  `VL_CUMMAX`. Improved the implementation of `VL_ROC` and
  VL_PR(). Added VL_DET() (Detection Error Trade-off (DET)
  curves). Improved the verbosity control to AIB. Added support for
  Xcode 4.3, improved support for past and future Xcode
  versions. Completed the migration of the old test code in
  `toolbox/test`, moving the functionality to the new unit tests
  `toolbox/xtest`.
- **0.9.14** Added SLIC superpixels. Added VL_ALPHANUM(). Improved
  Windows binary package and added support for Visual
  Studio 2010. Improved the documentation layout and added a proper
  bibliography. Bugfixes and other minor improvements. Moved from the
  GPL to the less restrictive BSD license.
- **0.9.13** Fixed Windows binary package.
- **0.9.12** Fixes `vl_compile` and the architecture string on Linux 32 bit.
- **0.9.11** Fixes a compatibility problem on older Mac OS X versions.
  A few bugfixes are included too.
- **0.9.10** Improves the homogeneous kernel map. Plenty of small
  tweaks and improvements. Make maci64 the default architecture on the
  Mac.
- **0.9.9** Added: sift matching example. Extended Caltech-101
  classification example to use kd-trees.
- **0.9.8** Added: image distance transform, PEGASOS, floating point
  K-means, homogeneous kernel maps, a Caltech-101 classification
  example. Improved documentation.
- **0.9.7** Changed the Mac OS X binary distribution to require a less
  recent version of Mac OS X (10.5).
- **0.9.6** Changed the GNU/Linux binary distribution to require a
  less recent version of the C library.
- **0.9.5** Added kd-tree and new SSE-accelerated vector/histogram
  comparison code.  Improved dense SIFT (dsift) implementation.  Added
  Snow Leopard and MATLAB R2009b support.
- **0.9.4** Added quick shift. Renamed dhog to dsift and improved
  implementation and documentation. Improved tutorials.  Added 64 bit
  Windows binaries. Many other small changes.
- **0.9.3** Namespace change (everything begins with a vl_ prefix
  now). Many other changes to provide compilation support on Windows
  with MATLAB 7.
- **beta-3** Completes to the ikmeans code.
- **beta-2** Many additions.
- **beta-1** Initial public release."
google/honggfuzz,191908,1903,131,395,Organization,False,3780,2,18,45,False,"Security oriented software fuzzer. Supports evolutionary, feedback-driven fuzzing based on code coverage (SW and HW based)",https://honggfuzz.dev,3,12,0,11,169,9,28,1,163,1,13,1839,11,331,7293,4831,0,0,1,1,,,"# Honggfuzz

## Description

A security oriented, feedback-driven, evolutionary, easy-to-use fuzzer with interesting analysis options. See the [Usage document](https://github.com/google/honggfuzz/blob/master/docs/USAGE.md) for a primer on Honggfuzz use.

## Code

  * Latest stable version: [2.2](https://github.com/google/honggfuzz/releases)
  * [Changelog](https://github.com/google/honggfuzz/blob/master/CHANGELOG)

## Features

  * It's __multi-process__ and __multi-threaded__: there's no need to run multiple copies of your fuzzer, as honggfuzz can unlock potential of all your available CPU cores with a single running instance. The file corpus is automatically shared and improved between all fuzzed processes.
  * It's blazingly fast when the [persistent fuzzing mode](https://github.com/google/honggfuzz/blob/master/docs/PersistentFuzzing.md)) is used. A simple/empty _LLVMFuzzerTestOneInput_ function can be tested with __up to 1mo iterations per second__ on a relatively modern CPU (e.g. i7-6700K).
  * Has a [solid track record](#trophies) of uncovered security bugs: the __only__ (to the date) __vulnerability in OpenSSL with the [critical](https://www.openssl.org/news/secadv/20160926.txt) score mark__ was discovered by honggfuzz. See the [Trophies](#trophies) paragraph for the summary of findings to the date.
  * Uses low-level interfaces to monitor processes (e.g. _ptrace_ under Linux and NetBSD). As opposed to other fuzzers, it __will discover and report hijacked/ignored signals from crashes__ (intercepted and potentially hidden by a fuzzed program).
  * Easy-to-use, feed it a simple corpus directory (can even be empty for the [feedback-driven fuzzing](https://github.com/google/honggfuzz/blob/master/docs/FeedbackDrivenFuzzing.md)), and it will work its way up, expanding it by utilizing feedback-based coverage metrics.
  * Supports several (more than any other coverage-based feedback-driven fuzzer) hardware-based (CPU: branch/instruction counting, __Intel BTS__, __Intel PT__) and software-based [feedback-driven fuzzing](https://github.com/google/honggfuzz/blob/master/docs/FeedbackDrivenFuzzing.md) modes. Also, see the new __[qemu mode](https://github.com/google/honggfuzz/tree/master/qemu_mode)__ for blackbox binary fuzzing.
  * Works (at least) under GNU/Linux, FreeBSD, NetBSD, Mac OS X, Windows/CygWin and [Android](https://github.com/google/honggfuzz/blob/master/docs/Android.md).
  * Supports the __persistent fuzzing mode__ (long-lived process calling a fuzzed API repeatedly). More on that can be found [here](https://github.com/google/honggfuzz/blob/master/docs/PersistentFuzzing.md).
  * It comes with the __[examples](https://github.com/google/honggfuzz/tree/master/examples) directory__, consisting of real world fuzz setups for widely-used software (e.g. Apache HTTPS, OpenSSL, libjpeg etc.).
  * Provides a __[corpus minimization](https://github.com/google/honggfuzz/blob/master/docs/USAGE.md#corpus-minimization--m)__ mode.

---

<p align=""center"">
 <img src=""https://raw.githubusercontent.com/google/honggfuzz/master/screenshot-honggfuzz-1.png"" width=""75%"" height=""75%"">
</p>

---

## Requirements

  * **Linux** - The BFD library (libbfd-dev) and libunwind (libunwind-dev/libunwind8-dev), clang-5.0 or higher for software-based coverage modes
  * **FreeBSD** - gmake, clang-5.0 or newer
  * **NetBSD** - gmake, clang, capstone, libBlocksRuntime
  * **Android** - Android SDK/NDK. Also see [this detailed doc](https://github.com/google/honggfuzz/blob/master/docs/Android.md) on how to build and run it
  * **Windows** - CygWin
  * **Darwin/OS X** - Xcode 10.8+
  * if **Clang/LLVM** is used to compile honggfuzz - link it with the BlocksRuntime Library (libblocksruntime-dev)

## Trophies

Honggfuzz has been used to find a few interesting security problems in major software packages; An incomplete list:


  * [Pre-auth remote crash in __OpenSSH__](https://anongit.mindrot.org/openssh.git/commit/?id=28652bca29046f62c7045e933e6b931de1d16737)
  * __Apache HTTPD__
    * [Remote crash in __mod\_http2__ • CVE-2017-7659](http://seclists.org/oss-sec/2017/q2/504)
    * [Use-after-free in __mod\_http2__ • CVE-2017-9789](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9789)
    * [Memory leak in __mod\_auth\_digest__ • CVE-2017-9788](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9788)
    * [Out of bound access • CVE-2018-1301](http://seclists.org/oss-sec/2018/q1/265)
    * [Write after free in HTTP/2 • CVE-2018-1302](http://seclists.org/oss-sec/2018/q1/268)
    * [Out of bound read • CVE-2018-1303](http://seclists.org/oss-sec/2018/q1/266)
  * Various __SSL__ libs
    * [Remote OOB read in __OpenSSL__ • CVE-2015-1789]( https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-1789)
    * [Remote Use-after-Free (potential RCE, rated as __critical__) in __OpenSSL__ • CVE-2016-6309](https://www.openssl.org/news/secadv/20160926.txt)
    * [Remote OOB write in __OpenSSL__ • CVE-2016-7054](https://www.openssl.org/news/secadv/20161110.txt)
    * [Remote OOB read in __OpenSSL__ • CVE-2017-3731](https://www.openssl.org/news/secadv/20170126.txt)
    * [Uninitialized mem use in __OpenSSL__](https://github.com/openssl/openssl/commit/bd5d27c1c6d3f83464ddf5124f18a2cac2cbb37f)
    * [Crash in __LibreSSL__](https://github.com/openbsd/src/commit/c80d04452814d5b0e397817ce4ed34edb4eb520d)
    * [Invalid free in __LibreSSL__](https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.6.2-relnotes.txt)
    * [Uninitialized mem use in __BoringSSL__](https://github.com/boringssl/boringssl/commit/7dccc71e08105b100c3acd56fa5f6fc1ba9b71d3)
  * [Adobe __Flash__ memory corruption • CVE-2015-0316](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-0316)
  * [Multiple bugs in the __libtiff__ library](http://bugzilla.maptools.org/buglist.cgi?query_format=advanced;emailreporter1=1;email1=robert@swiecki.net;product=libtiff;emailtype1=substring)
  * [Multiple bugs in the __librsvg__ library](https://bugzilla.gnome.org/buglist.cgi?query_format=advanced;emailreporter1=1;email1=robert%40swiecki.net;product=librsvg;emailtype1=substring)
  * [Multiple bugs in the __poppler__ library](http://lists.freedesktop.org/archives/poppler/2010-November/006726.html)
  * [Multiple exploitable bugs in __IDA-Pro__](https://www.hex-rays.com/bugbounty.shtml)
  * [Remote DoS in __Crypto++__ • CVE-2016-9939](http://www.openwall.com/lists/oss-security/2016/12/12/7)
  * Programming language interpreters
    * [__PHP/Python/Ruby__](https://github.com/dyjakan/interpreter-bugs)
    * [PHP WDDX](https://bugs.php.net/bug.php?id=74145)
    * [PHP](https://bugs.php.net/bug.php?id=74194)
    * Perl: [#1](https://www.nntp.perl.org/group/perl.perl5.porters/2018/03/msg250072.html), [#2](https://github.com/Perl/perl5/issues/16468), [#3](https://github.com/Perl/perl5/issues/16015)
  * [Double-free in __LibXMP__](https://github.com/cmatsuoka/libxmp/commit/bd1eb5cfcd802820073504c234c3f735e96c3355)
  * [Heap buffer overflow in SAPCAR • CVE-2017-8852](https://www.coresecurity.com/blog/sapcar-heap-buffer-overflow-crash-exploit)
  * [Crashes in __libbass__](http://seclists.org/oss-sec/2017/q4/185)
  * __FreeType 2__:
    * [CVE-2010-2497](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2497)
    * [CVE-2010-2498](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2498)
    * [CVE-2010-2499](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2499)
    * [CVE-2010-2500](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2500)
    * [CVE-2010-2519](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2519)
    * [CVE-2010-2520](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2520)
    * [CVE-2010-2527](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2527)
  * Stack corruption issues in the Windows OpenType parser: [#1](https://github.com/xinali/AfdkoFuzz/blob/4eadcb19eacb2fb73e4b0f0b34f382a9331bb3b4/CrashesAnalysis/CrashesAnalysis_3/README.md), [#2](https://github.com/xinali/AfdkoFuzz/blob/master/CVE-2019-1117/README.md), [#3](https://github.com/xinali/AfdkoFuzz/tree/f6d6562dd19403cc5a1f8cef603ee69425b68b20/CVE-2019-1118)
  * [Infinite loop in __NGINX Unit__](https://github.com/nginx/unit/commit/477e8177b70acb694759e62d830b8a311a736324)
  * A couple of problems in the [__MATLAB MAT File I/O Library__](https://sourceforge.net/projects/matio): [#1](https://github.com/tbeu/matio/commit/406438f497931f45fb3edf6de17d3a59a922c257), [#2](https://github.com/tbeu/matio/commit/406438f497931f45fb3edf6de17d3a59a922c257), [#3](https://github.com/tbeu/matio/commit/a55b9c2c01582b712d5a643699a13b5c41687db1), [#4](https://github.com/tbeu/matio/commit/3e6283f37652e29e457ab9467f7738a562594b6b), [#5](https://github.com/tbeu/matio/commit/783ee496a6914df68e77e6019054ad91e8ed6420)
  * __Samba__ [tdbdump + tdbtool](http://seclists.org/oss-sec/2018/q2/206), [#2](https://github.com/samba-team/samba/commit/183da1f9fda6f58cdff5cefad133a86462d5942a), [#3](https://github.com/samba-team/samba/commit/33e9021cbee4c17ee2f11d02b99902a742d77293), [#4](https://github.com/samba-team/samba/commit/ac1be895d2501dc79dcff2c1e03549fe5b5a930c), [#5](https://github.com/samba-team/samba/commit/b1eda993b658590ebb0a8225e448ce399946ed83), [#6](https://github.com/samba-team/samba/commit/f7f92803f600f8d302cdbb668c42ca8b186a797f) [CVE-2019-14907](https://www.samba.org/samba/security/CVE-2019-14907.html)
  * [Crash in __djvulibre__](https://github.com/barak/djvulibre/commit/89d71b01d606e57ecec2c2930c145bb20ba5bbe3)
  * [Multiple crashes in __VLC__](https://www.pentestpartners.com/security-blog/double-free-rce-in-vlc-a-honggfuzz-how-to/)
  * [Buffer overflow in __ClassiCube__](https://github.com/UnknownShadow200/ClassiCube/issues/591)
  * [Heap buffer-overflow (or UAF) in __MPV__](https://github.com/mpv-player/mpv/issues/6808)
  * [Heap buffer-overflow in __picoc__](https://gitlab.com/zsaleeba/picoc/issues/44)
  * Crashes in __OpenCOBOL__: [#1](https://sourceforge.net/p/open-cobol/bugs/586/), [#2](https://sourceforge.net/p/open-cobol/bugs/587/)
  * DoS in __ProFTPD__: [#1](https://twitter.com/SecReLabs/status/1186548245553483783), [#2](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18217)
  * [Multiple security problems in ImageIO (iOS/MacOS)](https://googleprojectzero.blogspot.com/2020/04/fuzzing-imageio.html)
  * [Memory corruption in __htmldoc__](https://github.com/michaelrsweet/htmldoc/issues/370)
  * [Memory corruption in __OpenDetex__](https://github.com/pkubowicz/opendetex/issues/60)
  * [Memory corruption in __Yabasic__](https://github.com/marcIhm/yabasic/issues/36)
  * [Memory corruption in __Xfig__](https://sourceforge.net/p/mcj/tickets/67/)
  * [Memory corruption in __LibreOffice__](https://github.com/LibreOffice/core/commit/0754e581b0d8569dd08cf26f88678754f249face)
  * [Memory corruption in __ATasm__](https://sourceforge.net/p/atasm/bugs/8/)
  * __Rust__:
    * panic() in regex [#1](https://github.com/rust-lang/regex/issues/464), [#2](https://github.com/rust-lang/regex/issues/465), [#3](https://github.com/rust-lang/regex/issues/465#issuecomment-381412816)
    * panic() in h2 [#1](https://github.com/carllerche/h2/pull/260), [#2](https://github.com/carllerche/h2/pull/261), [#3](https://github.com/carllerche/h2/pull/262)
    * panic() in sleep-parser [#1](https://github.com/datrs/sleep-parser/issues/3)
    * panic() in lewton [#1](https://github.com/RustAudio/lewton/issues/27)
    * panic()/DoS in Ethereum-Parity [#1](https://srlabs.de/bites/ethereum_dos/)
    * crash() in Parts - a GPT partition manager [#1](https://github.com/DianaNites/parts/commit/d8ab05d48d87814f362e94f01c93d9eeb4f4abf4)
    * crashes in rust-bitcoin/rust-lightning [#1](https://github.com/rust-bitcoin/rust-lightning/commit/a9aa3c37fe182dd266e0faebc788e0c9ee724783)
  * ... and more

## Projects utilizing or inspired-by Honggfuzz

  * [__QuickFuzz__ by CIFASIS](http://quickfuzz.org)
  * [__OSS-Fuzz__](https://github.com/google/oss-fuzz)
  * [__Frog And Fuzz__](https://github.com/warsang/FrogAndFuzz/tree/develop)
  * [__interpreters fuzzing__: by dyjakan](https://github.com/dyjakan/interpreter-bugs)
  * [__riufuzz__: honggfuzz with AFL-like UI](https://github.com/riusksk/riufuzz)
  * [__h2fuzz__: fuzzing Apache's HTTP/2 implementation](https://github.com/icing/h2fuzz)
  * [__honggfuzz-dharma__: honggfuzz with dharma grammar fuzzer](https://github.com/Sbouber/honggfuzz-dharma)
  * [__Owl__: a system for finding concurrency attacks](https://github.com/hku-systems/owl)
  * [__honggfuzz-docker-apps__](https://github.com/skysider/honggfuzz_docker_apps)
  * [__FFW__: Fuzzing For Worms](https://github.com/dobin/ffw)
  * [__honggfuzz-rs__: fuzzing Rust with Honggfuzz](https://docs.rs/honggfuzz/)
  * [__roughenough-fuzz__](https://github.com/int08h/roughenough-fuzz)
  * [__Monkey__: a HTTP server](https://github.com/monkey/monkey/blob/master/FUZZ.md)
  * [__Killerbeez API__: a modular fuzzing framework](https://github.com/grimm-co/killerbeez)
  * [__FuzzM__: a gray box model-based fuzzing framework](https://github.com/collins-research/FuzzM)
  * [__FuzzOS__: by Mozilla Security](https://github.com/MozillaSecurity/fuzzos)
  * [__Android__: by OHA](https://android.googlesource.com/platform/external/honggfuzz)
  * [__QDBI__: by Quarkslab](https://project.inria.fr/FranceJapanICST/files/2019/04/19-Kyoto-Fuzzing_Binaries_using_Dynamic_Instrumentation.pdf)
  * [__fuzzer-test-suite__: by Google](https://github.com/google/fuzzer-test-suite)
  * [__DeepState__: by Trail-of-Bits](https://github.com/trailofbits/deepstate)
  * [__Quiche-HTTP/3__: by Cloudflare](https://github.com/cloudflare/quiche/pull/179)
  * [__Bolero__: fuzz and property testing framework](https://github.com/camshaft/bolero)
  * [__pwnmachine__: a vagrantfile for exploit development on Linux](https://github.com/kapaw/pwnmachine/commit/9cbfc6f1f9547ed2d2a5d296f6d6cd8fac0bb7e1)
  * [__Quick700__: analyzing effectiveness of fuzzers on web browsers and web servers](https://github.com/Quick700/Quick700)
  * [__python-fuzz__: gluing honggfuzz and python3](https://github.com/thebabush/python-hfuzz)
  * [__Magma__: a ground-truth fuzzing benchmark](https://github.com/HexHive/magma)
  * [__arbitrary-model-tests__: a procedural macro for testing stateful models](https://github.com/jakubadamw/arbitrary-model-tests)
  * [__Clusterfuzz__: the fuzzing engine behind OSS-fuzz/Chrome-fuzzing](https://github.com/google/clusterfuzz/issues/1128)
  * [__Apache HTTP Server__](https://github.com/apache/httpd/commit/d7328a07d7d293deb5ce62a60c2ce6029104ebad)
  * [__centos-fuzz__](https://github.com/truelq/centos-fuzz)
  * [__FLUFFI__: Fully Localized Utility For Fuzzing Instantaneously by Siemens](https://github.com/siemens/fluffi)
  * [__Fluent Bit__: a fast log processor and forwarder for Linux](https://github.com/fluent/fluent-bit/search?q=honggfuzz&unscoped_q=honggfuzz)
  * [__Samba__: a SMB server](https://github.com/samba-team/samba/blob/2a90202052558c945e02675d1331e65aeb15f9fa/lib/fuzzing/README.md)
  * [__universal-fuzzing-docker__: by nnamon](https://github.com/nnamon/universal-fuzzing-docker)
  * [__Canokey Core__: core implementations of an open-source secure key](https://github.com/canokeys/canokey-core/search?q=honggfuzz&unscoped_q=honggfuzz)
  * [__uberfuzz2__: a cooperative fuzzing framework](https://github.com/acidghost/uberfuzz2)
  * [__TiKV__: a distributed transactional key-value database](https://github.com/tikv/tikv/tree/99a922564face31bdb59b5b38962339f79e0015c/fuzz)
  * [__fuzz-monitor__](https://github.com/acidghost/fuzz-monitor/search?q=honggfuzz&unscoped_q=honggfuzz)
  * [__libmutator__: a C library intended to generate random test cases by mutating legitimate test cases](https://github.com/denandz/libmutator)
  * [__StatZone__: a DNS zone file analyzer](https://github.com/fcambus/statzone)
  * [__shub-fuzz/honggfuzz__: singularity image for honggfuzz](https://github.com/shub-fuzz/honggfuzz)
  * [__Code Intelligence__: fuzzing-as-a-service](https://www.code-intelligence.com/technology.html)
  * [__SpecFuzz__: fuzzing for Spectre vulnerabilities](https://github.com/OleksiiOleksenko/SpecFuzz)
  * [__rcc__: a Rust C compiler](https://github.com/jyn514/rcc#testing)
  * [__EIP1962Fuzzing__: Fuzzy testing of various EIP1962 implementations](https://github.com/matter-labs/eip1962_fuzzing)
  * [__wasm-fuzz__: Fuzzing of wasmer](https://github.com/wasmerio/wasm-fuzz/blob/master/honggfuzz.md), [blog post](https://medium.com/wasmer/fuzz-testing-in-webassembly-vms-3a301f982e5a)
  * [__P0__: Fuzzing ImageIO](https://googleprojectzero.blogspot.com/2020/04/fuzzing-imageio.html)
    * [__TrapFuzz__: by P0](https://github.com/googleprojectzero/p0tools/tree/master/TrapFuzz)
  * [__Rust's fuzztest__](https://docs.rs/crate/fuzztest)
    * [_and multiple Rust projecs_](https://github.com/search?q=%22extern+crate+honggfuzz%22&type=Code)

## Contact

  * User mailing list: [honggfuzz@googlegroups.com](mailto:honggfuzz@googlegroups.com), sign up with [this link](https://groups.google.com/forum/#!forum/honggfuzz).

__This is NOT an official Google product__"
panda-re/panda,169826,1626,141,386,Organization,False,54809,34,16,770,False,Platform for Architecture-Neutral Dynamic Analysis,,3,7,1,62,211,28,19,7,398,4,111,,0,0,0,0,0,0,6,4,,,"# PANDA

[![Build Status](https://travis-ci.org/panda-re/panda.svg?branch=master)](https://travis-ci.org/panda-re/panda)
![Autobuild Docker Container](https://github.com/panda-re/panda/workflows/Build%20and%20Publish%20Docker%20Container/badge.svg)

PANDA is an open-source Platform for Architecture-Neutral Dynamic Analysis. It
is built upon the QEMU whole system emulator, and so analyses have access to all
code executing in the guest and all data. PANDA adds the ability to record and
replay executions, enabling iterative, deep, whole system analyses. Further, the
replay log files are compact and shareable, allowing for repeatable experiments.
A nine billion instruction boot of FreeBSD, e.g., is represented by only a few
hundred MB. PANDA leverages QEMU's support of thirteen different CPU
architectures to make analyses of those diverse instruction sets possible within
the LLVM IR. In this way, PANDA can have a single dynamic taint analysis, for
example, that precisely supports many CPUs. PANDA analyses are written in a
simple plugin architecture which includes a mechanism to share functionality
between plugins, increasing analysis code re-use and simplifying complex
analysis development.

It is currently being developed in collaboration with MIT Lincoln
Laboratory, NYU, and Northeastern University. PANDA is released under
the [GPLv2 license](LICENSE).

---------------------------------------------------------------------

## Building
### Quickstart: Docker
The latest version of PANDA's master branch is automatically built as a docker image
from both Ubuntu Bionic (18.04) and Xenial (16.04). These images are available [here](https://hub.docker.com/r/pandare/panda).

To pull the latest docker container and run PANDA
```
$ docker pull pandare/panda
$ docker run --rm pandare/panda -- /bin/panda-system-i386 --help
```

###  Debian, Ubuntu
Because PANDA has a few dependencies, we've encoded the build instructions into
the [install\_ubuntu.sh](panda/scripts/install\_ubuntu.sh). The script should
work on the latest Debian stable/Ubuntu LTS versions.
If you wish to build PANDA manually, you can also check the
[step-by-step instructions](panda/docs/build\_ubuntu.md) in the documentation
directory.

We currently only vouch for buildability on the latest Debian stable/Ubuntu LTS,
but we welcome pull requests to fix issues with other distros.
For other distributions, it should be straightforward to translate the `apt-get`
commands into whatever package manager your distribution uses.

Note that if you want to use our LLVM features (mainly the dynamic taint
system), you will need to install LLVM 3.3 from OS packages or compiled from
source. On Ubuntu this should happen automatically via `install_ubuntu.sh`.
Additionally, it is **strongly** recommended that you only build PANDA as 64bit
binary. Creating a 32bit build should be possible, but best avoided.
See the limitations section for details.

### Arch
The [install\_arch.sh](panda/scripts/install\_arch.sh) has been contributed
for building PANDA on Arch Linux.
Currently, the script has only been tested on Arch Linux 4.17.5-1-MANJARO.
You can also find
[step-by-step instructions for building on Arch](panda/docs/build\_arch.md)
in the documentation directory.

### MacOS
Building on Mac is less well-tested, but has been known to work. There is a script,
[install\_osx.sh](panda/scripts/install\_osx.sh) to build under OS X.
The script uses [homebrew](https://brew.sh) to install the PANDA dependencies.
As homebrew is known to be very fast in deprecating support for older versions
of OS X and supported packages, expect this to be broken.

### Installation
After successfully building PANDA, you can copy the build to a system-wide
location by running `make install`. The default installation path is `/usr/local`.
You can specify an alternate installation path through the `prefix` configuration
option. E.g. `--prefix=/opt/panda`.  Note that your system must have `chrpath`
installed in order for `make install` to succeed.

If the `bin` directory containing the PANDA binaries is in your `PATH` environment
variable, then you can run PANDA similarly to QEMU:

    panda-system-i386 -m 2G -hda guest.img -monitor stdio

---------------------------------------------------------------------

## Limitations

### LLVM Support
PANDA uses the LLVM architecture from the [S2E project](https://github.com/dslab-epfl/s2e).
This allows translating the TCG intermediate code representation used by QEMU,
to LLVM IR. The latter has the advantages of being easier to work with, as well
as platform independent. This enables the implementation of complex analyses
like the `taint2` plugin.
However, S2E is not actively updated to work with the latest LLVM toolchain.
As a consequence, PANDA still requires specifically LLVM 3.3 in order to be
built with taint analysis support.
of the plugins.

### Cross-architecture record/replay
Great effort is put to maintain the PANDA trace format stable so that existing
traces remain replayable in the future. Changes that will break existing traces
are avoided.
However, currently, record/replay is only guaranteed between PANDA builds of the
same address length. E.g. you can't replay a trace captured on a 32bit build of
PANDA on a 64bit of PANDA. The reason for this is that some raw pointers managed
to creep into the trace format (see headers in `panda/rr`).

Given the memory limitations of 32bit builds, almost all PANDA users use 64bit.
As a result, this issue should affect only a tiny minority of users.
This is also supported by the fact that the issue remained unreported for a
long time (>3 years). Therefore, when a fix is to be implemented, it may be
assessed that migrating existing recordings captured by 32bit builds is not
worth the effort.

For this, it is **strongly** recommended that you only create and use 64bit
builds of PANDA. If you happen to already have a dataset of traces captured
by a 32bit build of PANDA, you should contact the community ASAP to discuss
possible options.

---------------------------------------------------------------------

## Documentation and Support

### PANDA manual
PANDA currently supports whole-system record/replay execution, as well as
time-travel debugging, of x86, x86\_64, and ARM guests.
Details about the implementation and use of PANDA can be found in the
[PANDA manual](panda/docs/manual.md). Some of the topics covered are:

  * [details about record/replay](panda/docs/manual.md#recordreplay-details)
  * the [architecture-neutral plugin interface](panda/docs/manual.md#plugin-architecture)
  * the [callbacks provided by PANDA](panda/docs/manual.md#appendix-a-callback-list)
  * [plugin zoo](panda/docs/manual.md#plugin-zoo)

Documentation for individual plugins is provided by the `README.md` file
in the plugin directory. See [panda/plugins](panda/plugins) directory.

### Support
If you need help with PANDA, or want to discuss the project, you can join our
IRC channel at #panda-re on Freenode, or join the [PANDA mailing
list](http://mailman.mit.edu/mailman/listinfo/panda-users).

---------------------------------------------------------------------

## Publications

* [1] B. Dolan-Gavitt, T. Leek, J. Hodosh, W. Lee.  Tappan Zee (North) Bridge:
Mining Memory Accesses for Introspection. 20th ACM Conference on Computer and
Communications Security (CCS), Berlin, Germany, November 2013.

* [2] R. Whelan, T. Leek, D. Kaeli.  Architecture-Independent Dynamic
Information Flow Tracking. 22nd International Conference on Compiler
Construction (CC), Rome, Italy, March 2013.

* [3] B. Dolan-Gavitt, J. Hodosh, P. Hulin, T. Leek, R. Whelan.
Repeatable Reverse Engineering with PANDA. 5th Program Protection and Reverse
Engineering Workshop, Los Angeles, California, December 2015.

* [4] M. Stamatogiannakis, P. Groth, H. Bos. Decoupling Provenance
Capture and Analysis from Execution. 7th USENIX Workshop on the Theory
and Practice of Provenance, Edinburgh, Scotland, July 2015.

* [5] B. Dolan-Gavitt, P. Hulin, T. Leek, E. Kirda, A. Mambretti,
W. Robertson, F. Ulrich, R. Whelan. LAVA: Large-scale Automated Vulnerability
Addition. 37th IEEE Symposium on Security and Privacy, San Jose,
California, May 2016.

---------------------------------------------------------------------

## Acknowledgements

This material is based upon work supported under Air Force Contract No.
FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions, findings,
conclusions or recommendations expressed in this material are those of
the author(s) and do not necessarily reflect the views of the U.S. Air
Force."
sgminer-dev/sgminer,16123,565,129,854,Organization,False,6498,6,13,84,False,Scrypt GPU miner,,0,16,4,137,244,0,1,17,107,0,0,3400,0,0,0,0,0,0,1,0,,,"# sgminer


## Introduction

This is a multi-threaded multi-pool GPU miner with ATI GPU monitoring,
(over)clocking and fanspeed support for scrypt-based cryptocurrency. It is
based on cgminer by Con Kolivas (ckolivas), which is in turn based on
cpuminer by Jeff Garzik (jgarzik).

**releases**: https://github.com/sgminer-dev/sgminer/releases

**git tree**: https://github.com/sgminer-dev/sgminer

**bugtracker**: https://github.com/sgminer-dev/sgminer/issues

**irc**: `#sgminer` and `#sgminer-dev` on freenode

**mailing lists**: https://sourceforge.net/p/sgminer/mailman/

License: GPLv3.  See `COPYING` for details.


## Documentation

Documentation is available in directory `doc`. It is organised by topics:

* `API` for the RPC API specification;
* `configuration.md` for (largely incomplete) detailed information on all
  configuration options;
* `FAQ.md` for frequently asked questions;
* `GPU` for semi-obsolete information on GPU configuration options and mining
  SHA256d-based coins;
* `kernel.md` for OpenCL kernel-related information, including development
  procedure;
* `MINING.md` for how to find the right balance in GPU configuration to mine
  Scrypt-based coins efficiently;
* `windows-build.txt` for information on how to build on Windows.

Note that **most of the documentation is outdated or incomplete**. If
you want to contribute, fork this repository, update as needed, and
submit a pull request.


## Building

### Dependencies

Mandatory:

* [curl dev library](http://curl.haxx.se/libcurl/) - `libcurl4-openssl-dev` on Debian
* [pkg-config](http://www.freedesktop.org/wiki/Software/pkg-config)
* [libtool](http://www.gnu.org/software/libtool/)
* [AMD APP SDK](http://developer.amd.com/tools-and-sdks/heterogeneous-computing/amd-accelerated-parallel-processing-app-sdk/downloads/) - available under various names as a package on different GNU/Linux distributions

Optional:

* curses dev library - `libncurses5-dev` on Debian or `libpdcurses` on WIN32, for text user interface
* [AMD ADL SDK](http://developer.amd.com/tools-and-sdks/graphics-development/display-library-adl-sdk/) - version 6, required for ATI GPU monitoring & clocking

If building from git:

* autoconf
* automake

sgminer-specific configuration options:

    --disable-adl           Override detection and disable building with adl
 --disable-adl-checks
    --without-curses        Do not compile support for curses TUI

#### Debian Example

    apt-get install libcurl4-openssl-dev pkg-config libtool libncurses5-dev
AMD APP SDK and AMD ADL SDK must be downloaded from the amd websites.

### *nix build instructions

If needed, place include headers (`*.h` files) from `ADL_SDK_*<VERSION>*.zip` in `sgminer/ADL_SDK`.

Then:

    git submodule init
    git submodule update
    autoreconf -i
    CFLAGS=""-O2 -Wall -march=native -std=gnu99"" ./configure <options>
    make

To compile a version that can be used accross machines, remove
`-march=native`.

To compile a debug version, replace `-O2` with `-ggdb`.

Depending on your environment, replace `-std=gnu99` with `-std=c99`.

Systemwide installation is optional. You may run `sgminer` from the build
directory directly, or `make install` if you wish to install
`sgminer` to a system location or a location you specified with `--prefix`.

### Windows build instructions

See `doc/windows-build.txt` for MinGW compilation and cross-compiation,
`doc/cygwin-build.txt` for building using Cygwin, or use the provided
`winbuild` Microsoft Visual Studio project (tested on MSVS2010), with
instructions in `winbuild/README.txt`.


## Basic Usage

**WARNING**: documentation below this point has not been updated since the
fork.

After saving configuration from the menu, you do not need to give sgminer
any arguments and it will load your configuration.

Any configuration file may also contain a single

    ""include"" : ""filename""

to recursively include another configuration file.

Writing the configuration will save all settings from all files in the
output.

Single pool:

sgminer -o http://pool:port -u username -p password

Multiple pools:

sgminer -o http://pool1:port -u pool1username -p pool1password -o http://pool2:port -u pool2usernmae -p pool2password

Single pool with a standard http proxy, regular desktop:

sgminer -o ""http:proxy:port|http://pool:port"" -u username -p password

Single pool with a socks5 proxy, regular desktop:

sgminer -o ""socks5:proxy:port|http://pool:port"" -u username -p password

Single pool with stratum protocol support:

sgminer -o stratum+tcp://pool:port -u username -p password

The list of proxy types are:
 http:    standard http 1.1 proxy
 http0:   http 1.0 proxy
 socks4:  socks4 proxy
 socks5:  socks5 proxy
 socks4a: socks4a proxy
 socks5h: socks5 proxy using a hostname

If you compile sgminer with a version of CURL before 7.19.4 then some of
the above will not be available. All are available since CURL version
7.19.4.

If you specify the --socks-proxy option to sgminer, it will only be
applied to all pools that don't specify their own proxy setting like
above.

For more advanced usage , run `sgminer --help`.

See `doc/GPU` for more information regarding GPU mining and
`doc/SCRYPT` for more information regarding Scrypt mining.


## Runtime usage

The following options are available while running with a single keypress:

[P]ool management [G]PU management [S]ettings [D]isplay options [Q]uit

P gives you:

Current pool management strategy: Failover
[F]ailover only disabled
[A]dd pool [R]emove pool [D]isable pool [E]nable pool
[C]hange management strategy [S]witch pool [I]nformation


S gives you:

[Q]ueue: 1
[S]cantime: 60
[E]xpiry: 120
[W]rite config file
[C]gminer restart


D gives you:

[N]ormal [C]lear [S]ilent mode (disable all output)
[D]ebug:off
[P]er-device:off
[Q]uiet:off
[V]erbose:off
[R]PC debug:off
[W]orkTime details:off
co[M]pact: off
[L]og interval:5


Q quits the application.


G gives you something like:

GPU 0: [124.2 / 191.3 Mh/s] [A:77  R:33  HW:0  U:1.73/m  WU 1.73/m]
Temp: 67.0 C
Fan Speed: 35% (2500 RPM)
Engine Clock: 960 MHz
Memory Clock: 480 Mhz
Vddc: 1.200 V
Activity: 93%
Powertune: 0%
Last initialised: [2011-09-06 12:03:56]
Thread 0: 62.4 Mh/s Enabled ALIVE
Thread 1: 60.2 Mh/s Enabled ALIVE

[E]nable [D]isable [R]estart GPU [C]hange settings
Or press any other key to continue


The running log shows output like this:

 [2012-10-12 18:02:20] Accepted f0c05469 Diff 1/1 GPU 0 pool 1
 [2012-10-12 18:02:22] Accepted 218ac982 Diff 7/1 GPU 1 pool 1
 [2012-10-12 18:02:23] Accepted d8300795 Diff 1/1 GPU 3 pool 1
 [2012-10-12 18:02:24] Accepted 122c1ff1 Diff 14/1 GPU 1 pool 1

The 8 byte hex value are the 2nd 8 bytes of the share being submitted to the
pool. The 2 diff values are the actual difficulty target that share reached
followed by the difficulty target the pool is currently asking for.

The output line shows the following:
(5s):1713.6 (avg):1707.8 Mh/s | A:729  R:8  HW:0  WU:22.53/m

Each column is as follows:
5s:  A 5 second exponentially decaying average hash rate
avg: An all time average hash rate
A:  The total difficulty of Accepted shares
R:  The total difficulty of Rejected shares
HW:  The number of HardWare errors
WU:  The Work Utility defined as the number of diff1 shares work / minute
     (accepted or rejected).

 GPU 1: 73.5C 2551RPM | 427.3/443.0Mh/s | A:8 R:0 HW:0 WU:4.39/m

Each column is as follows:
Temperature (if supported)
Fanspeed (if supported)
A 5 second exponentially decaying average hash rate
An all time average hash rate
The total difficulty of accepted shares
The total difficulty of rejected shares
The number of hardware erorrs
The work utility defined as the number of diff1 shares work / minute

The sgminer status line shows:
 ST: 1  SS: 0  NB: 1  LW: 8  GF: 1  RF: 1

ST is STaged work items (ready to use).
SS is Stale Shares discarded (detected and not submitted so don't count as rejects)
NB is New Blocks detected on the network
LW is Locally generated Work items
GF is Getwork Fail Occasions (server slow to provide work)
RF is Remote Fail occasions (server slow to accept work)

The block display shows:
Block: 0074c5e482e34a506d2a051a...  Started: [17:17:22]  Best share: 2.71K

This shows a short stretch of the current block, when the new block started,
and the all time best difficulty share you've found since starting sgminer
this time.


## Multipool

### Failover strategies

A number of different strategies for dealing with multipool setups are
available. Each has their advantages and disadvantages so multiple strategies
are available by user choice, as per the following list:

#### Failover

The default strategy is failover. This means that if you input a number of
pools, it will try to use them as a priority list, moving away from the 1st
to the 2nd, 2nd to 3rd and so on. If any of the earlier pools recover, it will
move back to the higher priority ones.

#### Round robin

This strategy only moves from one pool to the next when the current one falls
idle and makes no attempt to move otherwise.

#### Rotate

This strategy moves at user-defined intervals from one active pool to the next,
skipping pools that are idle.

#### Load balance

This strategy sends work to all the pools on a quota basis. By default, all
pools are allocated equal quotas unless specified with --quota. This
apportioning of work is based on work handed out, not shares returned so is
independent of difficulty targets or rejected shares. While a pool is disabled
or dead, its quota is dropped until it is re-enabled. Quotas are forward
looking, so if the quota is changed on the fly, it only affects future work.
If all pools are set to zero quota or all pools with quota are dead, it will
fall back to a failover mode. See quota below for more information.

The failover-only flag has special meaning in combination with load-balance
mode and it will distribute quota back to priority pool 0 from any pools that
are unable to provide work for any reason so as to maintain quota ratios
between the rest of the pools.

#### Balance

This strategy monitors the amount of difficulty 1 shares solved for each pool
and uses it to try to end up doing the same amount of work for all pools.


### Quotas

The load-balance multipool strategy works off a quota based scheduler. The
quotas handed out by default are equal, but the user is allowed to specify any
arbitrary ratio of quotas. For example, if all the quota values add up to 100,
each quota value will be a percentage, but if 2 pools are specified and pool0
is given a quota of 1 and pool1 is given a quota of 9, pool0 will get 10% of
the work and pool1 will get 90%. Quotas can be changed on the fly by the API,
and do not act retrospectively. Setting a quota to zero will effectively
disable that pool unless all other pools are disabled or dead. In that
scenario, load-balance falls back to regular failover priority-based strategy.
While a pool is dead, it loses its quota and no attempt is made to catch up
when it comes back to life.

To specify quotas on the command line, pools should be specified with a
semicolon separated --quota(or -U) entry instead of --url. Pools specified with
--url are given a nominal quota value of 1 and entries can be mixed.

For example:
--url poola:porta -u usernamea -p passa --quota ""2;poolb:portb"" -u usernameb -p passb
Will give poola 1/3 of the work and poolb 2/3 of the work.

Writing configuration files with quotas is likewise supported. To use
the above quotas in a configuration file they would be specified thus:

    ""pools"" : [
        {
                ""url"" : ""poola:porta"",
                ""user"" : ""usernamea"",
                ""pass"" : ""passa""
        },
        {
                ""quota"" : ""2;poolb:portb"",
                ""user"" : ""usernameb"",
                ""pass"" : ""passb""
        }
    ]


### Extra File Configuration

If you want to store a number of pools in your configuration file, but
don't always want them automatically enabled at start up (or restart),
then the ""state"" option with a value of ""disabled"" can be used:

    ""pools"" : [
        {
                ""url"" : ""poola:porta"",
                ""user"" : ""usernamea"",
                ""pass"" : ""passa""
        },
        {
                ""quota"" : ""2;poolb:portb"",
                ""user"" : ""usernameb"",
                ""pass"" : ""passb"",
                ""state"" : ""disabled""
        }
    ]

It is then trivial to change the ""state"" setting to ""enabled"" in the
configuration file at anytime and then restart the miner (see below).
You can enable the pool whilst the miner is still running ('p' followed
by 'e' followed by pool number) - but the pool will still be disabled on
restart if the config file is not changed.

""state"" can also be set to ""hidden"". This allows the json file to
contain a large number of pools, of which some could be automatically
culled at start up. This makes it easy to swap pools in and out of the
runtime selection, without having a large list of pools cluttering up
the display.

    ""pools"" : [
        {
                ""poolname"" : ""Main Pool"",
                ""url"" : ""poola:porta"",
                ""user"" : ""usernamea"",
                ""pass"" : ""passa"",
                ""state"" : ""disabled""
        },
        {
                ""poolname"" : ""Joe's Weekend Pool"",
                ""quota"" : ""2;poolb:portb"",
                ""user"" : ""usernameb"",
                ""pass"" : ""passb"",
                ""state"" : ""hidden""
        }
    ]

These options are considered experimental and therefore will NOT be
created when the 'Write config file' option is used ('s' followed by
'w').

A restart of the miner ('s' followed by 'c') will reload the config
file and any changes that may have been made.


## Logging

sgminer will log to stderr if it detects stderr is being redirected to a
file. To enable logging simply append `2>logfile.txt` to your command line
and `logfile.txt` will contain all debug output unless you set `debug-log`
to `false`, in which case it will only contain output at the log level you
specified (notice by default).

There is also the -m option on Linux which will spawn a command of your choice
and pipe the output directly to that command.

The WorkTime details 'debug' option adds details on the end of each line
displayed for Accepted or Rejected work done. An example would be:

 <-00000059.ed4834a3 M:X D:1.0 G:17:02:38:0.405 C:1.855 (2.995) W:3.440 (0.000) S:0.461 R:17:02:47

The first 2 hex codes are the previous block hash, the rest are reported in
seconds unless stated otherwise:
The previous hash is followed by the getwork mode used M:X where X is one of
P:Pool, T:Test Pool, L:LP or B:Benchmark,
then D:d.ddd is the difficulty required to get a share from the work,
then G:hh:mm:ss:n.nnn, which is when the getwork or LP was sent to the pool and
the n.nnn is how long it took to reply,
followed by 'O' on it's own if it is an original getwork, or 'C:n.nnn' if it was
a clone with n.nnn stating how long after the work was recieved that it was cloned,
(m.mmm) is how long from when the original work was received until work started,
W:n.nnn is how long the work took to process until it was ready to submit,
(m.mmm) is how long from ready to submit to actually doing the submit, this is
usually 0.000 unless there was a problem with submitting the work,
S:n.nnn is how long it took to submit the completed work and await the reply,
R:hh:mm:ss is the actual time the work submit reply was received

If you start sgminer with the --sharelog option, you can get detailed
information for each share found. The argument to the option may be ""-"" for
standard output (not advisable with the ncurses UI), any valid positive number
for that file descriptor, or a filename.

To log share data to a file named ""share.log"", you can use either:
./sgminer --sharelog 50 -o xxx -u yyy -p zzz 50>share.log
./sgminer --sharelog share.log -o xxx -u yyy -p zzz

For every share found, data will be logged in a CSV (Comma Separated Value)
format:
    timestamp,disposition,target,pool,dev,thr,sharehash,sharedata
For example (this is wrapped, but it's all on one line for real):
    1335313090,reject,
    ffffffffffffffffffffffffffffffffffffffffffffffffffffffff00000000,
    http://localhost:8337,GPU0,0,
    6f983c918f3299b58febf95ec4d0c7094ed634bc13754553ec34fc3800000000,
    00000001a0980aff4ce4a96d53f4b89a2d5f0e765c978640fe24372a000001c5
    000000004a4366808f81d44f26df3d69d7dc4b3473385930462d9ab707b50498
    f681634a4f1f63d01a0cd43fb338000000000080000000000000000000000000
    0000000000000000000000000000000000000000000000000000000080020000"
haproxy-unofficial-obsolete-mirrors/haproxy,19218,919,98,317,Organization,False,6096,1,196,120,False,UNOFFICIAL fork of haproxy development repository - ISSUE REPORTS ARE IGNORED!,http://haproxy.1wt.eu/,0,6,0,,,,,0,20,0,0,4478,0,0,0,0,0,0,9,0,,,
eunyoung14/mtcp,123163,1484,151,365,Organization,False,384,8,2,18,False,mTCP: A Highly Scalable User-level TCP Stack for Multicore Systems,,0,8,0,62,189,14,5,3,45,3,2,2266,0,0,0,0,0,0,2,2,,mtcp-stack/mtcp,"[![Build Status](https://travis-ci.org/eunyoung14/mtcp.svg?branch=master)](https://travis-ci.org/eunyoung14/mtcp)
[![Build Status](https://scan.coverity.com/projects/11896/badge.svg)](https://scan.coverity.com/projects/eunyoung14-mtcp)

# README

mTCP is a highly scalable user-level TCP stack for multicore systems. 
mTCP source code is distributed under the Modified BSD License. For 
more detail, please refer to the LICENSE. The license term of io_engine 
driver and ported applications may differ from the mTCP’s.

## Prerequisites

We require the following libraries to run mTCP.
- `libdpdk` (Intel's DPDK package*) or `libps` (PacketShader I/O engine library) or `netmap` driver 
- `libnuma`
- `libpthread`
- `librt`
- `libgmp` (for DPDK/ONVM driver)

Compling PSIO/DPDK/NETMAP/ONVM driver requires kernel headers.
- For Debian/Ubuntu, try ``apt-get install linux-headers-$(uname -r)``

We have modified the dpdk package to export net_device stat data 
(for Intel-based Ethernet adapters only) to the OS. To achieve this, we have
created a new LKM dpdk-iface-kmow. We also modified 
``mk/rte.app.mk`` file to ease the compilation
process of mTCP applications. We recommend using our package for DPDK
installation.

### CCP support

You can optionally use [CCP](https://ccp-project.github.io/)'s congestion 
control implementation rather than mTCP's. You'll have wider selection of 
congestion control algorithms with CCP.
(Currently this feature is experimental and under revision.)

Using [CCP](https://ccp-project.github.io/) for congestion control (disabled by
default), requires the CCP library. If you would like to enable CCP, simply run
configure script with `--enable-ccp` option.

1. Install Rust. Any installation method should be fine. We recommend using
   rustup:

    ```bash
    curl https://sh.rustup.rs -sSf | sh -- -y -v --default-toolchain nightly
    ````

2. Install the CCP command line utility:

    ```bash
    cargo install portus --bin ccp
    ```

3. Build the library (comes with Reno and Cubic by default, use `ccp get` to add others):

    ```
    ccp makelib
    ```

4. You will also need to link your application against `-lccp` and `-lstartccp` as demonstrated in apps/example/Makefie.in

## Included directories

mtcp: mtcp source code directory
- mtcp/src: source code
- mtcp/src/include: mTCP’s internal header files
- mtcp/lib: library file
- mtcp/include: header files that applications will use

io_engine: event-driven packet I/O engine (io_engine)
- io_engine/driver - driver source code
- io_engine/lib - io_engine library
- io_engine/include - io_engine header files
- io_engine/samples - sample io_engine applications (not mTCP’s)

dpdk - Intel's Data Plane Development Kit
- dpdk/...

apps: mTCP applications
- apps/example - example applications (see README)
- apps/lighttpd-1.4.32 - mTCP-ported lighttpd (see INSTALL)
- apps/apache_benchmark - mTCP-ported apache benchmark (ab) (see README-mtcp)

util: useful source code for applications

config: sample mTCP configuration files (may not be necessary)


## Install guides

mTCP can be prepared in four ways.

### ***DPDK VERSION***

1. Download DPDK submodule.

    ```bash
    git submodule init
    git submodule update
    ```

2. Setup DPDK.

    ```bash
 ./setup_mtcp_dpdk_env.sh [<path to $RTE_SDK>]
    ```

    - Press [15] to compile x86_64-native-linuxapp-gcc version
    - Press [18] to install igb_uio driver for Intel NICs
    - Press [22] to setup 2048 2MB hugepages
    - Press [24] to register the Ethernet ports
    - Press [35] to quit the tool

    - Only those devices will work with DPDK drivers that are listed
      on this page: http://dpdk.org/doc/nics. Please make sure that your
      NIC is compatible before moving on to the next step.

    - We use `dpdk/` submodule as our DPDK driver. FYI, you can pass a different
      dpdk source directory as command line argument.

3. Bring the dpdk compatible interfaces up, and
   then set RTE_SDK and RTE_TARGET environment variables. If you are using Intel
   NICs, the interfaces will have dpdk prefix.

     ```bash
    sudo ifconfig dpdk0 x.x.x.x netmask 255.255.255.0 up
    export RTE_SDK=`echo $PWD`/dpdk
    export RTE_TARGET=x86_64-native-linuxapp-gcc
     ```

4. Setup mtcp library:

    ```bash
    ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET
    make
    ```

    - By default, mTCP assumes that there are 16 CPUs in your system.
      You can set the CPU limit, e.g. on a 32-core system, by using the following command:

        ```bash
        ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET CFLAGS=""-DMAX_CPUS=32""
        ```
    Please note that your NIC should support RSS queues equal to the MAX_CPUS value
    (since mTCP expects a one-to-one RSS queue to CPU binding).
    
    - In case `./configure` script prints an error, run the
      following command; and then re-do step-4 (configure again):
        ```bash
        autoreconf -ivf
        ```
   
    - checksum offloading in the NIC is now ENABLED (by default)!!!
        - this only works for dpdk at the moment
        - use ```./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --disable-hwcsum``` to disable checksum offloading.
    - check `libmtcp.a` in `mtcp/lib`
    - check header files in `mtcp/include`
    - check example binary files in `apps/example`

5. Check the configurations in `apps/example`
   - `epserver.conf` for server-side configuration
   - `epwget.conf` for client-side configuration
   - you may write your own configuration file for your application

6. Run the applications!

7. You can revert back all your changes by running the following script.

    ```bash
    ./setup_linux_env.sh [<path to $RTE_SDK>]
    ```
   
    - Press [29] to unbind the Ethernet ports
    - Press [30] to remove igb_uio.ko driver
    - Press [33] to remove hugepage mappings
    - Press [34] to quit the tool


### ***PSIO VERSION***

1. make in io_engine/driver:

    ```bash
    make
    ```

    - check ps_ixgbe.ko
    - please note that psio only runs on linux-2.6.x kernels
      (linux-2.6.32 ~ linux-2.6.38)

2. install the driver:
   
   ```bash
   ./install.py <# cores> <# cores>
   ```

    - refer to http://shader.kaist.edu/packetshader/io_engine/
    - you may need to change the ip address in install.py:46

3. Setup mtcp library:
   
    ```bash
    ./configure --with-psio-lib=<$path_to_ioengine>
    # e.g. ./configure --with-psio-lib=`echo $PWD`/io_engine
    make
    ```

    - By default, mTCP assumes that there are 16 CPUs in your system.
      You can set the CPU limit, e.g. on a 8-core system, by using the following command:

        ```bash
        ./configure --with-psio-lib=`echo $PWD`/io_engine CFLAGS=""-DMAX_CPUS=8""
        ```
    
    Please note that your NIC should support RSS queues equal to the MAX_CPUS value
    (since mTCP expects a one-to-one RSS queue to CPU binding).

    - In case `./configure` script prints an error, run the
      following command; and then re-do step-3 (configure again):

        ```bash
        autoreconf -ivf
        ```

    - check `libmtcp.a` in `mtcp/lib`
    - check header files in `mtcp/include`
    - check example binary files in `apps/example`

4. Check the configurations in `apps/example`
   - `epserver.conf` for server-side configuration
   - `epwget.conf` for client-side configuration
   - you may write your own configuration file for your application

5. Run the applications!


### ***ONVM VERSION***

***NEW***: Now you can run mTCP applications (server + client) locally.
A local setup is useful when only 1 machine is available for the experiment. 
ONVM configurations are placed as `.conf` files in apps/example directory.
ONVM basics are explained in https://github.com/sdnfv/openNetVM.

**Before running the applications make sure that onvm_mgr is running.**  
*Also, no core overlap between applications and onvm_mgr is allowed.*

1. [Install openNetVM following these instructions](https://github.com/sdnfv/openNetVM/blob/master/docs/Install.md)

2. Set up the dpdk interfaces:

    ```bash
 ./setup_mtcp_onvm_env.sh
    ```

3. Next bring the dpdk-registered interfaces up. This can be setup using:  

    ```bash
    sudo ifconfig dpdk0 x.x.x.x netmask 255.255.255.0 up
    ```

4. Setup mtcp library
    ```bash
    ./configure --with-dpdk-lib=$<path_to_dpdk> --with-onvm-lib=$<path_to_onvm_lib>
    # e.g. ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --with-onvm-lib=`echo $ONVM_HOME`/onvm
    make
    ```

    - By default, mTCP assumes that there are 16 CPUs in your system.
    You can set the CPU limit, e.g. on a 32-core system, by using the following command:
    
        ```bash
        ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --with-onvm-lib=$<path_to_onvm_lib> CFLAGS=""-DMAX_CPUS=32""
        ```

    Please note that your NIC should support RSS queues equal to the MAX_CPUS value
    (since mTCP expects a one-to-one RSS queue to CPU binding).
    
    - In case `./configure` script prints an error, run the
    following command; and then re-do step-4 (configure again):
    
        ```bash
        autoreconf -ivf
        ```

    - checksum offloading in the NIC is now ENABLED (by default)!!!
    - this only works for dpdk at the moment
    - use ```./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --with-onvm-lib=$<path_to_onvm_lib> --disable-hwcsum``` to disable checksum offloading.
    - check `libmtcp.a` in `mtcp/lib`
    - check header files in `mtcp/include`
    - check example binary files in `apps/example`

5. Check the configurations in `apps/example`
   - `epserver.conf` for server-side configuration
   - `epwget.conf` for client-side configuration
   - you may write your own configuration file for your application

6. Run the applications!

7. You can revert back all your changes by running the following script.

    ```bash
    ./setup_linux_env.sh
    ```
   
    - Press [29] to unbind the Ethernet ports
    - Press [30] to remove igb_uio.ko driver
    - Press [33] to remove hugepage mappings
    - Press [34] to quit the tool

**Notes**

Once you have started onvm_mgr, sometimes an mTCP application may fail to get launched due
to an error resembling the one mentioned below:

- ```EAL: FATAL: Cannot init memory```
- ``` Cannot mmap memory for rte_config at [0x7ffff7fb6000], got [0x7ffff7e74000] - please use '--base-virtaddr' option```
- ```EAL: Cannot mmap device resource file /sys/bus/pci/devices/0000:06:00.0/resource3 to address: 0x7ffff7ff1000```

To prevent this, use the base virtual address parameter to run the ONVM manager (core list arg `0xf8` isn't actually used by mtcp NFs but is required), e.g.:

```bash
cd openNetVM/onvm  
./go.sh 1,2,3 1 0xf8 -s stdout -a 0x7f000000000 
```

### ***NETMAP VERSION***

See README.netmap for details.


## Tested environments

mTCP runs on Linux-based operating systems (2.6.x for PSIO) with generic 
x86_64 CPUs, but to help evaluation, we provide our tested environments 
as follows.

    Intel Xeon E5-2690 octacore CPU @ 2.90 GHz 32 GB of RAM (4 memory channels)
    10 GbE NIC with Intel 82599 chipset (specifically Intel X520-DA2)
    Debian 6.0.7 (Linux 2.6.32-5-amd64)

    Intel Core i7-3770 quadcore CPU @ 3.40 GHz 16 GB of RAM (2 memory channels)
    10 GbE NIC with Intel 82599 chipset (specifically Intel X520-DA2)
    Ubuntu 10.04 (Linux 2.6.32-47)

Event-driven PacketShader I/O engine (extended io_engine-0.2)

- PSIO is currently only compatible with Linux-2.6.

We tested the DPDK version (polling driver) with Linux-3.13.0 kernel.

## Notes

1. mTCP currently runs with fixed memory pools. That means, the size of
   TCP receive and send buffers are fixed at the startup and does not 
   increase dynamically. This could be performance limit to the large 
   long-lived connections. Be sure to configure the buffer size 
   appropriately to your size of workload.

2. The client side of mTCP supports mtcp_init_rss() to create an 
   address pool that can be used to fetch available address space in 
   O(1). To easily congest the server side, this function should be 
   called at the application startup.

3. The supported socket options are limited for right now. Please refer 
   to the mtcp/src/api.c for more detail.

4. The counterpart of mTCP should enable TCP timestamp.

5. mTCP has been tested with the following Ethernet adapters:

    1. Intel-82598       ixgbe          (Max-queue-limit: 16)
    2. Intel-82599       ixgbe          (Max-queue-limit: 16)
    3. Intel-I350        igb            (Max-queue-limit: 08)
    4. Intel-X710        i40e           (Max-queue-limit: ~)
    5. Intel-X722        i40e           (Max-queue-limit: ~)
 
## Frequently asked questions

1. How can I quit the application?
    - Use ^C to gracefully shutdown the application. Two consecutive 
    ^C (separated by 1 sec) will force quit.

2. My application doesn't use the address specified from ifconfig.
    - For some Linux distros(e.g. Ubuntu), NetworkManager may re-assign
    a different IP address, or delete the assigned IP address.

    - Disable NetworkManager temporarily if that's the case.
    NetworkManager will be re-enabled upon reboot.

     ```bash
    sudo service network-manager stop
     ```

3. Can I statically set the routing or arp table?
    - Yes, mTCP allows static route and arp configuration. Go to the 
    config directory and see sample_route.conf or sample_arp.conf. 
    Copy and adapt it to your condition and link (ln -s) the config 
    directory to the application directory. mTCP will find 
    config/route.conf and config/arp.conf for static configuration.

## Caution

1. Do not remove I/O driver (```ps_ixgbe/igb_uio```) while running mTCP 
   applications. The application will panic!

2. Use the ps_ixgbe/dpdk driver contained in this package, not the one 
   from some other place (e.g., from io_engine github).

## Contacts

GitHub issue board is the preferred way to report bugs and ask questions about mTCP.

***CONTACTS FOR THE AUTHORS***

    User mailing list <mtcp-user at list.ndsl.kaist.edu>
    EunYoung Jeong <notav at ndsl.kaist.edu>
    M. Asim Jamshed <ajamshed at ndsl.kaist.edu>"
the-tcpdump-group/tcpdump,25285,1326,122,574,Organization,False,6157,20,42,119,False,the TCPdump network dissector,https://www.tcpdump.org/,8,60,2,60,491,12,10,33,273,2,19,,0,0,0,0,0,0,8,4,,,"# tcpdump

[![Build Status](https://travis-ci.org/the-tcpdump-group/tcpdump.svg?branch=master)](https://travis-ci.org/the-tcpdump-group/tcpdump)

[![Build Status](https://ci.appveyor.com/api/projects/status/github/the-tcpdump-group/tcpdump?branch=master&svg=true)](https://ci.appveyor.com/project/guyharris/tcpdump)

To report a security issue please send an e-mail to security@tcpdump.org.

To report bugs and other problems, contribute patches, request a
feature, provide generic feedback etc please see the file
CONTRIBUTING in the tcpdump source tree root.

TCPDUMP 4.x.y
Now maintained by ""The Tcpdump Group""
See   https://www.tcpdump.org

Anonymous Git is available via:

 git clone git://bpf.tcpdump.org/tcpdump

formerly from  Lawrence Berkeley National Laboratory
  Network Research Group <tcpdump@ee.lbl.gov>
  ftp://ftp.ee.lbl.gov/old/tcpdump.tar.Z (3.4)

This directory contains source code for tcpdump, a tool for network
monitoring and data acquisition.  This software was originally
developed by the Network Research Group at the Lawrence Berkeley
National Laboratory.  The original distribution is available via
anonymous ftp to `ftp.ee.lbl.gov`, in `tcpdump.tar.Z`.  More recent
development is performed at tcpdump.org, https://www.tcpdump.org/.

Tcpdump uses libpcap, a system-independent interface for user-level
packet capture.  Before building tcpdump, you must first retrieve and
build libpcap, also originally from LBL and now being maintained by
tcpdump.org; see https://www.tcpdump.org/.

Once libpcap is built (either install it or make sure it's in
`../libpcap`), you can build tcpdump using the procedure in the `INSTALL.txt`
file.

The program is loosely based on SMI's ""etherfind"" although none of the
etherfind code remains.  It was originally written by Van Jacobson as
part of an ongoing research project to investigate and improve tcp and
internet gateway performance.  The parts of the program originally
taken from Sun's etherfind were later re-written by Steven McCanne of
LBL.  To insure that there would be no vestige of proprietary code in
tcpdump, Steve wrote these pieces from the specification given by the
manual entry, with no access to the source of tcpdump or etherfind.

Over the past few years, tcpdump has been steadily improved by the
excellent contributions from the Internet community (just browse
through the `CHANGES` file).  We are grateful for all the input.

Richard Stevens gives an excellent treatment of the Internet protocols
in his book *""TCP/IP Illustrated, Volume 1""*. If you want to learn more
about tcpdump and how to interpret its output, pick up this book.

Some tools for viewing and analyzing tcpdump trace files are available
from the Internet Traffic Archive:

* http://ita.ee.lbl.gov/

Another tool that tcpdump users might find useful is tcpslice:

* https://github.com/the-tcpdump-group/tcpslice

It is a program that can be used to extract portions of tcpdump binary
trace files. See the above distribution for further details and
documentation.

Current versions can be found at https://www.tcpdump.org.

 - The TCPdump group

original text by: Steve McCanne, Craig Leres, Van Jacobson

-------------------------------------
```
This directory also contains some short awk programs intended as
examples of ways to reduce tcpdump data when you're tracking
particular network problems:

send-ack.awk
 Simplifies the tcpdump trace for an ftp (or other unidirectional
 tcp transfer).  Since we assume that one host only sends and
 the other only acks, all address information is left off and
 we just note if the packet is a ""send"" or an ""ack"".

 There is one output line per line of the original trace.
 Field 1 is the packet time in decimal seconds, relative
 to the start of the conversation.  Field 2 is delta-time
 from last packet.  Field 3 is packet type/direction.
 ""Send"" means data going from sender to receiver, ""ack""
 means an ack going from the receiver to the sender.  A
 preceding ""*"" indicates that the data is a retransmission.
 A preceding ""-"" indicates a hole in the sequence space
 (i.e., missing packet(s)), a ""#"" means an odd-size (not max
 seg size) packet.  Field 4 has the packet flags
 (same format as raw trace).  Field 5 is the sequence
 number (start seq. num for sender, next expected seq number
 for acks).  The number in parens following an ack is
 the delta-time from the first send of the packet to the
 ack.  A number in parens following a send is the
 delta-time from the first send of the packet to the
 current send (on duplicate packets only).  Duplicate
 sends or acks have a number in square brackets showing
 the number of duplicates so far.

 Here is a short sample from near the start of an ftp:
  3.00    0.20   send . 512
  3.20    0.20    ack . 1024  (0.20)
  3.20    0.00   send P 1024
  3.40    0.20    ack . 1536  (0.20)
  3.80    0.40 * send . 0  (3.80) [2]
  3.82    0.02 *  ack . 1536  (0.62) [2]
 Three seconds into the conversation, bytes 512 through 1023
 were sent.  200ms later they were acked.  Shortly thereafter
 bytes 1024-1535 were sent and again acked after 200ms.
 Then, for no apparent reason, 0-511 is retransmitted, 3.8
 seconds after its initial send (the round trip time for this
 ftp was 1sec, +-500ms).  Since the receiver is expecting
 1536, 1536 is re-acked when 0 arrives.

packetdat.awk
 Computes chunk summary data for an ftp (or similar
 unidirectional tcp transfer). [A ""chunk"" refers to
 a chunk of the sequence space -- essentially the packet
 sequence number divided by the max segment size.]

 A summary line is printed showing the number of chunks,
 the number of packets it took to send that many chunks
 (if there are no lost or duplicated packets, the number
 of packets should equal the number of chunks) and the
 number of acks.

 Following the summary line is one line of information
 per chunk.  The line contains eight fields:
    1 - the chunk number
    2 - the start sequence number for this chunk
    3 - time of first send
    4 - time of last send
    5 - time of first ack
    6 - time of last ack
    7 - number of times chunk was sent
    8 - number of times chunk was acked
 (all times are in decimal seconds, relative to the start
 of the conversation.)

 As an example, here is the first part of the output for
 an ftp trace:

 # 134 chunks.  536 packets sent.  508 acks.
 1       1       0.00    5.80    0.20    0.20    4       1
 2       513     0.28    6.20    0.40    0.40    4       1
 3       1025    1.16    6.32    1.20    1.20    4       1
 4       1561    1.86    15.00   2.00    2.00    6       1
 5       2049    2.16    15.44   2.20    2.20    5       1
 6       2585    2.64    16.44   2.80    2.80    5       1
 7       3073    3.00    16.66   3.20    3.20    4       1
 8       3609    3.20    17.24   3.40    5.82    4       11
 9       4097    6.02    6.58    6.20    6.80    2       5

 This says that 134 chunks were transferred (about 70K
 since the average packet size was 512 bytes).  It took
 536 packets to transfer the data (i.e., on the average
 each chunk was transmitted four times).  Looking at,
 say, chunk 4, we see it represents the 512 bytes of
 sequence space from 1561 to 2048.  It was first sent
 1.86 seconds into the conversation.  It was last
 sent 15 seconds into the conversation and was sent
 a total of 6 times (i.e., it was retransmitted every
 2 seconds on the average).  It was acked once, 140ms
 after it first arrived.

stime.awk
atime.awk
 Output one line per send or ack, respectively, in the form
  <time> <seq. number>
 where <time> is the time in seconds since the start of the
 transfer and <seq. number> is the sequence number being sent
 or acked.  I typically plot this data looking for suspicious
 patterns.


The problem I was looking at was the bulk-data-transfer
throughput of medium delay network paths (1-6 sec.  round trip
time) under typical DARPA Internet conditions.  The trace of the
ftp transfer of a large file was used as the raw data source.
The method was:

  - On a local host (but not the Sun running tcpdump), connect to
    the remote ftp.

  - On the monitor Sun, start the trace going.  E.g.,
      tcpdump host local-host and remote-host and port ftp-data >tracefile

  - On local, do either a get or put of a large file (~500KB),
    preferably to the null device (to minimize effects like
    closing the receive window while waiting for a disk write).

  - When transfer is finished, stop tcpdump.  Use awk to make up
    two files of summary data (maxsize is the maximum packet size,
    tracedata is the file of tcpdump tracedata):
      awk -f send-ack.awk packetsize=avgsize tracedata >sa
      awk -f packetdat.awk packetsize=avgsize tracedata >pd

  - While the summary data files are printing, take a look at
    how the transfer behaved:
      awk -f stime.awk tracedata | xgraph
    (90% of what you learn seems to happen in this step).

  - Do all of the above steps several times, both directions,
    at different times of day, with different protocol
    implementations on the other end.

  - Using one of the Unix data analysis packages (in my case,
    S and Gary Perlman's Unix|Stat), spend a few months staring
    at the data.

  - Change something in the local protocol implementation and
    redo the steps above.

  - Once a week, tell your funding agent that you're discovering
    wonderful things and you'll write up that research report
    ""real soon now"".
```"
cesanta/v7,19733,1248,93,160,Organization,False,1911,7,3,17,False,Embedded JavaScript engine for C/C++,,0,11,4,48,103,0,0,2,436,0,0,2378,0,0,0,0,0,0,58,1,,,"V7: Embedded JavaScript engine
==============================

**NOTE: this project is deprecated in favor of https://github.com/cesanta/mjs**

[![License](https://img.shields.io/badge/license-GPL_2-green.svg)](https://github.com/cesanta/v7/blob/master/LICENSE)

V7 is the smallest JavaScript engine written in C. V7 features are:

- Cross-platform: works on anything, starting from Arduino to MS Windows
- Small size. Compiled static size is in 40k - 120k range, RAM
  footprint on initialization is about 800 bytes with freeze feature,
  15k without freeze feature
- Simple and intuitive C/C++ API. It is easy to export existing C/C++
  functions into JavaScript environment
- Standard: V7 implements JavaScript 5.1
- Usable out-of-the-box: V7 provides an auxiliary library with
  Hardware (SPI, UART, etc), File, Crypto, Network API
- Source code is both ISO C and ISO C++ compliant
- Very easy to integrate: simply copy two files: [v7.h](v7.h)
   and [v7.c](v7.c) into your project

V7 makes it possible to program Internet of Things (IoT) embedded devices
in JavaScript. V7 is a part of the full stack
[Mongoose OS Platform](https://github.com/cesanta/mongoose-os).

## Examples & Documentation

- [Developer Centre](https://docs.cesanta.com/v7/dev) - User Guide and API reference
- [Examples](https://github.com/cesanta/v7/tree/master/examples) - Collection of well-commented examples
- [Support Forum](http://forum.cesanta.com/index.php?p=/categories/v7) - Ask questions on our support forum

# Contributions

To submit contributions, sign
[Cesanta CLA](https://docs.cesanta.com/contributors_la.shtml)
and send GitHub pull request. You retain the copyright on your contributions.

# Licensing

V7 is released under commercial and [GNU GPL v.2](http://www.gnu.org/licenses/old-licenses/gpl-2.0.html) open source licenses.

Commercial Projects:
Once your project becomes commercialised GPLv2 licensing dictates that you need to either open your source fully or purchase a commercial license. Cesanta offer full, royalty-free commercial licenses without any GPL restrictions. If your needs require a custom license, we’d be happy to work on a solution with you. [Contact us for pricing.] (https://www.cesanta.com/contact)

Prototyping:
While your project is still in prototyping stage and not for sale, you can use V7’s open source code without license restrictions."
a0rtega/pafish,1147,1399,159,303,User,False,166,2,16,9,False,Pafish is a demonstration tool that employs several techniques to detect sandboxes and analysis environments in the same way as malware families do.,,6,7,0,13,23,0,0,0,30,0,0,2399,0,0,0,0,0,0,15,,222,,"# Pafish
## (Paranoid Fish)

Pafish is a demonstration tool that employs several techniques to detect sandboxes and analysis environments in the same way as malware families do.

The project is open source, you can read the code of all anti-analysis checks. You can also **[download](https://github.com/a0rtega/pafish/raw/master/pafish.exe)** the executable of the latest stable version.

It is licensed under GNU/GPL version 3.

![Pafish screenshot](https://raw.githubusercontent.com/a0rtega/pafish/dev-chaos/screenshots/v057/pafish_vbox_win8.png)

# Scope

The objective of this project is to collect usual tricks seen in malware samples. This allows us to study them, and test if our analysis environments are properly implemented.

# Build

Pafish is written in C and can be built with MinGW (gcc + make).

Check out ""[How to build](https://github.com/a0rtega/pafish/wiki/How-to-build)"" for detailed instructions.

# Author

Alberto Ortega (@[a0rtega](https://twitter.com/#!/a0rtega) - [profile](http://aortega.badtrace.com))"
traviscross/mtr,1304,1447,87,245,User,False,656,4,73,53,False,"Official repository for mtr, a network diagnostic tool",http://www.bitwizard.nl/mtr/,0,6,0,97,92,12,7,7,155,1,5,7908,4,8,34,14,0,0,103,,65,,"WHAT IS MTR?
===

mtr combines the functionality of the 'traceroute' and 'ping' programs
in a single network diagnostic tool.

As mtr starts, it investigates the network connection between the host
mtr runs on and a user-specified destination host.  After it
determines the address of each network hop between the machines,
it sends a sequence of ICMP ECHO requests to each one to determine the
quality of the link to each machine.  As it does this, it prints
running statistics about each machine.

mtr is distributed under the GNU General Public License version 2.
See the COPYING file for details.

INSTALLING
===

If you're building this from a tarball, compiling mtr is as
simple as:

 ./configure && make

(in the past, there was a Makefile in the distribution that did
the `./configure` for you and then ran make again with the generated
Makefile, but this has suffered some bitrot. It didn't work well
with git.)

If you're building from the git repository, you'll need to run:

 ./bootstrap.sh && ./configure && make

When it looks as if the compilation was succesful, you can
test mtr with

 sudo ./mtr <host>

(fill in a hostname or IP address where it says <host>) or
immediately continue on to installing:

 make install

Note that mtr-packet must be suid-root because it requires access to
raw IP sockets.  See SECURITY for security information.

Older versions used to require a non-existent path to GTK for a
correct build of a non-gtk version while GTK was installed. This is
no longer necessary. `./configure --without-gtk` should now work.
If it doesn't, try `make WITHOUT_X11=YES` as the make step.

On Solaris, you'll need to use GNU make to build.
(Use `gmake` rather than `make`.)

On Solaris (and possibly other systems) the ""gtk"" library may be
installed in a directory where the dynamic linker refuses to look when
a binary is setuid. Roman Shterenzon reports that adding
        -Wl,-rpath=/usr/lib
to the commandline will work if you are using gnu LD. He tells me that
you're out of luck when you use the sun LD. That's not quite true, as
you can move the gtk libraries to `/usr/lib` instead of leaving them in
`/usr/local/lib`.  (when the ld tells you that `/usr/local/lib` is untrusted
and `/usr/lib` is trusted, and you trust the gtk libs enough to want them
in a setuid program, then there is something to say for moving them
to the ""trusted"" directory.)

Building on MacOS should not require any special steps.

BUILDING FOR WINDOWS
===

Building for Windows requires Cygwin.  To obtain Cygwin, see
https://cygwin.com/install.html.
Next, re-run cygwin's `setup-x86.exe` (or `setup-x86_64.exe` if you're using 64bit cygwin) with the following arguments,  
which will install the packages required for building:

        setup-x86.exe --package-manager --wait --packages automake,pkg-config,make,gcc-core,libncurses-devel

Build as under Unix:

        ./bootstrap.sh && ./configure && make

Finally, install the built binaries:

        make install


WHERE CAN I GET THE LATEST VERSION OR MORE INFORMATION?
===

mtr is now hosted on github.
https://github.com/traviscross/mtr

See the mtr web page at http://www.BitWizard.nl/mtr/

Bug reports and feature requests should be submitted to the Github bug tracking system.

Patches can be submitted by cloning the Github repository and issuing
a pull request, or by email to me. Please use unified diffs. Usually
the diff is sort of messy, so please check that the diff is clean and
doesn't contain too much of your local stuff (for example, I don't
want/need the ""configure"" script that /your/ automake made for you).

(There used to be a mailinglist, but all it got was spam. So
when the server was upgraded, the mailing list died.)


REW"
Celtoys/Remotery,1231,1466,77,162,User,False,666,2,0,30,False,"Single C file, Realtime CPU/GPU Profiler with Remote Web Viewer",,8,7,0,28,74,0,0,2,63,0,3,2287,4,11,296,193,0,0,17,,94,,
jgarff/rpi_ws281x,362,1225,107,465,User,False,211,3,0,49,False,Userspace Raspberry Pi PWM library for WS281X LEDs,,0,8,0,67,216,8,4,2,117,1,5,2112,4,4,20,13,0,0,2,,63,,"rpi_ws281x
==========

Userspace Raspberry Pi library for controlling WS281X LEDs.
This includes WS2812 and SK6812RGB RGB LEDs
Preliminary support is now included for SK6812RGBW LEDs (yes, RGB + W)
The LEDs can be controlled by either the PWM (2 independent channels)
or PCM controller (1 channel) or the SPI interface (1 channel).

### Bindings:

Language-specific bindings for rpi_ws281x are available in:

* Python - https://github.com/rpi-ws281x/rpi-ws281x-python
* Rust - https://github.com/rpi-ws281x/rpi-ws281x-rust
* Powershell - https://github.com/rpi-ws281x/rpi-ws281x-powershell
* Java - https://github.com/rpi-ws281x/rpi-ws281x-java
* CSharp - https://github.com/rpi-ws281x/rpi-ws281x-csharp
* Go - https://github.com/rpi-ws281x/rpi-ws281x-go

### Background:

The BCM2835 in the Raspberry Pi has both a PWM and a PCM module that
are well suited to driving individually controllable WS281X LEDs.
Using the DMA, PWM or PCM FIFO, and serial mode in the PWM, it's
possible to control almost any number of WS281X LEDs in a chain connected
to the appropriate output pin.
For SPI the Raspbian spidev driver is used (`/dev/spidev0.0`).
This library and test program set the clock rate to 3X the desired output
frequency and creates a bit pattern in RAM from an array of colors where
each bit is represented by 3 bits as follows.

    Bit 1 - 1 1 0
    Bit 0 - 1 0 0


### GPIO Usage:

The GPIOs that can be used are limited by the hardware of the Pi and will
vary based on the method used to drive them (PWM, PCM or SPI).
Beware that the GPIO numbers are not the same as the physical pin numbers
on the header.

PWM:
```
        PWM0, which can be set to use GPIOs 12, 18, 40, and 52.
        Only 12 (pin 32) and 18 (pin 12) are available on the B+/2B/3B

        PWM1 which can be set to use GPIOs 13, 19, 41, 45 and 53.
        Only 13 is available on the B+/2B/PiZero/3B, on pin 33
```

PCM:
```
        PCM_DOUT, which can be set to use GPIOs 21 and 31.
        Only 21 is available on the B+/2B/PiZero/3B, on pin 40.
```

SPI:
```
        SPI0-MOSI is available on GPIOs 10 and 38.
        Only GPIO 10 is available on all models.
        See also note for RPi 3 below.
```


### Power and voltage requirements

WS281X LEDs are generally driven at 5V. Depending on your actual
LED model and data line length you might be able to successfully drive
the data input with 3.3V. However in the general case you probably
want to use a level shifter to convert from the Raspberry Pi GPIO/PWM to 5V.

It is also possible to run the LEDs from a 3.3V - 3.6V power source, and
connect the GPIO directly at a cost of brightness, but this isn't
recommended.

The test program is designed to drive a 8x8 grid of LEDs e.g.from
Adafruit (http://www.adafruit.com/products/1487) or Pimoroni
(https://shop.pimoroni.com/products/unicorn-hat).
Please see the Adafruit and Pimoroni websites for more information.

Know what you're doing with the hardware and electricity.  I take no
reponsibility for damage, harm, or mistakes.

### Build:

- Install Scons (on raspbian, `apt-get install scons`).
- Make sure to adjust the parameters in main.c to suit your hardware.
  - Signal rate (400kHz to 800kHz).  Default 800kHz.
  - ledstring.invert=1 if using a inverting level shifter.
  - Width and height of LED matrix (height=1 for LED string).
- Type `scons` from inside the source directory.

### Running:

- Type `sudo ./test` (default uses PWM channel 0).
- That's it.  You should see a moving rainbow scroll across the
  display.
- More options are available, `./test -h` should show them:
```
./test version 1.1.0
Usage: ./test
-h (--help)    - this information
-s (--strip)   - strip type - rgb, grb, gbr, rgbw
-x (--width)   - matrix width (default 8)
-y (--height)  - matrix height (default 8)
-d (--dma)     - dma channel to use (default 10)
-g (--gpio)    - GPIO to use
                 If omitted, default is 18 (PWM0)
-i (--invert)  - invert pin output (pulse LOW)
-c (--clear)   - clear matrix on exit.
-v (--version) - version information
```

### Important warning about DMA channels

You must make sure that the DMA channel you choose to use for the LEDs is not [already in use](https://www.raspberrypi.org/forums/viewtopic.php?p=609380#p609380) by the operating system.

For example, **using DMA channel 5 [will cause](https://github.com/jgarff/rpi_ws281x/issues/224) filesystem corruption** on the Raspberry Pi 3 Model B.

The default DMA channel (10) should be safe for the Raspberry Pi 3 Model B, but this may change in future software releases.

### Limitations:

#### PWM

Since this library and the onboard Raspberry Pi audio
both use the PWM, they cannot be used together.  You will need to
blacklist the Broadcom audio kernel module by creating a file
`/etc/modprobe.d/snd-blacklist.conf` with

    blacklist snd_bcm2835

If the audio device is still loading after blacklisting, you may also
need to comment it out in the /etc/modules file.

On headless systems you may also need to force audio through hdmi
Edit config.txt and add:

    hdmi_force_hotplug=1
    hdmi_force_edid_audio=1

A reboot is required for this change to take effect

Some distributions use audio by default, even if nothing is being played.
If audio is needed, you can use a USB audio device instead.

#### PCM

When using PCM you cannot use digital audio devices which use I2S since I2S
uses the PCM hardware, but you can use analog audio.

#### SPI

When using SPI the ledstring is the only device which can be connected to
the SPI bus. Both digital (I2S/PCM) and analog (PWM) audio can be used.

Many distributions have a maximum SPI transfer of 4096 bytes. This can be
changed in `/boot/cmdline.txt` by appending
```
    spidev.bufsiz=32768
```
On a RPi 3 you have to change the GPU core frequency to 250 MHz, otherwise
the SPI clock has the wrong frequency.
Do this by adding the following line to /boot/config.txt and reboot.
```
    core_freq=250
```

SPI requires you to be in the `gpio` group if you wish to control your LEDs
without root.

### Comparison PWM/PCM/SPI

Both PWM and PCM use DMA transfer to output the control signal for the LEDs.
The max size of a DMA transfer is 65536 bytes. Since each LED needs 12 bytes
(4 colors, 8 symbols per color, 3 bits per symbol) this means you can
control approximately 5400 LEDs for a single strand in PCM and 2700 LEDs per string
for PWM (Only PWM can control 2 independent strings simultaneously)
SPI uses the SPI device driver in the kernel. For transfers larger than
96 bytes the kernel driver also uses DMA.
Of course there are practical limits on power and signal quality. These will
be more constraining in practice than the theoretical limits above.

When controlling a LED string of 240 LEDs the CPU load on the original Pi 2 (BCM2836) are:
  PWM  5%
  PCM  5%
  SPI  1%

### Usage:

The API is very simple.  Make sure to create and initialize the `ws2811_t`
structure as seen in [`main.c`](main.c).  From there it can be initialized
by calling `ws2811_init()`.  LEDs are changed by modifying the color in
the `.led[index]` array and calling `ws2811_render()`.
The rest is handled by the library, which either creates the DMA memory and
starts the DMA for PWM and PCM or prepares the SPI transfer buffer and sends
it out on the MISO pin.

Make sure to hook a signal handler for SIGKILL to do cleanup.  From the
handler make sure to call `ws2811_fini()`.  It'll make sure that the DMA
is finished before program execution stops and cleans up after itself."
EtchedPixels/FUZIX,13728,1172,128,143,User,False,7573,1,0,30,False,FuzixOS: Because Small Is Beautiful,,0,14,1,40,274,8,12,1,475,0,9,2056,5,14,85,67,0,0,24,,198,,"[![Build Status][travis-image]][travis-url]

**FuzixOS**: Because Small Is Beautiful

This is the initial public tree for the FuzixOS project. It is not yet useful although you can build and boot it and run
test application code. A lot of work is needed on the utilities and libraries.

# FUZIX

FUZIX is a fusion of various elements from the assorted UZI forks and
branches beaten together into some kind of semi-coherent platform and then
extended from V7 to somewhere in the SYS3 to SYS5.x world with bits of POSIX
thrown in for good measure. Various learnings and tricks from ELKS and from
OMU also got blended in

# Pre-built images

Some pre-built filesystems are now available on www.fuzix.org, and other
images should follow in time.

## What does FUZIX have over UZI


* Support for multiple processes in banked memory (as per UZI180) but
 with Minix style chmem and efficient use of bank allocations.
* Support for multiple processes via hard disk or non mappable RAM
    drive switching (as per UZI, UZIX).
* Support for ""real"" swapping combined with banked memory.
* Proper sane off_t and lseek
* Normal dev_t
* 30 character filenames
* Proper sane time_t
* System 5 signals
* Posix termios (does all the original UZI tty did but much can be added)
* Blocking on carrier for terminals
* Optimisations to avoid bogus uarea copying compared to UZI180
* More modern system call API: 3 argument open, mkdir, rmdir, rename,
 chroot (with correct .. semantics), fchdir, fchmod, fchown, fstat,
 fcntl, setpgrp, sighold and friends, waitpid, setpgrp, nice
 O_NDELAY, O_CLOEXEC, F_SETFL, F_DUPFD etc
* Address validation checks on all syscall copies
* Builds with a modern ANSI C compiler (SDCC)
* Kernel boots to userspace on 6303, 6502, 65C816, 68000, 6803, 6809, 8080, 8085, MSP430 (bitrotted) and Z80/Z180
* Core code can be built for 6303, 6502, 65C816, 68000, 6803, 6809, 8080, 8085, 8086, MSP430, pdp11, rabbit r2k/r3k and Z80/Z180 so should be far more portable
* Core architecture designed to support building and maintaining
 multiple target machines without forking each one
* Helpers to make many bits of implementation wrappers to core code
* Lots more bugs right now

## What does UZI have over FUZIX

* Can run in 64K of RAM (32K kernel/32K user). FUZIX would need
 banked ROM or similar to pull this off. If you have banked
 ROM then our kernel footprint in RAM is about 8K plus userspace
 plus any framebuffers and similar overhead. On a 6809 it's just
 about possible to run in a straight 64K

## What do the UZI branches have that FUZIX has not yet integrated

* Symbolic links (UZIX)
* Various clever fusions of syscalls that may save a few bytes
 (UZIX)
* setprio (UZIX)
* Rather crude loadable drivers (UZIX)
* Use of __naked and __asm for Z80 specific bits to avoid more
 .S files than are needed (UMZIX)

Plus OMU has a really clever function passing trick for open/creat and
friends, while UMZIX has a neat unified ""make anything"" function.

## What Key Features Are Missing Still

* ptrace, most of ulimit
* root reserved disk blocks
* banked executables
* TCP/IP (in progress)
* select/poll() (in progress)
* Support for > 32MB filesystems (but first figure out how to fsck
 a giant fs on a slow 8bit micro!)
* Smarter scheduler
* Optimisations for disk block/inode allocator (2.11BSD)

## Tool Issues

* 6809 gcc and cc65 don't have long long 64bit (for sane time_t)
* SDCC can generate ROMmable binaries but not banked ones (hack fixes done)
* None of the above have an O88 style common sequence compressor
* CC65 can't handle larger objects on stack, and lacks float support
* We need a 'proper' 65C816 C compiler
* ACK 8080 lacks floating point support

[travis-image]: https://travis-ci.org/EtchedPixels/FUZIX.png?branch=master
[travis-url]: https://travis-ci.org/EtchedPixels/FUZIX"
vmware/open-vm-tools,22368,1260,147,278,Organization,False,4654,30,88,9,False,Official repository of VMware open-vm-tools project,http://sourceforge.net/projects/open-…,0,8,0,139,239,16,19,33,24,3,4,2175,1,120,6681,3188,0,0,193,150,,,"# General
## What is the open-vm-tools project?
open-vm-tools is a set of services and modules that enable several features in VMware products for better management of, and seamless user interactions with, guests. It includes kernel modules for enhancing the performance of virtual machines running Linux or other VMware supported Unix like guest operating systems. 
 
open-vm-tools enables the following features in VMware products:

- The ability to perform virtual machine power operations gracefully.
- Execution of VMware provided or user configured scripts in guests during various power operations.
- The ability to run programs, commands and file system operation in guests to enhance guest automation.
- Authentication for guest operations. 
- Periodic collection of network, disk, and memory usage information from the guest.
- Generation of heartbeat from guests to hosts so VMware's HA solution can determine guests' availability.
- Clock synchronization between guests and hosts or client desktops.
- Quiescing guest file systems to allow hosts to capture file-system-consistent guest snapshots.
- Execution of pre-freeze and post-thaw scripts while quiescing guest file systems.
- The ability to customize guest operating systems immediately after powering on virtual machines.
- Enabling shared folders between host and guest file systems on VMware Workstation and VMware Fusion.
- Copying and pasting text, graphics, and files between guests and hosts or client desktops.

## Can you provide more details on the actual code being released?
The following components have been released as open source software:
- Linux, Solaris and FreeBSD drivers for various devices and file system access.
- The memory balloon driver for reclaiming memory from guests.
- The PowerOps plugin to perform graceful power operation and run power scripts.
- The VIX plugin to run programs and commands, and perform file system operations in guests.
- The GuestInfo plugin to periodically collect various statistics from guests.
- The TimeSync plugin to perform time synchronization.
- The dndcp plugin to support drag and drop, and text and file copy/paste operations.
- The ResolutionSet plugin to adjust guest screen resolutions automatically based on window sizes.
- The guest authentication service.
- The toolbox command to perform disk wiping and shrinking, manage power scripts, and time synchronization.
- The guest SDK libraries to provide information about virtual machines to guests.
- Clients and servers for shared folders support.
- Multiple monitor support.
- The GTK Toolbox UI.
 
## Is open-vm-tools available with Linux distributions?
Yes. open-vm-tools packages for user space components are available with new versions of major Linux distributions, and are installed as part of the OS installation in several cases. Please refer to VMware KB article http://kb.vmware.com/kb/2073803 for details. All leading Linux vendors support open-vm-tools and bundle it with their products. For information about OS compatibility for open-vm-tools, see the 
VMware Compatibility Guide at http://www.vmware.com/resources/compatibility
Automatic installation of open-vm-tools along with the OS installation eliminates the need to separately install open-vm-tools in guests. If open-vm-tools is not installed automatically, you may be able to manually install it from the guest OS vendor's public repository. Installing open-vm-tools from the Linux vendor's repository reduces virtual machine downtime because future updates to open-vm-tools are included with the OS maintenance patches and updates.
**NOTE**: Most of the Linux distributions ship two or more open-vm-tools packages. ""open-vm-tools"" is the core package without any dependencies on X libraries and ""open-vm-tools-desktop"" is an additional package with dependencies on ""open-vm-tools"" core package and X libraries. The ""open-vm-tools-sdmp"" package contains a plugin for Service Discovery. There may be additional packages, please refer to the documentation of the OS vendor. Note that the open-vm-tools packages available with Linux distributions do not include Linux drivers because Linux drivers are available as part of Linux kernel itself. Linux kernel versions 3.10 and later include all of the Linux drivers present in open-vm-tools except the vmhgfs driver. The vmhgfs driver was required for enabling shared folders feature, but is superseded by vmhgfs-fuse which does not require a kernel driver.

## Will there be continued support for VMware Tools and OSP? 
VMware Tools will continue to be available under a commercial license. It is recommended that open-vm-tools be used for the Linux distributions where open-vm-tools is available. VMware will not provide OSPs for operating systems where open-vm-tools is available.

## How does this benefit other open source projects?
Under the terms of the GPL, open source community members are able to use the open-vm-tools code to develop their own applications, extend it, and contribute to the community. They can also incorporate some or all of the code into their projects, provided they comply with the terms of the GPL.

# License Related
## What license is the code being released under?
The code is being released under GPL v2 and GPL v2 compatible licenses. To be more specific, the Linux kernel modules are being released under the GPL v2, while almost all of the user level components are being released under the LGPL v2.1. The SVGA and mouse drivers have been available under the X11 license for quite some time. There are certain third party components released under BSD style licenses, to which VMware has in some cases contributed, and will continue to distribute with open-vm-tools.
 
## Why did you choose these licenses?
We chose the GPL v2 for the kernel components to be consistent with the Linux kernel's license. We chose the LGPL v2.1 for the user level components because some of the code is implemented as shared libraries and we do not wish to restrict proprietary code from linking against those libraries. For consistency, we decided to license the rest of the userlevel code under the LGPL v2.1 as well.

## What are the obligations that the license(s) impose?
Each of these licenses have different obligations.
For questions about the GPL, LGPL licenses, the Free Software Foundation's GPL FAQ page provides lots of useful information. 
For questions about the other licenses like the X11, BSD licenses, the Open Source Initiative has numerous useful resources including mailing lists. 
The Software Freedom Law Center provides legal expertise and consulting for free and open source software (FOSS) developers.

## Can I use all or part of this code in my proprietary software? Do I have to release the source code if I do?
Different open source licenses have different requirements regarding the release of source code. Since the code is being released under various open source licenses, you will need to comply with the terms of the corresponding licenses.

## Am I required to contribute back any changes I make to the code?
No, you aren't required to contribute any changes that you make back to the open-vm-tools project. However, we encourage you to do so.

## Can I use all or part of this code in another open source package?
Yes, as long as you comply with the appropriate license(s).
 
## Can I package this for my favorite operating system?
Yes! Please do. 

## Will the commercial version (VMware Tools) differ from the open source version (open-vm-tools)? If so, how?
Our goal is to work towards making the open source version as close to the commercial version as possible. However, we do currently make use of certain components licensed from third parties as well as components from other VMware products which are only available in binary form.

## If I use the code from the open-vm-tools project in my project/product, can I call my project/product VMware Tools?
No, since your project/product is not a VMware project/product.

# Building open-vm-tools
## How do I build open-vm-tools?
open-vm-tools uses the GNU Automake tool for generating Makefiles to build all sources. More information about Automake can be found here: http://www.gnu.org/software/automake/
## Project build information:
The following steps will work on most recent Linux distributions:
```
autoreconf -i
./configure
make
sudo make install
sudo ldconfig
```

To build the optional sdmp (Service Discovery) plugin use the `--enable-servicediscovery` option to invoke the configure script:
```
./configure --enable-servicediscovery
```

## Getting configure options and help
If you are looking for help or additional settings for the building of this project, the following configure command will display a list of help options:
```
./configure --help
```
When using configure in the steps above it is only necessary to call ./configure once unless there was a problem after the first invocation.

# Getting Involved
## How can I get involved today?
You can get involved today in several different ways:
- Start using open-vm-tools today and give us feedback.
- Suggest feature enhancements.
- Identify and submit bugs under issues section: https://github.com/vmware/open-vm-tools/issues
- Start porting the code to other operating systems.   Here is the list of operating systems with open-vm-tools:

  * Red Hat Enterprise Linux 7.0 and later releases
  * SUSE Linux Enterprise 12 and later releases
  * Ubuntu 14.04 and later releases
  * CentOS 7 and later releases
  * Debian 7.x and later releases
  * Oracle Linux 7 and later 
  * Fedora 19 and later releases
  * openSUSE 11.x and later releases
 
## Will external developers be allowed to become committers to the project?
Yes. Initially, VMware engineers will be the only committers. As we roll out our development infrastructure, we will be looking to add external committers to the project as well.

## How can I submit code changes like bug fixes, patches, new features to the project?
Initially, you can submit bug fixes, patches and new features to the project development mailing list as attachments to emails or bug reports. To contribute source code, you will need to fill out a contribution agreement form as part of the submission process. We will have more details on this process shortly.

## What is the governance model for managing this as an open source project?
The feature roadmap and schedules for the open-vm-tools project will continue to be defined by VMware. Initially, VMware engineers will be the only approved committers. We will review incoming submissions for suitability for merging into the project. We will be looking to add community committers to the project based on their demonstrated contributions to the project. Finally, we also plan to set up a process for enhancement proposals, establishing sub-projects and so on.

## Will you ship code that I contribute with VMware products? If so, will I get credit for my contributions?
Contributions that are accepted into the open-vm-tools project's main source tree will likely be a part of VMware Tools. We also recognize the value of attribution and value your contributions. Consequently, we will acknowledge contributions from the community that are distributed with VMware's products.

## Do I need to sign something before making a contribution?
Yes. We have a standard contribution agreement that covers all contributions made to the project. It gives VMware and you joint copyright interests in the code you are contributing. The agreement also gives VMware flexibility with licensing and also helps avoid any copyright/licensing related issues that may arise in the future. In order for us to include your contribution in our source tree, we ask that you send us a signed copy of the agreement. You can do this in one of two ways:
Fax to +1.650.427.5003, Attn: Product & Technology Law Group
Scan and email it to oss-queries_at_vmware.com
Agreement: http://open-vm-tools.sourceforge.net/files/vca.pdf

# Compatibilty

## What Operating Systems are supported for customization?
The [Guest OS Customization Support Matrix](http://partnerweb.vmware.com/programs/guestOS/guest-os-customization-matrix.pdf) provides details about the guest operating systems supported for customization.

## Which versions of open-vm-tools are compatible with other VMware products?

The [VMware Product Interoperability Matrix](http://partnerweb.vmware.com/comp_guide2/sim/interop_matrix.php) provides details about the compatibility of different versions of VMware Tools (includes open-vm-tools) and other VMware Products.

#Internationalization
## Which languages are supported?

open-vm-tools supports the following languages:
- English
- French
- German
- Spanish
- Italian
- Japanese
- Korean
- Simplified Chinese
- Traditional Chinese

# Other
## Mailing Lists
Please send an email to one of these mailing lists based on the nature of your question.
- Development related questions : open-vm-tools-devel@lists.sourceforge.net
- Miscellaneous questions: open-vm-tools-discuss@lists.sourceforge.net
- General project announcements: open-vm-tools-announce@lists.sourceforge.net"
nesbox/TIC-80,121703,1892,88,189,User,False,1227,3,31,41,False,"TIC-80 is a fantasy computer for making, playing and sharing tiny games.",https://tic.computer,9,27,1,269,652,40,43,4,185,3,52,1440,12,136,94713,64208,0,0,22,,323,,"[![Build Status](https://travis-ci.org/nesbox/TIC-80.svg?branch=master)](https://travis-ci.org/nesbox/TIC-80)
[![Build status](https://ci.appveyor.com/api/projects/status/1pflw77cjd8mqggb/branch/master?svg=true)](https://ci.appveyor.com/project/nesbox/tic-80)

![TIC-80](https://tic.computer/img/logo64.png)
**TIC-80 TINY COMPUTER** - [https://tic.computer/](https://tic.computer/)

# About
TIC-80 is a **FREE** and **OPEN SOURCE** fantasy computer for making, playing and sharing tiny games.

With TIC-80 you get built-in tools for development: code, sprites, maps, sound editors and the command line, which is enough to create a mini retro game.

Games are packaged into a cartridge file, which can be easily distributed. TIC-80 works on all popular platforms. This means your cartridge can be played in any device.

To make a retro styled game, the whole process of creation and execution takes place under some technical limitations: 240x136 pixel display, 16 color palette, 256 8x8 color sprites, 4 channel sound, etc.

![TIC-80](https://user-images.githubusercontent.com/1101448/29687467-3ddc432e-8925-11e7-8156-5cec3700cc04.gif)

### Features
- Multiple programming languages: [Lua](https://www.lua.org),
  [Moonscript](https://moonscript.org),
  [Javascript](https://developer.mozilla.org/en-US/docs/Web/JavaScript),
  [Wren](http://wren.io/), and [Fennel](https://fennel-lang.org).
- Games can have mouse and keyboard as input
- Games can have up to 4 controllers as input (with up to 8 buttons, each)
- Built-in editors: for code, sprites, world maps, sound effects and music
- An aditional memory bank: load different assets from your cartridge while your game is executing

# Binary Downloads
You can download compiled versions for the major operating systems directly from our [releases page](https://github.com/nesbox/TIC-80/releases).

# Pro Version
To help support TIC-80 development, we have a [PRO Version](https://nesbox.itch.io/tic).
This version has a few additional features and binaries can only be downloaded on [our Itch.io page](https://nesbox.itch.io/tic).

For users who can't spend the money, we made it easy to build the pro version from the source code.

### Pro features

- Save/load cartridges in text format, and create your game in any editor you want, also useful for version control systems.
- Even more memory banks: instead of having only 1 memory bank you have 8.
- Export your game without editors, and then publish it to app stores (WIP).

# Community
You can play and share games, tools and music at [tic.computer](https://tic.computer/play).

The community also hangs out and discusses on [Discord chat](https://discord.gg/DkD73dP).

# Contributing
You are can contribute by issuing a bug or requesting a new feature on our [issues page](https://github.com/nesbox/tic.computer/issues).
Keep in mind when engaging on a discussion to follow our [Code of Conduct](https://github.com/nesbox/TIC-80/blob/master/CODE_OF_CONDUCT.md).

You can also contribute by reviewing or improving our [wiki](https://github.com/nesbox/tic.computer/wiki).
The [wiki](https://github.com/nesbox/tic.computer/wiki) holds TIC-80 documentation, code snippets and game development tutorials.

# Build instructions

## Windows
### with Visual Studio 2017
- install `Visual Studio 2017`
- install `git`
- run following commands in `cmd`
```
git clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/build
cmake -G ""Visual Studio 15 2017 Win64"" ..
```
- open `TIC-80.sln` and build
- enjoy :)

### with MinGW
- install `mingw-w64` (http://mingw-w64.org) and add `.../mingw/bin` path to the *System Variables Path*
- install `git`
- install `cmake` (https://cmake.org)
- run following commands in `terminal`
```
git clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/build
cmake -G ""MinGW Makefiles"" ..
mingw32-make -j4
```

## Linux 
### Ubuntu 14.04
run the following commands in the Terminal
```
sudo apt-get install git cmake libgtk-3-dev libgles1-mesa-dev libglu-dev -y
git clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/build
cmake ..
make -j4
```

to install the latest CMake:
```
wget ""https://cmake.org/files/v3.12/cmake-3.12.0-Linux-x86_64.sh""
sudo sh cmake-3.12.0-Linux-x86_64.sh --skip-license --prefix=/usr
```

### Ubuntu 18.04

run the following commands in the Terminal
```
sudo apt-get install git cmake libgtk-3-dev libglvnd-dev libglu1-mesa-dev freeglut3-dev -y
git clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/build
cmake ..
make -j4
```

## Mac
install `Command Line Tools for Xcode` and `brew` package manager

run the following commands in the Terminal
```
brew install git cmake
git clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/build
cmake ..
make -j4
```

## iOS / tvOS
You can find iOS/tvOS version here 
- 0.60.3: https://github.com/brunophilipe/TIC-80
- 0.45.0: https://github.com/CliffsDover/TIC-80"
raysan5/raylib,343468,4087,187,475,User,False,3975,3,16,144,False,A simple and easy-to-use library to enjoy videogames programming,http://www.raylib.com,10,14,0,15,695,12,136,1,558,1,85,2427,46,549,62606,88761,0,0,12,,663,,"<img align=""left"" src=""https://github.com/raysan5/raylib/blob/master/logo/raylib_256x256.png"" width=256>

**raylib is a simple and easy-to-use library to enjoy videogames programming.**

raylib is highly inspired by Borland BGI graphics lib and by XNA framework and it's specially well suited for prototyping, tooling, graphical applications, embedded systems and education.

*NOTE for ADVENTURERS: raylib is a programming library to enjoy videogames programming; no fancy interface, no visual helpers, no auto-debugging... just coding in the most pure spartan-programmers way.*

Ready to learn? Jump to [code examples!](http://www.raylib.com/examples.html)

<br>

[![GitHub contributors](https://img.shields.io/github/contributors/raysan5/raylib)](https://github.com/raysan5/raylib/graphs/contributors)
[![GitHub All Releases](https://img.shields.io/github/downloads/raysan5/raylib/total)](https://github.com/raysan5/raylib/releases)
[![GitHub commits since tagged version](https://img.shields.io/github/commits-since/raysan5/raylib/3.0.0)](https://github.com/raysan5/raylib/commits/master)
[![License](https://img.shields.io/badge/license-zlib%2Flibpng-blue.svg)](LICENSE)

[![Chat on Discord](https://img.shields.io/discord/426912293134270465.svg?logo=discord)](https://discord.gg/VkzNHUE)
[![GitHub stars](https://img.shields.io/github/stars/raysan5/raylib?style=social)](https://github.com/raysan5/raylib/stargazers)
[![Twitter Follow](https://img.shields.io/twitter/follow/raysan5?style=social)](https://twitter.com/raysan5)
[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/raylib?style=social)](https://www.reddit.com/r/raylib/)

[![Travis (.org)](https://img.shields.io/travis/raysan5/raylib?label=Travis%20CI%20Build%20Status%20-%20Linux,%20OSX,%20Android,%20Windows)](https://travis-ci.org/raysan5/raylib)
[![AppVeyor](https://img.shields.io/appveyor/build/raysan5/raylib?label=AppVeyor%20CI%20Build%20Status%20-%20Windows%20(mingw,%20msvc15))](https://ci.appveyor.com/project/raysan5/raylib)

[![Actions Status](https://github.com/raysan5/raylib/workflows/CI%20-%20Source%20&%20Examples%20-%20Windows/badge.svg)](https://github.com/raysan5/raylib/actions)
[![Actions Status](https://github.com/raysan5/raylib/workflows/CI%20-%20Source%20&%20Examples%20-%20Linux/badge.svg)](https://github.com/raysan5/raylib/actions)
[![Actions Status](https://github.com/raysan5/raylib/workflows/CI%20-%20Source%20&%20Examples%20-%20macOS/badge.svg)](https://github.com/raysan5/raylib/actions)

features
--------
  - **NO external dependencies**, all required libraries are bundled into raylib
  - Multiple platforms supported: **Windows, Linux, MacOS, Android, HTML5... and more!**
  - Written in plain C code (C99) in PascalCase/camelCase notation
  - Hardware accelerated with OpenGL (**1.1, 2.1, 3.3 or ES 2.0**)
  - **Unique OpenGL abstraction layer** (usable as standalone module): [rlgl](https://github.com/raysan5/raylib/blob/master/src/rlgl.h)
  - Multiple **Fonts** formats supported (TTF, XNA fonts, AngelCode fonts)
  - Outstanding texture formats support, including compressed formats (DXT, ETC, ASTC)
  - **Full 3D support**, including 3D Shapes, Models, Billboards, Heightmaps and more! 
  - Flexible Materials system, supporting classic maps and **PBR maps**
  - **Animated 3D models** supported (skeletal bones animation)
  - Shaders support, including model and **postprocessing** shaders.
  - **Powerful math module** for Vector, Matrix and Quaternion operations: [raymath](https://github.com/raysan5/raylib/blob/master/src/raymath.h)
  - Audio loading and playing with streaming support (WAV, OGG, MP3, FLAC, XM, MOD)
  - **VR stereo rendering** support with configurable HMD device parameters
  - Huge examples collection with [+115 code examples](https://github.com/raysan5/raylib/tree/master/examples)!
  - Bindings to [+40 programming languages](https://github.com/raysan5/raylib/blob/master/BINDINGS.md)!
  - Free and open source.

raylib uses on its [core](https://github.com/raysan5/raylib/blob/master/src/core.c) module the outstanding [GLFW3](http://www.glfw.org/) library, embedded in the form of [rglfw](https://github.com/raysan5/raylib/blob/master/src/rglfw.c) module, to avoid external dependencies.

raylib uses on its [raudio](https://github.com/raysan5/raylib/blob/master/src/raudio.c) module, the amazing [miniaudio](https://github.com/dr-soft/miniaudio) library to support multiple platforms and multiple audio backends.

raylib uses internally several single-file header-only libraries to support different fileformats loading and saving, all those libraries are embedded with raylib and available in [src/external](https://github.com/raysan5/raylib/tree/master/src/external) directory. Check [raylib Wiki](https://github.com/raysan5/raylib/wiki/raylib-dependencies) for a detailed list.

*On Android platform, `native_app_glue` module (provided by Android NDK) and native Android libraries are used to manage window/context, inputs and activity life cycle.*

*On Raspberry Pi platform (native mode), `Videocore API` and `EGL` libraries are used for window/context management. Inputs are processed using `evdev` Linux libraries*

*On Web platform, raylib uses `emscripten` provided libraries for several input events management, specially noticeable the touch events support.*

build and installation
----------------------

raylib binary releases for Windows, Linux and macOS are available at the [Github Releases page](https://github.com/raysan5/raylib/releases).

raylib is also available via multiple [package managers](https://github.com/raysan5/raylib/issues/613) on multiple OS distributions.

#### Installing and building raylib via vcpkg

You can download and install raylib using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:

      git clone https://github.com/Microsoft/vcpkg.git
      cd vcpkg
      ./bootstrap-vcpkg.sh
      ./vcpkg integrate install
      vcpkg install raylib

*The raylib port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.*

#### Building raylib on multiple platforms

[raylib Wiki](https://github.com/raysan5/raylib/wiki#development-platforms) contains detailed instructions on building and usage on multiple platforms.

 - [Working on Windows](https://github.com/raysan5/raylib/wiki/Working-on-Windows)
 - [Working on macOS](https://github.com/raysan5/raylib/wiki/Working-on-macOS)
 - [Working on GNU Linux](https://github.com/raysan5/raylib/wiki/Working-on-GNU-Linux)
 - [Working on FreeBSD](https://github.com/raysan5/raylib/wiki/Working-on-FreeBSD)
 - [Working on Raspberry Pi](https://github.com/raysan5/raylib/wiki/Working-on-Raspberry-Pi)
 - [Working for Android](https://github.com/raysan5/raylib/wiki/Working-for-Android)
 - [Working for Web (HTML5)](https://github.com/raysan5/raylib/wiki/Working-for-Web-(HTML5))
 - [Working for UWP (Universal Window Platform)](https://github.com/raysan5/raylib/wiki/Working-for-UWP)
 - [Working anywhere with CMake](https://github.com/raysan5/raylib/wiki/Working-with-CMake)

*Note that Wiki is open for edit, if you find some issue while building raylib for your target platform, feel free to edit the Wiki or open and issue related to it.*

#### Using raylib with multiple IDEs

raylib has been developed on Windows platform using [Notepad++](https://notepad-plus-plus.org/) and [MinGW GCC](http://mingw-w64.org/doku.php) compiler but it can be used with other IDEs on multiple platforms.

[Projects directory](https://github.com/raysan5/raylib/tree/master/projects) contains several ready-to-use **project templates** to build raylib and code examples with multiple IDEs.

*Note that there are lots of IDEs supported, some of the provided templates could require some review, please, if you find some issue with some template or you think they could be improved, feel free to send a PR or open a related issue.*

contact
-------

   * Webpage: [http://www.raylib.com](http://www.raylib.com)
   * Discord: [https://discord.gg/raylib](https://discord.gg/VkzNHUE)
   * Twitter: [http://www.twitter.com/raysan5](http://www.twitter.com/raysan5)
   * Twitch: [http://www.twitch.tv/raysan5](http://www.twitch.tv/raysan5)
   * Reddit: [https://www.reddit.com/r/raylib](https://www.reddit.com/r/raylib)
   * Patreon: [https://www.patreon.com/raylib](https://www.patreon.com/raylib)
   * YouTube: [https://www.youtube.com/channel/raylib](https://www.youtube.com/channel/UC8WIBkhYb5sBNqXO1mZ7WSQ)

If you are using raylib and enjoying it, please, join our [Discord server](https://discord.gg/VkzNHUE) and let us know! :)

license
-------

raylib is licensed under an unmodified zlib/libpng license, which is an OSI-certified, BSD-like license that allows static linking with closed source software. Check [LICENSE](LICENSE) for further details."
karlstav/cava,7041,1623,44,121,User,False,615,4,16,40,False,Console-based Audio Visualizer for Alsa,,0,6,0,12,203,3,26,0,146,0,43,2280,10,96,6610,4504,0,0,3,,29,,"C.A.V.A. [![Build Status](https://github.com/karlstav/cava/workflows/build/badge.svg)](https://github.com/karlstav/cava/actions)
====================

**C**onsole-based **A**udio **V**isualizer for **A**LSA

also supports audio input from Pulseaudio, fifo (mpd), sndio, squeezelite and portaudio.

Now also works on macOS!

by [Karl Stavestrand](mailto:karl@stavestrand.no)

![spectrum](https://raw.githubusercontent.com/karlstav/cava/gh-pages/cava_gradient.gif ""spectrum"")

thanks to [anko](https://github.com/anko) for the gif, here is the [recipe]( http://unix.stackexchange.com/questions/113695/gif-screencastng-the-unix-way).

[Demo video](https://youtu.be/9PSp8VA6yjU)

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [What it is](#what-it-is)
- [Installing](#installing)
  - [From Source](#from-source)
    - [Installing Build Requirements](#installing-build-requirements)
    - [Building](#building)
    - [Installing](#installing-1)
    - [Uninstalling](#uninstalling)
  - [Some distro specific pre-made binaries/recipes](#some-distro-specific-pre-made-binariesrecipes)
    - [openSUSE](#opensuse)
    - [Fedora](#fedora)
    - [Arch](#arch)
    - [Ubuntu](#ubuntu)
- [Capturing audio](#capturing-audio)
  - [From Pulseaudio monitor source (Easy, default if supported)](#from-pulseaudio-monitor-source-easy-default-if-supported)
  - [From ALSA-loopback device (Tricky)](#from-alsa-loopback-device-tricky)
  - [From mpd's fifo output](#from-mpds-fifo-output)
  - [sndio](#sndio)
  - [squeezelite](#squeezelite)
  - [macOS](#macos)
- [Running via ssh](#running-via-ssh)
  - [Raw Output](#raw-output)
- [Font notes](#font-notes)
  - [In ttys](#in-ttys)
  - [In terminal emulators](#in-terminal-emulators)
- [Latency notes](#latency-notes)
- [Usage](#usage)
  - [Controls](#controls)
- [Configuration](#configuration)
- [Contribution](#contribution)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->


What it is
----------

C.A.V.A. is a bar spectrum audio visualizer for the Linux terminal using ALSA, pulseaudio or fifo buffer for input.

This program is not intended for scientific use. It's written to look responsive and aesthetic when used to visualize music. 


Installing
------------------

### From Source

#### Installing Build Requirements

Required components:
* [FFTW](http://www.fftw.org/)
* libtool
* automake
* build-essentials
* [iniparser](https://github.com/ndevilla/iniparser)


Recomended components:
* [ncursesw dev files](http://www.gnu.org/software/ncurses/) (bundled in ncurses in arch)
* [ALSA dev files](http://alsa-project.org/), or
* [Pulseaudio dev files](http://freedesktop.org/software/pulseaudio/doxygen/), or
* Portaudio, or
* Sndio

Only FFTW and the other build tools are actually required for CAVA to compile, but this will only give you the ability to read from fifo files. To more easly grab audio from your system pulseaudio, alsa, sndio or portaudio dev files are recommended (depending on what audio system you are using). Not sure how to get the pulseaudio dev files for other distros than debian/ubuntu or if they are bundled in pulseaudio. 


For better a better visual experience ncurses is also recomended.

Iniparser is also required, but if it is not already installed, a bundled version will be used.

All the requirements can be installed easily in all major distros:

Debian Buster or higher/Ubuntu 18.04 or higher :

    apt-get install libfftw3-dev libasound2-dev libncursesw5-dev libpulse-dev libtool automake libiniparser-dev
    export CPPFLAGS=-I/usr/include/iniparser


older Debian/Ubuntu:

    apt-get install libfftw3-dev libasound2-dev libncursesw5-dev libpulse-dev libtool automake


ArchLinux:

    pacman -S base-devel fftw ncurses alsa-lib iniparser pulseaudio


openSUSE:

    zypper install alsa-devel ncurses-devel fftw3-devel libpulse-devel libtool


Fedora:

    dnf install alsa-lib-devel ncurses-devel fftw3-devel pulseaudio-libs-devel libtool

    
macOS:

First install homebrew if you have't already:

    /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""

Then install prerequisites:

    brew install fftw ncurses libtool automake portaudio
    
Then fix macOS not finding libtool and ncursesw:

    export LIBTOOL=`which glibtool`
    export LIBTOOLIZE=`which glibtoolize`
    ln -s `which glibtoolize` libtoolize
    ln -s /usr/lib/libncurses.dylib /usr/local/lib/libncursesw.dylib

Tested on macOS High Sierra.



#### Building
 First of all clone this repo and cd in to it, then run:
 
    ./autogen.sh
    ./configure
    make

If you have a recommended component installed, but do not wish to use it (perhaps if building a binary on one machine to be used on another), then the corresponding feature can be disabled during configuration (see configure --help for details).


    
#### Installing 

Install `cava` to default `/usr/local`:

    make install

Or you can change `PREFIX`, for example:

   ./configure --prefix=PREFIX

#### Uninstalling

    make uninstall


### Some distro specific pre-made binaries/recipes    
#### openSUSE

Tumbleweed users have cava in their repo. They can just use:

    zypper in cava

Leap users need to add the multimedia:apps repository first:

    zypper ar -f obs://multimedia:apps/openSUSE_Leap_42.2 multimedia

If you use another version just replace *openSUSE_Leap_42.2* with *openSUSE_13.2*, adjust it to your version.

#### Fedora

Cava is available in Fedora 26 and later.  You can install Cava by
running:

    dnf install cava

#### Arch

Cava is in [AUR](https://aur.archlinux.org/packages/cava/).

    pacaur -S cava

#### Ubuntu

Michael Nguyen has added CAVA to his PPA, it can be installed with:

    sudo add-apt-repository ppa:tehtotalpwnage/ppa
    sudo apt-get update
    sudo apt-get install cava
    
For Ubuntu 18 or newer, you can use Harshal Sheth's PPA:

    sudo add-apt-repository ppa:hsheth2/ppa
    sudo apt-get update
    sudo apt-get install cava

All distro specific instalation sources might be out of date.


Capturing audio
---------------

### From Pulseaudio monitor source (Easy, default if supported)

First make sure you have installed pulseaudio dev files and that cava has been built with pulseaudio support (it should be automatically if the dev files are found).

If you're lucky all you have to do is to uncomment this line in the config file under input:

    method = pulse
 
If nothing happens you might have to use a different source than the default. The default might also be your microphone. Look at the config file for help. 


### From ALSA-loopback device (Tricky)

Set

    method = alsa

in the config file.

ALSA can be difficult because there is no native way to grab audio from an output. If you want to capture audio straight fom the output (not just mic or line-in), you must create an ALSA loopback interface, then output the audio simultaneously to both the loopback and your normal interface.

To create a loopback interface simply run:

`sudo modprobe snd_aloop`

Hopefully your `aplay -l` should now contain a loopback interface.

To make it persistent across boot add the line `snd-aloop` to ""/etc/modules"". To keep it from being loaded as the first soundcard add the line `options snd-aloop index=1` to ""/etc/modprobe.d/alsa-base.conf"", this will load it at '1'. You can replace '1' with whatever makes most sense in your audio setup.

Playing the audio through your Loopback interface makes it possible for cava to capture it, but there will be no sound in your speakers. In order to play audio on the loopback interface and your actual interface you must make use of the ALSA multi channel.

Look at the included example file `example_files/etc/asound.conf` on how to use the multi channel. I was able to make this work on my laptop (an Asus UX31 running Ubuntu), but I had no luck with the ALSA method on my Raspberry Pi (Rasbian) with an USB DAC. The PulseAudio method however works perfectly on my Pi. 

Read more about the ALSA method [here](http://stackoverflow.com/questions/12984089/capture-playback-on-play-only-sound-card-with-alsa).

If you are having problems with the alsa method on Rasberry PI, try enabling `mmap` by adding the following line to `/boot/config.txt` and reboot:

```
dtoverlay=i2s-mmap
```

### From mpd's fifo output

Add these lines in mpd:

    audio_output {
        type                    ""fifo""
        name                    ""my_fifo""
        path                    ""/tmp/mpd.fifo""
        format                  ""44100:16:2""
    }

Uncomment and change input method to `fifo` in the config file.

The path of the fifo can be specified with the `source` parameter.

I had some trouble with sync (the visualizer was ahead of the sound). Reducing the ALSA buffer in mpd fixed it:

    audio_output {
            type            ""alsa""
            name            ""My ALSA""
            buffer_time     ""50000""   # (50ms); default is 500000 microseconds (0.5s)
    }

### sndio

sndio is the audio framework used on OpenBSD, but it's also available on
FreeBSD and Linux. So far this is only tested on FreeBSD.

To test it
```bash
# Start sndiod with a monitor sub-device
$ sndiod -dd -s default -m mon -s monitor

# Set the AUDIODEVICE environment variable to override the default
# sndio device and run cava
$ AUDIODEVICE=snd/0.monitor cava
```

### squeezelite
[squeezelite](https://en.wikipedia.org/wiki/Squeezelite) is one of several software clients available for the Logitech Media Server. Squeezelite can export its audio data as shared memory, which is what this input module uses.
Just adapt your config:
```
method = shmem
source = /squeezelite-AA:BB:CC:DD:EE:FF
```
where `AA:BB:CC:DD:EE:FF` is squeezelite's MAC address (check the LMS Web GUI (Settings>Information) if unsure).
Note: squeezelite must be started with the `-v` flag to enable visualizer support.

### macOS
Install [Soundflower](https://github.com/mattingalls/Soundflower) to create a loopback interface. Use Audio MIDI Setup to configure a virtual interface that outputs audio to both your speakers and the loopbacl interface, following [this](https://github.com/RogueAmoeba/Soundflower-Original/issues/44#issuecomment-151586106) recipe.

Then edit your config to use this interface with portaudio:

```
method = portaudio
source = ""Soundflower (2ch)""
```

Note: cava looks no good in the default macOS terminal. For a better look install [kitty](https://sw.kovidgoyal.net/kitty/index.html). Be where that you might run into #109, that can be fixed like [this](https://stackoverflow.com/questions/7165108/in-os-x-lion-lang-is-not-set-to-utf-8-how-to-fix-it).


Running via ssh
---------------

To run via ssh to an external monitor, redirect output to `/dev/console`:

     ~# ./cava  <> /dev/console >&0 2>&1

exit with ctrl+z then run 'bg' to keep it running after you log out.

(You must be root to redirect to console. Simple sudo is not enough: Run `sudo su` first.)

### Raw Output

You can also use Cava's output for other programs by using raw output mode, which will write bar data to `STDOUT` that can be piped into other processes. More information on this option is documented in [the example config file](/example_files/config).

Font notes
----------

Since the graphics are simply based on characters, performance is dependent on the terminal font.

### In ttys

If you run this in a TTY the program will change the font to the included `cava.psf` (actually a slightly modified ""unifont"").

In console fonts it seems that only 256 Unicode characters are supported, probably because they are bitmap fonts. I could not find a font with Unicode characters 2581-2587 (the 1/8 - 7/8 blocks used on the top of each bar to increase resolution).

So in `cava.psf`, the characters 1-7 are actually replaced by Unicode characters 2581-2587. When cava exits, it changes the font back. If cava exits abnormally and you notice that 1-7 are replaced by partial blocks, just change the font with `setfont`.

Actually, `setfont` is supposed to return the default font, but this usually isn't set. I haven't found another way to get the current font. So cava sets the font to ""Lat2-Fixed16"" when interrupted. All major distros should have it. It will revert to your default font at reboot.

### In terminal emulators

In terminal emulators like `xterm`, the font settings is chosen in the software and cannot be changed by an application. So find your terminal settings and try out different fonts and settings. Also character spacing affects the look of the bar spectrum.

Performance is also different, urxvt is the best I found so far, while Gnome-terminal is quite slow.

Cava also disables the terminal cursor, and turns it back on on exit, but in case it terminates unexpectedly, run `setterm -cursor on` to get it back.

Tip: Cava will look much nicer in small font sizes. Use a second terminal emulator for cava and set the font size to 1. Warning, can cause high CPU usage and latency if the terminal window is too large!


Latency notes
-------------

If you see latency issues (sound before image) in a terminal emulator, try increasing the font size. This will reduce the number of characters that have to be shown.

If your audio device has a huge buffer, you might experience that cava is actually faster than the audio you hear. This reduces the experience of the visualization. To fix this, try decreasing the buffer settings in your audio playing software.

Usage
-----

    Usage : cava [options]
    Visualize audio input in terminal. 

    Options:
         -p          path to config file
         -v          print version



Exit with ctrl+c or q.

If cava quits unexpectedly or is force killed, echo must be turned on manually with `stty -echo`.

### Controls

NOTE: only works in ncurses output mode.

| Key | Description |
| --- | ----------- |
| <kbd>up</kbd> / <kbd>down</kbd>| increase/decrease sensitivity |
| <kbd>left</kbd> / <kbd>right</kbd>| increase/decrease bar width |
| <kbd>f</kbd> / <kbd>b</kbd>| change foreground/background color |
| <kbd>r</kbd> | Reload configuration |
| <kbd>c</kbd> | Reload colors only |
| <kbd>q</kbd> or <kbd>CTRL-C</kbd>| Quit C.A.V.A. |

Configuration
-------------

As of version 0.4.0 all options are done in the config file, no more command-line arguments!

By default a configuration file is located in `$XDG_CONFIG_HOME/cava/config` or `$HOME/.config/cava/config`, but cava can also be made to use a different file with the `-p` option.

If for some reason the config file is not in the config dir, copy the [bundled configuration file](/example_files/config) to the config dir manually.

Sending cava a SIGUSR1 signal, will force cava to reload its configuration file. Thus, it behaves as if the user pressed <kbd>r</kbd> in the terminal. One might send a SIGUSR1 signal using `pkill` or `killall`.
For example:
```
$ pkill -USR1 cava
```

Similarly, sending cava a SIGUSR2 signal will only reload the colors from the configuration file, which is the same as pressing <kbd>c</kbd> in the terminal. This is slightly faster than reloading the entire config as the audio processing does not need to reinitialize.  
```
$ pkill -USR2 cava
```

**Examples on how the equalizer works:**

    [eq]
    1=0
    2=1
    3=0
    4=1
    5=0

![3_138](https://cloud.githubusercontent.com/assets/6376571/8670183/a54a851e-29e8-11e5-9eff-346bf6ed91e0.png)

    [eq]
    1=2
    2=2
    3=1
    4=1
    5=0.5

![3_139](https://cloud.githubusercontent.com/assets/6376571/8670181/9db0ef50-29e8-11e5-81bc-3e2bb9892da0.png)

Contribution
------

Please read CONTRIBUTING.md before opening a pull request.

Thanks to:
* [CelestialWalrus](https://github.com/CelestialWalrus)
* [anko](https://github.com/anko)
* [livibetter](https://github.com/livibetter)

for major contributions in the early development of this project.

Also thanks to [dpayne](https://github.com/dpayne/) for figuring out how to find the pulseaudio default sink name."
iNavFlight/inav,249989,1290,149,742,Organization,False,9689,60,66,261,False,INAV: Navigation-enabled flight control software,https://inavflight.github.io,6,22,2,301,2766,104,221,69,2697,31,215,2630,20,291,10825,7978,0,0,14,3,,,"# INAV - navigation capable flight controller

## F3 based flight controllers

> STM32 F3 flight controllers like Omnibus F3 or SP Racing F3 are deprecated and soon they will reach the end of support in INAV. If you are still using F3 boards, please migrate to F4 or F7.

![INAV](http://static.rcgroups.net/forums/attachments/6/1/0/3/7/6/a9088858-102-inav.png)
![Travis CI status](https://travis-ci.org/iNavFlight/inav.svg?branch=master)

## Features

* Runs on the most popular F4 and F7 flight controllers
* Outstanding performance out of the box
* Position Hold, Altitude Hold, Return To Home and Missions
* Excellent support for fixed wing UAVs: airplanes, flying wings 
* Fully configurable mixer that allows to run any hardware you want: multirotor, fixed wing, rovers, boats and other experimental devices
* Multiple sensor support: GPS, Pitot tube, sonar, lidar, temperature, ESC with BlHeli_32 telemetry
* SmartAudio and IRC Tramp VTX support
* DSHOT and Multishot ESCs
* Blackbox flight recorder logging
* On Screen Display (OSD) - both character and pixel style
* Telemetry: SmartPort, FPort, MAVlink, LTM
* Multi-color RGB LED Strip support
* Advanced gyro filtering: Matrix Filter and RPM filter
* Logic Conditions, Global Functions and Global Variables: you can program INAV with a GUI
* And many more!

For a list of features, changes and some discussion please review consult the releases [page](https://github.com/iNavFlight/inav/releases) and the documentation.

## Tools

### INAV Configurator

Official tool for INAV can be downloaded [here](https://github.com/iNavFlight/inav-configurator/releases). It can be run on Windows, MacOS and Linux machines and standalone application.  

### INAV Blackbox Explorer

Tool for Blackbox logs analysis is available [here](https://github.com/iNavFlight/blackbox-log-viewer/releases)

### Telemetry screen for OpenTX

Users of FrSky Taranis X9 and Q X7 can use INAV Lua Telemetry screen created by @teckel12 . Software and installation instruction are available here: [https://github.com/iNavFlight/LuaTelemetry](https://github.com/iNavFlight/LuaTelemetry)

## Installation

See: https://github.com/iNavFlight/inav/blob/master/docs/Installation.md

## Documentation, support and learning resources
* [Fixed Wing Guide](docs/INAV_Fixed_Wing_Setup_Guide.pdf)
* [Autolaunch Guide](docs/INAV_Autolaunch.pdf)
* [Modes Guide](docs/INAV_Modes.pdf)
* [Wing Tuning Masterclass](docs/INAV_Wing_Tuning_Masterclass.pdf)
* [Official documentation](https://github.com/iNavFlight/inav/tree/master/docs)
* [Official Wiki](https://github.com/iNavFlight/inav/wiki)
* [INAV Official on Telegram](https://t.me/INAVFlight)
* [INAV Official on Facebook](https://www.facebook.com/groups/INAVOfficial)
* [RC Groups Support](https://www.rcgroups.com/forums/showthread.php?2495732-Cleanflight-iNav-(navigation-rewrite)-project)
* [Video series by Painless360](https://www.youtube.com/playlist?list=PLYsWjANuAm4qdXEGFSeUhOZ10-H8YTSnH)
* [Video series by Paweł Spychalski](https://www.youtube.com/playlist?list=PLOUQ8o2_nCLloACrA6f1_daCjhqY2x0fB)

## Contributing

Contributions are welcome and encouraged.  You can contribute in many ways:

* Documentation updates and corrections.
* How-To guides - received help?  help others!
* Bug fixes.
* New features.
* Telling us your ideas and suggestions.
* Buying your hardware from this [link](https://inavflight.com/shop/u/bg/)

A good place to start is Telegram channel or Facebook group. Drop in, say hi.

Github issue tracker is a good place to search for existing issues or report a new bug/feature request:

https://github.com/iNavFlight/inav/issues

https://github.com/iNavFlight/inav-configurator/issues

Before creating new issues please check to see if there is an existing one, search first otherwise you waste peoples time when they could be coding instead!

## Developers

Please refer to the development section in the [docs/development](https://github.com/iNavFlight/inav/tree/master/docs/development) folder.


## INAV Releases
https://github.com/iNavFlight/inav/releases"
koush/android_system_core,2833,27,3,969,User,False,858,2,0,29,False,Android System Core (CM),,0,0,0,,,,,0,0,0,0,4247,0,0,0,0,0,0,297,,165,,
confluentinc/confluent-kafka-python,1816,1822,233,486,Organization,False,591,69,175,54,False,Confluent's Kafka Python Client,http://docs.confluent.io/current/clie…,5,22,1,184,427,74,30,21,264,8,39,1524,13,65,14983,1073,1300,107,168,170,,,"Confluent's Python Client for Apache Kafka<sup>TM</sup>
=======================================================

**confluent-kafka-python** provides a high-level Producer, Consumer and AdminClient compatible with all
[Apache Kafka<sup>TM<sup>](http://kafka.apache.org/) brokers >= v0.8, [Confluent Cloud](https://www.confluent.io/confluent-cloud/)
and the [Confluent Platform](https://www.confluent.io/product/compare/). The client is:

- **Reliable** - It's a wrapper around [librdkafka](https://github.com/edenhill/librdkafka) (provided automatically via binary wheels) which is widely deployed in a diverse set of production scenarios. It's tested using [the same set of system tests](https://github.com/confluentinc/confluent-kafka-python/tree/master/confluent_kafka/kafkatest) as the Java client [and more](https://github.com/confluentinc/confluent-kafka-python/tree/master/tests). It's supported by [Confluent](https://confluent.io).

- **Performant** - Performance is a key design consideration. Maximum throughput is on par with the Java client for larger message sizes (where the overhead of the Python interpreter has less impact). Latency is on par with the Java client.

- **Future proof** - Confluent, founded by the
creators of Kafka, is building a [streaming platform](https://www.confluent.io/product/compare/)
with Apache Kafka at its core. It's high priority for us that client features keep
pace with core Apache Kafka and components of the [Confluent Platform](https://www.confluent.io/product/compare/).


See the [API documentation](http://docs.confluent.io/current/clients/confluent-kafka-python/index.html) for more info.

**License**: [Apache License v2.0](http://www.apache.org/licenses/LICENSE-2.0)


Usage
=====

Below are some examples of typical usage. For more examples, see the [examples](examples) directory or the [confluentinc/examples](https://github.com/confluentinc/examples/tree/master/clients/cloud/python) github repo for a [Confluent Cloud](https://www.confluent.io/confluent-cloud/) example.


**Producer**

```python
from confluent_kafka import Producer


p = Producer({'bootstrap.servers': 'mybroker1,mybroker2'})

def delivery_report(err, msg):
    """""" Called once for each message produced to indicate delivery result.
        Triggered by poll() or flush(). """"""
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))

for data in some_data_source:
    # Trigger any available delivery report callbacks from previous produce() calls
    p.poll(0)

    # Asynchronously produce a message, the delivery report callback
    # will be triggered from poll() above, or flush() below, when the message has
    # been successfully delivered or failed permanently.
    p.produce('mytopic', data.encode('utf-8'), callback=delivery_report)

# Wait for any outstanding messages to be delivered and delivery report
# callbacks to be triggered.
p.flush()
```


**High-level Consumer**

```python
from confluent_kafka import Consumer


c = Consumer({
    'bootstrap.servers': 'mybroker',
    'group.id': 'mygroup',
    'auto.offset.reset': 'earliest'
})

c.subscribe(['mytopic'])

while True:
    msg = c.poll(1.0)

    if msg is None:
        continue
    if msg.error():
        print(""Consumer error: {}"".format(msg.error()))
        continue

    print('Received message: {}'.format(msg.value().decode('utf-8')))

c.close()
```

**AvroProducer**

```python
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer


value_schema_str = """"""
{
   ""namespace"": ""my.test"",
   ""name"": ""value"",
   ""type"": ""record"",
   ""fields"" : [
     {
       ""name"" : ""name"",
       ""type"" : ""string""
     }
   ]
}
""""""

key_schema_str = """"""
{
   ""namespace"": ""my.test"",
   ""name"": ""key"",
   ""type"": ""record"",
   ""fields"" : [
     {
       ""name"" : ""name"",
       ""type"" : ""string""
     }
   ]
}
""""""

value_schema = avro.loads(value_schema_str)
key_schema = avro.loads(key_schema_str)
value = {""name"": ""Value""}
key = {""name"": ""Key""}


def delivery_report(err, msg):
    """""" Called once for each message produced to indicate delivery result.
        Triggered by poll() or flush(). """"""
    if err is not None:
        print('Message delivery failed: {}'.format(err))
    else:
        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))


avroProducer = AvroProducer({
    'bootstrap.servers': 'mybroker,mybroker2',
    'on_delivery': delivery_report,
    'schema.registry.url': 'http://schema_registry_host:port'
    }, default_key_schema=key_schema, default_value_schema=value_schema)

avroProducer.produce(topic='my_topic', value=value, key=key)
avroProducer.flush()
```

**AvroConsumer**

```python
from confluent_kafka.avro import AvroConsumer
from confluent_kafka.avro.serializer import SerializerError


c = AvroConsumer({
    'bootstrap.servers': 'mybroker,mybroker2',
    'group.id': 'groupid',
    'schema.registry.url': 'http://127.0.0.1:8081'})

c.subscribe(['my_topic'])

while True:
    try:
        msg = c.poll(10)

    except SerializerError as e:
        print(""Message deserialization failed for {}: {}"".format(msg, e))
        break

    if msg is None:
        continue

    if msg.error():
        print(""AvroConsumer error: {}"".format(msg.error()))
        continue

    print(msg.value())

c.close()
```

**AdminClient**

Create topics:

```python
from confluent_kafka.admin import AdminClient, NewTopic

a = AdminClient({'bootstrap.servers': 'mybroker'})

new_topics = [NewTopic(topic, num_partitions=3, replication_factor=1) for topic in [""topic1"", ""topic2""]]
# Note: In a multi-cluster production scenario, it is more typical to use a replication_factor of 3 for durability.

# Call create_topics to asynchronously create topics. A dict
# of <topic,future> is returned.
fs = a.create_topics(new_topics)

# Wait for each operation to finish.
for topic, f in fs.items():
    try:
        f.result()  # The result itself is None
        print(""Topic {} created"".format(topic))
    except Exception as e:
        print(""Failed to create topic {}: {}"".format(topic, e))
```



Thread Safety
-------------

The `Producer`, `Consumer` and `AdminClient` are all thread safe.


Install
=======

**Install self-contained binary wheels**

    $ pip install confluent-kafka

**NOTE:** The pre-built Linux wheels do NOT contain SASL Kerberos/GSSAPI support.
          If you need SASL Kerberos/GSSAPI support you must install librdkafka and
          its dependencies using the repositories below and then build
          confluent-kafka  using the command in the ""Install from
          source from PyPi"" section below.

**Install AvroProducer and AvroConsumer**

    $ pip install ""confluent-kafka[avro]""

**Install from source from PyPi**
*(requires librdkafka + dependencies to be installed separately)*:

    $ pip install --no-binary :all: confluent-kafka


For source install, see *Prerequisites* below.


Broker Compatibility
====================
The Python client (as well as the underlying C library librdkafka) supports
all broker versions &gt;= 0.8.
But due to the nature of the Kafka protocol in broker versions 0.8 and 0.9 it
is not safe for a client to assume what protocol version is actually supported
by the broker, thus you will need to hint the Python client what protocol
version it may use. This is done through two configuration settings:

 * `broker.version.fallback=YOUR_BROKER_VERSION` (default 0.9.0.1)
 * `api.version.request=true|false` (default true)

When using a Kafka 0.10 broker or later you don't need to do anything
(`api.version.request=true` is the default).
If you use Kafka broker 0.9 or 0.8 you must set
`api.version.request=false` and set
`broker.version.fallback` to your broker version,
e.g `broker.version.fallback=0.9.0.1`.

More info here:
https://github.com/edenhill/librdkafka/wiki/Broker-version-compatibility


SSL certificates
================
If you're connecting to a Kafka cluster through SSL you will need to configure
the client with `'security.protocol': 'SSL'` (or `'SASL_SSL'` if SASL
authentication is used).

The client will use CA certificates to verify the broker's certificate.
The embedded OpenSSL library will look for CA certificates in `/usr/lib/ssl/certs/`
or `/usr/lib/ssl/cacert.pem`. CA certificates are typically provided by the
Linux distribution's `ca-certificates` package which needs to be installed
through `apt`, `yum`, et.al.

If your system stores CA certificates in another location you will need to
configure the client with `'ssl.ca.location': '/path/to/cacert.pem'`. 

Alternatively, the CA certificates can be provided by the [certifi](https://pypi.org/project/certifi/)
Python package. To use certifi, add an `import certifi` line and configure the
client's CA location with `'ssl.ca.location': certifi.where()`.


Prerequisites
=============

 * Python >= 2.7 or Python 3.x
 * [librdkafka](https://github.com/edenhill/librdkafka) >= 1.4.0 (latest release is embedded in wheels)

librdkafka is embedded in the macosx manylinux wheels, for other platforms, SASL Kerberos/GSSAPI support or
when a specific version of librdkafka is desired, following these guidelines:

  * For **Debian/Ubuntu** based systems, add this APT repo and then do `sudo apt-get install librdkafka-dev python-dev`:
http://docs.confluent.io/current/installation.html#installation-apt

 * For **RedHat** and **RPM**-based distros, add this YUM repo and then do `sudo yum install librdkafka-devel python-devel`:
http://docs.confluent.io/current/installation.html#rpm-packages-via-yum

 * On **OSX**, use **homebrew** and do `brew install librdkafka`


Developer Notes
===============

Instructions on building and testing confluent-kafka-python can be found [here](DEVELOPER.md)."
alibaba/ApsaraCache,69744,896,119,158,Organization,False,6408,3,0,229,False,ApsaraCache is a Redis branch originated from Alibaba Group.,,0,8,0,12,2,1,0,2,2,0,0,4100,0,0,0,0,0,0,318,151,,,"What is ApsaraCache ?
--------------

ApsaraCache is based on the Redis official release 4.0 and has many features and performance enhancements. ApsaraCache has proven to be very stable and efficient in production environment.

There are many features in ApsaraCache, the following two are included in this release and the other features will be gradually released in the subsequent, so stay tuned.

* ApsaraCache supports two main protocols of Memcached: the classic ASCII, and the newer binary. You can use ApsaraCache just as Memcached, and no client code need to be modified. You can persist your data by using ApsaraCache in Memcached mode just like Redis.
* In short connection scenario, ApsaraCache makes 30% performance increase compared with the open source version.


Building ApsaraCache
--------------

It is as simple as:

    % make
    
Running ApsaraCache
-------------
In default, ApsaraCache run in Redis mode. If you want ApsaraCache to run in Memcached mode, just add option 

       protocol  memcache
       
to redis.conf.

To run ApsaraCache with the default configuration just type:

    % cd src
    % ./redis-server

If you want to provide your redis.conf, you have to run it using an additional
parameter (the path of the configuration file):

    % cd src
    % ./redis-server /path/to/redis.conf

It is possible to alter the ApsaraCache configuration by passing parameters directly
as options using the command line. Examples:

    % ./redis-server --port 9999 --slaveof 127.0.0.1 6379
    % ./redis-server /etc/redis/6379.conf --loglevel debug

All the options in redis.conf are also supported as options using the command
line, with exactly the same name.

Playing with ApsaraCache in Redis mode
------------------

You can use redis-cli to play with ApsaraCache. Start a redis-server instance,
then in another terminal try the following:

    % cd src
    % ./redis-cli
    redis> ping
    PONG
    redis> set foo bar
    OK
    redis> get foo
    ""bar""
    redis> incr mycounter
    (integer) 1
    redis> incr mycounter
    (integer) 2
    redis>

You can find the list of all the available commands at http://redis.io/commands.

Playing with ApsaraCache in Memcached mode
------------------

You can use telnet to visit ApsaraCache(telnet use the classic ASCII protocol).

     % telnet your-host your-port(usually 11211)
     
       set key 10 3600 2
       ok
       STORED
       get key
       VALUE key 10 2
       ok
       END  



Enjoy!


Documentation
------------------
* [Documentation Home](https://github.com/alibaba/ApsaraCache/wiki/ApsaraCache-document)
* [Frequently Asked Questions](https://github.com/alibaba/ApsaraCache/wiki/frequently-ask-questions)

Contributing
------------------
See [ApsaraCache Contributing Guide](https://github.com/alibaba/ApsaraCache/wiki/CONTRIBUTING) for more information."
openssh/openssh-portable,21422,1104,108,849,Organization,False,10542,53,140,51,False,Portable OpenSSH,,6,6,0,,,,,37,147,14,17,,0,0,0,0,0,0,3,0,,,"# Portable OpenSSH

[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/openssh.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:openssh)

OpenSSH is a complete implementation of the SSH protocol (version 2) for secure remote login, command execution and file transfer. It includes a client ``ssh`` and server ``sshd``, file transfer utilities ``scp`` and ``sftp`` as well as tools for key generation (``ssh-keygen``), run-time key storage (``ssh-agent``) and a number of supporting programs.

This is a port of OpenBSD's [OpenSSH](https://openssh.com) to most Unix-like operating systems, including Linux, OS X and Cygwin. Portable OpenSSH polyfills OpenBSD APIs that are not available elsewhere, adds sshd sandboxing for more operating systems and includes support for OS-native authentication and auditing (e.g. using PAM).

## Documentation

The official documentation for OpenSSH are the man pages for each tool:

* [ssh(1)](https://man.openbsd.org/ssh.1)
* [sshd(8)](https://man.openbsd.org/sshd.8)
* [ssh-keygen(1)](https://man.openbsd.org/ssh-keygen.1)
* [ssh-agent(1)](https://man.openbsd.org/ssh-agent.1)
* [scp(1)](https://man.openbsd.org/scp.1)
* [sftp(1)](https://man.openbsd.org/sftp.1)
* [ssh-keyscan(8)](https://man.openbsd.org/ssh-keyscan.8)
* [sftp-server(8)](https://man.openbsd.org/sftp-server.8)

## Stable Releases

Stable release tarballs are available from a number of [download mirrors](https://www.openssh.com/portable.html#downloads). We recommend the use of a stable release for most users. Please read the [release notes](https://www.openssh.com/releasenotes.html) for details of recent changes and potential incompatibilities.

## Building Portable OpenSSH

### Dependencies

Portable OpenSSH is built using autoconf and make. It requires a working C compiler, standard library and headers, and [zlib](https://www.zlib.net/). ``libcrypto`` from either [LibreSSL](https://www.libressl.org/) or [OpenSSL](https://www.openssl.org) may also be used, but OpenSSH may be built without it supporting a subset of crypto algorithms.

FIDO security token support need [libfido2](https://github.com/Yubico/libfido2) and its dependencies. Also, certain platforms and build-time options may require additional dependencies, see README.platform for details.

### Building a release

Releases include a pre-built copy of the ``configure`` script and may be built using:

```
tar zxvf openssh-X.YpZ.tar.gz
cd openssh
./configure # [options]
make && make tests
```

See the [Build-time Customisation](#build-time-customisation) section below for configure options. If you plan on installing OpenSSH to your system, then you will usually want to specify destination paths.
 
### Building from git

If building from git, you'll need [autoconf](https://www.gnu.org/software/autoconf/) installed to build the ``configure`` script. The following commands will check out and build portable OpenSSH from git:

```
git clone https://github.com/openssh/openssh-portable # or https://anongit.mindrot.org/openssh.git
cd openssh-portable
autoreconf
./configure
make && make tests
```

### Build-time Customisation

There are many build-time customisation options available. All Autoconf destination path flags (e.g. ``--prefix``) are supported (and are usually required if you want to install OpenSSH).

For a full list of available flags, run ``configure --help`` but a few of the more frequently-used ones are described below. Some of these flags will require additional libraries and/or headers be installed.

Flag | Meaning
--- | ---
``--with-pam`` | Enable [PAM](https://en.wikipedia.org/wiki/Pluggable_authentication_module) support. [OpenPAM](https://www.openpam.org/), [Linux PAM](http://www.linux-pam.org/) and Solaris PAM are supported.
``--with-libedit`` | Enable [libedit](https://www.thrysoee.dk/editline/) support for sftp.
``--with-kerberos5`` | Enable Kerberos/GSSAPI support. Both [Heimdal](https://www.h5l.org/) and [MIT](https://web.mit.edu/kerberos/) Kerberos implementations are supported.
``--with-selinux`` | Enable [SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux) support.
``--with-security-key-builtin`` | Include built-in support for U2F/FIDO2 security keys. This requires [libfido2](https://github.com/Yubico/libfido2) be installed.

## Development

Portable OpenSSH development is discussed on the [openssh-unix-dev mailing list](https://lists.mindrot.org/mailman/listinfo/openssh-unix-dev) ([archive mirror](https://marc.info/?l=openssh-unix-dev)). Bugs and feature requests are tracked on our [Bugzilla](https://bugzilla.mindrot.org/).

## Reporting bugs

_Non-security_ bugs may be reported to the developers via [Bugzilla](https://bugzilla.mindrot.org/) or via the mailing list above. Security bugs should be reported to [openssh@openssh.com](mailto:openssh.openssh.com)."
danielgtaylor/jpeg-archive,2052,1021,47,106,User,False,159,2,8,18,False,Utilities for archiving JPEGs for long term storage.,,0,6,0,36,52,3,1,4,27,1,2,2350,2,2,6,6,0,0,86,,272,,"JPEG Archive [![Build Status](http://img.shields.io/travis/danielgtaylor/jpeg-archive.svg?style=flat)](https://travis-ci.org/danielgtaylor/jpeg-archive) [![Build status](https://ci.appveyor.com/api/projects/status/1p7hrrq380xuqlyh?svg=true)](https://ci.appveyor.com/project/danielgtaylor/jpeg-archive) [![Version](http://img.shields.io/badge/version-2.2.0-blue.svg?style=flat)](https://github.com/danielgtaylor/jpeg-archive/releases) [![License](http://img.shields.io/badge/license-MIT-red.svg?style=flat)](http://dgt.mit-license.org/)
============
Utilities for archiving photos for saving to long term storage or serving over the web. The goals are:

 * Use a common, well supported format (JPEG)
 * Minimize storage space and cost
 * Identify duplicates / similar photos

Approach:

 * Command line utilities and scripts
 * Simple options and useful help
 * Good quality output via sane defaults

Contributions to this project are very welcome.

Download
--------
You can download the latest source and binary releases from the [JPEG Archive releases page](https://github.com/danielgtaylor/jpeg-archive/releases). Windows binaries for the latest commit are available from the [Windows CI build server](https://ci.appveyor.com/project/danielgtaylor/jpeg-archive/build/artifacts).

If you are looking for an easy way to run these utilities in parallel over many files to utilize all CPU cores, please also download [Ladon](https://github.com/danielgtaylor/ladon) or [GNU Parallel](https://www.gnu.org/software/parallel/). You can then use the `jpeg-archive` command below or use `ladon` directly. Example:

```bash
# Re-compress JPEGs and replace the originals
ladon ""Photos/**/*.jpg"" -- jpeg-recompress FULLPATH FULLPATH

# Re-compress JPEGs into the new directory 'Comp'
ladon -m Comp/RELDIR ""Photos/**/*.jpg"" -- jpeg-recompress FULLPATH Comp/RELPATH
```

Utilities
---------
The following utilities are part of this project. All of them accept a `--help` parameter to see the available options.

### jpeg-archive
Compress RAW and JPEG files in a folder utilizing all CPU cores. This is a simple shell script that uses the utilities below. It requires:

* a POSIX-compatible shell such as Bash
* [Ladon](https://github.com/danielgtaylor/ladon) or [GNU Parallel](https://www.gnu.org/software/parallel/)
* [dcraw](http://www.cybercom.net/~dcoffin/dcraw/)
* [exiftool](http://www.sno.phy.queensu.ca/~phil/exiftool/)
* jpeg-recompress (part of this project)

```bash
# Compress a folder of images
cd path/to/photos
jpeg-archive

# Custom quality and metric
jpeg-archive --quality medium --method smallfry
```

### jpeg-recompress
Compress JPEGs by re-encoding to the smallest JPEG quality while keeping _perceived_ visual quality the same and by making sure huffman tables are optimized. This is a __lossy__ operation, but the images are visually identical and it usually saves 30-70% of the size for JPEGs coming from a digital camera, particularly DSLRs. By default all EXIF/IPTC/XMP and color profile metadata is copied over, but this can be disabled to save more space if desired.

There is no need for the input file to be a JPEG. In fact, you can use `jpeg-recompress` as a replacement for `cjpeg` by using PPM input and the `--ppm` option.

The better the quality of the input image is, the better the output will be.

Some basic photo-related editing options are available, such as removing fisheye lens distortion.

#### Demo
Below are two 100% crops of [Nikon's D3x Sample Image 2](http://static.nikonusa.com/D3X_gallery/index.html). The left shows the original image from the camera, while the others show the output of `jpeg-recompress` with the `medium` quality setting and various comparison methods. By default SSIM is used, which lowers the file size by **88%**. The recompression algorithm chooses a JPEG quality of 80. By comparison the `veryhigh` quality setting chooses a JPEG quality of 93 and saves 70% of the file size.

![JPEG recompression comparison](https://cloud.githubusercontent.com/assets/106826/3633843/5fde26b6-0eff-11e4-8c98-f18dbbf7b510.png)

Why are they different sizes? The default quality settings are set to average out to similar visual quality over large data sets. They may differ on individual photos (like above) because each metric considers different parts of the image to be more or less important for compression.

#### Image Comparison Metrics
The following metrics are available when using `jpeg-recompress`. SSIM is the default.

Name     | Option        | Description
-------- | ------------- | -----------
MPE      | `-m mpe`      | Mean pixel error (as used by [imgmin](https://github.com/rflynn/imgmin))
SSIM     | `-m ssim`     | [Structural similarity](http://en.wikipedia.org/wiki/Structural_similarity) **DEFAULT**
MS-SSIM* | `-m ms-ssim`  | Multi-scale structural similarity (slow!) ([2008 paper](https://doi.org/10.1117/12.768060))
SmallFry | `-m smallfry` | Linear-weighted BBCQ-like ([original project](https://github.com/dwbuiten/smallfry), [2011 BBCQ paper](http://spie.org/Publications/Proceedings/Paper/10.1117/12.872231))

**Note**: The SmallFry algorithm may be [patented](http://www.jpegmini.com/main/technology) so use with caution.

#### Subsampling
The JPEG format allows for subsampling of the color channels to save space. For each 2x2 block of pixels per color channel (four pixels total) it can store four pixels (all of them), two pixels or a single pixel. By default, the JPEG encoder subsamples the non-luma channels to two pixels (often referred to as 4:2:0 subsampling). Most digital cameras do the same because of limitations in the human eye. This may lead to unintended behavior for specific use cases (see #12 for an example), so you can use `--subsample disable` to disable this subsampling.

#### Example Commands

```bash
# Default settings
jpeg-recompress image.jpg compressed.jpg

# High quality example settings
jpeg-recompress --quality high --min 60 image.jpg compressed.jpg

# Slow high quality settings (3-4x slower than above, slightly more accurate)
jpeg-recompress --accurate --quality high --min 60 image.jpg compressed.jpg

# Use SmallFry instead of SSIM
jpeg-recompress --method smallfry image.jpg compressed.jpg

# Use 4:4:4 sampling (disables subsampling).
jpeg-recompress --subsample disable image.jpg compressed.jpg

# Remove fisheye distortion (Tokina 10-17mm on APS-C @ 10mm)
jpeg-recompress --defish 2.6 --zoom 1.2 image.jpg defished.jpg

# Read from stdin and write to stdout with '-' as the filename
jpeg-recompress - - <image.jpg >compressed.jpg

# Convert RAW to JPEG via PPM from stdin
dcraw -w -q 3 -c IMG_1234.CR2 | jpeg-recompress --ppm - compressed.jpg

# Disable progressive mode (not recommended)
jpeg-recompress --no-progressive image.jpg compressed.jpg

# Disable all output except for errors
jpeg-recompress --quiet image.jpg compressed.jpg
```

### jpeg-compare
Compare two JPEG photos to judge how similar they are. The `fast` comparison method returns an integer from 0 to 99, where 0 is identical. PSNR, SSIM, and MS-SSIM return floats but require images to be the same dimensions.

```bash
# Do a fast compare of two images
jpeg-compare image1.jpg image2.jpg

# Calculate PSNR
jpeg-compare --method psnr image1.jpg image2.jpg

# Calculate SSIM
jpeg-compare --method ssim image1.jpg image2.jpg
```

### jpeg-hash
Create a hash of an image that can be used to compare it to other images quickly.

```bash
jpeg-hash image.jpg
```

Building
--------
### Dependencies
 * [mozjpeg](https://github.com/mozilla/mozjpeg)

#### Ubuntu
Ubuntu users can install via `apt-get`:

```bash
sudo apt-get install build-essential autoconf pkg-config nasm libtool
git clone https://github.com/mozilla/mozjpeg.git
cd mozjpeg
autoreconf -fiv
./configure --with-jpeg8
make
sudo make install
```

#### Mac OS X
Mac users can install it via [Homebrew](http://brew.sh/):

```bash
brew install mozjpeg
```

#### FreeBSD

```bash
pkg install mozjpeg
git clone https://github.com/danielgtaylor/jpeg-archive.git
cd jpeg-archive/
gmake
sudo gmake install
```

#### Windows
The `Makefile` should work with MinGW/Cygwin/etc and standard GCC. Patches welcome.

To get everything you need to build, install these:

* [CMake](https://cmake.org/download/)
* [NASM](https://www.nasm.us/)
* [MinGW](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/installer/mingw-w64-install.exe/download) (installed to e.g. `C:\mingw`)
* [Github for Windows](https://windows.github.com/)

Run Github for windows. In the settings, set **Git Bash** as the shell. Open Git Shell from the start menu.

```bash
# Update PATH to include MinGW/NASM bin folder, location on your system may vary
export PATH=/c/mingw/mingw32/bin:/c/Program\ Files \(x68\)/nasm:$PATH

# Build mozjpeg or download https://www.dropbox.com/s/98jppfgds2xjblu/libjpeg.a
git clone https://github.com/mozilla/mozjpeg.git
cd mozjpeg
cmake -G ""MSYS Makefiles"" -D CMAKE_C_COMPILER=gcc.exe -D CMAKE_MAKE_PROGRAM=mingw32-make.exe  -D WITH_JPEG8=1
mingw32-make
cd ..

# Build jpeg-archive
git clone https://github.com/danielgtaylor/jpeg-archive
cd jpeg-archive
CC=gcc mingw32-make
```

JPEG-Archive should now be built.

### Compiling (Linux and Mac OS X)
The `Makefile` should work as-is on Ubuntu and Mac OS X. Other platforms may need to set the location of `libjpeg.a` or make other tweaks.

```bash
make
```

### Installation
Install the binaries into `/usr/local/bin`:

```bash
sudo make install
```

Links / Alternatives
--------------------
* https://github.com/rflynn/imgmin
* https://news.ycombinator.com/item?id=803839

License
-------
* JPEG-Archive is copyright &copy; 2015 Daniel G. Taylor
* Image Quality Assessment (IQA) is copyright 2011, Tom Distler (http://tdistler.com)
* SmallFry is copyright 2014, Derek Buitenhuis (https://github.com/dwbuiten)

All are released under an MIT license.

http://dgt.mit-license.org/"
orangeduck/Corange,240515,1165,95,145,User,False,355,1,0,9,False,Pure C Game Engine,http://www.youtube.com/watch?v=482Gxq…,0,0,0,10,24,7,0,1,24,1,4,3330,1,2,4,4,0,0,16,,1,,"Corange game engine
===================
 
Version 0.8.0

Written in Pure C, SDL and OpenGL.

Running
-------
 
Corange is a library, but to take a quick look at some of the things it does you can [Look at some of the Demos](http://www.youtube.com/watch?v=482GxqTWXtA). Warning: Some things shown are from a previous version and may not remain the same in this version.
 

Compiling
---------
 
To compile on Windows you need MinGW and then you should be able to run ""make"" as usual. You will need to have installed SDL, SDL_Mixer and SDL_Net.

    make

To compile on Linux you need to install SDL2. Then you should run ""make""

    sudo apt-get install libsdl2-dev
    sudo apt-get install libsdl2-mixer-dev
    sudo apt-get install libsdl2-net-dev
    make


Overview
--------

* Small, Simple, Powerful, Cross platform
* Clean and easy Asset, UI, Entity management
* Modern Deferred renderer


Demos
-----

I'm a graphics programmer by trade so apologies that most of the demos are graphical apps; they're just what I love!

* __renderers__ Shows off the various renderers with shaders, shadows, animation etc.
* __metaballs__ Uses OpenCL/OpenGL interop to do Metaball rendering.
* __noise__ Feedback based noise pattern on screen using shader. Can generate tileable perlin noise in software.
* __platformer__ Basic platforming game. Fairly well commented.
* __sea__ Renders a sea-like surface, a ship, and some collision detection.
* __scotland__ Demonstrates terrain system.
* __tessellation__ Demo showing tessellation shaders in OpenGL 4.

 
FAQ
---

* __How is that pronounced?__

 Rhymes with Purple.

* __Why not C++?__
 
 There are plenty of C++ engines which do what I've done here and better. Pure C game engines on the other hand are much rarer. Corange provided me an outlet to practice my C skills. Of course if you are just linking to it you can still program your game/executable using C++.
 
* __What stuff does it do?__

 I've used it as a platform for trying out all sorts of techniques and effects. These features are not out-of-the-box or plug-in-and-play, but if you are a developer who has knowledge of what they are, you should be able to utilize what I have written. Some are WIP or rough around the edges.
 
 Deferred Rendering / UI Rendering / Text Rendering. Multiple Lights. Post effects. SSAO. Shadow Mapping. Color Correction. Skeletal Animation. Inverse Kinematics. Collision Detection. OpenCL support. Asset / Entity / UI Management. Terrain. File loaders including .dds, .wav, .bmp, .obj, .smd. Maths and Geometry. And More...
 
* __Can I use this for 2D stuff?__
 
 Certainly. Though Corange doesn't provide a 2D renderer for you. That you can write yourself. Believe it or not, making a generalized 2D renderer can be exceedingly complicated when you have to optimise for different sprites, tile sets, dynamic objects and all sorts of other effects. You're better off writing the rendering code application specific.
 
* __Can I contact you about something?__

 Yes - `contact@theorangeduck.com`

  
Using / Contributing
--------------------
 
This is still mainly a personal project and so there are going to be lots of bugs, unfinished features and messy bits of code. The engine is heavily WIP and subject to sweeping changes. It isn't really viable to use without also being part of the project development and in communication with me. Rather than a full game engine like Unity, Corange is more of a framework and gives you access to features at about the same level as XNA.

I have a big backlog of Work in Progress changes I need to push up to the repository once they get to a reasonable point so if you are interested in those please contact me.
  
Saying that, it is a great excuse to practise your C and I very much welcome help. If the project appeals to you here are a couple of quick things that might help get you started.
  
* First take a look at the demos. These give a brief overview of how Corange can be used. The platformer demo is probably the most commented.

* There is no real documentation so your first port of call is the header files and your second is the c files. The code has very minimal comments but should be pretty clear most of the time.

* Corange doesn't hide anything from you. OpenGL and SDL calls are in the namespace so you've got access to the basics. The corange_init and corange_finish functions are fairly short so it is even possible to not call them and only use the components you want.

* Structs are typedefed without their pointer. The reason for this is a personal choice but there are also quite a few data types which are passed by value on the stack (vectors, matrices, spheres, boxes). I didn't want the notion of these to get confused.

* Some important parts of the engine are the asset, UI and entity managers. These basically let you access and store assets (models, textures - objects in the file system) and entities (lights, cameras, engine objects) and UI elements. They clean up memory on destruction and let you get pointers from all parts of the code.

* Corange mangles the namespace pretty badly, taking names such as ""error"", ""warning"", ""vec2"" and ""image"". It isn't a general purpose library. But I've still tried to decouple stuff so it should be possible to extract certain code if you need it."
bitcraze/crazyflie-firmware,13423,628,100,628,Organization,False,1560,17,26,56,False,"The main firmware for the Crazyflie Nano Quadcopter, Crazyflie Bolt Quadcopter and Roadrunner Positioning Tag.",,0,9,1,70,323,22,23,2,196,2,17,,0,0,0,0,0,0,46,8,,,"# Crazyflie Firmware  [![Build Status](https://api.travis-ci.org/bitcraze/crazyflie-firmware.svg)](https://travis-ci.org/bitcraze/crazyflie-firmware)

This project contains the source code for the firmware used in the Crazyflie range of platforms, including
the Crazyflie 2.X and the Roadrunner.

### Crazyflie 1.0 support

The 2017.06 release was the last release with Crazyflie 1.0 support. If you want
to play with the Crazyflie 1.0 and modify the code, please clone this repo and
branch off from the 2017.06 tag.

## Dependencies

You'll need to use either the [Crazyflie VM](https://wiki.bitcraze.io/projects:virtualmachine:index),
[the toolbelt](https://wiki.bitcraze.io/projects:dockerbuilderimage:index) or
install some ARM toolchain.

### Install a toolchain

#### OS X
```bash
brew tap PX4/homebrew-px4
brew install gcc-arm-none-eabi
```

#### Debian/Ubuntu

Tested on Ubuntu 14.04 64b, Ubuntu 16.04 64b, and Ubuntu 18.04 64b:

For Ubuntu 14.04 :

```bash
sudo add-apt-repository ppa:terry.guo/gcc-arm-embedded
sudo apt-get update
sudo apt-get install libnewlib-arm-none-eabi
```

For Ubuntu 16.04 and Ubuntu 18.04:

```bash
sudo add-apt-repository ppa:team-gcc-arm-embedded/ppa
sudo apt-get update
sudo apt install gcc-arm-embedded
```

Note: Do not use the `gcc-arm-none-eabi` package that is part of the Ubuntu repository as this is outdated.

#### Arch Linux

```bash
sudo pacman -S community/arm-none-eabi-gcc community/arm-none-eabi-gdb community/arm-none-eabi-newlib
```

#### Windows

The GCC ARM Embedded toolchain for Windows is available at [launchpad.net](https://launchpad.net/gcc-arm-embedded/+download). Download the zip archive rather than the executable installer. There are a few different systems for running UNIX-style shells and build systems on Windows; the instructions below are for [Cygwin](https://www.cygwin.com/).

Install Cygwin with [setup-x86_64.exe](https://www.cygwin.com/setup-x86_64.exe). Use the standard `C:\cygwin64` installation directory and install at least the `make` and `git` packages.

Download the latest `gcc-arm-none-eabi-*-win32.zip` archive from [launchpad.net](https://launchpad.net/gcc-arm-embedded/+download). Create the directory `C:\cygwin64\opt\gcc-arm-none-eabi` and extract the contents of the zip file to it.

Launch a Cygwin terminal and run the following to append to your `~/.bashrc` file:
```bash
echo '[[ $PATH == */opt/gcc-arm-none-eabi/bin* ]] || export PATH=/opt/gcc-arm-none-eabi/bin:$PATH' >>~/.bashrc
source ~/.bashrc
```

Verify the toolchain installation with `arm-none-eabi-gcc --version`

### Cloning

This repository uses git submodules. Clone with the `--recursive` flag

```bash
git clone --recursive https://github.com/bitcraze/crazyflie-firmware.git
```

If you already have cloned the repo without the `--recursive` option, you need to
get the submodules manually

```bash
cd crazyflie-firmware
git submodule init
git submodule update
```


## Compiling

### Crazyflie 2.X

This is the default build so just running ```make``` is enough or:
```bash
make PLATFORM=cf2
```

or with the toolbelt

```bash
tb make PLATFORM=cf2
```

### Roadrunner

Use the ```tag``` platform

```bash
make PLATFORM=tag
```

or with the toolbelt

```bash
tb make PLATFORM=tag
```


### config.mk
To create custom build options create a file called `config.mk` in the `tools/make/`
folder and fill it with options. E.g.
```
PLATFORM=CF2
DEBUG=1
```
More information can be found on the
[Bitcraze documentation](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/)


```
# Make targets:
```
all        : Shortcut for build
compile    : Compile cflie.hex. WARNING: Do NOT update version.c
build      : Update version.c and compile cflie.elf/hex
clean_o    : Clean only the Objects files, keep the executables (ie .elf, .hex)
clean      : Clean every compiled files
mrproper   : Clean every compiled files and the classical editors backup files

cload      : If the crazyflie-clients-python is placed on the same directory level and
             the Crazyradio/Crazyradio PA is inserted it will try to flash the firmware
             using the wireless bootloader.
flash      : Flash .elf using OpenOCD
halt       : Halt the target using OpenOCD
reset      : Reset the target using OpenOCD
openocd    : Launch OpenOCD
```

# Unit testing

## Running all unit tests

With the environment set up locally

        make unit

with the docker builder image and the toolbelt

        tb make unit

## Running one unit test

When working with one specific file it is often convenient to run only one unit test

       make unit FILES=test/utils/src/test_num.c

or with the toolbelt

       tb make unit FILES=test/utils/src/test_num.c

## Running unit tests with specific build settings

Defines are managed by make and are passed on to the unit test code. Use the
normal ways of configuring make when running tests. For instance to run test
for Crazyflie 1

      make unit LPS_TDOA_ENABLE=1

## Dependencies

Frameworks for unit testing and mocking are pulled in as git submodules.

The testing framework uses ruby and rake to generate and run code.

To minimize the need for installations and configuration, use the docker builder
image (bitcraze/builder) that contains all tools needed. All scripts in the
tools/build directory are intended to be run in the image. The
[toolbelt](https://wiki.bitcraze.io/projects:dockerbuilderimage:index) makes it
easy to run the tool scripts."
libav/libav,88763,791,79,350,Organization,False,45232,9,116,495,False,"Libav github mirror, clone of git://git.libav.org/libav",http://www.libav.org,3,0,0,,,,,7,27,1,0,7103,0,0,0,0,0,0,3,4,,,"Libav
=====

[![Build Status](https://travis-ci.org/libav/libav.svg)](https://travis-ci.org/libav/libav)

Libav is a collection of libraries and tools to process multimedia content
such as audio, video, subtitles and related metadata.

## Libraries

* `libavcodec` provides implementation of a wider range of codecs.
* `libavformat` implements streaming protocols, container formats and basic I/O access.
* `libavutil` includes hashers, decompressors and miscellaneous utility functions.
* `libavfilter` provides a mean to alter decoded Audio and Video through chain of filters.
* `libavdevice` provides an abstraction to access capture and playback devices.
* `libavresample` implements audio mixing and resampling routines.
* `libswscale` implements color conversion and scaling routines.

## Tools

* [avconv](http://libav.org/avconv.html) is a command line toolbox to
  manipulate, convert and stream multimedia content.
* [avplay](http://libav.org/avplay.html) is a minimalistic multimedia player.
* [avprobe](http://libav.org/avprobe.html) is a simple analisys tool to inspect
  multimedia content.
* Additional small tools such as `aviocat`, `ismindex` and `qt-faststart`.

## Documentation

The offline documentation is available in the **doc/** directory.

The online documentation is available in the main [website](http://libav.org)
and in the [wiki](http://wiki.libav.org).

### Examples

Conding examples are available in the **doc/example** directory.

## License

Libav codebase is mainly LGPL-licensed with optional components licensed under
GPL. Please refer to the LICENSE file for detailed information."
seemoo-lab/nexmon,370760,1436,126,312,Organization,False,820,4,8,19,False,"The C-based Firmware Patching Framework for Broadcom/Cypress WiFi Chips that enables Monitor Mode, Frame Injection and much more",,7,13,0,204,132,24,20,18,54,4,2,1328,3,15,20280,49,0,0,47,6,,,"![NexMon logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/nexmon.png)

# What is nexmon?
Nexmon is our C-based firmware patching framework for Broadcom/Cypress WiFi chips 
that enables you to write your own firmware patches, for example, to enable monitor
mode with radiotap headers and frame injection.

Before we started to work on this repository, we developed patches for the Nexus 5 (with bcm4339 WiFi chip) in the [bcm-public](https://github.com/seemoo-lab/bcm-public)  repository and those for the Raspberry Pi 3 (with bcm43430a1 WiFi chip) in the [bcm-rpi3](https://github.com/seemoo-lab/bcm-rpi3) repository. To remove the development overhead of maintaining multiple separate repositories, we decided to merge them in this repository and add support for some additional devices. In contrast to the former repositories, here, you can only build the firmware patch without drivers and kernels. The Raspberry Pi 3 makes an exception, as here it is always required to also build the driver.

# Give Feedback
We setup a survey to learn about who uses Nexmon to which purpose and how we could improve Nexmon. We would be happy if every Nexmon user filled out this survey: https://nexmon.org/survey

# WARNING
Our software may damage your hardware and may void your hardware’s warranty! You use our tools at your own risk and responsibility! If you don't like these terms, don't use nexmon!

# Important changes
* We started to collect usage statistics. In the file [STATISTICS.md](STATISTICS.md), you can find information on which data we collect and how you can opt-out of the statistics collection
* Starting with commit 4f8697743dc46ffc37d87d960825367531baeef9 the brcmfmac driver for the RPi3 can now be used as a regular interface. You need to use nexutil to activate monitor mode (`nexutil -m2` for monitor mode with radiotap headers), which will automtically adjust the interface type.
* Starting with commit 184480edd6696392aae5f818f305f244606f2d17 you can choose different monitor mode options using nexutil. Use `nexutil -m1` to activate monitor mode without radiotap headers, `nexutil -m2` to activate it with radiotap headers. The numbers were chosen as non-Nexmon firmwares also support native monitor mode without radiotap headers by activating monitor mode with `nexutil -m1`.
* Starting with commit 1bcfdc95b4395c2e8bdd962791ae20c4ba602f5b we changed the nexutil interface. Instead of calling `nexutil -m true` to activate monitor mode, you should now write `nexutil -m1`. To get the current monitor mode state execute `nexutil -m` instead of `nexutil -n`.

# Supported Devices
The following devices are currently supported by our nexmon firmware patch.

WiFi Chip                 | Firmware Version     | Used in                   | Operating System          |  M  | RT  |  I  | FP  | UC  | CT 
------------------------- | -------------------- | ------------------------- | ------------------------- | --- | --- | --- | --- | --- | ---
bcm4330                   | 5_90_100_41_sta      | Samsung Galaxy S2         | Cyanogenmod 13.0          |  X  |  X  |     |  X  |  X  |  O 
bcm4335b0                 | 6.30.171.1_sta       | Samsung Galaxy S4         | LineageOS 14.1            |  X  |  X  |  X  |     |  X  |  O 
bcm4339                   | 6_37_34_43           | Nexus 5                   | Android 6 Stock           |  X  |  X  |  X  |  X  |  X  |  O 
bcm43430a1<sup>1</sup>    | 7_45_41_26           | Raspberry Pi 3 and Zero W | Raspbian 8                |  X  |  X  |  X  |  X  |  X  |  O 
bcm43430a1<sup>1</sup>    | 7_45_41_46           | Raspberry Pi 3 and Zero W | Raspbian Stretch          |  X  |  X  |  X  |  X  |  X  |  O 
bcm43451b1                | 7_63_43_0            | iPhone 6                  | iOS 10.1.1 (14B100)       |     |     |     |  X  |  X  |    
bcm43455                  | 7_45_77_0_hw         | Huawei P9                 | Android 7 Stock           |  X  |  X  |  X  |  X  |  X  |    
bcm43455                  | 7_120_5_1_sta_C0     | Galaxy J7 2017            | ?                         |     |     |     |  X  |  X  |    
bcm43455                  | 7_45_77_0_hw(8-2017) | Huawei P9                 | Android 7 Stock           |  X  |  X  |  X  |  X  |  X  |    
bcm43455<sup>5</sup>      | 7_46_77_11_hw        | Huawei P9                 | Android 8 China Stock     |  X  |  X  |  X  |  X  |  X  |    
bcm43455                  | 7_45_59_16           | Sony Xperia Z5 Compact    | LineageOS 14.1            |  X  |  X  |  X  |  X  |  X  |    
bcm43455c0                | 7_45_154             | Raspberry Pi B3+/B4       | Raspbian Kernel 4.9/14/19 |  X  |  X  |     |  X  |  X  |    
bcm43455c0                | 7_45_189             | Raspberry Pi B3+/B4       | Raspbian Kernel 4.14/19   |  X  |  X  |     |  X  |  X  |    
bcm4356                   | 7_35_101_5_sta       | Nexus 6                   | Android 7.1.2             |  X  |  X  |     |  X  |  X  |  O 
bcm4358                   | 7_112_200_17_sta     | Nexus 6P                  | Android 7 Stock           |  X  |  X  |     |  X  |  X  |  O 
bcm4358                   | 7_112_201_3_sta      | Nexus 6P                  | Android 7.1.2 Stock       |  X  |  X  |     |  X  |  X  |  O 
bcm4358<sup>2</sup>       | 7_112_300_14_sta     | Nexus 6P                  | Android 8.0.0 Stock       |  X  |  X  |  X  |  X  |  X  |  O 
bcm43596a0<sup>3</sup>    | 9_75_155_45_sta_c0   | Samsung Galaxy S7         | Android 7 Stock           |  X  |     |     |  O  |  X  |    
bcm43596a0<sup>3,2</sup>  | 9_96_4_sta_c0        | Samsung Galaxy S7         | LineageOS 14.1            |  X  |  X  |  X  |  O  |  X  |    
bcm4375b1<sup>3,5,6</sup> | 18_38_18_sta         | Samsung Galaxy S10        | LineageOS 16              |     |     |     |  O  |  X  |    
qca9500<sup>4</sup>       | 4-1-0_55             | TP-Link Talon AD7200      | Custom LEDE Image         |     |     |     |     |     |    

<sup>1</sup> bcm43430a1 was wrongly labeled bcm43438 in the past.

<sup>2</sup> use LD_PRELOAD=libnexmon.so instead of LD_PRELOAD=libfakeioctl.so to inject frames through ioctls

<sup>3</sup> flash patches need to be 8 bytes long and aligned on an 8 byte boundary

<sup>4</sup> 802.11ad Wi-Fi chip from first 60 GHz Wi-Fi router Talon AD7200. Patch your firmware using [nexmon-arc](https://github.com/seemoo-lab/nexmon-arc) and run it with our custom LEDE image [lede-ad7200](https://github.com/seemoo-lab/lede-ad7200)

<sup>5</sup> Disabled the execution protection (called Execute Never) on region 1, because it interferes with the nexmon code (Permission fault on Section)

<sup>6</sup> To use nexutil, you need to deactivate SELinux or set it to permissive

## Legend
- M = Monitor Mode
- RT = Monitor Mode with RadioTap headers
- I = Frame Injection
- FP = Flash Patching
- UC = Ucode Compression
- CT = c't Article Support (for consistent support, use our ct-artikel branch)

# Steps to create your own firmware patches

## Build patches for bcm4330, bcm4339 and bcm4358 using a x86 computer running Linux (e.g. Ubuntu 16.04)
* Install some dependencies: `sudo apt-get install git gawk qpdf adb flex bison`
* **Only necessary for x86_64 systems**, install i386 libs: 

  ```
  sudo dpkg --add-architecture i386
  sudo apt-get update
  sudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386
  ```
* Clone our repository: `git clone https://github.com/seemoo-lab/nexmon.git`
* In the root directory of the repository: `cd nexmon`
  * Setup the build environment: `source setup_env.sh`
  * Compile some build tools and extract the ucode and flashpatches from the original firmware files: `make`
* Go to the *patches* folder of your target device (e.g. bcm4339 for the Nexus 5): `cd patches/bcm4339/6_37_34_43/nexmon/`
  * Compile a patched firmware: `make`
  * Generate a backup of your original firmware file: `make backup-firmware`
  * Install the patched firmware on your smartphone: `make install-firmware` (make sure your smartphone is connected to your machine beforehand)

### Using the Monitor Mode patch
* Install at least *nexutil* and *libfakeioctl* from our utilities. The easiest way to do this is by using this app: https://nexmon.org/app. But you can also build it from the source by executing `make` in the *utilties* folder (Note: you will need the Android NDK properly installed for this).
* Connect to your Android phone using the ADB tools: `adb shell`
* Make sure you are **not** connected to an access point
* Use *nexutil* to enable monitor mode: `nexutil -m2`
* At this point the monitor mode is active. There is no need to call *airmon-ng*. 
* **Important:** Most tools need a Radiotap interface to work properly. *libfakeioctl* emulates this type of interface for you, therefore, use LD_PRELOAD to load this library when you call the favourite tool (e.g. tcpdump or airodump-ng): `LD_PRELOAD=libfakeioctl.so tcpdump -i wlan0`
* *untested hint:* Thanks to XDA member ruleh, there is a bcmdhd driver patch to activate native monitor mode, see: https://github.com/ruleh/misc/tree/master/monitor

### Using nexutil over UDP on Nexus 5
To be able to communicate with the firmware without root priviledges, we created a UDP interface accessible through the `libnexio`, which is also used by `nexutil`. You first have to prove to the firmware that you generally have root priviledges by setting a security cookie. Then you can use it for UDP based connections. Your wlan0 interface also needs an IP address in the 192.168.222.0/24 range or you have to change the default nexutil `broadcast-ip`:
* Set the IP address of the wlan0 interface: `ifconfig wlan0 192.168.222.1 netmask 255.255.255.0`
* Set the security cookie as root: `nexutil -x<cookie (uint)>`
* Start a UDP connection for example to activate monitor mode: `nexutil -X<cookie> -m1`

## Build patches for bcm43430a1 on the RPI3/Zero W or bcm434355c0 on the RPI3+/RPI4 using Raspbian (recommended)
**Note:** We currently support Kernel Version 4.4 (depricated), 4.9, 4.14 and 4.19. Raspbian contains firmware version 7.45.154 for the bcm43455c0. We also support the newer firmware release 7.45.189 from Cypress. Please, try which works best for you.
* Make sure the following commands are executed as root: `sudo su`
* Upgrade your Raspbian installation: `apt-get update && apt-get upgrade`
* Install the kernel headers to build the driver and some dependencies: `sudo apt install raspberrypi-kernel-headers git libgmp3-dev gawk qpdf bison flex make`
* Clone our repository: `git clone https://github.com/seemoo-lab/nexmon.git`
* Go into the root directory of our repository: `cd nexmon`
* Check if `/usr/lib/arm-linux-gnueabihf/libisl.so.10` exists, if not, compile it from source:
  * `cd buildtools/isl-0.10`, `./configure`, `make`, `make install`, `ln -s /usr/local/lib/libisl.so /usr/lib/arm-linux-gnueabihf/libisl.so.10`
* Check if `/usr/lib/arm-linux-gnueabihf/libmpfr.so.4` exists, if not, compile it from source:
  * `cd buildtools/mpfr-3.1.4`, `autoreconf -f -i`, `./configure`, `make`, `make install`, `ln -s /usr/local/lib/libmpfr.so /usr/lib/arm-linux-gnueabihf/libmpfr.so.4`
* Then you can setup the build environment for compiling firmware patches
  * Setup the build environment: `source setup_env.sh`
  * Compile some build tools and extract the ucode and flashpatches from the original firmware files: `make`
* Go to the *patches* folder for the bcm43430a1/bcm43455c0 chipset: `cd patches/bcm43430a1/7_45_41_46/nexmon/` / `patches/bcm43455c0/<7_45_154 or 7_45_189>/nexmon/`
  * Compile a patched firmware: `make`
  * Generate a backup of your original firmware file: `make backup-firmware`
  * Install the patched firmware on your RPI3: `make install-firmware`
* Install nexutil: from the root directory of our repository switch to the nexutil folder: `cd utilities/nexutil/`. Compile and install nexutil: `make && make install`.
* *Optional*: remove wpa_supplicant for better control over the WiFi interface: `apt-get remove wpasupplicant`
* **Note:** To connect to regular access points you have to execute `nexutil -m0` first

### Using the Monitor Mode patch
* Thanks to the prior work of Mame82, you can setup a new monitor mode interface by executing:
```iw phy `iw dev wlan0 info | gawk '/wiphy/ {printf ""phy"" $2}'` interface add mon0 type monitor```
* To activate monitor mode in the firmware, simply set the interface up: `ifconfig mon0 up`.
* At this point, monitor mode is active. There is no need to call *airmon-ng*. 
* The interface already set the Radiotap header, therefore, tools like *tcpdump* or *airodump-ng* can be used out of the box: `tcpdump -i mon0`
* *Optional*: To make the RPI3 load the modified driver after reboot:
  * Find the path of the default driver at reboot: `modinfo brcmfmac` #the first line should be the full path
  * Backup the original driver: `mv ""<PATH TO THE DRIVER>/brcmfmac.ko"" ""<PATH TO THE DRIVER>/brcmfmac.ko.orig""`
  * Copy the modified driver (Kernel 4.9): `cp /home/pi/nexmon/patches/bcm43430a1/7_45_41_46/nexmon/brcmfmac_kernel49/brcmfmac.ko ""<PATH TO THE DRIVER>/""`
  * Copy the modified driver (Kernel 4.14): `cp /home/pi/nexmon/patches/bcm43430a1/7_45_41_46/nexmon/brcmfmac_4.14.y-nexmon/brcmfmac.ko ""<PATH TO THE DRIVER>/""`
  * Probe all modules and generate new dependency: `depmod -a`
  * The new driver should be loaded by default after reboot: `reboot`
  * **Note:** It is possible to connect to an access point or run your own access point in parallel to the monitor mode interface on the `wlan0` interface.

# How to build the utilities
To build the utilities such as nexmon or dhdutil for Android, you need to download the **old** NDK version 11c,
extract it and export the environment variable `NDK_ROOT` pointing to the directory where you extracted the NDK 
files.

# How to extract the ROM
The Wi-Fi firmware consists of a read-only part stored in the ROM of every Wi-Fi chip and another part that is 
loaded by the driver into the RAM. To analyze the whole firmware, one needs to extract the ROM. There are two 
options to do this. Either you write a firmware patch that simply copies the contents of the ROM to RAM and then 
you dump the RAM, or you directly dump the ROM after loading the regular firmware into the RAM. Even though, 
the second option is easier, it only works, if the ROM can be directly accessed by the driver, which is not always 
the case. Additionally, the firmware loaded into RAM can contain ROM patches that overlay the data stored in ROM. 
By dumping the ROM after loading the original RAM firmware, it contains flash patches. Hence, the ROM needs to be 
dumped again for every RAM firmware update to be consistent. As a conclusion, we prefer to dump the clean ROM after 
copying it to RAM.

## Dumping the ROM directly
To dump the ROM directly, you need to know, where to find it and how large it is. On chips with Cortex-M3 it is 
usually at upper addresses such as 0x800000, while on chips with Cortex-R4 it is likely at 0x0. Run dhdutil to 
perform the dump:
```
dhdutil membytes -r 0x0 0xA0000 > rom.bin
```

## Dumping a clean ROM after copying to RAM
For the BCM4339 and BCM4358, we created `rom_extraction` projects that load a firmware patch that copies ROM to 
RAM and them dumps it using dhdutil. To dump the ROM simply execute the following in the project directory:
```
make dump-rom
```

After ROM extraction, the `rom.bin` file will be copies to the corresponding firmwares subdirectory. To apply the 
flash patches of a specific RAM firmware version, enter its directory and execute:
```
make rom.bin
```



# Structure of this repository
* `buildtools`: Contains compilers and other tools to build the firmware
* `firmwares`
  * `<chip version>`
    * `<firmware version>`
      * `<firmware file>`: The original firmware that will be loaded into the RAM of the WiFi Chip
      * `definitions.mk`: Contains mainly firmware specific addresses
      * `structs.h`: Structures only valid for this firmware version
      * `Makefile`: Used to extract flashpatches and ucode
      * `flashpatches.c` (generated by Makefile): Contains flashpatches
      * `ucode.bin` (extracted by Makefile): Contains uncompressed Ucode
    * `structs.common.h`: Structures that are common between firmware versions
* `patches`
  * `<chip version>`
    * `<firmware version>`
      * `nexmon`
        * `Makefile`: Used to build the firmware
        * `patch.ld`: Linker file
        * `src`
          * `patch.c`: General patches to the firmware
          * `injection.c`: Code related to frame injection
          * `monitormode.c`: Code related to monitor mode with radiotap headers
          * `ioctl.c`: Handling of custom IOCTLs
          * ...
        * `obj` (generated by Makefile): Object files created from C files
        * `log` (generated by Makefile): Logs written during compilation
        * `gen` (generated by Makefile): Files generated during the build process
          * `nexmon.pre` (generated by gcc plugin): Extracted at-attributes and targetregion-pragmas
          * `nexmon.ld` (generated from nexmon.pre): Linker file use to place patch code at defined addresses in the firmware
          * `nexmon.mk` (generated from nexmon.pre): Make file used take code from patch.elf and place it into firmware
          * `flashpatches.ld` (generated from nexmon.pre): Linker file that places flashpatches at target locations in firmware ROM
          * `flashpatches.mk` (generated from nexmon.pre): Make file used to insert flashpatch config and data structures into firmware
          * `patch.elf` (generated from object files and linker scripts): contains the newly compiled code placed at predefined addresses
    * `common`
      * `wrapper.c`: Wrappers for functions that already exist in the firmware
      * `ucode_compression.c`: [tinflate](http://achurch.org/tinflate.c) based ucode decompression
      * `radiotap.c`: RadioTap header parser
      * `helper.c`: Helpful utility functions
    * `include`: Common include files
      * `firmware_version.h`: Definitions of chip and firmware versions
      * `patcher.h`: Macros use to perform patching for existing firmware code (e.g., BPatch patches a branch instruction)
      * `capabilities.h`: Allows to indicate capabilities (such as, monitor mode and frame injection)
      * `nexioctl.h`: Defines custom IOCTL numbers

# Related projects
* [bcmon](https://bcmon.blogspot.de/): Monitor Mode and Frame Injection for the bcm4329 and bcm4330
* [monmob](https://github.com/tuter/monmob): Monitor Mode and Frame Injection for the bcm4325, bcm4329 and bcm4330
* [P4wnP1](https://github.com/mame82/P4wnP1): Highly customizable attack platform, based on Raspberry Pi Zero W and Nexmon
* [kali Nethunter OS](https://github.com/nethunteros): ROM that brings Kali Linux to smartphones with Nexmon support
* [dustcloud-nexmon](https://github.com/dgiese/dustcloud-nexmon): Nexmon for Xiaomi IoT devices (ARM based)
* [InternalBlue](https://github.com/seemoo-lab/internalblue): Bluetooth experimentation framework based on Reverse Engineering of Broadcom Bluetooth Controllers

# Interesting articles on firmware hacks
If you know more projects that use nexmon or perform similar firmware hacks, let us know and we will add a link.

* [Project Zero](https://googleprojectzero.blogspot.de/2017/09/over-air-vol-2-pt-1-exploiting-wi-fi.html): Over The Air - Vol. 2, Pt. 1: Exploiting The Wi-Fi Stack on Apple Devices
* [broadpwn](https://blog.exodusintel.com/2017/07/26/broadpwn/): Remotely Compromising Android and IOS via a Bug in Broadcom's Wi-Fi Chipsets
* [Project Zero](https://googleprojectzero.blogspot.de/2017/04/over-air-exploiting-broadcoms-wi-fi_4.html): Over The Air: Exploiting Broadcom's Wi-Fi Stack (Part 1)
* [Project Zero](https://googleprojectzero.blogspot.de/2017/04/over-air-exploiting-broadcoms-wi-fi_11.html): Over The Air: Exploiting Broadcom's Wi-Fi Stack (Part 2) 

# Read my PhD thesis
* Matthias Schulz. [**Teaching Your Wireless Card New Tricks: Smartphone Performance and Security Enhancements through Wi-Fi Firmware Modifications**](http://tuprints.ulb.tu-darmstadt.de/7243/). Dr.-Ing. thesis, Technische Universität Darmstadt, Germany, February 2018. [pdf](http://tuprints.ulb.tu-darmstadt.de/7243/7/dissertation_2018_matthias_thomas_schulz.pdf)

# Read our papers
* F. Gringoli, M. Schulz, J. Link, and M. Hollick. [**Free Your CSI: A Channel State Information Extraction Platform For Modern Wi-Fi Chipsets**](https://doi.org/10.1145/3349623.3355477). Accepted to appear in *Proceedings of the 13th Workshop on Wireless Network Testbeds, Experimental evaluation & CHaracterization (WiNTECH 2019)*, October 2019. [code](https://nexmon.org/csi)
* D. Mantz, J. Classen, M. Schulz, and M. Hollick. [**InternalBlue - Bluetooth Binary Patching and Experimentation Framework**](https://dl.acm.org/citation.cfm?id=3326089). *In Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys '19)*. June 2019.
* M. Schuß, C. A. Boano, M. Weber, M. Schulz, M. Hollick, K. Römer. [**JamLab-NG: Benchmarking Low-Power Wireless Protocols under Controlable and Repeatable Wi-Fi Interference**](https://dl.acm.org/citation.cfm?id=3324331). *Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks (EWSN 2019)*, February 2019.
* M. Schulz, D. Wegemer, and M. Hollick. [**The Nexmon Firmware Analysis and Modification Framework: Empowering Researchers to Enhance Wi-Fi Devices**](https://doi.org/10.1016/j.comcom.2018.05.015). *Elsevier Computer Communications (COMCOM) Journal*. 2018.
* M. Schulz, J. Link, F. Gringoli, and M. Hollick. [**Shadow Wi-Fi: Teaching Smart- phones to Transmit Raw Signals and to Extract Channel State Information to Implement Practical Covert Channels over Wi-Fi**](https://dl.acm.org/citation.cfm?id=3210333). Accepted to appear in *Proceedings of the 16th ACM International Conference on Mobile Systems, Applications, and Services*, MobiSys 2018, June 2018.
* D. Steinmetzer, D. Wegemer, M. Schulz, J. Widmer, M. Hollick. [**Compressive Millimeter-Wave Sector Selection in Off-the-Shelf IEEE 802.11ad Devices**](https://dl.acm.org/citation.cfm?id=3143384). *Proceedings of the 13th International Conference on emerging Networking EXperiments and Technologies*, CoNEXT 2017, December 2017.
* M. Schulz, D. Wegemer, M. Hollick. [**Nexmon: Build Your Own Wi-Fi Testbeds With Low-Level MAC and PHY-Access Using Firmware Patches on Off-the-Shelf Mobile Devices**](https://dl.acm.org/citation.cfm?id=3131476). *Proceedings of the 11th ACM International Workshop on Wireless Network Testbeds, Experimental Evaluation & Characterization (WiNTECH 2017)*, October 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wintech2017) [video](https://youtu.be/m5Zrk4n4hoE)
* M. Schulz, F. Knapp, E. Deligeorgopoulos, D. Wegemer, F. Gringoli, M. Hollick. [**DEMO: Nexmon in Action: Advanced Applications Powered by the Nexmon Firmware Patching Framework**](https://dl.acm.org/citation.cfm?id=3133333), Accepted for publication in *Proceedings of the 11th ACM International Workshop on Wireless Network Testbeds, Experimental Evaluation & Characterization (WiNTECH 2017)*, October 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wintech2017demo)
* M. Schulz, F. Gringoli, D. Steinmetzer, M. Koch and M. Hollick. [**Massive Reactive Smartphone-Based Jamming using Arbitrary Waveforms and Adaptive Power Control**](https://dl.acm.org/citation.cfm?id=3098253). Proceedings of the *10th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec 2017)*, July 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wisec2017) [video](https://youtu.be/S2XPBK0KdiQ)
* M. Schulz, E. Deligeorgopoulos, M. Hollick and F. Gringoli. [**DEMO: Demonstrating Reactive Smartphone-Based Jamming**](https://dl.acm.org/citation.cfm?id=3106022). Proceedings of the *10th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec 2017)*, July 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wisec2017demo)
* M. Schulz. [**Nexmon - Wie man die eigene WLAN-Firmware hackt**](http://heise.de/-3538660), 
c't 26/2016, S. 168, Heise Verlag, 2016.
* M. Schulz, D. Wegemer, M. Hollick. [**DEMO: Using NexMon, the C-based WiFi 
firmware modification framework**](https://dl.acm.org/citation.cfm?id=2942419), 
Proceedings of the *9th ACM Conference on Security and Privacy in Wireless and 
Mobile Networks (WiSec 2016)*, July 2016. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wisec2016demo1)
* M. Schulz, D. Wegemer and M. Hollick. [**NexMon: A Cookbook for Firmware 
Modifications on Smartphones to Enable Monitor Mode**](http://arxiv.org/abs/1601.07077), 
CoRR, vol. abs/1601.07077, December 2015. 
[bibtex](http://dblp.uni-trier.de/rec/bibtex/journals/corr/SchulzWH16)

[Get references as bibtex file](https://nexmon.org/bib)

# Reference our project
Any use of this project which results in an academic publication or other publication which includes a bibliography should include a citation to the Nexmon project and probably one of our papers depending on the code you use. Find all references in our [bibtex file](nexmon.bib). Here is the reference for the project only:
```
@electronic{nexmon:project,
 author = {Schulz, Matthias and Wegemer, Daniel and Hollick, Matthias},
 title = {Nexmon: The C-based Firmware Patching Framework},
 url = {https://nexmon.org},
 year = {2017}
}
```

# Contact
* [Matthias Schulz](https://seemoo.tu-darmstadt.de/mschulz) <mschulz@seemoo.tu-darmstadt.de>
* Daniel Wegemer <dwegemer@seemoo.tu-darmstadt.de>

# Powered By
## Secure Mobile Networking Lab (SEEMOO)
<a href=""https://www.seemoo.tu-darmstadt.de"">![SEEMOO logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/seemoo.png)</a>
## Networked Infrastructureless Cooperation for Emergency Response (NICER)
<a href=""https://www.nicer.tu-darmstadt.de"">![NICER logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/nicer.png)</a>
## Multi-Mechanisms Adaptation for the Future Internet (MAKI)
<a href=""http://www.maki.tu-darmstadt.de/"">![MAKI logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/maki.png)</a>
## Technische Universität Darmstadt
<a href=""https://www.tu-darmstadt.de/index.en.jsp"">![TU Darmstadt logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/tudarmstadt.png)</a>"
openbsd/src,1111198,1512,133,431,Organization,False,206039,1,0,80,False,Public git conversion mirror of OpenBSD's official CVS src repository. Pull requests not accepted - send diffs to the tech@ mailing list.,https://www.openbsd.org,1,7,0,,,,,0,18,0,4,8328,53,3055,1816034,1227626,0,0,4,2,,,
kaltura/nginx-vod-module,3985,1148,125,286,Organization,False,1283,9,31,18,False,NGINX-based MP4 Repackager,,13,7,0,100,543,23,33,8,498,2,12,2161,3,19,1012,307,0,0,325,4,,,"# NGINX-based VOD Packager
## nginx-vod-module [![Build Status](https://travis-ci.org/kaltura/nginx-vod-module.svg?branch=master)](https://travis-ci.org/kaltura/nginx-vod-module)

[Join the list of organizations using this video packager project](https://github.com/kaltura/nginx-vod-module/issues/730/).

### Features

* On-the-fly repackaging of MP4 files to DASH, HDS, HLS, MSS

* Working modes:
  1. Local - serve locally accessible files (local disk/NFS mounted)
  2. Remote - serve files accessible via HTTP using range requests
  3. Mapped - serve files according to a specification encoded in JSON format. The JSON can pulled from a remote server, or read from a local file

* Adaptive bitrate support

* Playlist support (playing several different media files one after the other) - mapped mode only

* Simulated live support (generating a live stream from MP4 files) - mapped mode only

* Fallback support for file not found in local/mapped modes (useful in multi-datacenter environments)
  
* Video codecs: H264, H265 (DASH/HLS), VP9 (DASH)

* Audio codecs: AAC, MP3 (HLS/HDS/MSS), AC-3 (DASH/HLS), E-AC-3 (DASH/HLS), OPUS (DASH)

* Captions support - 
  
  Input:
  1. WebVTT
  2. SRT
  3. DFXP/TTML
  4. CAP (Cheetah)
  
  Output:
  1. DASH - served as a single WebVTT
  2. HLS - segmented WebVTT (m3u8)
  3. MSS - converted to TTML and packaged in fragmented MP4 (no support for styling)

* Audio only/video only files

* Alternative audio renditions - supporting both:
  1. Generation of manifest with different audio renditions, allowing selection on the client side
  2. Muxing together audio and video streams from separate files / tracks - provides the ability
 to serve different audio renditions of a single video, without the need for any special support
 on the client side.

* Track selection for multi audio/video MP4 files

* Playback rate change - 0.5x up to 2x (requires libavcodec and libavfilter)

* Source file clipping (only from I-Frame to P-frame)

* Support for variable segment lengths - enabling the player to select the optimal bitrate fast,
without the overhead of short segments for the whole duration of the video

* Clipping of MP4 files for progressive download playback

* Thumbnail capture (requires libavcodec) and resize (requires libswscale)

* Volume map (requires libavcodec) - returns a CSV containing the volume level in each interval

* Decryption of CENC-encrypted MP4 files (it is possible to create such files with MP4Box)

* DASH: common encryption (CENC) support

* MSS: PlayReady encryption support

* HLS: Generation of I-frames playlist (EXT-X-I-FRAMES-ONLY)

* HLS: support for AES-128 / SAMPLE-AES encryption

### Limitations

* Track selection and playback rate change are not supported in progressive download

* I-frames playlist generation is not supported when encryption is enabled

* Tested on Linux only

### Compilation

#### Dependencies

In general, if you have the dependencies that are required to build nginx, you should be able to build nginx-vod-module.
However, some optional features of this module depend on additional packages. The module detects these packages 
during `configure` - if a package is missing, the respective feature will be disabled.

The optional features are:
1. Thumbnail capture & volume map - depend on ffmpeg (3.0 or newer)
2. Audio filtering (for changing playback rate / gain) - depends on ffmpeg (3.0 or newer) and also on libfdk_aac.
 Due to licensing issues, libfdk_aac is not built into kaltura ffmpeg packages
3. Encryption / decryption (DRM / HLS AES) - depends on openssl
4. DFXP captions - depends on libxml2
5. UTF-16 encoded SRT files - depends on iconv

#### Build

To link statically against nginx, cd to nginx source directory and execute:

    ./configure --add-module=/path/to/nginx-vod-module
    make
    make install

To compile as a dynamic module (nginx 1.9.11+), use:
  
 ./configure --add-dynamic-module=/path/to/nginx-vod-module

In this case, the `load_module` directive should be used in nginx.conf in order to load the module.

Optional recommended settings:
1. `--with-file-aio` - enable asynchronous I/O support, highly recommended, relevant only to local and mapped modes
2. `--with-threads` (nginx 1.7.11+) - enable asynchronous file open using thread pool (also requires `vod_open_file_thread_pool` in nginx.conf), relevant only to local and mapped modes
3. `--with-cc-opt=""-O3""` - enable additional compiler optimizations (we saw about 8% reduction in the mp4 parse time
 and frame processing time compared to the nginx default `-O`)

Debug settings:
1. `--with-debug` - enable debug messages (also requires passing `debug` in the `error_log` directive in nginx.conf).
2. `--with-cc-opt=""-O0""` - disable compiler optimizations (for debugging with gdb)

### Installation

#### RHEL/CentOS 6/7 RPM
```
# rpm -ihv http://installrepo.kaltura.org/releases/kaltura-release.noarch.rpm
# yum install kaltura-nginx
```

#### Debian/Ubuntu deb package
*Ubuntu NOTE: before trying to install kaltura-nginx, you must also make sure the multiverse repo is enabled*

For Debian Wheezy [7], Debian Jessie [8], Ubuntu 14.04 and 14.10, add this repo:
```
# wget -O - http://installrepo.kaltura.org/repo/apt/debian/kaltura-deb-curr.gpg.key|apt-key add -
# echo ""deb [arch=amd64] http://installrepo.kaltura.org/repo/apt/debian propus main"" > /etc/apt/sources.list.d/kaltura.list
```

For Ubuntu 16.04, 16.10 add this repo:
```
# wget -O - http://installrepo.kaltura.org/repo/apt/xenial/kaltura-deb-curr-256.gpg.key|apt-key add -
# echo ""deb [arch=amd64] http://installrepo.kaltura.org/repo/apt/xenial propus main"" > /etc/apt/sources.list.d/kaltura.list
```

Then install the kaltura-nginx package:
```
# apt-get update
# apt-get install kaltura-nginx
```


If you wish to make use of the following features:
- Thumbnail capture
- Playback rate change - 0.5x up to 2x

You will also need to install the kaltura-ffmpeg (>= 3.1) package.

### URL structure

#### Basic URL structure

The basic structure of an nginx-vod-module URL is:
`http://<domain>/<location>/<fileuri>/<filename>`

Where:
* domain - the domain of the nginx-vod-module server
* location - the location specified in the nginx conf
* fileuri - a URI to the mp4 file:
  * local mode - the full file path is determined according to the root / alias nginx.conf directives
  * mapped mode - the full file path is determined according to the JSON received from the upstream / local file
  * remote mode - the mp4 file is read from upstream in chunks
  * Note: in mapped & remote modes, the URL of the upstream request is `http://<upstream>/<location>/<fileuri>?<extraargs>`
  (extraargs is determined by the `vod_upstream_extra_args` parameter)
* filename - detailed below

#### Multi URL structure

Multi URLs are used to encode several URLs on a single URL. A multi URL can be used to specify
the URLs of several different MP4 files that should be included together in a DASH MPD for example.

The structure of a multi URL is:
`http://<domain>/<location>/<prefix>,<middle1>,<middle2>,<middle3>,<postfix>.urlset/<filename>`

The sample URL above represents 3 URLs:
* `http://<domain>/<location>/<prefix><middle1><postfix>/<filename>`
* `http://<domain>/<location>/<prefix><middle2><postfix>/<filename>`
* `http://<domain>/<location>/<prefix><middle3><postfix>/<filename>`

The suffix `.urlset` (can be changed using `vod_multi_uri_suffix`) indicates that the URL should be treated as a multi URL.
For example - the URL `http://example.com/hls/videos/big_buck_bunny_,6,9,15,00k.mp4.urlset/master.m3u8` will return a manifest containing:
* http://example.com/hls/videos/big_buck_bunny_600k.mp4/index.m3u8
* http://example.com/hls/videos/big_buck_bunny_900k.mp4/index.m3u8
* http://example.com/hls/videos/big_buck_bunny_1500k.mp4/index.m3u8

#### URL path parameters

The following parameters are supported on the URL path:
* clipFrom - an offset in milliseconds since the beginning of the video, where the generated stream should start. 
 For example, `.../clipFrom/10000/...` will generate a stream that starts 10 seconds into the video.
* clipTo - an offset in milliseconds since the beginning of the video, where the generated stream should end.
 For example, `.../clipTo/60000/...` will generate a stream truncated to 60 seconds.
* tracks - can be used to select specific audio/video tracks. The structure of the parameter is: `v<id1>-v<id2>-a<id1>-a<id2>...`
 For example, `.../tracks/v1-a1/...` will select the first video track and first audio track.
 The default is to include all tracks.
* shift - can be used to apply a timing shift to one or more streams. The structure of the parameter is: `v<vshift>-a<ashift>-s<sshift>`
 For example, `.../shift/v100/...` will apply a forward shift of 100ms to the video timestamps.

#### Filename structure

The structure of filename is:
`<basename>[<seqparams>][<fileparams>][<trackparams>][<langparams>].<extension>`

Where:
* basename + extension - the set of options is packager specific (the list below applies to the default settings):
  * dash - manifest.mpd
  * hds - manifest.f4m
  * hls master playlist - master.m3u8
  * hls media playlist - index.m3u8
  * mss - manifest
  * thumb - `thumb-<offset>[<resizeparams>].jpg` (offset is the thumbnail video offset in milliseconds)
  * volume_map - `volume_map.csv`
* seqparams - can be used to select specific sequences by id (provided in the mapping JSON), e.g. master-sseq1.m3u8.
* fileparams - can be used to select specific sequences by index when using multi URLs.
 For example, manifest-f1.mpd will return an MPD only from the first URL.
* trackparams - can be used to select specific audio/video tracks.
 For example, manifest-a1.f4m will return an F4M containing only the first audio stream of each sequence.
 The default is to include the first audio and first video tracks of each file.
 The tracks selected on the file name are AND-ed with the tracks selected with the /tracks/ path parameter.
 v0/a0 select all video/audio tracks respectively.
 The a/v parameters can be combined with f/s, e.g. f1-v1-f2-a1 = video1 of file1 + audio1 of file2, f1-f2-v1 = video1 of file1 + video1 of file2.
* langparams - can be used to filter audio tracks/subtitles according to their language (ISO639-3 code).
 For example, master-leng.m3u8 will return only english audio tracks.
* resizeparams - can be used to resize the returned thumbnail image. For example, thumb-1000-w150-h100.jpg captures a thumbnail
 1 second into the video, and resizes it to 150x100. If one of the dimensions is omitted, its value is set so that the 
 resulting image will retain the aspect ratio of the video frame.

### Mapping response format

When configured to run in mapped mode, nginx-vod-module issues an HTTP request to a configured upstream server 
in order to receive the layout of media streams it should generate.
The response has to be in JSON format. 

This section contains a few simple examples followed by a reference of the supported objects and fields. 
But first, a couple of definitions:

1. `Source Clip` - a set of audio and/or video frames (tracks) extracted from a single media file
2. `Generator` - a component that can generate audio/video frames. Currently, the only supported generator is the silence generator.
3. `Filter` - a manipulation that can be applied on audio/video frames. The following filters are supported: 
  * rate (speed) change - applies to both audio and video
  * audio volume change
  * mix - can be used to merge several audio tracks together, or to merge the audio of source A with the video of source B
4. `Clip` - the result of applying zero or more filters on a set of source clips
5. `Dynamic Clip` - a clip whose contents is not known in advance, e.g. targeted ad content
6. `Sequence` - a set of clips that should be played one after the other. 
7. `Set` - several sequences that play together as an adaptive set, each sequence must have the same number of clips.

#### Simple mapping

The JSON below maps the request URI to a single MP4 file:
```
{
 ""sequences"": [
  {
   ""clips"": [
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video.mp4""
    }
   ]
  }
 ]
}
```

When using multi URLs, this is the only allowed JSON pattern. In other words, it is not
possible to combine more complex JSONs using multi URL.

#### Adaptive set

As an alternative to using multi URL, an adaptive set can be defined via JSON:
```
{
 ""sequences"": [
  {
   ""clips"": [
    {
     ""type"": ""source"",
     ""path"": ""/path/to/bitrate1.mp4""
    }
   ]
  },
  {
   ""clips"": [
    {
     ""type"": ""source"",
     ""path"": ""/path/to/bitrate2.mp4""
    }
   ]
  }
 ]
}
```

#### Playlist

The JSON below will play 35 seconds of video1 followed by 22 seconds of video2:
```
{
 ""durations"": [ 35000, 22000 ],
 ""sequences"": [
  {
   ""clips"": [
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video1.mp4""
    },
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video2.mp4""
    }
   ]
  }
 ]
}
```

#### Filters

The JSON below takes video1, plays it at x1.5 and mixes the audio of the result with the audio of video2,
after reducing it to 50% volume:
```
{
 ""sequences"": [
  {
   ""clips"": [
    {
     ""type"": ""mixFilter"",
     ""sources"": [
      {
       ""type"": ""rateFilter"",
       ""rate"": 1.5,
       ""source"": {
        ""type"": ""source"",
        ""path"": ""/path/to/video1.mp4""
       }
      },
      {
       ""type"": ""gainFilter"",
       ""gain"": 0.5,
       ""source"": {
        ""type"": ""source"",
        ""path"": ""/path/to/video2.mp4"",
        ""tracks"": ""a1""
       }
      }
     ]
    }
   ]
  }
 ]
}
```

#### Continuous live

The JSON below is a sample of a continuous live stream (=a live stream in which all videos have exactly the same encoding parameters).
In practice, this JSON will have to be generated by some script, since it is time dependent.
(see test/playlist.php for a sample implementation)
```
{
 ""playlistType"": ""live"",
 ""discontinuity"": false,
 ""segmentBaseTime"": 1451904060000,
 ""firstClipTime"": 1451917506000,
 ""durations"": [83000, 83000],
 ""sequences"": [
  {
   ""clips"": [
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video1.mp4""
    },
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video2.mp4""
    }
   ]
  }
 ]
}
```

#### Non-continuous live

The JSON below is a sample of a non-continuous live stream (=a live stream in which the videos have different encoding parameters).
In practice, this JSON will have to be generated by some script, since it is time dependent 
(see test/playlist.php for a sample implementation)
```
{
 ""playlistType"": ""live"",
 ""discontinuity"": true,
 ""initialClipIndex"": 171,
 ""initialSegmentIndex"": 153,
 ""firstClipTime"": 1451918170000,
 ""durations"": [83000, 83000],
 ""sequences"": [
  {
   ""clips"": [
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video1.mp4""
    },
    {
     ""type"": ""source"",
     ""path"": ""/path/to/video2.mp4""
    }
   ]
  }
 ]
}
```

### Mapping reference

#### Set (top level object in the mapping JSON)

Mandatory fields:
* `sequences` - array of Sequence objects. 
 The mapping has to contain at least one sequence and up to 32 sequences.
 
Optional fields:
* `id` - a string that identifies the set. The id can be retrieved by `$vod_set_id`.
* `playlistType` - string, can be set to `live` or `vod`, default is `vod`.
* `durations` - an array of integers representing clip durations in milliseconds.
 This field is mandatory if the mapping contains more than a single clip per sequence.
 If specified, this array must contain at least one element and up to 128 elements.
* `discontinuity` - boolean, indicates whether the different clips in each sequence have
 different media parameters. This field has different manifestations according to the 
 delivery protocol - a value of true will generate `#EXT-X-DISCONTINUITY` in HLS, 
 and a multi period MPD in DASH. The default value is true, set to false only if the media
 files were transcoded with exactly the same parameters (in AVC for example, 
 the clips should have exactly the same SPS/PPS).
* `segmentDuration` - integer, sets the segment duration in milliseconds. This field, 
 if specified, takes priority over the value set in `vod_segment_duration`.
* `consistentSequenceMediaInfo` - boolean, currently affects only DASH. When set to true (default)
 the MPD will report the same media parameters in each period element. Setting to false
 can have severe performance implications for long sequences (nginx-vod-module has 
 to read the media info of all clips included in the mapping in order to generate the MPD)
* `referenceClipIndex` - integer, sets the (1-based) index of the clip that should be used 
 to retrieve the video metadata for manifest requests (codec, width, height etc.)
 If `consistentSequenceMediaInfo` is set to false, this parameter has no effect -
 all clips are parsed. If this parameter is not specified, nginx-vod-module uses the last clip 
 by default.
* `notifications` - array of notification objects (see below), when a segment is requested,
 all the notifications that fall between the start/end times of the segment are fired.
 the notifications must be ordered in an increasing offset order.
* `clipFrom` - integer, contains a timestamp indicating where the returned stream should start.
 Setting this parameter is equivalent to passing /clipFrom/ on the URL.
* `clipTo` - integer, contains a timestamp indicating where the returned stream should end.
 Setting this parameter is equivalent to passing /clipTo/ on the URL.
 
Live fields:
* `firstClipTime` - integer, mandatory for all live playlists unless `clipTimes` is specified.
 Contains the absolute time of the first clip in the playlist, in milliseconds since the epoch (unixtime x 1000)
* `clipTimes` - array of integers, sets the absolute time of all the clips in the playlist, 
 in milliseconds since the epoch (unixtime x 1000). This field can be used only when 
 `discontinuity` is set to true. The timestamps may contain gaps, but they are not allowed to overlap
 (`clipTimes[n + 1] >= clipTimes[n] + durations[n]`)
* `segmentBaseTime` - integer, mandatory for continuous live streams, contains the absolute
 time of the first segment of the stream, in milliseconds since the epoch (unixtime x 1000).
 This value must not change during playback.
 For discontinuous live streams, this field is optional:
 * if not set, sequential segment indexes will be used throughout the playlist.
  In this case, the upstream server generating the mapping json has to maintain state,
  and update initialSegmentIndex every time a clip is removed from the playlist.
 * if set, the timing gaps between clips must not be lower than `vod_segment_duration`.
* `firstClipStartOffset` - integer, optional, measured in milliseconds. This field contains the
 difference between first clip time, and the original start time of the first clip -
 the time it had when it was initially added (before the live window shifted)
* `initialClipIndex` - integer, mandatory for non-continuous live streams that mix videos having
 different encoding parameters (SPS/PPS), contains the index of the first clip in the playlist. 
 Whenever a clip is pushed out of the head of the playlist, this value must be incremented by one.
* `initialSegmentIndex` - integer, mandatory for live streams that do not set `segmentBaseTime`, 
 contains the index of the first segment in the playlist. Whenever a clip is pushed out of the head of
 the playlist, this value must be incremented by the number of segments in the clip.
* `presentationEndTime` - integer, optional, measured in milliseconds since the epoch.
 when supplied, the module will compare the current time to the supplied value, 
 and signal the end of the live presentation if `presentationEndTime` has passed. 
 In HLS, for example, this parameter controls whether an `#EXT-X-ENDLIST` tag should be 
 included in the media playlist.
 When the parameter is not supplied, the module will not signal live presentation end.
* `expirationTime` - integer, optional, measured in milliseconds since the epoch.
 when supplied, the module will compare the current time to the supplied value, 
 and if `expirationTime` has passed, the module will return a 404 error for manifest requests 
 (segment requests will continue to be served).
 when both presentationEndTime and expirationTime have passed, presentationEndTime takes
 priority, i.e. manifest requests will be served and signal presentation end.
* `liveWindowDuration` - integer, optional, provides a way to override `vod_live_window_duration`
 specified in the configuration. If the value exceeds the absolute value specified in 
 `vod_live_window_duration`, it is ignored.
 
#### Sequence

Mandatory fields:
* `clips` - array of Clip objects (mandatory). The number of elements must match the number
 the durations array specified on the set. If the durations array is not specified,
 the clips array must contain a single element.
 
Optional fields:
* `id` - a string that identifies the sequence. The id can be retrieved by `$vod_sequence_id`.
* `language` - a 3-letter (ISO-639-2) language code, this field takes priority over any language
 specified on the media file (MP4 mdhd atom)
* `label` - a friendly string that identifies the sequence. If a language is specified,
 a default label will be automatically derived by it - e.g. if language is `ita`, 
 by default `italiano` will be used as the label.
* `bitrate` - an object that can be used to set the bitrate for the different media types,
 in bits per second. For example, `{""v"": 900000, ""a"": 64000}`. If the bitrate is not supplied,
 nginx-vod-module will estimate it based on the last clip in the sequence.
* `avg_bitrate` - an object that can be used to set the average bitrate for the different media types,
 in bits per second. See `bitrate` above for a sample object. If specified, the module will use
 the value to populate the AVERAGE-BANDWIDTH attribute of `#EXT-X-STREAM-INF` in HLS.

#### Clip (abstract)

Mandatory fields:
* `type` - a string that defines the type of the clip. Allowed values are:
 * source
 * rateFilter
 * mixFilter
 * gainFilter
 * silence
 * concat
 * dynamic

Optional fields:
* `keyFrameDurations` - array of integers, containing the durations in milliseconds of the video key frames
 in the clip. This property can only be supplied on the top level clips of each sequence,
 supplying this property on nested clips has no effect.
 Supplying the key frame durations enables the module to both:
 1. align the segments to key frames 
 2. report the correct segment durations in the manifest - providing an alternative to setting
  `vod_manifest_segment_durations_mode` to `accurate`, which is not supported for multi clip
  media sets (for performance reasons).
* `firstKeyFrameOffset` - integer, offset of the first video key frame in the clip, 
 measured in milliseconds relative to `firstClipTime`. Defaults to 0 if not supplied.

#### Source clip

Mandatory fields:
* `type` - a string with the value `source`
* `path` - a string containing the path of the MP4 file. The string `""empty""` can be used to represent
 an empty captions file (useful in case only some videos in a playlist have captions)

Optional fields:
* `tracks` - a string that specifies the tracks that should be used, the default is ""v1-a1"",
 which means the first video track and the first audio track
* `clipFrom` - an integer that specifies an offset in milliseconds, from the beginning of the 
 media file, from which to start loading frames
* `encryptionKey` - a base64 encoded string containing the key (128/192/256 bit) that should be used
 to decrypt the file.
* `encryptionIv` - a base64 encoded string containing the iv (128 bit) that should be used
 to decrypt the file.
* `encryptionScheme` - the encryption scheme that was used to encrypt the file. Currently,
 only two schemes are supported - `cenc` for MP4 files, `aes-cbc` for caption files.

#### Rate filter clip

Mandatory fields:
* `type` - a string with the value `rateFilter`
* `rate` - a float that specified the acceleration factor, e.g. a value of 2 means double speed.
 Allowed values are in the range 0.5 - 2 with up to two decimal points
* `source` - a clip object on which to perform the rate filtering

#### Gain filter clip

Mandatory fields:
* `type` - a string with the value `gainFilter`
* `gain` - a float that specified the amplification factor, e.g. a value of 2 means twice as loud.
 The gain must be positive with up to two decimal points
* `source` - a clip object on which to perform the gain filtering

#### Mix filter clip

Mandatory fields:
* `type` - a string with the value `mixFilter`
* `sources` - an array of Clip objects to mix. This array must contain at least one clip and
 up to 32 clips.

#### Concat clip

Mandatory fields:
* `type` - a string with the value `concat`
* `durations` - an array of integers representing MP4 durations in milliseconds,
 this array must match the `paths` array in count and order.

Optional fields:
* `paths` - an array of strings, containing the paths of the MP4 files. Either `paths` or `clipIds` must be specified.
* `clipIds` - an array of strings, containing the ids of source clips. 
 The ids are translated to paths by issuing a request to the uri specified in `vod_source_clip_map_uri`.
 Either `paths` or `clipIds` must be specified.
* `tracks` - a string that specifies the tracks that should be used, the default is ""v1-a1"",
 which means the first video track and the first audio track
* `offset` - an integer in milliseconds that indicates the timestamp offset of the 
 first frame in the concatenated stream relative to the clip start time
* `basePath` - a string that should be added as a prefix to all the paths
* `notifications` - array of notification objects (see below), when a segment is requested,
 all the notifications that fall between the start/end times of the segment are fired.
 the notifications must be ordered in an increasing offset order.

#### Dynamic clip

Mandatory fields:
* `type` - a string with the value `dynamic`
* `id` - a string that uniquely identifies the dynamic clip, used for mapping the clip to its content

#### Notification

Mandatory fields:
* `offset` - an integer in milliseconds that indicates the time in which the notification should be fired.
 when the notification object is contained in the media set, `offset` is relative to `firstClipTime`
 (0 for vod). when the notification object is contained in a concat clip, `offset` is relative to
 the beginning of the concat clip.
* `id` - a string that identifies the notification, this id can be referenced by `vod_notification_uri`
 using the variable `$vod_notification_id`

### Security

#### Authorization

##### CDN-based delivery

Media packaged by nginx-vod-module can be protected using CDN tokens, this works as follows:
* Some application authenticates the user and decides whether the user should be allowed 
 to watch a specific video. If the user is allowed, the application generates a tokenized
 URL for the manifest of the video.
* The CDN validates the token, and if found to be valid, forwards the request to nginx-vod-module 
 on the origin. 
* The nginx server builds the manifest response and generates tokens for the segment URLs
 contained inside it. The module https://github.com/kaltura/nginx-secure-token-module can
 be used to accomplish this task, it currently support Akamai tokens and CloudFront tokens.
 See the readme of this module for more details.
* The CDN validates the token on each segment that is requested.

In this setup it also highly recommended to block direct access to the origin server by
authenticating the CDN requests. Without this protection, a user who somehow gets the address
of the origin will be able to bypass the CDN token enforcement. If using Akamai, this can
be accomplished using https://github.com/refractalize/nginx_mod_akamai_g2o.
For other CDNs, it may be possible to configure the CDN to send a secret header to the origin
and then simply enforce the header using an nginx if statement:
```
  if ($http_x_secret_origin_header != ""secret value"") {
   return 403;
  }
```

In addition to the above, most CDNs support other access control settings, such as geo-location.
These restrictions are completely transparent to the origin and should work well. 

##### Direct delivery

Deployments in which the media is pulled directly from nginx-vod-module can protect the media
using nginx access control directives, such `allow`, `deny`, or `access_by_lua` (for more complex
scenarios).

In addition, it is possible to build a token based solution (as detailed in the previous section) 
without a CDN, by having the nginx server validate the token. 
The module https://github.com/kaltura/nginx-akamai-token-validate-module can be used
to validate Akamai tokens. Locations on which the module is enabled will return 403 unless the 
request contains a valid Akamai token. See the readme of this module for more details.

#### URL encryption

As an alternative to tokenization, URL encryption can be used to prevent an attacker from being
able to craft a playable URL. URL encryption can be implemented with 
https://github.com/kaltura/nginx-secure-token-module, and is supported for HLS and DASH (with 
manifest format set to segmentlist). 

In terms of security, the main advantage of CDN tokens over URL encryption is that CDN tokens
usually expire, while encrypted URLs do not (someone who obtains a playable URL will be able to
use it indefinitely)

#### Media encryption

Nginx-vod-module supports AES-128 and SAMPLE-AES HLS encryption schemes. The main difference between
media encryption and DRM (detailed below) is the mechanism used to transfer the encryption key to 
the client. With media encryption the key is fetched by the client by performing a simple GET request
to nginx-vod-module, while with DRM the key is returned inside a vendor specific license response.

Media encryption reduces the problem of securing the media to the need to secure the encryption key. 
The media segment URLs (which compose the vast majority of the traffic) can be completely unprotected, 
and easily cacheable by any proxies between the client and servers (unlike tokenization). 
The encryption key request can then be protected using one of the methods mentioned above (CDN tokens,
nginx access rules etc.). 

In addition, it is possible to configure nginx-vod-module to return the encryption key over HTTPS
while having the segments delivered over HTTP. The way to configure this is to set `vod_segments_base_url`
to `http://nginx-vod-host` and set `vod_base_url` to `https://nginx-vod-host`.

#### DRM

Nginx-vod-module has the ability to perform on-the-fly encryption for MPEG DASH (CENC), MSS Play Ready and FairPlay HLS.
As in the case of media encryption, the encryption is performed while serving a video/audio segment to the client, 
therefore, when working with DRM it is recommended not to serve the content directly from nginx-vod-module to end-users.
A more scalable architecture would be to use proxy servers or a CDN in order to cache the encrypted segments.

In order to perform the encryption, nginx-vod-module needs several parameters, including key & key_id, these parameters
are fetched from an external server via HTTP GET requests.
The `vod_drm_upstream_location` parameter specifies an nginx location that is used to access the DRM server,
and the request uri is configured using `vod_drm_request_uri` (this parameter can include nginx variables). 
The response of the DRM server is a JSON, with the following format:

```
[{
 ""pssh"": [{
   ""data"": ""CAESEGMyZjg2MTczN2NjNGYzODIaB2thbHR1cmEiCjBfbmptaWlwbXAqBVNEX0hE"", 
   ""uuid"": ""edef8ba9-79d6-4ace-a3c8-27dcd51d21ed""
  }], 
 ""key"": ""GzoNU9Dfwc//Iq3/zbzMUw=="", 
 ""key_id"": ""YzJmODYxNzM3Y2M0ZjM4Mg==""
}]
```

* `pssh.data` - base64 encoded binary data, the format of this data is drm vendor specific
* `pssh.uuid` - the drm system UUID, in this case, edef8ba9-79d6-4ace-a3c8-27dcd51d21ed stands for Widevine
* `key` - base64 encoded encryption key (128 bit)
* `key_id` - base64 encoded key identifier (128 bit)
* `iv` - optional base64 encoded initialization vector (128 bit). The IV is currently used only in HLS (FairPlay), 
 in the other protocols an IV is generated automatically by nginx-vod-module.

##### Sample configurations

Apple FairPlay HLS:
```
location ~ ^/fpshls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {
 vod hls;
 vod_hls_encryption_method sample-aes;
 vod_hls_encryption_key_uri ""skd://entry-$2"";
 vod_hls_encryption_key_format ""com.apple.streamingkeydelivery"";
 vod_hls_encryption_key_format_versions ""1"";

 vod_drm_enabled on;
 vod_drm_request_uri ""/udrm/system/ovp/$vod_suburi"";

 vod_last_modified_types *;
 add_header Access-Control-Allow-Headers '*';
 add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
 add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
 add_header Access-Control-Allow-Origin '*';
 expires 100d;
}
```

Common Encryption HLS:
```
location ~ ^/cenchls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {
 vod hls;
 vod_hls_encryption_method sample-aes-cenc;
 vod_hls_encryption_key_format ""urn:uuid:edef8ba9-79d6-4ace-a3c8-27dcd51d21ed"";
 vod_hls_encryption_key_format_versions ""1"";

 vod_drm_enabled on;
 vod_drm_request_uri ""/udrm/system/ovp/$vod_suburi"";

 vod_last_modified_types *;
 add_header Access-Control-Allow-Headers '*';
 add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
 add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
 add_header Access-Control-Allow-Origin '*';
 expires 100d;
}
```

##### Verified configurations

Following is a list of configurations that were tested and found working:
* DASH/CENC with PlayReady & Widevine PSSH together
* MSS PlayReady
* HLS FairPlay

### Performance recommendations

1. For medium/large scale deployments, don't have users play the videos directly from nginx-vod-module.
 Since all the different streaming protocols supported by nginx vod are HTTP based, they can be cached by standard HTTP proxies / CDNs. 
 For medium scale add a layer of caching proxies between the vod module and the end users 
 (can use standard nginx servers with proxy_pass & proxy_cache). 
 For large scale deployments, it is recommended to use a CDN (such as Akamai, Level3 etc.). 
 
 In general, it's best to have nginx vod as close as possible to where the mp4 files are stored, 
 and have the caching proxies as close as possible to the end users.
2. Enable nginx-vod-module caches:
 * `vod_metadata_cache` - saves the need to re-read the video metadata for each segment. This cache should be rather large, in the order of GBs.
 * `vod_response_cache` - saves the responses of manifest requests. This cache may not be required when using a second layer of caching servers before nginx vod. 
  No need to allocate a large buffer for this cache, 128M is probably more than enough for most deployments.
 * `vod_mapping_cache` - for mapped mode only, few MBs is usually enough.
 * nginx's open_file_cache - caches open file handles.

 The hit/miss ratios of these caches can be tracked by enabling performance counters (`vod_performance_counters`)
 and setting up a status page for nginx vod (`vod_status`)
3. In local & mapped modes, enable aio. - nginx has to be compiled with aio support, and it has to be enabled in nginx conf (aio on). 
 You can verify it works by looking at the performance counters on the vod status page - read_file (aio off) vs. async_read_file (aio on)
4. In local & mapped modes, enable asynchronous file open - nginx has to be compiled with threads support, and `vod_open_file_thread_pool`
 has to be specified in nginx.conf. You can verify it works by looking at the performance counters on the vod status page - 
 open_file vs. async_open_file. Note that open_file may be nonzero with vod_open_file_thread_pool enabled, due to the open file cache - 
 open requests that are served from cache will be counted as synchronous open_file.
5. When using DRM enabled DASH/MSS, if the video files have a single nalu per frame, set `vod_min_single_nalu_per_frame_segment` to non-zero.
6. The muxing overhead of the streams generated by this module can be reduced by changing the following parameters:
 * HDS - set `vod_hds_generate_moof_atom` to off
 * HLS - set `vod_hls_mpegts_align_frames` to off and `vod_hls_mpegts_interleave_frames` to on
7. Enable gzip compression on manifest responses - 

 `gzip_types application/vnd.apple.mpegurl video/f4m application/dash+xml text/xml`
8. Apply common nginx performance best practices, such as tcp_nodelay=on, client_header_timeout etc.

### Configuration directives - base

#### vod
* **syntax**: `vod segmenter`
* **default**: `n/a`
* **context**: `location`

Enables the nginx-vod module on the enclosing location. 
The allowed values for `segmenter` are:

1. `none` - serves the MP4 files as is / clipped
2. `dash` - Dynamic Adaptive Streaming over HTTP packager
3. `hds` - Adobe HTTP Dynamic Streaming packager
4. `hls` - Apple HTTP Live Streaming packager
5. `mss` - Microsoft Smooth Streaming packager
6. `thumb` - thumbnail capture
7. `volume_map` - audio volume map

#### vod_mode
* **syntax**: `vod_mode mode`
* **default**: `local`
* **context**: `http`, `server`, `location`

Sets the file access mode - local, remote or mapped (see the features section above for more details)

#### vod_status
* **syntax**: `vod_status`
* **default**: `n/a`
* **context**: `location`

Enables the nginx-vod status page on the enclosing location. 

### Configuration directives - segmentation

#### vod_segment_duration
* **syntax**: `vod_segment_duration duration`
* **default**: `10s`
* **context**: `http`, `server`, `location`

Sets the segment duration in milliseconds. It is highly recommended to use a segment duration that is a multiple of the GOP duration.
If the segment duration is not a multiple of GOP duration, and `vod_align_segments_to_key_frames` is enabled, there could be significant
differences between the segment duration that is reported in the manifest and the actual segment duration. This could also lead to
the appearance of empty segments within the stream.

#### vod_live_window_duration
* **syntax**: `vod_live_window_duration duration`
* **default**: `30000`
* **context**: `http`, `server`, `location`

Sets the total duration in milliseconds of the segments that should be returned in a live manifest.
If the value is positive, nginx vod returns a range of maximum `vod_live_window_duration` milliseconds, ending at the current server time.
If the value is negative, nginx vod returns a range of maximum `-vod_live_window_duration` milliseconds from the end of the mapping json.
If the value is set to zero, the live manifest will contain all the segments that are fully contained in the mapping json time frame.

#### vod_force_playlist_type_vod
* **syntax**: `vod_force_playlist_type_vod on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Generate a vod stream even when the media set has `playlistType=live`. 
Enabling this setting has the following effects:
1. Frame timestamps will be continuous and start from zero
2. Segment indexes will start from one
3. In case of HLS, the returned manifest will have both `#EXT-X-PLAYLIST-TYPE:VOD` and `#EXT-X-ENDLIST`

This can be useful for clipping vod sections out of a live stream.

#### vod_force_continuous_timestamps
* **syntax**: `vod_force_continuous_timestamps on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Generate continuous timestamps even when the media set has gaps (gaps can created by the use of `clipTimes`)
If ID3 timestamps are enabled (`vod_hls_mpegts_output_id3_timestamps`), they contain the original timestamps that were set in `clipTimes`.

#### vod_bootstrap_segment_durations
* **syntax**: `vod_bootstrap_segment_durations duration`
* **default**: `none`
* **context**: `http`, `server`, `location`

Adds a bootstrap segment duration in milliseconds. This setting can be used to make the first few segments
shorter than the default segment duration, thus making the adaptive bitrate selection kick-in earlier without 
the overhead of short segments throughout the video.

#### vod_align_segments_to_key_frames
* **syntax**: `vod_align_segments_to_key_frames on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module forces all segments to start with a key frame. Enabling this setting can lead to differences
between the actual segment durations and the durations reported in the manifest (unless `vod_manifest_segment_durations_mode` is set to accurate).

#### vod_segment_count_policy
* **syntax**: `vod_segment_count_policy last_short/last_long/last_rounded`
* **default**: `last_short`
* **context**: `http`, `server`, `location`

Configures the policy for calculating the segment count, for segment_duration = 10 seconds:
* last_short - a file of 33 sec is partitioned as - 10, 10, 10, 3
* last_long - a file of 33 sec is partitioned as - 10, 10, 13
* last_rounded - a file of 33 sec is partitioned as - 10, 10, 13, a file of 38 sec is partitioned as 10, 10, 10, 8

#### vod_manifest_duration_policy
* **syntax**: `vod_manifest_duration_policy min/max`
* **default**: `max`
* **context**: `http`, `server`, `location`

Configures the policy for calculating the duration of a manifest containing multiple streams:
* max - uses the maximum stream duration (default)
* min - uses the minimum non-zero stream duration

#### vod_manifest_segment_durations_mode
* **syntax**: `vod_manifest_segment_durations_mode estimate/accurate`
* **default**: `estimate`
* **context**: `http`, `server`, `location`

Configures the calculation mode of segment durations within manifest requests:
* estimate - reports the duration as configured in nginx.conf, e.g. if `vod_segment_duration` has the value 10000,
an HLS manifest will contain #EXTINF:10
* accurate - reports the exact duration of the segment, taking into account the frame durations, e.g. for a 
frame rate of 29.97 and 10 second segments it will report the first segment as 10.01. accurate mode also
takes into account the key frame alignment, in case `vod_align_segments_to_key_frames` is on

#### vod_media_set_override_json
* **syntax**: `vod_media_set_override_json json`
* **default**: `{}`
* **context**: `http`, `server`, `location`

This parameter provides a way to override portions of the media set JSON (mapped mode only).
For example, `vod_media_set_override_json '{""clipTo"":20000}'` clips the media set to 20 sec.
The parameter value can contain variables.

### Configuration directives - upstream

#### vod_upstream_location
* **syntax**: `vod_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets an nginx location that is used to read the MP4 file (remote mode) or mapping the request URI (mapped mode).

#### vod_remote_upstream_location
* **syntax**: `vod_remote_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets an nginx location that is used to read the MP4 file on remote or mapped mode. If this directive is set on mapped mode, the module reads 
the MP4 files over HTTP, treating the paths in the mapping JSON as URIs (the default behavior is to read from local files)

#### vod_max_upstream_headers_size
* **syntax**: `vod_max_upstream_headers_size size`
* **default**: `4k`
* **context**: `http`, `server`, `location`

Sets the size that is allocated for holding the response headers when issuing upstream requests (to vod_xxx_upstream_location).

#### vod_upstream_extra_args
* **syntax**: `vod_upstream_extra_args ""arg1=value1&arg2=value2&...""`
* **default**: `empty`
* **context**: `http`, `server`, `location`

Extra query string arguments that should be added to the upstream request (remote/mapped modes only).
The parameter value can contain variables.

#### vod_media_set_map_uri
* **syntax**: `vod_media_set_map_uri uri`
* **default**: `$vod_suburi`
* **context**: `http`, `server`, `location`

Sets the uri of media set mapping requests, the parameter value can contain variables.
In case of multi url, `$vod_suburi` will be the current sub uri (a separate request is issued per sub URL)

#### vod_path_response_prefix
* **syntax**: `vod_path_response_prefix prefix`
* **default**: `{""sequences"":[{""clips"":[{""type"":""source"",""path"":""`
* **context**: `http`, `server`, `location`

Sets the prefix that is expected in URI mapping responses (mapped mode only).

#### vod_path_response_postfix
* **syntax**: `vod_path_response_postfix postfix`
* **default**: `""}]}]}`
* **context**: `http`, `server`, `location`

Sets the postfix that is expected in URI mapping responses (mapped mode only).

#### vod_max_mapping_response_size
* **syntax**: `vod_max_mapping_response_size length`
* **default**: `1K`
* **context**: `http`, `server`, `location`

Sets the maximum length of a path returned from upstream (mapped mode only).

### Configuration directives - fallback

#### vod_fallback_upstream_location
* **syntax**: `vod_fallback_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets an nginx location to which the request is forwarded after encountering a file not found error (local/mapped modes only).

#### vod_proxy_header_name
* **syntax**: `vod_proxy_header_name name`
* **default**: `X-Kaltura-Proxy`
* **context**: `http`, `server`, `location`

Sets the name of an HTTP header that is used to prevent fallback proxy loops (local/mapped modes only).

#### vod_proxy_header_value
* **syntax**: `vod_proxy_header_value name`
* **default**: `dumpApiRequest`
* **context**: `http`, `server`, `location`

Sets the value of an HTTP header that is used to prevent fallback proxy loops (local/mapped modes only).

### Configuration directives - performance

#### vod_metadata_cache
* **syntax**: `vod_metadata_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the video metadata cache. For MP4 files, this cache holds the moov atom.

#### vod_mapping_cache
* **syntax**: `vod_mapping_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the mapping cache for vod (mapped mode only).

#### vod_live_mapping_cache
* **syntax**: `vod_live_mapping_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the mapping cache for live (mapped mode only).

#### vod_response_cache
* **syntax**: `vod_response_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the response cache. The response cache holds manifests
and other non-video content (like DASH init segment, HLS encryption key etc.). Video segments are not cached.

#### vod_live_response_cache
* **syntax**: `vod_live_response_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the response cache for time changing live responses. 
This cache holds the following types of responses for live: DASH MPD, HLS index M3U8, HDS bootstrap, MSS manifest.

#### vod_initial_read_size
* **syntax**: `vod_initial_read_size size`
* **default**: `4K`
* **context**: `http`, `server`, `location`

Sets the size of the initial read operation of the MP4 file.

#### vod_max_metadata_size
* **syntax**: `vod_max_metadata_size size`
* **default**: `128MB`
* **context**: `http`, `server`, `location`

Sets the maximum supported video metadata size (for MP4 - moov atom size)

#### vod_max_frames_size
* **syntax**: `vod_max_frames_size size`
* **default**: `16MB`
* **context**: `http`, `server`, `location`

Sets the limit on the total size of the frames of a single segment

#### vod_cache_buffer_size
* **syntax**: `vod_cache_buffer_size size`
* **default**: `256K`
* **context**: `http`, `server`, `location`

Sets the size of the cache buffers used when reading MP4 frames.

#### vod_open_file_thread_pool
* **syntax**: `vod_open_file_thread_pool pool_name`
* **default**: `off`
* **context**: `http`, `server`, `location`

Enables the use of asynchronous file open via thread pool.
The thread pool must be defined with a thread_pool directive, if no pool name is specified the default pool is used.
This directive is supported only on nginx 1.7.11 or newer when compiling with --add-threads.
Note: this directive currently disables the use of nginx's open_file_cache by nginx-vod-module

#### vod_output_buffer_pool
* **syntax**: `vod_output_buffer_pool size count`
* **default**: `off`
* **context**: `http`, `server`, `location`

Pre-allocates buffers for generating response data, saving the need allocate/free the buffers on every request.

#### vod_performance_counters
* **syntax**: `vod_performance_counters zone_name`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the shared memory object name of the performance counters

### Configuration directives - url structure

#### vod_base_url
* **syntax**: `vod_base_url url`
* **default**: `see below`
* **context**: `http`, `server`, `location`

Sets the base URL (scheme + domain) that should be returned in manifest responses.
The parameter value can contain variables, if the parameter evaluates to an empty string, relative URLs will be used.
If the parameter evaluates to a string ending with /, it is assumed to be a full URL - the module only appends the
file name to it, instead of a full URI.
If not set, the base URL is determined as follows:
1. If the request did not contain a host header (HTTP/1.0) relative URLs will be returned
2. Otherwise, the base URL will be `$scheme://$http_host`
The setting currently affects only HLS and DASH. In MSS and HDS, relative URLs are always returned.

#### vod_segments_base_url
* **syntax**: `vod_segments_base_url url`
* **default**: `see below`
* **context**: `http`, `server`, `location`

Sets the base URL (scheme + domain) that should be used for delivering video segments.
The parameter value can contain variables, if the parameter evaluates to an empty string, relative URLs will be used.
If not set, vod_base_url will be used.
The setting currently affects only HLS.

#### vod_multi_uri_suffix
* **syntax**: `vod_multi_uri_suffix suffix`
* **default**: `.urlset`
* **context**: `http`, `server`, `location`

A URL suffix that is used to identify multi URLs. A multi URL is a way to encode several different URLs
that should be played together as an adaptive streaming set, under a single URL. When the default suffix is
used, an HLS set URL may look like: 
http://host/hls/common-prefix,bitrate1,bitrate2,common-suffix.urlset/master.m3u8

#### vod_clip_to_param_name
* **syntax**: `vod_clip_to_param_name name`
* **default**: `clipTo`
* **context**: `http`, `server`, `location`

The name of the clip to request parameter.

#### vod_clip_from_param_name
* **syntax**: `vod_clip_from_param_name name`
* **default**: `clipFrom`
* **context**: `http`, `server`, `location`

The name of the clip from request parameter.

#### vod_tracks_param_name
* **syntax**: `vod_tracks_param_name name`
* **default**: `tracks`
* **context**: `http`, `server`, `location`

The name of the tracks request parameter.

#### vod_time_shift_param_name
* **syntax**: `vod_time_shift_param_name name`
* **default**: `shift`
* **context**: `http`, `server`, `location`

The name of the shift request parameter.

#### vod_speed_param_name
* **syntax**: `vod_speed_param_name name`
* **default**: `speed`
* **context**: `http`, `server`, `location`

The name of the speed request parameter.

#### vod_lang_param_name
* **syntax**: `vod_lang_param_name name`
* **default**: `lang`
* **context**: `http`, `server`, `location`

The name of the language request parameter.

#### vod_force_sequence_index
* **syntax**: `vod_force_sequence_index on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

Use sequence index in segment uris even if there is only one sequence

### Configuration directives - response headers

#### vod_expires
* **syntax**: `vod_expires time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the ""Expires"" and ""Cache-Control"" response headers for successful requests.
This directive is similar to nginx's built-in `expires` directive, except that it only supports the expiration interval scenario
(epoch, max, off, day time are not supported)
Main motivation for using this directive instead of the built-in `expires` is to have different expiration for VOD and dynamic live content.
If this directive is not specified, nginx-vod-module will not set the ""Expires"" / ""Cache-Control"" headers.
This setting affects all types of requests in VOD playlists and segment requests in live playlists.

#### vod_expires_live
* **syntax**: `vod_expires_live time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Same as `vod_expires` (above) for live requests that are not time dependent and not segments (e.g. HLS - master.m3u8, HDS - manifest.f4m).

#### vod_expires_live_time_dependent
* **syntax**: `vod_expires_live_time_dependent time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Same as `vod_expires` (above) for live requests that are time dependent (HLS - index.m3u8, HDS - bootstrap.abst, MSS - manifest, DASH - manifest.mpd).

#### vod_last_modified
* **syntax**: `vod_last_modified time`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the Last-Modified header returned on the response, by default the module does not return a Last-Modified header.
The reason for having this parameter here is in order to support If-Modified-Since / If-Unmodified-Since.
Since nginx's builtin ngx_http_not_modified_filter_module runs before any other header filter module, it will not see any headers set by add_headers / more_set_headers.
This makes nginx always reply as if the content changed (412 for If-Unmodified-Since / 200 for If-Modified-Since)
For live requests that are not segments (e.g. live DASH MPD), Last-Modified is set to the current server time.

#### vod_last_modified_types
* **syntax**: `vod_last_modified_types mime-type1 mime-type2 ...`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the MIME types for which the Last-Modified header should be set.
The special value ""*"" matches any MIME type.

### Configuration directives - ad stitching (mapped mode only)

#### vod_dynamic_mapping_cache
* **syntax**: `vod_dynamic_mapping_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the cache that stores the mapping of dynamic clips.

#### vod_dynamic_clip_map_uri
* **syntax**: `vod_dynamic_clip_map_uri uri`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the uri that should be used to map dynamic clips. 
The parameter value can contain variables, specifically, `$vod_clip_id` contains the id of the clip that should be mapped.
The expected response from this uri is a JSON containing a concat clip object.

#### vod_source_clip_map_uri
* **syntax**: `vod_source_clip_map_uri uri`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the uri that should be used to map source clips defined using the clipIds property of concat. 
The parameter value can contain variables, specifically, `$vod_clip_id` contains the id of the clip that should be mapped.
The expected response from this uri is a JSON containing a source clip object.

#### vod_redirect_segments_url
* **syntax**: `vod_redirect_segments_url url`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets a url to which requests for segments should be redirected.
The parameter value can contain variables, specifically, `$vod_dynamic_mapping` contains a serialized representation of the mapping of dynamic clips.

#### vod_apply_dynamic_mapping
* **syntax**: `vod_apply_dynamic_mapping mapping`
* **default**: `none`
* **context**: `http`, `server`, `location`

Maps dynamic clips to concat clips using the given expression, previously generated by `$vod_dynamic_mapping`.
The parameter value can contain variables.

#### vod_notification_uri
* **syntax**: `vod_notification_uri uri`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the uri that should be used to issue notifications. 
The parameter value can contain variables, specifically, `$vod_notification_id` contains the id of the notification that is being fired.
The response from this uri is ignored.

### Configuration directives - DRM / encryption

#### vod_secret_key
* **syntax**: `vod_secret_key string`
* **default**: `empty`
* **context**: `http`, `server`, `location`

Sets the seed that is used to generate the TS encryption key and DASH/MSS encryption IVs.
The parameter value can contain variables, and will usually have the structure ""secret-$vod_filepath"".
See the list of nginx variables added by this module below.

#### vod_encryption_iv_seed
* **syntax**: `vod_encryption_iv_seed string`
* **default**: `empty`
* **context**: `http`, `server`, `location`

Sets the seed that is used to generate the encryption IV, currently applies only to HLS/fMP4 with AES-128 encryption.
The parameter value can contain variables.

#### vod_drm_enabled
* **syntax**: `vod_drm_enabled on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module encrypts the media segments according to the response it gets from the drm upstream.
Currently supported only for dash and mss (play ready).

#### vod_drm_single_key
* **syntax**: `vod_drm_single_key on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module requests the drm info only for the first sequence and applies it to all sequences.
When disabled, the drm info is requested for each sequence separately.
In addition, in DASH, enabling this setting makes the module place the ContentProtection tag under AdaptationSet,
otherwise, it is placed under Representation.

#### vod_drm_clear_lead_segment_count
* **syntax**: `vod_drm_clear_lead_segment_count count`
* **default**: `1`
* **context**: `http`, `server`, `location`

Sets the number of clear (unencrypted) segments in the beginning of the stream. A clear lead enables the player to start playing without having to wait for the license response.

#### vod_drm_max_info_length
* **syntax**: `vod_drm_max_info_length length`
* **default**: `4K`
* **context**: `http`, `server`, `location`

Sets the maximum length of a drm info returned from upstream.

#### vod_drm_upstream_location
* **syntax**: `vod_drm_upstream_location location`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the nginx location that should be used for getting the DRM info for the file.

#### vod_drm_info_cache
* **syntax**: `vod_drm_info_cache zone_name zone_size [expiration]`
* **default**: `off`
* **context**: `http`, `server`, `location`

Configures the size and shared memory object name of the drm info cache.

#### vod_drm_request_uri
* **syntax**: `vod_drm_request_uri uri`
* **default**: `$vod_suburi`
* **context**: `http`, `server`, `location`

Sets the uri of drm info requests, the parameter value can contain variables.
In case of multi url, `$vod_suburi` will be the current sub uri (a separate drm info request is issued per sub URL)

#### vod_min_single_nalu_per_frame_segment
* **syntax**: `vod_min_single_nalu_per_frame_segment index`
* **default**: `0`
* **context**: `http`, `server`, `location`

Sets the minimum segment index (1-based) that should be assumed to have a single h264 nalu per frame.
If the value is 0, no assumption is being made on the number of nal units per frame.
This setting only affects DASH and MSS configurations that have DRM enabled.

When transcoding videos using libx264, by default, all frames have a single nal unit, except the first frame
that contains an additional nalu with the libx264 copyright information. Setting this parameter to a value
greater than 0 can provide a significant performance improvement, since the layout of the segment can be
calculated in advance, allowing the module to:
* Output segment buffers as they are generated (it doesn't have to wait for the whole segment to complete)
* Avoid frame processing for requests that do not need the segment data (e.g. HEAD, range 0-0, etc.)

### Configuration directives - DASH

#### vod_dash_absolute_manifest_urls
* **syntax**: `vod_dash_absolute_manifest_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute URLs in MPD requests

#### vod_dash_manifest_file_name_prefix
* **syntax**: `vod_dash_manifest_file_name_prefix name`
* **default**: `manifest`
* **context**: `http`, `server`, `location`

The name of the MPD file (an mpd extension is implied).

#### vod_dash_profiles
* **syntax**: `vod_dash_profiles profiles`
* **default**: `urn:mpeg:dash:profile:isoff-main:2011`
* **context**: `http`, `server`, `location`

Sets the profiles that are returned in the MPD tag in manifest responses.

#### vod_dash_init_file_name_prefix
* **syntax**: `vod_dash_init_file_name_prefix name`
* **default**: `init`
* **context**: `http`, `server`, `location`

The name of the MP4 initialization file (an mp4 extension is implied).

#### vod_dash_fragment_file_name_prefix
* **syntax**: `vod_dash_fragment_file_name_prefix name`
* **default**: `frag`
* **context**: `http`, `server`, `location`

The name of the fragment files (an m4s extension is implied).

#### vod_dash_manifest_format
* **syntax**: `vod_dash_manifest_format format`
* **default**: `segmenttimeline`
* **context**: `http`, `server`, `location`

Sets the MPD format, available options are:
* `segmentlist` - uses SegmentList and SegmentURL tags, in this format the URL of each fragment is explicitly set in the MPD
* `segmenttemplate` - uses SegmentTemplate, reporting a single duration for all fragments
* `segmenttimeline` - uses SegmentTemplate and SegmentTimeline to explicitly set the duration of the fragments

#### vod_dash_subtitle_format
* **syntax**: `vod_dash_subtitle_format format`
* **default**: `webvtt`
* **context**: `http`, `server`, `location`

Sets the format of the subtitles returned in the MPD, available options are:
* `webvtt` - WebVTT
* `smpte-tt` - SMPTE Timed Text

#### vod_dash_init_mp4_pssh
* **syntax**: `vod_dash_init_mp4_pssh on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled, the DRM pssh boxes are returned in the DASH init segment and in the manifest.
When disabled, the pssh boxes are returned only in the manifest.

#### vod_dash_duplicate_bitrate_threshold
* **syntax**: `vod_dash_duplicate_bitrate_threshold threshold`
* **default**: `4096`
* **context**: `http`, `server`, `location`

The bitrate threshold for removing identical bitrates, streams whose bitrate differences are less than
this value will be considered identical.

#### vod_dash_use_base_url_tag
* **syntax**: `vod_dash_use_base_url_tag on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, a BaseURL tag will be used to specify the fragments/init segment base url.
Otherwise, the media/initialization attributes under SegmentTemplate will contain absolute URLs. 

### Configuration directives - HDS

#### vod_hds_absolute_manifest_urls
* **syntax**: `vod_hds_absolute_manifest_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns the base URL in the F4M manifest

#### vod_hds_manifest_file_name_prefix
* **syntax**: `vod_hds_manifest_file_name_prefix name`
* **default**: `manifest`
* **context**: `http`, `server`, `location`

The name of the HDS manifest file (an f4m extension is implied).

#### vod_hds_fragment_file_name_prefix
* **syntax**: `vod_hds_fragment_file_name_prefix name`
* **default**: `frag`
* **context**: `http`, `server`, `location`

The prefix of fragment file names, the actual file name is `frag-f<file-index>-v<video-track-index>-a<audio-track-index>-Seg1-Frag<index>`.

#### vod_hds_generate_moof_atom
* **syntax**: `vod_hds_generate_moof_atom on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the module generates a moof atom in the HDS fragments, when disabled only an mdat atom is generated.
Turning this parameter off reduces the packaging overhead, however the default is on since Adobe tools are generating this atom.

### Configuration directives - HLS

#### vod_hls_encryption_method
* **syntax**: `vod_hls_encryption_method method`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the encryption method of HLS segments, allowed values are: none (default), aes-128, sample-aes, sample-aes-cenc.

#### vod_hls_force_unmuxed_segments
* **syntax**: `vod_hls_force_unmuxed_segments on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled the server returns the audio stream in separate segments than the ones used by the video stream (using EXT-X-MEDIA)

#### vod_hls_container_format
* **syntax**: `vod_hls_container_format mpegts/fmp4/auto`
* **default**: `auto`
* **context**: `http`, `server`, `location`

Sets the container format of the HLS segments. 
The default behavior is to use fmp4 for HEVC, and mpegts otherwise (Apple does not support HEVC over MPEG TS).

#### vod_hls_absolute_master_urls
* **syntax**: `vod_hls_absolute_master_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute playlist URLs in master playlist requests

#### vod_hls_absolute_index_urls
* **syntax**: `vod_hls_absolute_index_urls on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute segment URLs in media playlist requests

#### vod_hls_absolute_iframe_urls
* **syntax**: `vod_hls_absolute_iframe_urls on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled the server returns absolute segment URLs in iframe playlist requests

#### vod_hls_output_iframes_playlist
* **syntax**: `vod_hls_output_iframes_playlist on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When disabled iframe playlists are not returned as part of master playlists

#### vod_hls_master_file_name_prefix
* **syntax**: `vod_hls_master_file_name_prefix name`
* **default**: `master`
* **context**: `http`, `server`, `location`

The name of the HLS master playlist file (an m3u8 extension is implied).

#### vod_hls_index_file_name_prefix
* **syntax**: `vod_hls_index_file_name_prefix name`
* **default**: `index`
* **context**: `http`, `server`, `location`

The name of the HLS media playlist file (an m3u8 extension is implied).

#### vod_hls_iframes_file_name_prefix
* **syntax**: `vod_hls_iframes_file_name_prefix name`
* **default**: `iframes`
* **context**: `http`, `server`, `location`

The name of the HLS I-frames playlist file (an m3u8 extension is implied).

#### vod_hls_segment_file_name_prefix
* **syntax**: `vod_hls_segment_file_name_prefix name`
* **default**: `seg`
* **context**: `http`, `server`, `location`

The prefix of segment file names, the actual file name is `seg-<index>-v<video-track-index>-a<audio-track-index>.ts`.

#### vod_hls_init_file_name_prefix
* **syntax**: `vod_hls_init_file_name_prefix name`
* **default**: `init`
* **context**: `http`, `server`, `location`

The name of the init segment file name, only relevant when using fmp4 container.

#### vod_hls_encryption_key_file_name
* **syntax**: `vod_hls_encryption_key_file_name name`
* **default**: `encryption.key`
* **context**: `http`, `server`, `location`

The name of the encryption key file name, only relevant when encryption method is not `none`.

#### vod_hls_encryption_key_uri
* **syntax**: `vod_hls_encryption_key_uri uri`
* **default**: `a url pointing to encryption.key`
* **context**: `http`, `server`, `location`

Sets the value of the URI attribute of EXT-X-KEY, only relevant when encryption method is not `none`.
The parameter value can contain variables.

#### vod_hls_encryption_key_format
* **syntax**: `vod_hls_encryption_key_format format`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the KEYFORMAT attribute of EXT-X-KEY, only relevant when encryption method is not `none`.

#### vod_hls_encryption_key_format_versions
* **syntax**: `vod_hls_encryption_key_format_versions versions`
* **default**: `none`
* **context**: `http`, `server`, `location`

Sets the value of the KEYFORMATVERSIONS attribute of EXT-X-KEY, only relevant when encryption method is not `none`.

#### vod_hls_mpegts_interleave_frames
* **syntax**: `vod_hls_mpegts_interleave_frames on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the HLS muxer interleaves frames of different streams (audio / video).
When disabled, on every switch between audio / video the muxer flushes the MPEG TS packet.

#### vod_hls_mpegts_align_frames
* **syntax**: `vod_hls_mpegts_align_frames on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled, every video / audio frame is aligned to MPEG TS packet boundary,
padding is added as needed.

#### vod_hls_mpegts_output_id3_timestamps
* **syntax**: `vod_hls_mpegts_output_id3_timestamps on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, an ID3 TEXT frame will be outputted in each TS segment, containing a JSON with the absolute segment timestamp.
The timestamp is measured in milliseconds since the epoch (unixtime x 1000), the JSON structure is: `{""timestamp"":1459779115000}`

### Configuration directives - MSS

#### vod_mss_manifest_file_name_prefix
* **syntax**: `vod_mss_manifest_file_name_prefix name`
* **default**: `manifest`
* **context**: `http`, `server`, `location`

The name of the manifest file (has no extension).

#### vod_mss_duplicate_bitrate_threshold
* **syntax**: `vod_mss_duplicate_bitrate_threshold threshold`
* **default**: `4096`
* **context**: `http`, `server`, `location`

The bitrate threshold for removing identical bitrates, streams whose bitrate differences are less than
this value will be considered identical.

### Configuration directives - thumbnail capture

#### vod_thumb_file_name_prefix
* **syntax**: `vod_thumb_file_name_prefix name`
* **default**: `thumb`
* **context**: `http`, `server`, `location`

The name of the thumbnail file (a jpg extension is implied).

#### vod_thumb_accurate_positioning
* **syntax**: `vod_thumb_accurate_positioning on/off`
* **default**: `on`
* **context**: `http`, `server`, `location`

When enabled, the module grabs the frame that is closest to the requested offset.
When disabled, the module uses the keyframe that is closest to the requested offset.
Setting this parameter to off can result in faster thumbnail capture, since the module 
always decodes a single video frame per request.

#### vod_gop_look_behind
* **syntax**: `vod_gop_look_behind millis`
* **default**: `10000`
* **context**: `http`, `server`, `location`

Sets the interval (in milliseconds) before the thumbnail offset that should be loaded.
This setting should be set to the maximum GOP size, setting it to a lower value may result in capture failure.
Note that the metadata of all frames between `offset - vod_gop_look_behind` and `offset + vod_gop_look_ahead`
is loaded, however only the frames of the minimum GOP containing `offset` will be read and decoded.

#### vod_gop_look_ahead
* **syntax**: `vod_gop_look_ahead millis`
* **default**: `1000`
* **context**: `http`, `server`, `location`

Sets the interval (in milliseconds) after the thumbnail offset that should be loaded.

### Configuration directives - volume map

#### vod_volume_map_file_name_prefix
* **syntax**: `vod_volume_map_file_name_prefix name`
* **default**: `volume_map`
* **context**: `http`, `server`, `location`

The name of the volume map file (a csv extension is implied).

#### vod_volume_map_interval
* **syntax**: `vod_volume_map_interval millis`
* **default**: `1000`
* **context**: `http`, `server`, `location`

Sets the interval/resolution (in milliseconds) of the volume map.

### Configuration directives - misc

#### vod_ignore_edit_list
* **syntax**: `vod_ignore_edit_list on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module ignores any edit lists (elst) in the MP4 file.

#### vod_parse_hdlr_name
* **syntax**: `vod_parse_hdlr_name on/off`
* **default**: `off`
* **context**: `http`, `server`, `location`

When enabled, the module parses the name field of the hdlr MP4 atom, and uses it as the stream label.

### Nginx variables

The module adds the following nginx variables:
* `$vod_suburi` - the current sub uri. For example, if the url is:
  `http://<domain>/<location>/<prefix>,<middle1>,<middle2>,<middle3>,<postfix>.urlset/<filename>`
  `$vod_suburi` will have the value `http://<domain>/<location>/<prefix><middle1><postfix>/<filename>` 
  when processing the first uri.
* `$vod_filepath` - in local / mapped modes, the file path of current sub uri. In remote mode, has the same value as `$vod_suburi`.
* `$vod_set_id` - contains the id of the set.
* `$vod_sequence_id` - contains the id of the current sequence, if no id was specified in the mapping json this variable will be the same as `$vod_suburi`.
* `$vod_clip_id` - the id of the current clip, this variable has a value during these phases:
  1. Mapping of dynamic clips to concat clips
  2. Mapping of source clip to paths
* `$vod_notification_id` - the id of the current notification, the value is non-empty only when referenced by `vod_notification_uri`
* `$vod_dynamic_mapping` - a serialized representation of the mapping of dynamic clips to concat clips.
* `$vod_request_params` - a serialized representation of the request params, e.g. 12-f2-v1-a1. The variable contains:
  1. The segment index (for a segment request)
  2. The sequence index
  3. A selection of audio/video tracks
* `$vod_status` - the internal error code of the module, provides a more fine grained classification of errors than http status.
 the following values are defined:
 `BAD_REQUEST` - the request is invalid, for example, `clipFrom` is larger than the video duration
 `NO_STREAMS` - an invalid segment index was requested
 `EMPTY_MAPPING` - the mapping response is empty
 `BAD_MAPPING` - the mapping json is invalid, for example, the `sequences` element is missing
 `BAD_DATA` - the video file is corrupt
 `EXPIRED` - the current server time is larger than `expirationTime`
 `ALLOC_FAILED` - the module failed to allocate memory
 `UNEXPECTED` - a scenario that is not supposed to happen, most likely a bug in the module
* `$vod_segment_duration` - for segment requests, contains the duration of the segment in milliseconds
* `$vod_frames_bytes_read` - for segment requests, total number of bytes read while processing media frames

Note: Configuration directives that can accept variables are explicitly marked as such.

### Sample configurations

#### Local configuration

 http {
  upstream fallback {
   server fallback.kaltura.com:80;
  }

  server {
   # vod settings
   vod_mode local;
   vod_fallback_upstream_location /fallback;
   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
   vod_last_modified_types *;

   # vod caches
   vod_metadata_cache metadata_cache 512m;
   vod_response_cache response_cache 128m;
   
   # gzip manifests
   gzip on;
   gzip_types application/vnd.apple.mpegurl;

   # file handle caching / aio
   open_file_cache          max=1000 inactive=5m;
   open_file_cache_valid    2m;
   open_file_cache_min_uses 1;
   open_file_cache_errors   on;
   aio on;
   
   location ^~ /fallback/ {
    internal;
    proxy_pass http://fallback/;
    proxy_set_header Host $http_host;
   }

   location /content/ {
    root /web/;
    vod hls;
    
    add_header Access-Control-Allow-Headers '*';
    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
    add_header Access-Control-Allow-Origin '*';
    expires 100d;
   }
  }
 }

#### Mapped configuration

 http {
  upstream kalapi {
   server www.kaltura.com:80;
  }

  upstream fallback {
   server fallback.kaltura.com:80;
  }

  server {
   # vod settings
   vod_mode mapped;
   vod_upstream_location /kalapi;
   vod_upstream_extra_args ""pathOnly=1"";
   vod_fallback_upstream_location /fallback;
   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
   vod_last_modified_types *;

   # vod caches
   vod_metadata_cache metadata_cache 512m;
   vod_response_cache response_cache 128m;
   vod_mapping_cache mapping_cache 5m;
   
   # gzip manifests
   gzip on;
   gzip_types application/vnd.apple.mpegurl;

   # file handle caching / aio
   open_file_cache          max=1000 inactive=5m;
   open_file_cache_valid    2m;
   open_file_cache_min_uses 1;
   open_file_cache_errors   on;
   aio on;
   
   location ^~ /fallback/ {
    internal;
    proxy_pass http://fallback/;
    proxy_set_header Host $http_host;
   }

   location ^~ /kalapi/ {
    internal;
    proxy_pass http://kalapi/;
    proxy_set_header Host $http_host;
   }

   location ~ ^/p/\d+/(sp/\d+/)?serveFlavor/ {
    # encrypted hls
    vod hls;
    vod_secret_key ""mukkaukk$vod_filepath"";
    vod_hls_encryption_method aes-128;
    
    add_header Access-Control-Allow-Headers '*';
    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
    add_header Access-Control-Allow-Origin '*';
    expires 100d;
   }
  }
 }

#### Mapped + Remote configuration

 http {
  upstream jsonupstream {
   server jsonserver:80;
  }

  server {
   # vod settings
   vod_mode mapped;
   vod_upstream_location /json;
   vod_remote_upstream_location /proxy;
   vod_upstream_extra_args ""pathOnly=1"";
   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
   vod_last_modified_types *;

   # vod caches
   vod_metadata_cache metadata_cache 512m;
   vod_response_cache response_cache 128m;
   vod_mapping_cache mapping_cache 5m;

   # gzip manifests
   gzip on;
   gzip_types application/vnd.apple.mpegurl;

   # file handle caching / aio
   open_file_cache   max=1000 inactive=5m;
   open_file_cache_valid    2m;
   open_file_cache_min_uses 1;
   open_file_cache_errors   on;
   aio on;

   location ^~ /json/hls/ {
    internal;
    proxy_pass http://jsonupstream/;
    proxy_set_header Host $http_host;
   }

   location ~ /proxy/([^/]+)/(.*) {
    internal;
    proxy_pass $1://$2;
    resolver 8.8.8.8;
   }

   location ~ ^/hls/ {
    vod hls;

    add_header Access-Control-Allow-Headers '*';
    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
    add_header Access-Control-Allow-Origin '*';
    expires 100d;
   }
  }
 }

Set it up so that http://jsonserver:80/test.json returns the following JSON:

 {
  ""sequences"": [{
   ""clips"": [{
    ""type"": ""source"",
    ""path"": ""/http/commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4""
   }]
  }]
 }

And use this stream URL - http://nginx-vod-server/hls/test.json/master.m3u8

#### Remote configuration

 http {
  upstream kalapi {
   server www.kaltura.com:80;
  }

  server {
   # vod settings
   vod_mode remote;
   vod_upstream_location /kalapi;
   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';
   vod_last_modified_types *;

   # vod caches
   vod_metadata_cache metadata_cache 512m;
   vod_response_cache response_cache 128m;
   
   # gzip manifests
   gzip on;
   gzip_types application/vnd.apple.mpegurl;
   
   location ^~ /kalapi/ {
    internal;
    proxy_pass http://kalapi/;
    proxy_set_header Host $http_host;
   }

   location ~ ^/p/\d+/(sp/\d+/)?serveFlavor/ {
    vod hls;
    
    add_header Access-Control-Allow-Headers '*';
    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';
    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';
    add_header Access-Control-Allow-Origin '*';
    expires 100d;
   }
  }
 }

### Copyright & License

All code in this project is released under the [AGPLv3 license](http://www.gnu.org/licenses/agpl-3.0.html) unless a different license for a particular library is specified in the applicable library path. 

Copyright © Kaltura Inc. All rights reserved."
commonmark/cmark,4681,986,46,332,Organization,False,2444,6,34,74,False,CommonMark parsing and rendering library and program in C,,0,7,0,24,155,6,9,6,153,0,12,2154,5,80,1276,752,0,0,5,1,,,"cmark
=====

[![CI
tests](https://github.com/commonmark/cmark/workflows/CI%20tests/badge.svg)](https://github.com/commonmark/cmark/actions)

`cmark` is the C reference implementation of [CommonMark], a
rationalized version of Markdown syntax with a [spec][the spec].
(For the JavaScript reference implementation, see
[commonmark.js].)

It provides a shared library (`libcmark`) with functions for parsing
CommonMark documents to an abstract syntax tree (AST), manipulating
the AST, and rendering the document to HTML, groff man, LaTeX,
CommonMark, or an XML representation of the AST.  It also provides a
command-line program (`cmark`) for parsing and rendering CommonMark
documents.

Advantages of this library:

- **Portable.**  The library and program are written in standard
  C99 and have no external dependencies.  They have been tested with
  MSVC, gcc, tcc, and clang.

- **Fast.** cmark can render a Markdown version of *War and Peace* in
  the blink of an eye (127 milliseconds on a ten year old laptop,
  vs. 100-400 milliseconds for an eye blink).  In our [benchmarks],
  cmark is 10,000 times faster than the original `Markdown.pl`, and
  on par with the very fastest available Markdown processors.

- **Accurate.** The library passes all CommonMark conformance tests.

- **Standardized.** The library can be expected to parse CommonMark
  the same way as any other conforming parser.  So, for example,
  you can use `commonmark.js` on the client to preview content that
  will be rendered on the server using `cmark`.

- **Robust.** The library has been extensively fuzz-tested using
  [american fuzzy lop].  The test suite includes pathological cases
  that bring many other Markdown parsers to a crawl (for example,
  thousands-deep nested bracketed text or block quotes).

- **Flexible.** CommonMark input is parsed to an AST which can be
  manipulated programmatically prior to rendering.

- **Multiple renderers.**  Output in HTML, groff man, LaTeX, CommonMark,
  and a custom XML format is supported. And it is easy to write new
  renderers to support other formats.

- **Free.** BSD2-licensed.

It is easy to use `libcmark` in python, lua, ruby, and other dynamic
languages: see the `wrappers/` subdirectory for some simple examples.

There are also libraries that wrap `libcmark` for
[Go](https://github.com/rhinoman/go-commonmark),
[Haskell](https://hackage.haskell.org/package/cmark),
[Ruby](https://github.com/gjtorikian/commonmarker),
[Lua](https://github.com/jgm/cmark-lua),
[Perl](https://metacpan.org/release/CommonMark),
[Python](https://pypi.python.org/pypi/paka.cmark),
[R](https://cran.r-project.org/package=commonmark) and
[Scala](https://github.com/sparsetech/cmark-scala).

Installing
----------

Building the C program (`cmark`) and shared library (`libcmark`)
requires [cmake].  If you modify `scanners.re`, then you will also
need [re2c] \(>= 0.14.2\), which is used to generate `scanners.c` from
`scanners.re`.  We have included a pre-generated `scanners.c` in
the repository to reduce build dependencies.

If you have GNU make, you can simply `make`, `make test`, and `make
install`.  This calls [cmake] to create a `Makefile` in the `build`
directory, then uses that `Makefile` to create the executable and
library.  The binaries can be found in `build/src`.  The default
installation prefix is `/usr/local`.  To change the installation
prefix, pass the `INSTALL_PREFIX` variable if you run `make` for the
first time: `make INSTALL_PREFIX=path`.

For a more portable method, you can use [cmake] manually. [cmake] knows
how to create build environments for many build systems.  For example,
on FreeBSD:

    mkdir build
    cd build
    cmake ..  # optionally: -DCMAKE_INSTALL_PREFIX=path
    make      # executable will be created as build/src/cmark
    make test
    make install

Or, to create Xcode project files on OSX:

    mkdir build
    cd build
    cmake -G Xcode ..
    open cmark.xcodeproj

The GNU Makefile also provides a few other targets for developers.
To run a benchmark:

    make bench

For more detailed benchmarks:

    make newbench

To run a test for memory leaks using `valgrind`:

    make leakcheck

To reformat source code using `clang-format`:

    make format

To run a ""fuzz test"" against ten long randomly generated inputs:

    make fuzztest

To do a more systematic fuzz test with [american fuzzy lop]:

    AFL_PATH=/path/to/afl_directory make afl

Fuzzing with [libFuzzer] is also supported but, because libFuzzer is still
under active development, may not work with your system-installed version of
clang. Assuming LLVM has been built in `$HOME/src/llvm/build` the fuzzer can be
run with:

    CC=""$HOME/src/llvm/build/bin/clang"" LIB_FUZZER_PATH=""$HOME/src/llvm/lib/Fuzzer/libFuzzer.a"" make libFuzzer

To make a release tarball and zip archive:

    make archive

Installing (Windows)
--------------------

To compile with MSVC and NMAKE:

    nmake

You can cross-compile a Windows binary and dll on linux if you have the
`mingw32` compiler:

    make mingw

The binaries will be in `build-mingw/windows/bin`.

Usage
-----

Instructions for the use of the command line program and library can
be found in the man pages in the `man` subdirectory.

Security
--------

By default, the library will scrub raw HTML and potentially
dangerous links (`javascript:`, `vbscript:`, `data:`, `file:`).

To allow these, use the option `CMARK_OPT_UNSAFE` (or
`--unsafe`) with the command line program. If doing so, we
recommend you use a HTML sanitizer specific to your needs to
protect against [XSS
attacks](http://en.wikipedia.org/wiki/Cross-site_scripting).

Contributing
------------

There is a [forum for discussing
CommonMark](http://talk.commonmark.org); you should use it instead of
github issues for questions and possibly open-ended discussions.
Use the [github issue tracker](http://github.com/commonmark/CommonMark/issues)
only for simple, clear, actionable issues.

Authors
-------

John MacFarlane wrote the original library and program.
The block parsing algorithm was worked out together with David
Greenspan. Vicent Marti optimized the C implementation for
performance, increasing its speed tenfold.  Kārlis Gaņģis helped
work out a better parsing algorithm for links and emphasis,
eliminating several worst-case performance issues.
Nick Wellnhofer contributed many improvements, including
most of the C library's API and its test harness.

[benchmarks]: benchmarks.md
[the spec]: http://spec.commonmark.org
[CommonMark]: http://commonmark.org
[cmake]: http://www.cmake.org/download/
[re2c]: http://re2c.org
[commonmark.js]: https://github.com/commonmark/commonmark.js
[Build Status]: https://img.shields.io/travis/commonmark/cmark/master.svg?style=flat
[Windows Build Status]: https://ci.appveyor.com/api/projects/status/h3fd91vtd1xfmp69?svg=true
[american fuzzy lop]: http://lcamtuf.coredump.cx/afl/
[libFuzzer]: http://llvm.org/docs/LibFuzzer.html"
GNOME/glib,97385,866,75,364,Organization,False,21824,303,478,609,False,Low level core library,https://gitlab.gnome.org/GNOME/glib,0,6,0,,,,,0,32,0,1,,0,0,0,0,0,0,393,2,,,"# GLib

GLib is the low-level core library that forms the basis for projects such
as GTK and GNOME. It provides data structure handling for C, portability
wrappers, and interfaces for such runtime functionality as an event loop,
threads, dynamic loading, and an object system.

The official download locations are:
  <https://download.gnome.org/sources/glib>

The official web site is:
  <https://www.gtk.org/>

## Installation

See the file '[INSTALL.in](INSTALL.in)'

## How to report bugs

Bugs should be reported to the GNOME issue tracking system.
(<https://gitlab.gnome.org/GNOME/glib/issues/new>). You will need
to create an account for yourself.

In the bug report please include:

* Information about your system. For instance:
  * What operating system and version
  * For Linux, what version of the C library
  * And anything else you think is relevant.
* How to reproduce the bug.
  * If you can reproduce it with one of the test programs that are built
  in the tests/ subdirectory, that will be most convenient.  Otherwise,
  please include a short test program that exhibits the behavior.
  As a last resort, you can also provide a pointer to a larger piece
  of software that can be downloaded.
* If the bug was a crash, the exact text that was printed out
  when the crash occured.
* Further information such as stack traces may be useful, but
  is not necessary.

## Patches

Patches should also be submitted as merge requests to gitlab.gnome.org. If the
patch fixes an existing issue, please refer to the issue in your commit message
with the following notation (for issue 123):
Closes: #123

Otherwise, create a new merge request that introduces the change, filing a
separate issue is not required.

## Notes

### Notes about GLib 2.48

* The system copy of PCRE is now used by default to implement GRegex.
  Configure with --with-pcre=internal if a system PCRE version
  is unavailable or undesired.

### Notes about GLib 2.46

* GTask no longer imposes a fixed limit on the number of tasks that
  can be run_in_thread() simultaneously, since doing this inevitably
  results in deadlocks in some use cases. Instead, it now has a base
  number of threads that can be used ""for free"", but will gradually
  add more threads to the pool if too much time passes without any
  tasks completing.

  The exact behavior may continue to change in the future, and it's
  possible that some future version of GLib may not do any
  rate-limiting at all. As a result, you should no longer assume that
  GTask will rate-limit tasks itself (or, by extension, that calls to
  certain async gio methods will automatically be rate-limited for
  you). If you have a very large number of tasks to run, and don't
  want them to all run at once, you should rate-limit them yourself.

### Notes about GLib 2.40

* g_test_run() no longer runs tests in exactly the order they are
  registered; instead, it groups them according to test suites (ie,
  path components) like the documentation always claimed it did. In
  some cases, this can result in a sub-optimal ordering of tests,
  relative to the old behavior. The fix is to change the test paths to
  properly group together the tests that should run together. (eg, if
  you want to run test_foo_simple(), test_bar_simple(), and
  test_foo_using_bar() in that order, they should have test paths like
  ""/simple/foo"", ""/simple/bar"", ""/complex/foo-using-bar"", not
  ""/foo/simple"", ""/bar/simple"", ""/foo/using-bar"" (which would result
  in test_foo_using_bar() running before test_bar_simple()).

  (The behavior actually changed in GLib 2.36, but it was not
  documented at the time, since we didn't realize it mattered.)

### Notes about GLib 2.36

* It is no longer necessary to call g_type_init().  If you are
  loading GLib as a dynamic module, you should be careful to avoid
  unloading it, then subsequently loading it again.  This never
  really worked before, but it is now explicitly undefined behavior.
  Note that if g_type_init() was the only explicit use of a GObject
  API and you are using linker flags such as --no-add-needed, then
  you may have to artificially use some GObject call to keep the
  linker from optimizing away -lgobject. We recommend to use
  g_type_ensure (G_TYPE_OBJECT) for this purpose.

* This release contains an incompatible change to the g_get_home_dir()
  function.  Previously, this function would effectively ignore the HOME
  environment variable and always return the value from /etc/password.
  As of this version, the HOME variable is used if it is set and the
  value from /etc/passwd is only used as a fallback.

* The 'flowinfo' and 'scope_id' fields of GInetSocketAddress
  (introduced in GLib 2.32) have been fixed to be in host byte order
  rather than network byte order. This is an incompatible change, but
  the previous behavior was clearly broken, so it seems unlikely that
  anyone was using it.

### Notes about GLib 2.34

* GIO now looks for thumbnails in XDG_CACHE_HOME, following a
  recent alignment of the thumbnail spec with the basedir spec.

* The default values for GThreadPools max_unused_threads and
  max_idle_time settings have been changed to 2 and 15*1000,
  respectively.

### Notes about GLib 2.32

* It is no longer necessary to use g_thread_init() or to link against
  libgthread.  libglib is now always thread-enabled. Custom thread
  system implementations are no longer supported (including errorcheck
  mutexes).

* The thread and synchronisation APIs have been updated.
  GMutex and GCond can be statically allocated without explicit
  initialisation, as can new types GRWLock and GRecMutex.  The
  GStatic_______ variants of these types have been deprecated.  GPrivate
  can also be statically allocated and has a nicer API (deprecating
  GStaticPrivate).  Finally, g_thread_create() has been replaced with a
  substantially simplified g_thread_new().

* The g_once_init_enter()/_leave() functions have been replaced with
  macros that allow for a pointer to any gsize-sized object, not just a
  gsize*.  The assertions to ensure that a pointer to a correctly-sized
  object is being used will not work with generic pointers (ie: (void*)
  and (gpointer) casts) which would have worked with the old version.

* It is now mandatory to include glib.h instead of individual headers.

* The -uninstalled variants of the pkg-config files have been dropped.

* For a long time, gobject-2.0.pc mistakenly declared a public
  dependency on gthread-2.0.pc (when the dependency should have been
  private).  This means that programs got away with calling
  g_thread_init() without explicitly listing gthread-2.0.pc among their
  dependencies.

  gthread has now been removed as a gobject dependency, which will cause
  such programs to break.

  The fix for this problem is either to declare an explicit dependency
  on gthread-2.0.pc (if you care about compatibility with older GLib
  versions) or to stop calling g_thread_init().

* g_debug() output is no longer enabled by default.  It can be enabled
  on a per-domain basis with the G_MESSAGES_DEBUG environment variable
  like
    G_MESSAGES_DEBUG=domain1,domain2
  or
    G_MESSAGES_DEBUG=all

### Notes about GLib 2.30

* GObject includes a generic marshaller, g_cclosure_marshal_generic.
  To use it, simply specify NULL as the marshaller in g_signal_new().
  The generic marshaller is implemented with libffi, and consequently
  GObject depends on libffi now.

### Notes about GLib 2.28

* The GApplication API has changed compared to the version that was
  included in the 2.25 development snapshots. Existing users will need
  adjustments.

### Notes about GLib 2.26

* Nothing noteworthy.

### Notes about GLib 2.24

* It is now allowed to call g_thread_init(NULL) multiple times, and
  to call glib functions before g_thread_init(NULL) is called
  (although the later is mainly a change in docs as this worked before
  too). See the GThread reference documentation for the details.

* GObject now links to GThread and threads are enabled automatically
  when g_type_init() is called.

* GObject no longer allows to call g_object_set() on construct-only properties
  while an object is being initialized. If this behavior is needed, setting a
  custom constructor that just chains up will re-enable this functionality.

* GMappedFile on an empty file now returns NULL for the contents instead of
  returning an empty string. The documentation specifically states that code
  may not rely on nul-termination here so any breakage caused by this change
  is a bug in application code.

### Notes about GLib 2.22

* Repeated calls to g_simple_async_result_set_op_res_gpointer used
  to leak the data. This has been fixed to always call the provided
  destroy notify.

### Notes about GLib 2.20

* The functions for launching applications (e.g. g_app_info_launch() +
  friends) now passes a FUSE file:// URI if possible (requires gvfs
  with the FUSE daemon to be running and operational). With gvfs 2.26,
  FUSE file:// URIs will be mapped back to gio URIs in the GFile
  constructors. The intent of this change is to better integrate
  POSIX-only applications, see bug #528670 for the rationale.  The
  only user-visible change is when an application needs to examine an
  URI passed to it (e.g. as a positional parameter). Instead of
  looking at the given URI, the application will now need to look at
  the result of g_file_get_uri() after having constructed a GFile
  object with the given URI.

### Notes about GLib 2.18

* The recommended way of using GLib has always been to only include the
  toplevel headers glib.h, glib-object.h and gio.h. GLib enforces this by
  generating an error when individual headers are directly included.
  To help with the transition, the enforcement is not turned on by
  default for GLib headers (it is turned on for GObject and GIO).
  To turn it on, define the preprocessor symbol G_DISABLE_SINGLE_INCLUDES.

### Notes about GLib 2.16

* GLib now includes GIO, which adds optional dependencies against libattr
  and libselinux for extended attribute and SELinux support. Use
  --disable-xattr and --disable-selinux to build without these.

### Notes about GLib 2.10

* The functions g_snprintf() and g_vsnprintf() have been removed from
  the gprintf.h header, since they are already declared in glib.h. This
  doesn't break documented use of gprintf.h, but people have been known
  to include gprintf.h without including glib.h.

* The Unicode support has been updated to Unicode 4.1. This adds several
  new members to the GUnicodeBreakType enumeration.

* The support for Solaris threads has been retired. Solaris has provided
  POSIX threads for long enough now to have them available on every
  Solaris platform.

* 'make check' has been changed to validate translations by calling
  msgfmt with the -c option. As a result, it may fail on systems with
  older gettext implementations (GNU gettext < 0.14.1, or Solaris gettext).
  'make check' will also fail on systems where the C compiler does not
  support ELF visibility attributes.

* The GMemChunk API has been deprecated in favour of a new 'slice
  allocator'. See the g_slice documentation for more details.

* A new type, GInitiallyUnowned, has been introduced, which is
  intended to serve as a common implementation of the 'floating reference'
  concept that is e.g. used by GtkObject. Note that changing the
  inheritance hierarchy of a type can cause problems for language
  bindings and other code which needs to work closely with the type
  system. Therefore, switching to GInitiallyUnowned should be done
  carefully. g_object_compat_control() has been added to GLib 2.8.5
  to help with the transition.

### Notes about GLib 2.6.0

* GLib 2.6 introduces the concept of 'GLib filename encoding', which is the
  on-disk encoding on Unix, but UTF-8 on Windows. All GLib functions
  returning or accepting pathnames have been changed to expect
  filenames in this encoding, and the common POSIX functions dealing
  with pathnames have been wrapped. These wrappers are declared in the
  header <glib/gstdio.h> which must be included explicitly; it is not
  included through <glib.h>.

  On current (NT-based) Windows versions, where the on-disk file names
  are Unicode, these wrappers use the wide-character API in the C
  library. Thus applications can handle file names containing any
  Unicode characters through GLib's own API and its POSIX wrappers,
  not just file names restricted to characters in the system codepage.

  To keep binary compatibility with applications compiled against
  older versions of GLib, the Windows DLL still provides entry points
  with the old semantics using the old names, and applications
  compiled against GLib 2.6 will actually use new names for the
  functions. This is transparent to the programmer.

  When compiling against GLib 2.6, applications intended to be
  portable to Windows must take the UTF-8 file name encoding into
  consideration, and use the gstdio wrappers to access files whose
  names have been constructed from strings returned from GLib.

* Likewise, g_get_user_name() and g_get_real_name() have been changed
  to return UTF-8 on Windows, while keeping the old semantics for
  applications compiled against older versions of GLib.

* The GLib uses an '_' prefix to indicate private symbols that
  must not be used by applications. On some platforms, symbols beginning
  with prefixes such as _g will be exported from the library, on others not.
  In no case can applications use these private symbols. In addition to that,
  GLib+ 2.6 makes several symbols private which were not in any installed
  header files and were never intended to be exported.

* To reduce code size and improve efficiency, GLib, when compiled
  with the GNU toolchain, has separate internal and external entry
  points for exported functions. The internal names, which begin with
  IA__, may be seen when debugging a GLib program.

* On Windows, GLib no longer opens a console window when printing
  warning messages if stdout or stderr are invalid, as they are in
  ""Windows subsystem"" (GUI) applications. Simply redirect stdout or
  stderr if you need to see them.

* The child watch functionality tends to reveal a bug in many
  thread implementations (in particular the older LinuxThreads
  implementation on Linux) where it's not possible to call waitpid()
  for a child created in a different thread. For this reason, for
  maximum portability, you should structure your code to fork all
  child processes that you want to wait for from the main thread.

* A problem was recently discovered with g_signal_connect_object();
  it doesn't actually disconnect the signal handler once the object being
  connected to dies, just disables it. See the API docs for the function
  for further details and the correct workaround that will continue to
  work with future versions of GLib."
checkpoint-restore/criu,17418,1112,73,264,Organization,False,10244,9,60,94,False,Checkpoint/Restore tool,http://criu.org,17,33,3,283,489,41,48,32,298,24,109,,0,0,0,0,0,0,4,12,,,"[![master](https://travis-ci.org/checkpoint-restore/criu.svg?branch=master)](https://travis-ci.org/checkpoint-restore/criu)
[![development](https://travis-ci.org/checkpoint-restore/criu.svg?branch=criu-dev)](https://travis-ci.org/checkpoint-restore/criu)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/55251ec7db28421da4481fc7c1cb0cee)](https://www.codacy.com/app/xemul/criu?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=xemul/criu&amp;utm_campaign=Badge_Grade)
<p align=""center""><img src=""https://criu.org/w/images/1/1c/CRIU.svg"" width=""256px""/></p>

## CRIU -- A project to implement checkpoint/restore functionality for Linux

CRIU (stands for Checkpoint and Restore in Userspace) is a utility to checkpoint/restore Linux tasks.

Using this tool, you can freeze a running application (or part of it) and checkpoint 
it to a hard drive as a collection of files. You can then use the files to restore and run the
application from the point it was frozen at. The distinctive feature of the CRIU
project is that it is mainly implemented in user space. There are some more projects
doing C/R for Linux, and so far CRIU [appears to be](https://criu.org/Comparison_to_other_CR_projects) 
the most feature-rich and up-to-date with the kernel.

The project [started](https://criu.org/History) as the way to do live migration for OpenVZ
Linux containers, but later grew to more sophisticated and flexible tool. It is currently 
used by (integrated into) OpenVZ, LXC/LXD, Docker, and other software, project gets tremendous 
help from the community, and its packages are included into many Linux distributions.

The project home is at http://criu.org. This wiki contains all the knowledge base for CRIU we have.
Pages worth starting with are:
- [Installation instructions](http://criu.org/Installation)
- [A simple example of usage](http://criu.org/Simple_loop)
- [Examples of more advanced usage](https://criu.org/Category:HOWTO)
- Troubleshooting can be hard, some help can be found [here](https://criu.org/When_C/R_fails), [here](https://criu.org/What_cannot_be_checkpointed) and [here](https://criu.org/FAQ)

### Checkpoint and restore of simple loop process 
[<p align=""center""><img src=""https://asciinema.org/a/232445.png"" width=""572px"" height=""412px""/></p>](https://asciinema.org/a/232445)

## Advanced features

As main usage for CRIU is live migration, there's a library for it called P.Haul. Also the
project exposes two cool core features as standalone libraries. These are libcompel for parasite code 
injection and libsoccr for TCP connections checkpoint-restore.

### Live migration

True [live migration](https://criu.org/Live_migration) using CRIU is possible, but doing
all the steps by hands might be complicated. The [phaul sub-project](https://criu.org/P.Haul)
provides a Go library that encapsulates most of the complexity. This library and the Go bindings
for CRIU are stored in the [go-criu](https://github.com/checkpoint-restore/go-criu) repository.


### Parasite code injection

In order to get state of the running process CRIU needs to make this process execute
some code, that would fetch the required information. To make this happen without
killing the application itself, CRIU uses the [parasite code injection](https://criu.org/Parasite_code)
technique, which is also available as a standalone library called [libcompel](https://criu.org/Compel).

### TCP sockets checkpoint-restore

One of the CRIU features is the ability to save and restore state of a TCP socket
without breaking the connection. This functionality is considered to be useful by
itself, and we have it available as the [libsoccr library](https://criu.org/Libsoccr).

## How to contribute

CRIU project is (almost) the never-ending story, because we have to always keep up with the
Linux kernel supporting checkpoint and restore for all the features it provides. Thus we're
looking for contributors of all kinds -- feedback, bug reports, testing, coding, writing, etc.
Here are some useful hints to get involved.

* We have both -- [very simple](https://github.com/checkpoint-restore/criu/issues?q=is%3Aissue+is%3Aopen+label%3Aenhancement) and [more sophisticated](https://github.com/checkpoint-restore/criu/issues?q=is%3Aissue+is%3Aopen+label%3A%22new+feature%22) coding tasks;
* CRIU does need [extensive testing](https://github.com/checkpoint-restore/criu/issues?q=is%3Aissue+is%3Aopen+label%3Atesting);
* Documentation is always hard, we have [some information](https://criu.org/Category:Empty_articles) that is to be extracted from people's heads into wiki pages as well as [some texts](https://criu.org/Category:Editor_help_needed) that all need to be converted into useful articles;
* Feedback is expected on the github issues page and on the [mailing list](https://lists.openvz.org/mailman/listinfo/criu);
* We accept github pull requests and this is the preferred way to contribute to CRIU. If you prefer to send patches by email, you are welcome to send them to [the devel list](http://criu.org/How_to_submit_patches);
* Spread the word about CRIU in [social networks](http://criu.org/Contacts);
* If you're giving a talk about CRIU -- let us know, we'll mention it on the [wiki main page](https://criu.org/News/events);

## Licence

The project is licensed under GPLv2 (though files sitting in the lib/ directory are LGPLv2.1)."
gpac/gpac,121966,1003,87,310,Organization,False,9813,7,7,44,False,GPAC main code repository,http://www.gpac.io,4,17,1,78,1103,29,96,5,320,1,15,,0,0,0,0,0,0,22,0,,,"[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu64)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/linux64)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu32)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/linux32)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/windows64)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/win64)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/windows32)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/win32)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/macos)](https://buildbot.gpac.io/#/grid?branch=master)
[![Tests](https://tests.gpac.io/testres/badge/tests/macos)](https://tests.gpac.io/)

[![Build Status](https://tests.gpac.io/testres/badge/build/ios)](https://buildbot.gpac.io/#/grid?branch=master)

[![Build Status](https://tests.gpac.io/testres/badge/build/android)](https://buildbot.gpac.io/#/grid?branch=master)

[![Coverage](https://tests.gpac.io/testres/badge/cov/linux64)](https://tests.gpac.io/)

![License](https://img.shields.io/badge/license-LGPL-blue.svg)

README for GPAC version 0.8.0

GPAC is a multimedia framework oriented towards rich media and distributed under the LGPL license (see COPYING).

GPAC supports many multimedia formats, from simple audiovisual containers (avi, mov, mpg) to complex
presentation formats (MPEG-4 Systems, SVG Tiny 1.2, VRML/X3D) and 360 videos. GPAC supports presentation scripting for MPEG4/VRML/X3D through mozilla SpiderMonkey javascript engine.

GPAC currently supports local playback, http progressive download, Adaptive HTTP Streaming (MPEG-DASH, HLS), RTP/RTSP streaming over UDP (unicast or multicast) or TCP and TS demuxing (from file, IP or DVB4Linux).

GPAC also features MP4Box, a multimedia swiss-army knife for the prompt, and MP42TS, a fast TS multiplexer from MP4 and RTP sources.

For compilation and installation instruction, check INSTALLME file

For GPAC configuration instruction, check gpac/doc/configuration.html or gpac/doc/man/gpac.1 (man gpac when installed)

For more information, visit the GPAC website:
 http://gpac.io"
buserror/simavr,4564,995,75,249,User,False,952,4,18,66,False,"simavr is a lean, mean and hackable AVR simulator for linux & OSX",,7,7,0,59,93,11,4,22,208,4,8,3855,6,7,46,20,0,0,26,,114,,"simavr - a lean and mean Atmel AVR simulator for linux
======

_simavr_ is a new AVR simulator for linux, or any platform that uses avr-gcc. It uses 
avr-gcc's own register definition to simplify creating new targets for supported AVR
devices. The core was made to be small and compact, and hackable so allow quick 
prototyping of an AVR project. The AVR core is now stable for use with parts 
with <= 128KB flash, and with preliminary support for the bigger parts. The 
simulator loads ELF files directly, and there is even a way to specify simulation 
parameters directly in the emulated code using an .elf section. You can also 
load multipart HEX files.

Installation
------------
On OSX, we recommend using [homebrew](https://brew.sh):

    brew tap osx-cross/avr
    brew install --HEAD simavr

On Ubuntu, SimAVR is available in the Bionic package source:

    apt-get install simavr

(Note that the command is made available under the name `simavr` not `run_avr`.)

Otherwise, `make` is enough to just start using __bin/simavr__. To install the __simavr__ command system-wide, `make install RELEASE=1`.

Supported IOs
--------------
* _eeprom_
* _watchdog_
* _IO ports_ (including pin interrupts)
* _Timers_, 8 &16 (Normal, CTC and Fast PWM, the overflow interrupt too)
* The _UART_, including tx & rx interrupts (there is a loopback/local echo test mode too)
* _SPI_, master/slave including the interrupt
* _i2c_ Master & Slave
* External _Interrupts_, INT0 and so on.
* _ADC_
* Self-programming (ie bootloaders!)

Emulated Cores (very easy to add new ones!)
--------------
+ ATMega2560
+ AT90USB162 (with USB!)
+ ATMega1281
+ ATMega1280
+ ATMega128
+ ATMega128rf1
+ ATMega16M1
+ ATMega169
+ ATMega162
+ ATMega164/324/644
+ ATMega48/88/168/328
+ ATMega8/16/32
+ ATTiny25/45/85
+ ATTIny44/84
+ ATTiny2313/2313v
+ ATTiny13/13a

Extras:
-------
* fully working _gdb_ support including some pretty cool “passive modes”.
* There is also very easy support for “VCD” (Value Change Dump) that can be visualized 
graphically as “waveforms” with tools like _gtkwave_ (see below).
* There are a few examples of real life firmwares running on simavr, including OpenGL rendering of the display…
* There is support for _Arduino_, but no IDE integration

Documentation And Further Information
-------------------------------------

* Manual / Developer Guide: https://github.com/buserror-uk/simavr/blob/master/doc/manual/manual.pdf?raw=true
* Examples: https://github.com/buserror-uk/simavr/tree/master/examples
* Mailing List: http://groups.google.com/group/simavr
* IRC: _#simavr_ on Freenode

Contributing
------------

Patches are always welcome! Please submit your changes via Github pull requests.

VCD Support -- built in logic analyzer 
-----------
_simavr_ can output most of its pins, firmware variables, interrupts and a few other
things as signals to be dumped into a file that can be plotted using gtkwave for
further, precise analysis.
A firmware can contain instructions for _simavr_ to know what to trace, and the file is
automatically generated.
Example:

 const struct avr_mmcu_vcd_trace_t _mytrace[]  _MMCU_ = {
  { AVR_MCU_VCD_SYMBOL(""UDR0""), .what = (void*)&UDR0, },
  { AVR_MCU_VCD_SYMBOL(""UDRE0""), .mask = (1 << UDRE0), .what = (void*)&UCSR0A, },
 };

Will tell _simavr_ to generate a trace everytime the UDR0 register changes and everytime
the interrupt is raised (in UCSR0A). The *_MMCU_* tag tells gcc that it needs compiling,
but it won't be linked in your program, so it takes literally zero bytes, this is a code
section that is private to _simavr_, it's free!
A program running with these instructions and writing to the serial port will generate
a file that will display:

 $ ./simavr/run_avr tests/atmega88_example.axf
 AVR_MMCU_TAG_VCD_TRACE 00c6:00 - UDR0
 AVR_MMCU_TAG_VCD_TRACE 00c0:20 - UDRE0
 Loaded 1780 .text
 Loaded 114 .data
 Loaded 4 .eeprom
 Starting atmega88 - flashend 1fff ramend 04ff e2end 01ff
 atmega88 init
 avr_eeprom_ioctl: AVR_IOCTL_EEPROM_SET Loaded 4 at offset 0
 Creating VCD trace file 'gtkwave_trace.vcd'
 Read from eeprom 0xdeadbeef -- should be 0xdeadbeef..
 Read from eeprom 0xcafef00d -- should be 0xcafef00d..
 simavr: sleeping with interrupts off, quitting gracefully

And when the file is loaded in gtkwave, you see:
![gtkwave](https://github.com/buserror-uk/simavr/raw/master/doc/img/gtkwave1.png)

You get a very precise timing breakdown of any change that you add to the trace, down
to the AVR cycle. 

Example:
--------
_simavr_ is really made to be the center for emulating your own AVR projects, not just
a debugger, but also the emulating the peripherals you will use in your firmware, so 
you can test and develop offline, and now and then try it on the hardware.

You can also use _simavr_ to do test units on your shipping firmware to validate it
before you ship a new version, to prevent regressions or mistakes.

_simavr_ has a few 'complete projects/ that demonstrate this, most of them were made
using real hardware at some point, and the firmware binary is _exactly_ the one that
ran on the hardware. The key here is to emulate the _parts_ or peripherals that
are hooked to the AVR. Of course, you don't have to emulate the full hardware, you just
need to generate the proper stimulus so that the AVR is fooled.

HD44780 LCD Board Demo
----------------------

![lcd](https://github.com/buserror-uk/simavr/raw/master/doc/img/hd44780.png)

This example board hooks up an Atmega48 to an emulated HD44780 LCD and display a running
counter in the 'lcd'. Everything is emulated, the firmware runs exactly like this
on a real hardware.

![lcd-gtkwave](https://github.com/buserror-uk/simavr/raw/master/doc/img/hd44780-wave.png)

And this is a gtkwave trace of what the firmware is doing. You can zoom in, measure, etc
in gtkwave, select trades to see etc.

Quite a few other examples are available!"
strongswan/strongswan,93414,957,100,444,Organization,False,17484,84,218,75,False,strongSwan - IPsec-based VPN,https://www.strongswan.org,6,0,0,,,,,13,160,1,11,5339,5,172,11273,3334,0,0,8,1,,,"# strongSwan Configuration #

## Overview ##

strongSwan is an OpenSource IPsec-based VPN solution.

This document is just a short introduction of the strongSwan **swanctl** command
which uses the modern [**vici**](src/libcharon/plugins/vici/README.md) *Versatile
IKE Configuration Interface*. The deprecated **ipsec** command using the legacy
**stroke** configuration interface is described [**here**](README_LEGACY.md).
For more detailed information consult the man pages and
[**our wiki**](https://wiki.strongswan.org).


## Quickstart ##

Certificates for users, hosts and gateways are issued by a fictitious
strongSwan CA. In our example scenarios the CA certificate `strongswanCert.pem`
must be present on all VPN endpoints in order to be able to authenticate the
peers. For your particular VPN application you can either use certificates from
any third-party CA or generate the needed private keys and certificates yourself
with the strongSwan **pki** tool, the use of which will be explained in one of
the sections following below.


### Site-to-Site Case ###

In this scenario two security gateways _moon_ and _sun_ will connect the
two subnets _moon-net_ and _sun-net_ with each other through a VPN tunnel
set up between the two gateways:

    10.1.0.0/16 -- | 192.168.0.1 | === | 192.168.0.2 | -- 10.2.0.0/16
      moon-net          moon                 sun           sun-net

Configuration on gateway _moon_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/moonCert.pem
    /etc/swanctl/private/moonKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            net-net {
                remote_addrs = 192.168.0.2

                local {
                    auth = pubkey
                    certs = moonCert.pem
                }
                remote {
                    auth = pubkey
                    id = ""C=CH, O=strongSwan, CN=sun.strongswan.org""
                }
                children {
                    net-net {
                        local_ts  = 10.1.0.0/16
                        remote_ts = 10.2.0.0/16
                        start_action = trap
                    }
                }
            }
        }

Configuration on gateway _sun_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/sunCert.pem
    /etc/swanctl/private/sunKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            net-net {
                remote_addrs = 192.168.0.1

                local {
                    auth = pubkey
                    certs = sunCert.pem
                }
                remote {
                    auth = pubkey
                    id = ""C=CH, O=strongSwan, CN=moon.strongswan.org""
                }
                children {
                    net-net {
                        local_ts  = 10.2.0.0/16
                        remote_ts = 10.1.0.0/16
                        start_action = trap
                    }
                }
            }
        }

The local and remote identities used in this scenario are the
*subjectDistinguishedNames* contained in the end entity certificates.
The certificates and private keys are loaded into the **charon** daemon with
the command

    swanctl --load-creds

whereas

    swanctl --load-conns

loads the connections defined in `swanctl.conf`. With `start_action = trap` the
IPsec connection is automatically set up with the first plaintext payload IP
packet wanting to go through the tunnel.

### Host-to-Host Case ###

This is a setup between two single hosts which don't have a subnet behind
them.  Although IPsec transport mode would be sufficient for host-to-host
connections we will use the default IPsec tunnel mode.

    | 192.168.0.1 | === | 192.168.0.2 |
         moon                sun

Configuration on host _moon_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/moonCert.pem
    /etc/swanctl/private/moonKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            host-host {
                remote_addrs = 192.168.0.2

                local {
                    auth=pubkey
                    certs = moonCert.pem
                }
                remote {
                    auth = pubkey
                    id = ""C=CH, O=strongSwan, CN=sun.strongswan.org""
                }
                children {
                    net-net {
                        start_action = trap
                    }
                }
            }
        }

Configuration on host _sun_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/sunCert.pem
    /etc/swanctl/private/sunKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            host-host {
                remote_addrs = 192.168.0.1

                local {
                    auth = pubkey
                    certs = sunCert.pem
                }
                remote {
                    auth = pubkey
                    id = ""C=CH, O=strongSwan, CN=moon.strongswan.org""
                }
                children {
                    host-host {
                        start_action = trap
                    }
                }
            }
        }


### Roadwarrior Case ###

This is a very common case where a strongSwan gateway serves an arbitrary
number of remote VPN clients usually having dynamic IP addresses.

    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x |
      moon-net          moon              carol

Configuration on gateway _moon_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/moonCert.pem
    /etc/swanctl/private/moonKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            rw {
                local {
                    auth = pubkey
                    certs = moonCert.pem
                    id = moon.strongswan.org
                }
                remote {
                    auth = pubkey
                }
                children {
                    net-net {
                        local_ts  = 10.1.0.0/16
                    }
                }
            }
        }

Configuration on roadwarrior _carol_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/carolCert.pem
    /etc/swanctl/private/carolKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            home {
                remote_addrs = moon.strongswan.org

                local {
                    auth = pubkey
                    certs = carolCert.pem
                    id = carol@strongswan.org
                }
                remote {
                    auth = pubkey
                    id = moon.strongswan.org
                }
                children {
                    home {
                        local_ts  = 10.1.0.0/16
                        start_action = start
                    }
                }
            }
        }

For `remote_addrs` the hostname `moon.strongswan.org` was chosen which will be
resolved by DNS at runtime into the corresponding IP destination address.
In this scenario the identity of the roadwarrior `carol` is the email address
`carol@strongswan.org` which must be included as a *subjectAlternativeName* in
the roadwarrior certificate `carolCert.pem`.


### Roadwarrior Case with Virtual IP ###

Roadwarriors usually have dynamic IP addresses assigned by the ISP they are
currently attached to.  In order to simplify the routing from _moon-net_ back
to the remote access client _carol_ it would be desirable if the roadwarrior had
an inner IP address chosen from a pre-defined pool.

    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x | -- 10.3.0.1
      moon-net          moon              carol       virtual IP

In our example the virtual IP address is chosen from the address pool
`10.3.0.0/16` which can be configured by adding the section

    pools {
        rw_pool {
            addrs = 10.3.0.0/16
        }
    }

to the gateway's `swanctl.conf` from where they are loaded into the **charon**
daemon using the command

    swanctl --load-pools

To request an IP address from this pool a roadwarrior can use IKEv1 mode config
or IKEv2 configuration payloads. The configuration for both is the same

    vips = 0.0.0.0

Configuration on gateway _moon_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/moonCert.pem
    /etc/swanctl/private/moonKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            rw {
                pools = rw_pool

                local {
                    auth = pubkey
                    certs = moonCert.pem
                    id = moon.strongswan.org
                }
                remote {
                    auth = pubkey
                }
                children {
                    net-net {
                        local_ts  = 10.1.0.0/16
                    }
                }
            }
        }

        pools {
            rw_pool {
                addrs = 10.30.0.0/16
            }
        }

Configuration on roadwarrior _carol_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/carolCert.pem
    /etc/swanctl/private/carolKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            home {
                remote_addrs = moon.strongswan.org
                vips = 0.0.0.0

                local {
                    auth = pubkey
                    certs = carolCert.pem
                    id = carol@strongswan.org
                }
                remote {
                    auth = pubkey
                    id = moon.strongswan.org
                }
                children {
                    home {
                        local_ts  = 10.1.0.0/16
                        start_action = start
                    }
                }
            }
        }


### Roadwarrior Case with EAP Authentication ###

This is a very common case where a strongSwan gateway serves an arbitrary
number of remote VPN clients which authenticate themselves via a password
based *Extended Authentication Protocol* as e.g. *EAP-MD5* or *EAP-MSCHAPv2*.

    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x |
      moon-net          moon              carol

Configuration on gateway _moon_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/moonCert.pem
    /etc/swanctl/private/moonKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            rw {
                local {
                    auth = pubkey
                    certs = moonCert.pem
                    id = moon.strongswan.org
                }
                remote {
                    auth = eap-md5
                }
                children {
                    net-net {
                        local_ts  = 10.1.0.0/16
                    }
                }
                send_certreq = no
            }
        }

The  `swanctl.conf` file additionally contains a `secrets` section defining all
client credentials

        secrets {
            eap-carol {
                id = carol@strongswan.org
                secret = Ar3etTnp
            }
            eap-dave {
                id = dave@strongswan.org
                secret = W7R0g3do
            }
        }

Configuration on roadwarrior _carol_:

    /etc/swanctl/x509ca/strongswanCert.pem

    /etc/swanctl/swanctl.conf:

        connections {
            home {
                remote_addrs = moon.strongswan.org

                local {
                    auth = eap
                    id = carol@strongswan.org
                }
                remote {
                    auth = pubkey
                    id = moon.strongswan.org
                }
                children {
                    home {
                        local_ts  = 10.1.0.0/16
                        start_action = start
                    }
                }
            }
        }

        secrets {
            eap-carol {
                id = carol@strongswan.org
                secret = Ar3etTnp
            }
        }


### Roadwarrior Case with EAP Identity ###

Often a client EAP identity is exchanged via EAP which differs from the
external IKEv2 identity. In this example the IKEv2 identity defaults to
the IPv4 address of the client.

    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x |
      moon-net          moon              carol

Configuration on gateway _moon_:

    /etc/swanctl/x509ca/strongswanCert.pem
    /etc/swanctl/x509/moonCert.pem
    /etc/swanctl/private/moonKey.pem

    /etc/swanctl/swanctl.conf:

        connections {
            rw {
                local {
                    auth = pubkey
                    certs = moonCert.pem
                    id = moon.strongswan.org
                }
                remote {
                    auth = eap-md5
                    eap_id = %any
                }
                children {
                    net-net {
                        local_ts  = 10.1.0.0/16
                    }
                }
                send_certreq = no
            }
        }

        secrets {
            eap-carol {
                id = carol
                secret = Ar3etTnp
            }
            eap-dave {
                id = dave
                secret = W7R0g3do
            }
        }

Configuration on roadwarrior _carol_:

    /etc/swanctl/x509ca/strongswanCert.pem

    /etc/swanctl/swanctl.conf:

        connections {
            home {
                remote_addrs = moon.strongswan.org

                local {
                    auth = eap
                    eap_id = carol
                }
                remote {
                    auth = pubkey
                    id = moon.strongswan.org
                }
                children {
                    home {
                        local_ts  = 10.1.0.0/16
                        start_action = start
                    }
                }
            }
        }

        secrets {
            eap-carol {
                id = carol
                secret = Ar3etTnp
            }
        }


## Generating Certificates and CRLs ##

This section is not a full-blown tutorial on how to use the strongSwan **pki**
tool. It just lists a few points that are relevant if you want to generate your
own certificates and CRLs for use with strongSwan.


### Generating a CA Certificate ###

The pki statement

    pki --gen --type ed25519 --outform pem > strongswanKey.pem

generates an elliptic Edwards-Curve key with a cryptographic strength of 128
bits. The corresponding public key is packed into a self-signed CA certificate
with a lifetime of 10 years (3652 days)

    pki --self --ca --lifetime 3652 --in strongswanKey.pem \
               --dn ""C=CH, O=strongSwan, CN=strongSwan Root CA"" \
               --outform pem > strongswanCert.pem

which can be listed with the command

    pki --print --in strongswanCert.pem

    subject:  ""C=CH, O=strongSwan, CN=strongSwan Root CA""
    issuer:   ""C=CH, O=strongSwan, CN=strongSwan Root CA""
    validity:  not before May 18 08:32:06 2017, ok
               not after  May 18 08:32:06 2027, ok (expires in 3651 days)
    serial:    57:e0:6b:3a:9a:eb:c6:e0
    flags:     CA CRLSign self-signed
    subjkeyId: 2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5d
    pubkey:    ED25519 256 bits
    keyid:     a7:e1:6a:3f:e7:6f:08:9d:89:ec:23:92:a9:a1:14:3c:78:a8:7a:f7
    subjkey:   2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5d

If you prefer the CA private key and X.509 certificate to be in binary DER format
then just omit the `--outform pem` option. The directory `/etc/swanctl/x509ca`
contains all required CA certificates either in binary DER or in Base64 PEM
format. Irrespective of the file suffix the correct format will be determined
by strongSwan automagically.


### Generating a Host or User End Entity Certificate ###

Again we are using the command

    pki --gen --type ed25519 --outform pem > moonKey.pem

to generate an Ed25519 private key for the host `moon`. Alternatively you could
type

    pki --gen --type rsa --size 3072 > moonKey.der

to generate a traditional 3072 bit RSA key and store it in binary DER format.
As an alternative a **TPM 2.0** *Trusted Platform Module* available on every
recent Intel platform could be used as a virtual smartcard to securely store an
RSA or ECDSA private key. For details, refer to the TPM 2.0
[HOWTO](https://wiki.strongswan.org/projects/strongswan/wiki/TpmPlugin).

In a next step the command

    pki --req --type priv --in moonKey.pem \
              --dn ""C=CH, O=strongswan, CN=moon.strongswan.org \
              --san moon.strongswan.org --outform pem > moonReq.pem

creates a PKCS#10 certificate request that has to be signed by the CA.
Through the [multiple] use of the `--san` parameter any number of desired
*subjectAlternativeNames* can be added to the request. These can be of the
form

    --san sun.strongswan.org     # fully qualified host name
    --san carol@strongswan.org   # RFC822 user email address
    --san 192.168.0.1            # IPv4 address
    --san fec0::1                # IPv6 address

Based on the certificate request the CA issues a signed end entity certificate
with the following command

    pki --issue --cacert strongswanCert.pem --cakey strongswanKey.pem \
                --type pkcs10 --in moonReq.pem --serial 01 --lifetime 1826 \
                --outform pem > moonCert.pem

If the `--serial` parameter with a hexadecimal argument is omitted then a random
serial number is generated. Some third party VPN clients require that a VPN
gateway certificate contains the *TLS Server Authentication* Extended Key Usage
(EKU) flag which can be included with the following option

    --flag serverAuth

If you want to use the dynamic CRL fetching feature described in one of the
following sections then you may include one or several *crlDistributionPoints*
in your end entity certificates using the `--crl` parameter

    --crl  http://crl.strongswan.org/strongswan.crl
    --crl ""ldap://ldap.strongswan.org/cn=strongSwan Root CA, o=strongSwan,c=CH?certificateRevocationList""

The issued host certificate can be listed with

    pki --print --in moonCert.pem

    subject:  ""C=CH, O=strongSwan, CN=moon.strongswan.org""
    issuer:   ""C=CH, O=strongSwan, CN=strongSwan Root CA""
    validity:  not before May 19 10:28:19 2017, ok
               not after  May 19 10:28:19 2022, ok (expires in 1825 days)
    serial:    01
    altNames:  moon.strongswan.org
    flags:     serverAuth
    CRL URIs:  http://crl.strongswan.org/strongswan.crl
    authkeyId: 2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5d
    subjkeyId: 60:9d:de:30:a6:ca:b9:8e:87:bb:33:23:61:19:18:b8:c4:7e:23:8f
    pubkey:    ED25519 256 bits
    keyid:     39:1b:b3:c2:34:72:1a:01:08:40:ce:97:75:b8:be:ce:24:30:26:29
    subjkey:   60:9d:de:30:a6:ca:b9:8e:87:bb:33:23:61:19:18:b8:c4:7e:23:8f

Usually, a Windows, OSX, Android or iOS based VPN client needs its private key,
its host or user certificate and the CA certificate.  The most convenient way
to load this information is to put everything into a PKCS#12 container:

    openssl pkcs12 -export -inkey carolKey.pem \
                   -in carolCert.pem -name ""carol"" \
                   -certfile strongswanCert.pem -caname ""strongSwan Root CA"" \
                   -out carolCert.p12

The strongSwan **pki** tool currently is not able to create PKCS#12 containers
so that **openssl** must be used.


### Generating a CRL ###

An empty CRL that is signed by the CA can be generated with the command

    pki --signcrl --cacert strongswanCert.pem --cakey strongswanKey.pem \
                  --lifetime 30 > strongswan.crl

If you omit the `--lifetime` option then the default value of 15 days is used.
CRLs can either be uploaded to a HTTP or LDAP server or put in binary DER or
Base64 PEM format into the `/etc/swanctl/x509crl` directory from where they are
loaded into the **charon** daemon with the command

    swanctl --load-creds


### Revoking a Certificate ###

A specific end entity certificate is revoked with the command

    pki --signcrl --cacert strongswanCert.pem --cakey strongswanKey.pem \
                  --lifetime 30 --lastcrl strongswan.crl \
                  --reason key-compromise --cert moonCert.pem > new.crl

Instead of the certificate file (in our example moonCert.pem), the serial number
of the certificate to be revoked can be indicated using the `--serial`
parameter. The `pki --signcrl --help` command documents all possible revocation
reasons but the `--reason` parameter can also be omitted. The content of the new
CRL file can be listed with the command

    pki --print --type crl --in new.crl

    issuer:   ""C=CH, O=strongSwan, CN=strongSwan Root CA""
    update:    this on May 19 11:13:01 2017, ok
               next on Jun 18 11:13:01 2017, ok (expires in 29 days)
    serial:    02
    authKeyId: 2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5d
    1 revoked certificate:
      01: May 19 11:13:01 2017, key compromise


### Local Caching of CRLs ###

The `strongswan.conf` option

    charon {
        cache_crls = yes
    }

activates the local caching of CRLs that were dynamically fetched from an
HTTP or LDAP server.  Cached copies are stored in `/etc/swanctl/x509crl` using a
unique filename formed from the issuer's *subjectKeyIdentifier* and the
suffix `.crl`.

With the cached copy the CRL is immediately available after startup.  When the
local copy has become stale, an updated CRL is automatically fetched from one of
the defined CRL distribution points during the next IKEv2 authentication."
coreutils/coreutils,37867,1633,90,431,Organization,False,29052,6,365,161,False,upstream mirror,http://git.savannah.gnu.org/gitweb/?p…,0,6,0,2,13,1,2,1,16,1,1,10092,11,58,2367,1454,0,0,3,2,,,
gnab/rtl8812au,2059,1061,99,397,User,False,138,1,0,50,False,Realtek 802.11n WLAN Adapter Linux driver,,0,6,0,101,30,6,3,1,61,0,6,2399,5,5,12,6,0,0,29,,182,,"## Changes
2019-07-11: Updated to compile against kernel 5.2

## Realtek 802.11ac (rtl8812au)

This is a fork of the Realtek 802.11ac (rtl8812au) v4.2.2 (7502.20130507)
driver altered to build on Linux kernel version >= 3.10.

### Purpose

My D-Link DWA-171 wireless dual-band USB adapter needs the Realtek 8812au
driver to work under Linux.

The current rtl8812au version (per nov. 20th 2013) doesn't compile on Linux
kernels >= 3.10 due to a change in the proc entry API, specifically the
deprecation of the `create_proc_entry()` and `create_proc_read_entry()`
functions in favor of the new `proc_create()` function.

### Building

The Makefile is preconfigured to handle most x86/PC versions.  If you are compiling for something other than an intel x86 architecture, you need to first select the platform, e.g. for the Raspberry Pi, you need to set the I386 to n and the ARM_RPI to y:
```sh
...
CONFIG_PLATFORM_I386_PC = n
...
CONFIG_PLATFORM_ARM_RPI = y
```

There are many other platforms supported and some other advanced options, e.g. PCI instead of USB, but most won't be needed.

The driver is built by running `make`, and can be tested by loading the
built module using `insmod`:

```sh
$ make
$ sudo insmod 8812au.ko
```

After loading the module, a wireless network interface named __Realtek 802.11n WLAN Adapter__ should be available.

### Installing

Installing the driver is simply a matter of copying the built module
into the correct location and updating module dependencies using `depmod`:

```sh
$ sudo cp 8812au.ko /lib/modules/$(uname -r)/kernel/drivers/net/wireless
$ sudo depmod
```

The driver module should now be loaded automatically.

### DKMS

Automatically rebuilds and installs on kernel updates. DKMS is in official sources of Ubuntu, for installation do:

```sh
$ sudo apt-get install build-essential dkms 
```

The driver source must be copied to /usr/src/8812au-4.2.3

Then add it to DKMS:

```sh
$ sudo dkms add -m 8812au -v 4.2.3
$ sudo dkms build -m 8812au -v 4.2.3
$ sudo dkms install -m 8812au -v 4.2.3
```

Check with:
```sh
$ sudo dkms status
```
Automatically load at boot:
```sh
$ echo 8812au | sudo tee -a /etc/modules
```
Eventually remove from DKMS with:
```sh
$ sudo dkms remove -m 8812au -v 4.2.3 --all
```

### References

- D-Link DWA-171
  - [D-Link page](http://www.dlink.com/no/nb/home-solutions/connect/adapters/dwa-171-wireless-ac-dual-band-usb-adapter)
  - [wikidevi page](http://wikidevi.com/wiki/D-Link_DWA-171_rev_A1)"
FRRouting/frr,73876,1376,170,565,Organization,False,19989,20,50,223,False,The FRRouting Protocol Suite,https://frrouting.org/,17,65,7,372,1270,101,105,72,4859,61,770,4807,41,1092,88990,35809,0,0,6,25,,,"<p align=""center"">
<img src=""http://docs.frrouting.org/en/latest/_static/frr-icon.svg"" alt=""Icon"" width=""20%""/>
</p>

FRRouting
=========

FRR is free software that implements and manages various IPv4 and IPv6 routing
protocols. It runs on nearly all distributions of Linux and BSD as well as
Solaris and supports all modern CPU architectures.

FRR currently supports the following protocols:

* BGP
* OSPFv2
* OSPFv3
* RIPv1
* RIPv2
* RIPng
* IS-IS
* PIM-SM/MSDP
* LDP
* BFD
* Babel
* PBR
* OpenFabric
* VRRP
* EIGRP (alpha)
* NHRP (alpha)

Installation & Use
------------------

For source tarballs, see the
[releases page](https://github.com/FRRouting/frr/releases).

For Debian and its derivatives, use the APT repository at
[https://deb.frrouting.org/](https://deb.frrouting.org/).

Instructions on building and installing from source for supported platforms may
be found in the
[developer docs](http://docs.frrouting.org/projects/dev-guide/en/latest/building.html).

Once installed, please refer to the [user guide](http://docs.frrouting.org/)
for instructions on use.

Community
---------

The FRRouting email list server is located
[here](https://lists.frrouting.org/listinfo) and offers the following public
lists:

| Topic             | List                         |
|-------------------|------------------------------|
| Development       | dev@lists.frrouting.org      |
| Users & Operators | frog@lists.frrouting.org     |
| Announcements     | announce@lists.frrouting.org |

For chat, we currently use [Slack](https://frrouting.slack.com). You can join
by clicking the ""Slack"" link under the
[Participate](https://frrouting.org/#participate) section of our website.


Contributing
------------

FRR maintains [developer's documentation](http://docs.frrouting.org/projects/dev-guide/en/latest/index.html)
which contains the [project workflow](http://docs.frrouting.org/projects/dev-guide/en/latest/workflow.html)
and expectations for contributors. Some technical documentation on project
internals is also available.

We welcome and appreciate all contributions, no matter how small!


Security
--------

To report security issues, please use our security mailing list:

```
security [at] lists.frrouting.org
```"
yasm/yasm,27906,854,79,214,Organization,False,2159,23,21,21,False,Yasm Assembler mainline development tree,http://yasm.tortall.net/,0,7,0,85,18,1,1,17,25,4,1,6977,4,4,37,392,0,0,5,1,,,
OP-TEE/optee_os,22087,789,149,567,Organization,False,3797,10,41,125,False,Trusted side of the TEE,,0,10,1,22,1635,22,160,14,2259,14,260,,0,0,0,0,0,0,8,0,,,"# OP-TEE Trusted OS
This git contains source code for the secure side implementation of OP-TEE
project.

All official OP-TEE documentation has moved to http://optee.readthedocs.io.

// OP-TEE core maintainers"
TheOfficialFloW/VitaShell,52239,945,134,177,User,False,996,1,52,85,False,Multi-functional file manager for PS Vita,,0,14,0,66,337,8,3,0,163,0,12,1608,9,15,756,548,0,0,20,,1,,"# VitaShell

VitaShell is an alternative replacement of the PS Vita's LiveArea. It offers you a file manager, package installer, built-in FTP and much more.
This homebrew was an entry of the Revitalize PS Vita homebrew competition and won the first prize. HENkaku's molecularShell is also based on VitaShell.

## Changelog
See [CHANGELOG.md](CHANGELOG.md)

## How to use an USB flash drive as Memory Card on a PS TV
- Format your USB flash drive as exFAT or FAT32.
- Launch VitaShell and press `▲` in the `home` section.
- Select `Mount uma0:` and attach your USB flash drive. You can now copy stuff from/to your USB stick.
- Once `uma0:` is listed under the partitions, press `▲` again and choose `Mount USB ux0:`. This will copy important apps like VitaShell, molecularShell, and other files.
- Your USB flash drive is now acting as a Memory Card.
- To sync all your apps on your USB flash drive, press `▲` and choose `Refresh livearea`. This will NOT refresh PSP games.
- If you wish to revert the patch, press `▲` and select `Umount USB ux0:`.
- Note that this patch is only temporary and you need to redo the procedure everytime you launch your PS TV.

## Customization
You can customize those files:

| File                   | Note                        |
| ---------------------- | --------------------------- |
| colors.txt             | All colors adjustable       |
| archive_icon.png       | Archive icon                |
| audio_icon.png         | Audio icon                  |
| battery.png            | Battery border icon         |
| battery_bar_charge.png | Charging battery bar        |
| battery_bar_green.png  | Green battery bar           |
| battery_bar_red.png    | Red battery bar             |
| bg_audioplayer.png     | Background for audio player |
| bg_browser.png         | Background for file browser |
| bg_hexeditor.png       | Background for hex editor   |
| bg_photoviewer.png     | Background for photo viewer |
| bg_texteditor.png      | Background for text editor  |
| context.png            | Context menu image (Can be any size. Suggestion: It will look great if you add alpha channel to your image)  |
| context_more.png       | Context menu more image (Can be any size. Suggestion: It will look great if you add alpha channel to your image)  |
| cover.png              | Default album cover         |
| dialog.png             | Dialog menu image (Can be any size. This image file will be stretched by VitaShell to fit the dialog box. Suggestion: Don't use motives, as it will not look good with wrong proportion)  |
| fastforward.png        | Fastforward icon            |
| fastrewind.png         | Fastrewind icon             |
| file_icon.png          | File icon                   |
| folder_icon.png        | Folder icon                 |
| ftp.png                | FTP icon                    |
| image_icon.png         | Image icon                  |
| pause.png              | Pause icon                  |
| play.png               | Play icon                   |
| settings.png           | Settings icon               |
| sfo_icon.png           | SFO icon                    |
| text_icon.png          | Text icon                   |
| wallpaper.png          | Wallpaper                   |

**Theme setting:** VitaShell will load the theme that is set in `ux0:VitaShell/theme/theme.txt` (`THEME_NAME = ""YOUR_THEME_NAME""`)

**General info:** You don't need to have all these files in your custom theme, if one of them is missing, the default image file will be loaded instead.

**Dialog and context image:** If these files are not available, the colors `DIALOG_BG_COLOR` and `CONTEXT_MENU_COLOR` from `colors.txt` will be used instead.

## Multi-language
Put your language file at `ux0:VitaShell/language/x.txt`, where the file must be UTF-8 encoded and `x` is one of the language listed below:

- japanese
- english_us
- french
- spanish
- german
- italian
- dutch
- portuguese
- russian
- korean
- chinese_t
- chinese_s
- finnish
- swedish
- danish
- norwegian
- polish
- portuguese_br
- turkish

VitaShell does automatically load the language that matches to the current system language.
If your system language is for example french, it will load from `ux0:VitaShell/language/french.txt`.

Languages files are available in the `l10n` folder of this repository.

## Building
Install [vitasdk](https://github.com/vitasdk) and build VitaShell using:

```
mkdir build && cd build && cmake .. && make
```

## Credits
* Team Molecule for HENkaku
* xerpi for ftpvitalib and vita2dlib
* wololo for the Revitalize contest
* sakya for Lightmp3
* Everybody who contributed on vitasdk"
lanoxx/tilda,3821,904,40,139,User,False,592,14,34,45,False,A Gtk based drop down terminal for Linux and Unix,,0,14,2,90,229,9,11,9,94,1,1,2917,2,37,6629,6322,0,0,26,,37,,"# What is Tilda?

Tilda is a terminal emulator and can be compared with other popular terminal emulators such as
gnome-terminal (Gnome), Konsole (KDE), xterm and many others. The specialities of Tilda
are that it does not behave like a normal window but instead it can be pulled up and down from the top
of the screen with a special hotkey. Additionally Tilda is highly configurable. It is possible to configure the
hotkeys for keybindings, change the appearance and many options that affect the behavior of Tilda. The screen shots
below show some of the options that Tilda provides.

## Tilda Terminal
![Tilda window with search bar](images/tilda_terminal_with_search_bar.png)

Starting with version 1.3 Tilda comes with a search bar. Currently we support searching forwards and backwards
as well as options to search case sensitive and to use regular expressions. The search bar can be activated from
the context menu or with a configurable hotkey that defaults to `<Ctrl><Shift>F`
## General options
![General](images/tilda_general-16-9.png)

## Title and Command
![Title And Command](images/tilda_title_and_command-16-9.png)

## Appearance options
![Appearance](images/tilda_appearance-16-9.png)

## Colors
![Colors](images/tilda_colors-16-9.png)

## Keybindings options

Starting with version 1.4 Tilda's keybindings page switches to
a new list based layout that is easier to use.

![Keybindings](images/tilda_keybindings-16-9.png)

## Supported Platforms
Tilda currently works only on Xorg-based desktops. Previously that
meant that virtually all Linux distributions and some BSD's would be supported.
Recently however, some Linux distributions
(such as Ubuntu 17.10) have started to use Wayland as their
default display server. Tilda currently does not support Wayland and will not
work on such desktops. As a result it will fail to start.
Patches that introduce wayland support for tilda are very welcome. Please
look into the issue section or write me a mail if you would like to contribute
to tilda.

# Installing Tilda

Tilda should be packaged for your distribution if you are running Debian or any Debian derived distribution such as
Ubuntu or Linux Mint. For other distributions please check your package manager if it provides Tilda. You can also
install Tilda from source. For instructions to compile, install and optionally package tilda please see
**[HACKING.md](HACKING.md)**.

# Running Tilda

Once you have installed tilda, it should have automatically registered a menu entry in your desktops application menu.
Alternatively, you can run `tilda` from your command line.

The first time you run Tilda, it will create the default configuration file for
you, and show the configuration wizard. If you do not want to change any
settings, just press the ""OK"" button to accept the defaults.

The default keybindings to show and hide Tilda are as follows:

 * F1 - the first instance
 * F2 - the second instance
 * F3 - the third instance
 * ...

Other default keybindings are:

 * Shift-Ctrl-T - Open new tab
 * Ctrl-PageUp - Next tab
 * Ctrl-PageDown - Previous tab
 * Shift-Ctrl-W - Close current tab
 * Shift-Ctrl-Q - Exit Tilda

# Specifying your own keybinding to hide / show Tilda

We, the developers, have attempted to make the keybinding setting work with as
little trouble as possible. Some example keybindings follow:

| Keybinding String | Notes                                                             |
|-------------------|-------------------------------------------------------------------|
| `grave`           | This will use the tilde key. Many people want this.               |
| `~`               | This is the same as `<Shift>grave`                                |
| `space`           | This will use the spacebar to show / hide Tilda. NOT RECOMMENDED! |
| `<Shift><Ctrl>A`  | Press Shift, Control, and the ""a"" key at the same time            |
| `<Shift>space`    | Press Shift and the spacebar at the same time                     |
| `<Ctrl>z`         | Press Control and the ""z"" key at the same time                    |

That should cover most of the cases. If you want to use something else, it
probably follows the pattern, so give it a try. Alternatively, you can use the
configuration wizard, and press the ""Pull Down Terminal"" button on the
""Keybindings"" tab, then type the combination you want to use. Hopefully, Tilda
will be able to grab it for you. The `<Tab>` key cannot be grabbed, so at the moment
combinations such as `<Ctrl><Tab>` are not possible.

# Files that Tilda creates

Since approximately version 0.9.6 Tilda adheres to the XDG Base Directory Specification and
creates its files in the $XDG_CONFIG_HOME and $XDG_CACHE_HOME folders which normally default to
~/.config/ and ~/.cache/. Tilda will create a lock file in the cache directory
each time it starts, to keep track of how many instances are running:

    ~/.cache/tilda/locks/lock_$PID_$INSTANCE

Tilda will also create the config files in:

    ~/.config/tilda/config_$INSTANCE

where `$INSTANCE` is the number of how many instances are running and
`$PID` the process id. Tilda automatically migrates the files for you if it detects configuration file
at the old location `~/.tilda`.

# Getting more help / Questions and Comments

To get more help, you should first open a command prompt, and run `tilda
--help`. You can also take a look at the **[Wiki](https://github.com/lanoxx/tilda/wiki)**, or email
questions and comments to anyone listed in the [AUTHORS](AUTHORS) file.

# Reporting Bugs

We have done our best to make sure that Tilda is free from bugs, but it is
inevitable that we have missed some.

You may open bugs in the **[issue section](http://github.com/lanoxx/tilda/issues)** or email them to the
developers directly.

# Contributing to Tilda

Tilda is an open source project that lives by the help of volunteers
who fix bugs and implement new features in their spare time. Everybody is
welcome to join and help us to fix bugs or to implement new features.
Pull requests and patches are always welcome.

## Prerequisites

Tilda is written in C with the use of the libraries
**glib**, **GTK+**, **libconfuse** and **X11**. You should have a
good background in C and some experience with GTK and glib already. Some
areas of tilda will also require to know a little about X11 programming.

## What to work on?

If you already have the necessary background feel free to submit a patch
that fixes an issue or implements a new feature. If you are unsure if
your patch will be accepted then open an issue first, describe your issue
and ask if its likely that the patch gets accepted.

You can also look into the [TODO.md](TODO.md) file and see if there
is something there that you would like to do.

## Getting help

Feel free to mail the developers if you have questions about the
tilda source code or if you are unsure how something works."
pgbouncer/pgbouncer,2789,1073,65,260,Organization,False,1218,3,55,37,False,lightweight connection pooler for PostgreSQL,https://www.pgbouncer.org/,1,9,0,110,223,22,10,39,119,7,5,,0,0,0,0,0,0,2,0,,,"PgBouncer
=========

Lightweight connection pooler for PostgreSQL.

Homepage: <https://www.pgbouncer.org/>

Sources, bug tracking: <https://github.com/pgbouncer/pgbouncer>

Building
---------

PgBouncer depends on few things to get compiled:

* [GNU Make] 3.81+
* [Libevent] 2.0+
* [pkg-config]
* [OpenSSL] 1.0.1+ for TLS support
* (optional) [c-ares] as alternative to Libevent's evdns
* (optional) PAM libraries

[GNU Make]: https://www.gnu.org/software/make/
[Libevent]: http://libevent.org/
[pkg-config]: https://www.freedesktop.org/wiki/Software/pkg-config/
[OpenSSL]: https://www.openssl.org/
[c-ares]: http://c-ares.haxx.se/

When dependencies are installed just run:

    $ ./configure --prefix=/usr/local
    $ make
    $ make install

If you are building from Git, or are building for Windows, please see
separate build instructions below.

DNS lookup support
------------------

PgBouncer does host name lookups at connect time instead of just once
at configuration load time.  This requires an asynchronous DNS
implementation.  The following table shows supported backends and
their probing order:

| backend                    | parallel | EDNS0 (1) | /etc/hosts | SOA lookup (2) | note                                  |
|----------------------------|----------|-----------|------------|----------------|---------------------------------------|
| c-ares                     | yes      | yes       | yes        | yes            | IPv6+CNAME buggy in <=1.10            |
| udns                       | yes      | yes       | no         | yes            | IPv4 only                             |
| evdns, libevent 2.x        | yes      | no        | yes        | no             | does not check /etc/hosts updates     |
| getaddrinfo_a, glibc 2.9+  | yes      | yes (3)   | yes        | no             | N/A on non-glibc                      |
| getaddrinfo, libc          | no       | yes (3)   | yes        | no             | N/A on Windows, requires pthreads     |

1. EDNS0 is required to have more than 8 addresses behind one host name.
2. SOA lookup is needed to re-check host names on zone serial change.
3. To enable EDNS0, add `options edns0` to `/etc/resolv.conf`.

c-ares is the most fully-featured implementation and is recommended
for most uses and binary packaging (if a sufficiently new version is
available).  Libevent's built-in evdns is also suitable for many uses,
with the listed restrictions.  The other backends are mostly legacy
options at this point and don't receive much testing anymore.

By default, c-ares is used if it can be found.  Its use can be forced
with `configure --with-cares` or disabled with `--without-cares`.  If
c-ares is not used (not found or disabled), then specify `--with-udns`
to pick udns, else Libevent is used.  Specify `--disable-evdns` to
disable the use of Libevent's evdns and fall back to a libc-based
implementation.

PAM authentication
------------------

To enable PAM authentication, `./configure` has a flag `--with-pam`
(default value is no).  When compiled with PAM support, a new global
authentication type `pam` is available to validate users through PAM.

systemd integration
-------------------

To enable systemd integration, use the `configure` option
`--with-systemd`.  This allows using `Type=notify` service units as
well as socket activation.  See `etc/pgbouncer.service` and
`etc/pgbouncer.socket` for examples.

Building from Git
-----------------

Building PgBouncer from Git requires that you fetch the libusual
submodule and generate the header and configuration files before
you can run `configure`:

 $ git clone https://github.com/pgbouncer/pgbouncer.git
 $ cd pgbouncer
 $ git submodule init
 $ git submodule update
 $ ./autogen.sh
 $ ./configure ...
 $ make
 $ make install

Additional packages required: autoconf, automake, libtool, pandoc

Building on Windows
-------------------

The only supported build environment on Windows is MinGW.  Cygwin and
Visual $ANYTHING are not supported.

To build on MinGW, do the usual:

 $ ./configure ...
 $ make

If cross-compiling from Unix:

 $ ./configure --host=i586-mingw32msvc ...

Running on Windows
------------------

Running from the command line goes as usual, except that the `-d` (daemonize),
`-R` (reboot), and `-u` (switch user) switches will not work.

To run PgBouncer as a Windows service, you need to configure the
`service_name` parameter to set name for service.  Then:

 $ pgbouncer -regservice config.ini

To uninstall service:

 $ pgbouncer -unregservice config.ini

To use the Windows event log, set `syslog = 1` in the configuration file.
But before that you need to register `pgbevent.dll`:

 $ regsvr32 pgbevent.dll

To unregister it, do:

 $ regsvr32 /u pgbevent.dll"
emsec/ChameleonMini,9716,948,127,250,Organization,False,332,2,1,25,False,The ChameleonMini is a versatile contactless smartcard emulator compliant to NFC. The ChameleonMini was developed by https://kasper-oswald.de. The device is available at https://shop.kasper.it. For further information see the Getting Started Page https://rawgit.com/emsec/ChameleonMini/master/Doc/Doxygen/html/_page__getting_started.html or the Wi…,,10,7,0,48,148,10,4,11,56,2,4,2364,3,7,6871,2838,0,0,12,2,,,"Chameleon-Mini
==============
This is the official repository of ChameleonMini, a freely programmable, portable tool for NFC security analysis that can emulate and clone contactless cards, read RFID tags and sniff/log RF data. Thanks to over 1700 backers from our [Kickstarter project](https://www.kickstarter.com/projects/1980078555/chameleonmini-a-versatile-nfc-card-emulator-and-mo), the current Revision G has been realized by Kasper & Oswald GmbH.

The ChameleonMini RevG is now also available via the Kasper & Oswald [Webshop](https://shop.kasper.it/). Thank you for supporting the project!

IMPORTANT: Third-party Clones
------------------
We are aware of various third-party ChameleonMini clones or modified variants that are available on the Internet. Warning: We have evidence that some of these devices are defective or suffer from reading problems et cetera. Please understand that we cannot give support for these non-official devices, as we have no schematics / layout or other information, nor do we know the manufacturers. In case of problems, please contact the manufacturers of your device directly.

Note to the manufacturers: Some of the third-party ChameleonMini are violating the ChameleonMini license, please obey the license (see LICENSE.txt)!

First Steps
-----------
To upgrade the firmware of your ChameleonMini, please visit the [Getting Started page](https://cdn.statically.io/gh/emsec/ChameleonMini/master/Doc/Doxygen/html/_page__getting_started.html) from the [doxygen documentation](https://cdn.statically.io/gh/emsec/ChameleonMini/master/Doc/Doxygen/html/index.html).

Supported Cards and Codecs
--------------------------
See [here](https://github.com/emsec/ChameleonMini/wiki/Supported-Cards-and--Codecs).


Questions
---------
If you have any questions, please visit the [Issues page](https://github.com/emsec/ChameleonMini/issues) and ask your questions there so everyone benefits from the answer.

**Please note that we cannot give any support if you have issues with the RevE-rebooted hardware or any GUI. Please use this issues page only for problems with the RevG hardware as distributed by Kasper & Oswald and for problems with the firmware from this page.**

External Contributions
----------------------
We appreciate every contribution to the project. Have a look at the [External Contributions page](https://github.com/emsec/ChameleonMini/wiki/External-Contributions).

Repository Structure
--------------------
The code repository contains
* Doc: A doxygen documentation
* Drivers: Chameleon drivers for Windows and Linux
* Dumps: Dumps of different smartcards
* Hardware: The layout and schematics of the PCB
* Firmware: The complete firmware including a modified Atmel DFU bootloader and LUFA
* Software: Contains a python tool for an easy configuration (and more) of the ChameleonMini, Note that this is currently under construction
* RevE: Contains the whole contents of the discontinued RevE repository.
* RevE-light: Contains our development files for the RevE-light - **WARNING:** currently not supported / not functional"
DuyTrandeLion/BLE_Bike_mbed_os,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
netsniff-ng/netsniff-ng,3063,834,100,205,Organization,False,1328,2,22,40,False,A Swiss army knife for your daily Linux network plumbing.,http://netsniff-ng.org,7,7,0,40,92,2,1,4,85,0,5,2651,4,11,28,12,0,0,2,0,,,
corosync/corosync,21882,812,96,172,Organization,False,4144,37,84,66,False,The Corosync Cluster Engine,http://corosync.github.com/corosync,0,3,0,33,153,8,6,11,362,4,26,5465,5,27,1090,750,0,0,5,3,,,
antirez/disque,2493,7668,444,528,User,False,602,1,1,53,False,Disque is a distributed message broker,,0,11,1,42,64,0,0,18,92,1,0,2049,0,0,0,0,0,0,63,,14,,"[![Build Status](https://travis-ci.org/antirez/disque.svg)](https://travis-ci.org/antirez/disque)

Disque, an in-memory, distributed job queue
===

Disque is an ongoing experiment to build a distributed, in-memory, message
broker.
Its goal is to capture the essence of the ""Redis as a jobs queue"" use case,
which is usually implemented using blocking list operations, and move
it into an ad-hoc, self-contained, scalable, and fault tolerant design, with
simple to understand properties and guarantees, but still resembling Redis
in terms of simplicity, performance, and implementation as a C non-blocking
networked server.

Currently (2 Jan 2016) the project is in release candidate state. People are
encouraged to start evaluating it and report bugs and experiences.

**WARNING: This is beta code and may not be suitable for production usage. The API is considered to be stable if not for details that may change in the next release candidates, however it's new code, so handle with care!**

What is a message queue?
---

*Hint: skip this section if you are familiar with message queues.*

You know how humans use text messages to communicate, right? I could write
my wife ""please get the milk at the store"", and she maybe will reply ""Ok message
received, I'll get two bottles on my way home"".

A message queue is the same as human text messages, but for computer programs.
For example a web application, when an user subscribes, may send another
process, that handles sending emails, ""please send the confirmation email
to tom@example.net"".

Message systems like Disque allow communication between processes using
different queues. So a process can send a message to a queue with a given
name, and only processes which fetch messages from this queue will return those
messages. Moreover, multiple processes can listen for messages in a given
queue, and multiple processes can send messages to the same queue.

The important part of a message queue is to be able to provide guarantees so
that messages are eventually delivered even in the face of failures. So even
if in theory implementing a message queue is very easy, to write a very
robust and scalable one is harder than it may appear.

Give me the details!
---

Disque is a distributed and fault tolerant message broker, so it works as a
middle layer among processes that want to exchange messages.

Producers add messages that are served to consumers.
Since message queues are often used in order to process delayed
jobs, Disque often uses the term ""job"" in the API and in the documentation,
however jobs are actually just messages in the form of strings, so Disque
can be used for other use cases. In this documentation ""jobs"" and ""messages""
are used in an interchangeable way.

Job queues with a producer-consumer model are pretty common, so the devil is
in the details. A few details about Disque are:

Disque is a **synchronously replicated job queue**. By default when a new job is added, it is replicated to W nodes before the client gets an acknowledgement about the job being added. W-1 nodes can fail and the message will still be delivered.

Disque supports both **at-least-once and at-most-once** delivery semantics. At-least-once delivery semantics is where most effort was spent in the design and implementation, while at-most-once semantics is a trivial result of using a *retry time* set to 0 (which means, never re-queue the message again) and a replication factor of 1 for the message (not strictly needed, but it is useless to have multiple copies of a message around if it will be delivered at most one time). You can have, at the same time, both at-least-once and at-most-once jobs in the same queues and nodes, since this is a per message setting.

Disque at-least-once delivery is designed to **approximate single delivery** when possible, even during certain kinds of failures. This means that while Disque can only guarantee a number of deliveries equal or greater to one, it will try hard to avoid multiple deliveries whenever possible.

Disque is a distributed system where **all nodes have the same role** (aka, it is multi-master). Producers and consumers can attach to whatever node they like, and there is no need for producers and consumers of the same queue to stay connected to the same node. Nodes will automatically exchange messages based on load and client requests.

Disque is Available (it is an eventually consistent AP system in CAP terms): producers and consumers can make progress as long as a single node is reachable.

Disque supports **optional asynchronous commands** that are low latency for the client but provide less guarantees. For example a producer can add a job to a queue with a replication factor of 3, but may want to run away before knowing if the contacted node was really able to replicate it to the specified number of nodes or not. The node will replicate the message in the background in a best effort way.

Disque **automatically re-queues messages that are not acknowledged** as already processed by consumers, after a message-specific retry time. There is no need for consumers to re-queue a message if it was not processed.

Disque uses **explicit acknowledges** in order for a consumer to signal a message as delivered (or, using a different terminology, to signal a job as already processed).

Disque queues only provides **best effort ordering**. Each queue sorts messages based on the job creation time, which is obtained using the *wall clock* of the local node where the message was created (plus an incremental counter for messages created in the same millisecond), so messages created in the same node are normally delivered in the same order they were created. This is not causal ordering since correct ordering is violated in different cases: when messages are re-issued because they are not acknowledged, because of nodes local clock drifts, and when messages are moved to other nodes for load balancing and federation (in this case you end with queues having jobs originated in different nodes with different wall clocks). However all this also means that normally messages are not delivered in random order and usually messages created first are delivered first.

Note that since Disque does not provide strict FIFO semantics, technically speaking it should not be called a *message queue*, and it could better identified as a message broker. However I believe that at this point in the IT industry a *message queue* is often more lightly used to identify a generic broker that may or may not be able to guarantee order in all cases. Given that we document the semantics very clearly, I grant myself the right to call Disque a message queue anyway.

Disque provides the user with fine-grained control for each job **using three time related parameters**, and one replication parameter. For each job, the user can control:

1. The replication factor (how many nodes have a copy).
2. The delay time (the min time Disque will wait before putting the message in a queue, making the message deliverable).
3. The retry time (how much time should elapse since the last time the job was queued and without an acknowledge about the job delivery, before the job is re-queued for delivery).
4. The expire time (how much time should elapse for the job to be deleted regardless of whether it was successfully delivered, i.e. acknowledged, or not).

Finally, Disque supports optional disk persistence, which is not enabled by default, but that can be handy in single data center setups and during restarts.

Other minor features are:

* Ability to block queues.
* A few statistics about queue activity.
* Stateless iterators for queues and jobs.
* Commands to control the visibility of single jobs.
* Easy resize of the cluster (adding nodes is trivial).
* Graceful removal of nodes without losing job replicas.

ACKs and retries
---

Disque's implementation of *at-least-once* delivery semantics is designed in
order to avoid multiple delivery during certain classes of failures. It is not able to guarantee that no multiple deliveries will occur. However there are many *at-least-once* workloads where duplicated deliveries are acceptable (or explicitly handled), but not desirable either. A trivial example is sending emails to users (it is not terrible if an user gets a duplicated email, but is important to avoid it when possible), or doing idempotent operations that are expensive (all the times where it is critical for performance to avoid multiple deliveries).

In order to avoid multiple deliveries when possible, Disque uses client ACKs. When a consumer processes a message correctly, it should acknowledge this fact to Disque. ACKs are replicated to multiple nodes, and are garbage collected as soon as the system believes it is unlikely that more nodes in the cluster have the job (the ACK refers to) still active. Under memory pressure or under certain failure scenarios, ACKs are eventually discarded.

More explicitly:

1. A job is replicated to multiple nodes, but usually only *queued* in a single node. There is a difference between having a job in memory, and queueing it for delivery.
2. Nodes having a copy of a message, if a certain amount of time has elapsed without getting the ACK for the message, will re-queue it. Nodes will run a best-effort protocol to avoid re-queueing the message multiple times.
3. ACKs are replicated and garbage collected across the cluster so that eventually processed messages are evicted (this happens ASAP if there are no failures nor network partitions).

For example, if a node having a copy of a job gets partitioned away during the time the job gets acknowledged by the consumer, it is likely that when it returns (in a reasonable amount of time, that is, before the retry time is reached) it will be informed about the ACK and will avoid to re-queue the message. Similarly, jobs can be acknowledged during a partition to just a single available node, and when the partition heals the ACK will be propagated to other nodes that may still have a copy of the message.

So an ACK is just a **proof of delivery** that is replicated and retained for
some time in order to make multiple deliveries less likely to happen in practice.

As already mentioned, in order to control replication and retries, a Disque job has the following associated properties: number of replicas, delay, retry and expire.

If a job has a retry time set to 0, it will get queued exactly once (and in this case a replication factor greater than 1 is useless, and signaled as an error to the user), so it will get delivered either a single time or will never get delivered. While jobs can be persisted on disk for safety, queues aren't, so this behavior is guaranteed even when nodes restart after a crash, whatever the persistence configuration is. However when nodes are manually restarted by the sysadmin, for example for upgrades, queues are persisted correctly and reloaded at startup, since the store/load operation is atomic in this case, and there are no race conditions possible (it is not possible that a job was delivered to a client and is persisted on disk as queued at the same time).

Fast acknowledges
---

Disque supports a faster way to acknowledge processed messages, via the
`FASTACK` command. The normal acknowledge is very expensive from the point of
view of messages exchanged between nodes, this is what happens during a normal
acknowledge:

1. The client sends ACKJOB to one node.
2. The node sends a SETACK message to everybody it believes to have a copy.
3. The receivers of SETACK reply with GOTACK to confirm.
4. The node finally sends DELJOB to all the nodes.

*Note: actual garbage collection is more complex in case of failures and is explained in the state machine later. The above is what happens 99% of times.*

If a message is replicated to 3 nodes, acknowledging requires 1+2+2+2 messages,
for the sake of retaining the ack if some nodes may not be reached when the
message is acknowledged. This makes the probability of multiple deliveries of
this message less likely.

However the alternative **fast ack**, while less reliable, is much faster
and invovles exchanging less messages. This is how a fast acknowledge works:

1. The client sends `FASTACK` to one node.
2. The node evicts the job and sends a best effort DELJOB to all the nodes that may have a copy, or to all the cluster if the node was not aware of the job.

If during a fast acknowledge a node having a copy of the message is not
reachable, for example because of a network partition, the node will deliver
the message again, since it has a non-acknowledged copy of the message and
there is nobody able to inform it the message has been acknowledged when the
partition heals.

If the network you are using is pretty reliable, and you are very concerned with
performance, and multiple deliveries in the context of your applications are
a non issue, then `FASTACK` is probably the way to go.


Dead letter queue
---

Many message queues implement a feature called *dead letter queue*. It is
a special queue used in order to accumulate messages that cannot be processed
for some reason. Common causes could be:

1. The message was delivered too many times but never correctly processed.
2. The message time-to-live reached zero before it was processed.
3. Some worker explicitly asked the system to flag the message as having issues.

The idea is that the administrator of the system checks (usually via automatic
systems) if there is something in the dead letter queue in order to understand
if there is some software error or other kind of error preventing messages
from being processed as expected.

Since Disque is an in-memory system, the message time-to-live is an important
property. When it is reached, we want messages to go away, since the TTL should
be chosen so that after such a time it is no longer meaningful to process
the message. In such a system, to use memory and create a queue in response
to an error or to messages timing out looks like a non optimal idea. Moreover,
due to the distributed nature of Disque, dead letters could end up spawning
multiple nodes and having duplicated entries in them.

So Disque uses a different approach. Each node message representation has
two counters: a **nacks counter** and an **additional deliveries** counter.
The counters are not consistent among nodes having a copy of the same message,
they are just best effort counters that may not increment in some node during
network partitions.

The idea of these two counters is that one is incremented every time a worker
uses the `NACK` command to tell the queue the message was not processed correctly
and should be put back on the queue ASAP. The other is incremented for every other condition (different than the `NACK` call) that requires a message to be put back
on the queue again. This includes messages that get lost and are enqueued again
or messages that are enqueued on one side of the partition since the message
was processed on the other side and so forth.

Using the `GETJOB` command with the `WITHCOUNTERS` option, or using the
`SHOW` command to inspect a job, it is possible to retrieve these two counters
together with the other job information, so if a worker, before processing
a message, sees the counters have values over some application-defined limit, it
can notify operations people in multiple ways:

1. It may send an email.
2. Set a flag in a monitoring system.
3. Put the message in a special queue (simulating the dead letter feature).
4. Attempt to process the message and report the stack trace of the error if any.

Basically the exact handling of the feature is up to the application using
Disque. Note that the counters don't need to be consistent in the face of
failures or network partitions: the idea is that eventually if a message has
issues the counters will get incremented enough times to reach the limit
selected by the application as a warning threshold.

The reason for having two distinct counters is that applications may want
to handle the case of explicit negative acknowledges via `NACK` differently
than multiple deliveries because of timeouts or messages getting lost.

Disque and disk persistence
---

Disque can be operated in-memory only, using synchronous replication as a
durability guarantee, or can be operated using the Append Only File where
jobs creations and evictions are logged on disk (with configurable fsync
policies) and reloaded at restart.

AOF is recommended especially if you run in a single availability zone
where a mass reboot of all your nodes is possible.

Normally Disque only reloads job data in memory, without populating queues,
since unacknowledged jobs are requeued eventually. Moreover, reloading
queue data is not safe in the case of at-most-once jobs having the retry value
set to 0. However a special option is provided in order to reload the full
state from the AOF. This is used together with an option that allows shutting
down the server just after the AOF is generated from scratch, in order to
make it safe even to reload jobs with retry set to 0, since the AOF is generated
while the server no longer accepts commands from clients, so no race condition
is possible.

Even when running memory-only, Disque is able to dump its memory on disk and reload from disk on controlled restarts, for example in order to upgrade the software.

This is how to perform a controlled restart, that works whether AOF is enabled
or not:

1. CONFIG SET aof-enqueue-jobs-once yes
2. CONFIG REWRITE
3. SHUTDOWN REWRITE-AOF

At this point we have a freshly generated AOF on disk, and the server is
configured in order to load the full state only at the next restart
(`aof-enqueue-jobs-once` is automatically turned off after the restart).

We can just restart the server with the new software, or in a new server, and
it will restart with the full state. Note that `aof-enqueue-jobs-once`
implies loading the AOF even if AOF support is switched off, so there is
no need to enable AOF just for the upgrade of an in-memory only server.

Job IDs
---

Disque jobs are uniquely identified by an ID like the following:

    D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1

Job IDs are composed of exactly 40 characters and start with the prefix `D-`.

We can split an ID into multiple parts:

    D- | dcb833cf | 8YL1NT17e9+wsA/09NqxscQI | 05a1

1. `D-` is the prefix.
2. `dcb833cf` is the first 8 bytes of the node ID where the message was generated.
3. `8YL1NT17e9+wsA/09NqxscQI` is the 144 bit ID pseudo-random part encoded in base64.
4. `05a1` is the Job TTL in minutes. Because of it, message IDs can be expired safely even without having the job representation.

IDs are returned by ADDJOB when a job is successfully created, are part of
the GETJOB output, and are used in order to acknowledge that a job was
correctly processed by a worker.

Part of the node ID is included in the message so that a worker processing
messages for a given queue can easily guess what are the nodes where jobs
are created, and move directly to these nodes to increase efficiency instead
of listening for messages in a node that will require to fetch messages from
other nodes.

Only 32 bits of the original node ID is included in the message, however
in a cluster with 100 Disque nodes, the probability of two nodes having
identical 32 bit ID prefixes is given by the birthday paradox:

    P(100,2^32) = .000001164

In case of collisions, the workers may just make a non-efficient choice.

Collisions in the 144 bits random part are believed to be impossible,
since it is computed as follows.

    144 bit ID = HIGH_144_BITS_OF_SHA1(seed || counter)

Where:

* **seed** is a seed generated via `/dev/urandom` at startup.
* **counter** is a 64 bit counter incremented at every ID generation.

So there are 22300745198530623141535718272648361505980416 possible IDs,
selected in a uniform way. While the probability of a collision is non-zero
mathematically, in practice each ID can be regarded as unique.

The encoded TTL in minutes has a special property: it is always even for
at most once jobs (job retry value set to 0), and is always odd otherwise.
This changes the encoded TTL precision to 2 minutes, but allows to tell
if a Job ID is about a job with deliveries guarantees or not.
Note that this fact does not mean that Disque jobs TTLs have a precision of
two minutes. The TTL field is only used to expire job IDs of jobs a given
node does not actually have a copy, search ""dummy ACK"" in this documentation
for more information.

Setup
===

To play with Disque please do the following:

1. Compile Disque - if you can compile Redis, you can compile Disque, it's the usual ""no external deps"" thing. Just type `make`. Binaries (`disque` and `disque-server`) will end up in the `src` directory.
2. Run a few Disque nodes on different ports. Create different `disque.conf` files following the example `disque.conf` in the source distribution.
3. After you have them running, you need to join the cluster. Just select a random node among the nodes you are running, and send the command `CLUSTER MEET <ip> <port>` for every other node in the cluster.

**Please note that you need to open two TCP ports on each node**, the base port of the Disque instance, for example 7711, plus the cluster bus port, which is always at a fixed offset, obtained summing 10000 to the base port, so in the above example, you need to open both 7711 and 17711. Disque uses the base port to communicate with clients and the cluster bus port to communicate with other Disque processes.

To run a node, just call `./disque-server`.

For example, if you are running three Disque servers in port 7711, 7712, 7713, in order to join the cluster you should use the `disque` command line tool and run the following commands:

    ./disque -p 7711 cluster meet 127.0.0.1 7712
    ./disque -p 7711 cluster meet 127.0.0.1 7713

Your cluster should now be ready. You can try to add a job and fetch it back
in order to test if everything is working:

    ./disque -p 7711
    127.0.0.1:7711> ADDJOB queue body 0
    D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1
    127.0.0.1:7711> GETJOB FROM queue
    1) 1) ""queue""
       2) ""D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1""
       3) ""body""

Remember that you can add and get jobs from different nodes as Disque
is multi master. Also remember that you need to acknowledge jobs otherwise
they'll never go away from the server memory (unless the time-to-live is
reached).

Main API
===

The Disque API is composed of a small set of commands, since the system solves a
single very specific problem. The three main commands are:

#### `ADDJOB queue_name job <ms-timeout> [REPLICATE <count>] [DELAY <sec>] [RETRY <sec>] [TTL <sec>] [MAXLEN <count>] [ASYNC]`

Adds a job to the specified queue. Arguments are as follows:

* *queue_name* is the name of the queue, any string, basically. You don't need to create queues, if a queue does not exist, it gets created automatically. If one has no more jobs, it gets removed.
* *job* is a string representing the job. Disque is job meaning agnostic, for it a job is just a message to deliver. Job max size is 4GB.
* *ms-timeout* is the command timeout in milliseconds. If no ASYNC is specified, and the replication level specified is not reached in the specified number of milliseconds, the command returns with an error, and the node does a best-effort cleanup, that is, it will try to delete copies of the job across the cluster. However the job may still be delivered later. Note that the actual timeout resolution is 1/10 of second or worse with the default server hz.
* *REPLICATE count* is the number of nodes the job should be replicated to.
* *DELAY sec* is the number of seconds that should elapse before the job is queued by any server. By default there is no delay.
* *RETRY sec* period after which, if no ACK is received, the job is put into the queue again for delivery. If RETRY is 0, the job has at-most-once delivery semantics. The default retry time is 5 minutes, with the exception of jobs having a TTL so small that 10% of TTL is less than 5 minutes. In this case the default RETRY is set to TTL/10 (with a minimum value of 1 second).
* *TTL sec* is the max job life in seconds. After this time, the job is deleted even if it was not successfully delivered. If not specified, the default TTL is one day.
* *MAXLEN count* specifies that if there are already *count* messages queued for the specified queue name, the message is refused and an error reported to the client.
* *ASYNC* asks the server to let the command return ASAP and replicate the job to other nodes in the background. The job gets queued ASAP, while normally the job is put into the queue only when the client gets a positive reply.

The command returns the Job ID of the added job, assuming ASYNC is specified, or if the job was replicated correctly to the specified number of nodes. Otherwise an error is returned.

#### `GETJOB [NOHANG] [TIMEOUT <ms-timeout>] [COUNT <count>] [WITHCOUNTERS] FROM queue1 queue2 ... queueN`

Return jobs available in one of the specified queues, or return NULL
if the timeout is reached. A single job per call is returned unless a count greater than 1 is specified. Jobs are returned as a three-element array containing the queue name, the Job ID, and the job body itself. If jobs are available in multiple queues, queues are processed left to right.

If there are no jobs for the specified queues, the command blocks, and messages are exchanged with other nodes, in order to move messages about these queues to this node, so that the client can be served.

Options:

* **NOHANG**: Ask the command to not block even if there are no jobs in all the specified queues. This way the caller can just check if there are available jobs without blocking at all.
* **WITHCOUNTERS**: Return the best-effort count of NACKs (negative acknowledges) received by this job, and the number of additional deliveries performed for this job. See the *Dead Letters* section for more information.

#### `ACKJOB jobid1 jobid2 ... jobidN`

Acknowledges the execution of one or more jobs via job IDs. The node receiving the ACK will replicate it to multiple nodes and will try to garbage collect both the job and the ACKs from the cluster so that memory can be freed.

A node receiving an ACKJOB command about a job ID it does not know will create
a special empty job, with the state set to ""acknowledged"", called a ""dummy ACK"".
The dummy ACK is used in order to retain the acknolwedge during a netsplit if
the ACKJOB is sent to a node that does not have a copy of the job. When the
partition heals, job garbage collection will be attempted.

However, since the job ID encodes information about the job being an ""at-most-
once"" or an ""at-least-once"" job, the dummy ACK is only created for at-least-
once jobs.

#### `FASTACK jobid1 jobid2 ... jobidN`

Performs a best-effort cluster-wide deletion of the specified job IDs. When the
network is well connected and there are no node failures, this is equivalent to
`ACKJOB` but much faster (due to less messages being exchanged), however during
failures it is more likely that fast acknowledges will result in multiple
deliveries of the same messages.

#### `WORKING jobid`

Claims to be still working with the specified job, and asks Disque to postpone
the next time it will deliver the job again. The next delivery is postponed
for the job retry time, however the command works in a **best effort** way
since there is no way to guarantee during failures that another node in a
different network partition won't perform a delivery of the same job.

Another limitation of the `WORKING` command is that it cannot be sent to
nodes not knowing about this particular job. In such a case the command replies
with a `NOJOB` error. Similarly, if the job is already acknowledged an error
is returned.

Note that the `WORKING` command is refused by Disque nodes if 50% of the job
time to live has already elapsed. This limitation makes Disque safer since
usually the *retry* time is much smaller than the time-to-live of a job, so
it can't happen that a set of broken workers monopolize a job with `WORKING`
and never process it. After 50% of the TTL has elapsed, the job will be delivered
to other workers anyway.

Note that `WORKING` returns the number of seconds you (likely) postponed the
message visibility for other workers (the command basically returns the
*retry* time of the job), so the worker should make sure to send the next
`WORKING` command before this time elapses. Moreover, a worker that may want
to use this interface may fetch the retry value with the `SHOW` command
when starting to process a message, or may simply send a `WORKING` command
ASAP, like in the following example (in pseudo code):

    retry = WORKING(jobid)
    RESET timer
    WHILE ... work with the job still not finished ...
        IF timer reached 80% of the retry time
            WORKING(jobid)
            RESET timer
        END
    END

#### `NACK <job-id> ... <job-id>`

The `NACK` command tells Disque to put the job back in the queue ASAP. It
is very similar to `ENQUEUE` but it increments the job `nacks` counter
instead of the `additional-deliveries` counter. The command should be used
when the worker was not able to process a message and wants the message to
be put back into the queue in order to be processed again.

Other commands
===

#### `INFO`

Generic server information / stats.

#### `HELLO`

Returns hello format version, this node ID, all the nodes IDs, IP addresses,
ports, and priority (lower is better, means a node is more available).
Clients should use this as a handshake command when connecting with a
Disque node.

#### `QLEN <queue-name>`

Return the length of the queue.

#### `QSTAT <queue-name>`

Show information about a queue as an array of key-value pairs.
Below is an example of the output, however, implementations should not rely
on the order of the fields nor on the existence of the fields listed.
They may be (unlikely) removed or more can be (likely) added
in the future.

If a queue does not exist, NULL is returned. Note that queues are
automatically evicted after some time if empty and without clients blocked
waiting for jobs, even if there are active jobs for the queue. So the
non existence of a queue does not mean there are not jobs in the node
or in the whole cluster about this queue. The queue will be immediately
created again when needed to serve requests.

Example output:

```
QSTAT foo
 1) ""name""
 2) ""foo""
 3) ""len""
 4) (integer) 56520
 5) ""age""
 6) (integer) 601
 7) ""idle""
 8) (integer) 3
 9) ""blocked""
10) (integer) 50
11) ""import-from""
12) 1) ""dcb833cf8f42fbb7924d92335ff6d67d3cea6e3d""
    2) ""4377bdf656040a18d8caf4d9f409746f1f9e6396""
13) ""import-rate""
14) (integer) 19243
15) ""jobs-in""
16) (integer) 3462847
17) ""jobs-out""
18) (integer) 3389522
19) ""pause""
20) ""none""
```

Most fields should be obvious. The `import-from` field shows a list of node
IDs this node is importing jobs from, for this queue, in order to serve
clients requests. The `import-rate` is the instantaneous amount of jos/sec
we import in order to handle our outgoing traffic (GETJOB commands).
`blocked` is the number of clients blocked on this queue right now.
`age` and `idle` are reported in seconds. The `jobs-in` and `-out` counters are
incremented every time a job is enqueued or dequeued for any reason.

#### `QPEEK <queue-name> <count>`

Return, without consuming from the queue, *count* jobs. If *count* is positive
the specified number of jobs are returned from the oldest to the newest
(in the same best-effort FIFO order as GETJOB). If *count* is negative the
commands changes behavior and shows the *count* newest jobs, from the newest
from the oldest.

#### `ENQUEUE <job-id> ... <job-id>`

Queue jobs if not already queued.

#### `DEQUEUE <job-id> ... <job-id>`

Remove the job from the queue.

#### `DELJOB <job-id> ... <job-id>`

Completely delete a job from a node.
Note that this is similar to `FASTACK`, but limited to a single node since
no `DELJOB` cluster bus message is sent to other nodes.

#### `SHOW <job-id>`

Describe the job.

#### `QSCAN [COUNT <count>] [BUSYLOOP] [MINLEN <len>] [MAXLEN <len>] [IMPORTRATE <rate>]`

The command provides an interface to iterate all the existing queues in
the local node, providing a cursor in the form of an integer that is passed
to the next command invocation. During the first call, the cursor must be 0,
in the next calls the cursor returned in the previous call is used in the
next. The iterator guarantees to return all the elements but may return
duplicated elements.

Options:

* `COUNT <count>` A hint about how much work to do per iteration.
* `BUSYLOOP` Block and return all the elements in a busy loop.
* `MINLEN <count>` Don't return elements with less than `count` jobs queued.
* `MAXLEN <count>` Don't return elements with more than `count` jobs queued.
* `IMPORTRATE <rate>` Only return elements with a job import rate (from other nodes) `>=` `rate`.

The cursor argument can be in any place, the first non matching option
that has valid cursor form of an unsigned number will be sensed as a valid
cursor.

#### `JSCAN [<cursor>] [COUNT <count>] [BUSYLOOP] [QUEUE <queue>] [STATE <state1> STATE <state2> ... STATE <stateN>] [REPLY all|id]`

The command provides an interface to iterate all the existing jobs in
the local node, providing a cursor in the form of an integer that is passed
to the next command invocation. During the first call the cursor must be 0,
in the next calls the cursor returned in the previous call is used in the
next. The iterator guarantees to return all the elements but may return
duplicated elements.

Options:

* `COUNT <count>` A hint about how much work to do per iteration.
* `BUSYLOOP` Block and return all the elements in a busy loop.
* `QUEUE <queue>` Return only jobs in the specified queue.
* `STATE <state>` Return jobs in the specified state. Can be used multiple times for a logical OR.
* `REPLY <type>` Job reply type. Type can be `all` or `id`. Default is to report just the job ID. If `all` is specified the full job state is returned like for the SHOW command.

The cursor argument can be in any place, the first non matching option
that has valid cursor form of an unsigned number will be sensed as a valid
cursor.

#### `PAUSE <queue-name> option1 [option2 ... optionN]`

Control the paused state of a queue, possibly broadcasting the command to
other nodes in the cluster. Disque queues can be paused in both directions,
input and output, or both. Pausing a queue makes it unavailable for input
or output operations. Specifically:

A queue paused in input will have changed behavior in the following ways:

1. ADDJOB returns a `-PAUSED` error for queues paused in input.
2. The node where the queue is paused, no longer accepts to replicate jobs for this queue when requested by other nodes. Since ADDJOB by default uses synchronous replication, it means that if the queue is paused in enough nodes, adding jobs with a specified level of replication may fail. In general the node where the queue is paused will not create new jobs in the local node about this queue.
3. The job no longer accepts ENQUEUE messages from other nodes. Those messages are usually used by nodes in out of memory conditions that replicate jobs externally (not holding a copy), in order to put the job in the queue of some random node, among the nodes having a copy of a job.
4. Active jobs that reach their retry time, are not put back into the queue. Instead their retry timer is updated and the node will try again later.

Basically a queue paused in input never creates new jobs for this queue, and never puts active jobs (jobs for which the node has a copy but are not currently queued) back in the queue, for all the time the queue is paused.

A queue paused in output instead will behave in the following way:

1. GETJOB will block even if there are jobs available in the specified queue, instead of serving the jobs. But GETJOB will unblock if the queue output pause is cleared later.
2. The node will not provide jobs to other nodes in the context of node federation, for paused queues.

So a queue paused in output will stop acting as a source of messages for both
local and non local clients.

The paused state can be set for each queue using the PAUSE command followed
by options to specify how to change the paused state. Possible options are:

* **in**: pause the queue in input.
* **out**: pause the queue in output.
* **all**: pause the queue in input and output (same as specifying both the **in** and **out** options).
* **none**: clear the paused state in input and output.
* **state**: just report the current queue state.
* **bcast**: send a PAUSE command to all the reachable nodes of the cluster to set the same queue in the other nodes to the same state.

The command always returns the state of the queue after the execution of the specified options, so the return value is one of **in**, **out**, **all**, **none**.

Queues paused in input or output are never evicted to reclaim memory, even if
they are empty and inactive for a long time, since otherwise the paused state
would be forgotten.

For example, in order to block output for the queue `myqueue` in all the
currently reachable nodes, the following command should be send to a single node:

    PAUSE myqueue out bcast

To specify **all** is the same as to specify both **in** and **out**, so the two following
forms are equivalent:

    PAUSE myqueue in out
    PAUSE myqueue all

To just get the current state use:

    PAUSE myqueue state
    ""none""

Special handling of messages with RETRY set to 0
===

In order to provide a coherent API, messages with at-most-once delivery
semantics are still retained after being delivered a first time, and should
be acknowledged like any other message. Of course, the acknowledge is not
mandatory, since the message may be lost and there is no way for the receiver
to get the same message again, since the message is associated with a retry
value of 0.

In order to avoid non acknowledged messages with retry set to 0 from leaking
into Disque and eating all the memory, when the Disque server memory is full
and starts to evict, it does not just evict acknowledged messages, but also
can evict non acknowledged messages having, at the same time, the following
two properties:

1. Their retry is set to 0.
2. The job was already delivered.

In theory, acknowledging a job that will never be retried is a waste of time
and resources, however this design has hidden advantages:

1. The API is exactly the same for all the kinds of jobs.
2. After the job is delivered, it is still possible to examine it. Observability is a very good property of messaging systems.

However, not acknowledging the job does not result in big issues since they
are evicted eventually during memory pressure.

Adding and removing nodes at runtime
===

Adding nodes is trivial, and just consists in starting a new node and sending it
a `CLUSTER MEET` command. Assuming the node you just started is located
at address 192.168.1.10 port 7714, and a random (you can use any) node of
the existing cluster is located at 192.168.1.9 port 7711, all you need to do
is:

    ./disque -h 192.168.1.10 -p 7714 cluster meet 192.168.1.9 7711

Note that you can invert the source and destination arguments and the
new node will still join the cluster. It does not matter if it's the old node to
meet the new one or the other way around.

In order to remove a node, it is possible to use the crude way of just
shutting it down, and then use `CLUSTER FORGET <old-node-id>` in all the
other nodes in order to remove references to it from the configuration of
the other nodes. However this means that, for example, messages that had
a replication factor of 3, and one of the replicas was the node you are
shutting down, suddenly are left with just 2 replicas even if no *actual*
failure happened. Moreover if the node you are removing had messages in
queue, you'll need to wait the retry time before the messages will be
queued again. For all these reasons, Disque has a better way to remove nodes
which is described in the next section.

Gracefully removal of nodes
===

In order to empty a node of its content before removing it, it is possible
to use a feature that puts a node in *leaving* state. To enable this feature
just contact the node to remove, and use the following command:

    CLUSTER LEAVING yes

The node will start advertising itself as *leaving*, so in a matter of seconds
all the cluster will know (if there are partitions, when the partition heals
all the nodes will eventually be informed), and this is what happens
when the node is in this state:

1. When the node receives `ADDJOB` commands, it performs external replication, like when a node is near the memory limits. This means that it will make sure
to create the number of replicas of the message in the cluster **without using itself** as a replica. So no new messages are created in the context of a node which is leaving.
2. The node starts to send `-LEAVING` messages to all clients that use `GETJOB` but would block waiting for jobs. The `-LEAVING` error means the clients should connect to another node. Clients that were already blocked waiting for messages will be unblocked and a `-LEAVING` error will be sent to them as well.
3. The node no longer sends `NEEDJOBS` messages in the context of Disque federation, so it will never ask other nodes to transfer messages to it.
4. The node and all the other nodes will advertise it with a bad priority in the `HELLO` command output, so that clients will select a different node.
5. The node will no longer create *dummy acks* in response to an `ACKJOB` command about a job it does not know.

All these behavior changes result in the node participating only as a source of messages, so eventually its message count will drop to zero (it is possible to check for this condition using `INFO jobs`). When this happens the node can be stopped and removed from the other nodes tables using `CLUSTER FORGET` as described in the section above.

Client libraries
===

Disque uses the same protocol as Redis itself. To adapt Redis clients, or to use them directly, should be pretty easy. However note that Disque's default port is 7711 and not 6379.

While a vanilla Redis client may work well with Disque, clients should optionally use the following protocol in order to connect with a Disque cluster:

1. The client should be given a number of IP addresses and ports where nodes are located. The client should select random nodes and should try to connect until an available one is found.
2. On a successful connection the `HELLO` command should be used in order to retrieve the Node ID and other potentially useful information (server version, number of nodes).
3. If a consumer sees a high message rate received from foreign nodes, it may optionally have logic in order to retrieve messages directly from the nodes where producers are producing the messages for a given topic. The consumer can easily check the source of the messages by checking the Node ID prefix in the messages IDs.
4. The `GETJOB` command, or other commands, may return a `-LEAVING` error instead of blocking. This error should be considered by the client library as a request to connect to a different node, since the node it is connected to is not able to serve the request since it is leaving the cluster. Nodes in this state have a very high *priority* number published via `HELLO`, so will be unlikely to be picked at the next connection attempt.

This way producers and consumers will eventually try to minimize node message exchanges whenever possible.

So basically you could perform basic usage using just a Redis client, however
there are already specialized client libraries implementing a more specialized
API on top of Disque:

*C++*

- [disque C++ client](https://github.com/zhengshuxin/acl/tree/master/lib_acl_cpp/samples/disque)

*Common Lisp*

- [cl-disque](https://github.com/CodyReichert/cl-disque)

*Elixir*

- [exdisque](https://github.com/mosic/exdisque)

*Erlang*

- [edisque](https://github.com/nacmartin/edisque)

*Go*

- [disque-go](https://github.com/zencoder/disque-go)
- [go-disque](https://github.com/EverythingMe/go-disque)
- [disque](https://github.com/goware/disque)

*Java*

- [jedisque](https://github.com/xetorthio/jedisque)
- [spinach](https://github.com/mp911de/spinach)

*Node.js*

- [disque.js](https://www.npmjs.com/package/disque.js)
- [thunk-disque](https://github.com/thunks/thunk-disque)
- [disqueue-node](https://www.npmjs.com/package/disqueue-node)

*Perl*

- [perl-disque](https://github.com/lovelle/perl-disque)

*PHP*

- [phpque](https://github.com/s12v/phpque) (PHP/HHVM)
- [disque-php](https://github.com/mariano/disque-php) ([Composer/Packagist](https://packagist.org/packages/mariano/disque-php))
- [disque-client-php](https://github.com/mavimo/disque-client-php) ([Composer/Packagist](https://packagist.org/packages/mavimo/disque-client))
- [phloppy](https://github.com/0x20h/phloppy) ([Composer/Packagist](https://packagist.org/packages/0x20h/phloppy))

*Python*

- [disq](https://github.com/ryansb/disq) ([PyPi](https://pypi.python.org/pypi/disq))
- [pydisque](https://github.com/ybrs/pydisque) ([PyPi](https://pypi.python.org/pypi/pydisque))
- [django-q](https://github.com/koed00/django-q) ([PyPi](https://pypi.python.org/pypi/django-q))

*Ruby*

- [disque-rb](https://github.com/soveran/disque-rb)
- [disque_jockey](https://github.com/DevinRiley/disque_jockey)
- [Disc](https://github.com/pote/disc)

*Rust*

- [disque-rs](https://github.com/seppo0010/disque-rs)

*.NET*

- [Disque.Net](https://github.com/ziyasal/Disque.Net)

Implementation details
===

Job replication strategy
---

1. Disque tries to replicate to W-1 (or W during out of memory) reachable nodes, shuffled.
2. The cluster REPLJOB message is used to replicate a job to multiple nodes, the job is sent together with the list of nodes that may have a copy.
2. If the required replication is not reached promptly, the job is send to one additional node every 50 milliseconds. When this happens, a new REPLJOB message is also re-sent to each node that may already have a copy, in order to refresh the list of nodes that have a copy.
3. If the specified synchronous replication timeout is reached, the node that originally received the ADDJOB command from the client gives up and returns an error to the client. When this happens the node performs a best-effort procedure to delete the job from nodes that may have already received a copy of the job.

Cluster topology
---

Disque is a full mesh, with each node connected to each other. Disque performs
distributed failure detection via gossip, only in order to adjust the
replication strategy (try reachable nodes first when trying to replicate
a message), and in order to inform clients about non reachable nodes when
they want the list of nodes they can connect to.

As Disque is multi-master, the event of nodes failing is not handled in any
special way.

Cluster messages
---

Nodes communicate via a set of messages, using the node-to-node message bus.
A few of the messages are used in order to check that other nodes are
reachable and to mark nodes as failing. Those messages are PING, PONG and
FAIL. Since failure detection is only used to adjust the replication strategy
(talk with reachable nodes first in order to improve latency), the details
are yet not described. Other messages are more important since they are used
in order to replicate jobs, re-issue jobs while trying to minimize multiple
deliveries, and in order to auto-federate to serve consumers when messages
are produced in different nodes compared to where consumers are.

The following is a list of messages and what they do, split by category.
Note that this is just an informal description, while in the next sections
describing the Disque state machine, there is a more detailed description
of the behavior caused by message reception, and in what cases they are
generated.

Cluster messages related to jobs replication and queueing
---

* REPLJOB: ask the receiver to replicate a job, that is, to add a copy of the job among the registered jobs in the target node. When a job is accepted, the receiver replies with GOTJOB to the sender. A job may not be accepted if the receiving node is near out of memory. In this case GOTJOB is not sent and the message discarded.
* GOTJOB: The reply to REPLJOB to confirm the job was replicated.
* ENQUEUE: Ask a node to put a given job into its queue. This message is used when a job is created by a node that does not want to take a copy, so it asks another node (among the ones that acknowledged the job replication) to queue it for the first time. If this message is lost, after the retry time some node will try to re-queue the message, unless retry is set to zero.
* WILLQUEUE: This message is sent 500 milliseconds before a job is re-queued to all the nodes that may have a copy of the message, according to the sender table. If some of the receivers already have the job queued, they'll reply with QUEUED in order to prevent the sender to queue the job again (avoid multiple delivery when possible).
* QUEUED: When a node re-queues a job, it sends QUEUED to all the nodes that may have a copy of the message, so that the other nodes will update the time at which they'll retry to queue the job. Moreover, every node that already has the same job in queue, but with a node ID which is lexicographically smaller than the sending node, will de-queue the message in order to best-effort de-dup messages that may be queued in multiple nodes at the same time.

Cluster messages related to ACK propagation and garbage collection
---

* SETACK: This message is sent to force a node to mark a job as successfully delivered (acknowledged by the worker): the job will no longer be considered active, and will never be re-queued by the receiving node. Also SETACK is send to the sender if the receiver of QUEUED or WILLQUEUE message has the same job marked as acknowledged (successfully delivered) already.
* GOTACK: This message is sent in order to acknowledge a SETACK message. The receiver can mark a given node that may have a copy of a job, as informed about the fact that the job was acknowledged by the worker. Nodes delete (garbage collect) a message cluster wide when they believe all the nodes that may have a copy are informed about the fact the job was acknowledged.
* DELJOB: Ask the receiver to remove a job. Is only sent in order to perform garbage collection of jobs by nodes that are sure the job was already delivered correctly. Usually the node sending DELJOB only does that when its sure that all the nodes that may have a copy of the message already marked the message ad delivered, however after some time the job GC may be performed anyway, in order to reclaim memory, and in that case, an otherwise avoidable multiple delivery of a job may happen. The DELJOB message is also used in order to implement *fast acknowledges*.

Cluster messages related to nodes federation
---

* NEEDJOBS(queue,count): The sender asks the receiver to obtain messages for a given queue, possibly *count* messages, but this is only an hit for congestion control and messages optimization, the receiver is free to reply with whatever number of messages. NEEDJOBS messages are delivered in two ways: broadcasted to every node in the cluster from time to time, in order to discover new source nodes for a given queue, or more often, to a set of nodes that recently replies with jobs for a given queue. This latter mechanism is called an *ad hoc* delivery, and is possible since every node remembers for some time the set of nodes that were recent providers of messages for a given queue. In both cases, NEEDJOBS messages are delivered with exponential delays, with the exception of queues that drop to zero-messages and have a positive recent import rate, in this case an ad hoc NEEDJOBS delivery is performed regardless of the last time the message was delivered in order to allow a continuous stream of messages under load.

* YOURJOBS(array of messages): The reply to NEEDJOBS. An array of serialized jobs, usually all about the same queue (but future optimization may allow to send different jobs from different queues). Jobs into YOURJOBS replies are extracted from the local queue, and queued at the receiver node's queue with the same name. So even messages with a retry set to 0 (at most once delivery) still guarantee the safety rule since a given message may be in the source node, on the wire, or already received in the destination node. If a YOURJOBS message is lost, at least once delivery jobs will be re-queued later when the retry time is reached.

Disque state machine
---

This section shows the most interesting (as in less obvious) parts of the state machine each Disque node implements. While practically it is a single state machine, it is split in sections. The state machine description uses a convention that is not standard but should look familiar, since it is event driven, made of actions performed upon: message receptions in the form of commands received from clients, messages received from other cluster nodes, timers, and procedure calls.

Note that: `job` is a job object with the following fields:

1. `job.delivered`: A list of nodes that may have this message. This list does not need to be complete, is used for best-effort algorithms.
2. `job.confirmed`: A list of nodes that confirmed reception of ACK by replying with a GOTJOB message.
3. `job.id`: The job 48 chars ID.
4. `job.state`: The job state among: `wait-repl`, `active`, `queued`, `acked`.
5. `job.replicate`: Replication factor for this job.
5. `job.qtime`: Time at which we need to re-queue the job.

List fields such as `.delivered` and `.confirmed` support methods like `.size` to get the number of elements.

States are as follows:

1. `wait-repl`: the job is waiting to be synchronously replicated.
2. `active`: the job is active, either it reached the replication factor in the originating node, or it was created because the node received an `REPLJOB` message from another node.
3. `queued`: the job is active and also is pending into a queue in this node.
4. `acked`: the job is no longer active since a client confirmed the reception using the `ACKJOB` command or another Disque node sent a `SETACK` message for the job.

Generic functions
---

PROCEDURE `LOOKUP-JOB(string job-id)`:

1. If job with the specified id is found, returns the corresponding job object.
2. Otherwise returns NULL.

PROCEDURE `UNREGISTER(object job)`:

1. Delete the job from memory, and if queued, from the queue.

PROCEDURE `ENQUEUE(job)`:

1. If `job.state == queued` return ASAP.
2. Add `job` into `job.queue`.
3. Change `job.state` to `queued`.

PROCEDURE `DEQUEUE(job)`:

1. If `job.state != queued` return ASAP.
2. Remove `job` from `job.queue`.
3. Change `job.state` to `active`.

ON RECV cluster message: `DELJOB(string job.id)`:

1. job = Call `LOOKUP-JOB(job-id)`.
2. IF `job != NULL` THEN call `UNREGISTER(job)`.

Job replication state machine
---

This part of the state machine documents how clients add jobs to the cluster
and how the cluster replicates jobs across different Disque nodes.

ON RECV client command `ADDJOB(string queue-name, string body, integer replicate, integer retry, integer ttl, ...):

1. Create a job object in `wait-repl` state, having as body, ttl, retry, queue name, the specified values.
2. Send REPLJOB(job.serialized) cluster message to `replicate-1` nodes.
3. Block the client without replying.

Step 3: We'll reply to the client in step 4 of `GOTJOB` message processing.

ON RECV cluster message `REPLJOB(object serialized-job)`:

1. job = Call `LOOKUP-JOB(serialized-job.id)`.
2. IF `job != NULL` THEN: job.delivered = UNION(job.delivered,serialized-job.delivered). Return ASAP, since we have the job.
3. Create a job from serialized-job information.
4. job.state = `active`.
5. Reply to the sender with `GOTJOB(job.id)`.

Step 1: We may already have the job, since REPLJOB may be duplicated.

Step 2: If we already have the same job, we update the list of jobs that may have a copy of this job, performing the union of the list of nodes we have with the list of nodes in the serialized job.

ON RECV cluster message `GOTJOB(object serialized-job)`:

1. job = Call `LOOKUP-JOB(serialized-job.id)`.
2. IF `job == NULL` OR `job.state != wait-repl` Return ASAP.
3. Add sender node to `job.confirmed`.
4. IF `job.confirmed.size == job.replicate` THEN change `job.state` to `active`, call ENQUEUE(job), and reply to the blocked client with `job.id`.

Step 4: As we receive enough confirmations via `GOTJOB` messages, we finally reach the replication factor required by the user and consider the message active.

TIMER, firing every next 50 milliseconds while a job still did not reached the expected replication factor.

1. Select an additional node not already listed in `job.delivered`, call it `node`.
2. Add `node` to `job.delivered`.
3. Send REPLJOB(job.serialized) cluster message to each node in `job.delivered`.

Step 3: We send the message to every node again, so that each node will have a chance to update `job.delivered` with the new nodes. It is not required for each node to know the full list of nodes that may have a copy, but doing so improves our approximation of single delivery whenever possible.

Job re-queueing state machine
---

This part of the state machine documents how Disque nodes put a given job
back into the queue after the specified retry time elapsed without the
job being acknowledged.

TIMER, firing 500 milliseconds before the retry time elapses:

1. Send `WILLQUEUE(job.id)` to every node in `jobs.delivered`.

TIMER, firing when `job.qtime` time is reached.

1. If `job.retry == 0` THEN return ASAP.
2. Call ENQUEUE(job).
3. Update `job.qtime` to NOW + job.retry.
4. Send `QUEUED(job.id)` message to each node in `job.delivered`.

Step 1: At most once jobs never get enqueued again.

Step 3: We'll retry again after the retry period.

ON RECV cluster message `WILLQUEUE(string job-id)`:

1. job = Call `LOOKUP-JOB(job-id)`.
2. IF `job == NULL` THEN return ASAP.
3. IF `job.state == queued` SEND `QUEUED(job.id)` to `job.delivered`.
4. IF `job.state == acked` SEND `SETACK(job.id)` to the sender.

Step 3: We broadcast the message since likely the other nodes are going to retry as well.

Step 4: SETACK processing is documented below in the acknowledges section of the state machine description.

ON RECV cluster message `QUEUED(string job-id)`:

1. job = Call `LOOKUP-JOB(job-id)`.
2. IF `job == NULL` THEN return ASAP.
3. IF `job.state == acked` THEN return ASAP.
4. IF `job.state == queued` THEN if sender node ID is greater than my node ID call DEQUEUE(job).
5. Update `job.qtime` setting it to NOW + job.retry.

Step 4: If multiple nodes re-queue the job about at the same time because of race conditions or network partitions that make `WILLQUEUE` not effective, then `QUEUED` forces receiving nodes to dequeue the message if the sender has a greater node ID, lowering the probability of unwanted multiple delivery.

Step 5: Now the message is already queued somewhere else, but the node will retry again after the retry time.

Acknowledged jobs garbage collection state machine
---

This part of the state machine is used in order to garbage collect
acknowledged jobs, when a job finally gets acknowledged by a client.

PROCEDURE `ACK-JOB(job)`:

1. If job state is already `acked`, do nothing and return ASAP.
2. Change job state to `acked`, dequeue the job if queued, schedule first call to TIMER.

PROCEDURE `START-GC(job)`:

1. Send `SETACK(job.delivered.size)` to each node that is listed in `job.delivered` but is not listed in `job.confirmed`.
2. IF `job.delivered.size == 0`, THEN send `SETACK(0)` to every node in the cluster.

Step 2: this is an ACK about a job we don’t know. In that case, we can just broadcast the acknowledged hoping somebody knows about the job and replies.

ON RECV client command `ACKJOB(string job-id)`:

1. job = Call `LOOKUP-JOB(job-id)`.
1. if job is `NULL`, ignore the message and return.
2. Call `ACK-JOB(job)`.
3. Call `START-GC(job)`.

ON RECV cluster message `SETACK(string job-id, integer may-have)`:

1. job = Call `LOOKUP-JOB(job-id)`.
2. Call ACK-JOB(job) IF job is not `NULL`.
3. Reply with GOTACK IF `job == NULL OR job.delivered.size <= may-have`.
4. IF `job != NULL` and `jobs.delivered.size > may-have` THEN call `START-GC(job)`.
5. IF `may-have == 0 AND job  != NULL`, reply with `GOTACK(1)` and call `START-GC(job)`.

Steps 3 and 4 makes sure that among the reachable nodes that may have a message, garbage collection will be performed by the node that is aware of more nodes that may have a copy.

Step 5 instead is used in order to start a GC attempt if we received a SETACK message from a node just hacking a dummy ACK (an acknowledge about a job it was not aware of).

ON RECV cluster message `GOTACK(string job-id, bool known)`:

1. job = Call `LOOKUP-JOB(job-id)`. Return ASAP IF `job == NULL`.
2. Call `ACK-JOB(job)`.
3. IF `known == true AND job.delivered.size > 0` THEN add the sender node to `job.delivered`.
4. IF `(known == true) OR (known == false AND job.delivered.size > 0) OR (known == false AND sender is an element of job.delivered)` THEN add the sender node to `jobs.confirmed`.
5. IF `job.delivered.size > 0 AND job.delivered.size == job.confirmed.size`, THEN send `DELJOB(job.id)` to every node in the `job.delivered` list and call `UNREGISTER(job)`.
6. IF `job.delivered == 0 AND known == true`, THEN call `UNREGISTER(job)`.
7. IF `job.delivered == 0 AND job.confirmed.size == cluster.size` THEN call `UNREGISTER(job)`.

Step 3: If `job.delivered.size` is zero, it means that the node just holds a *dummy ack* for the job. It means the node has an acknowledged job it created on the fly because a client acknowledged (via ACKJOB command) a job it was not aware of.

Step 6: we don't have to hold a dummy acknowledged jobs if there are nodes that have the job already acknowledged.

Step 7: this happens when nobody knows about a job, like when a client acknowledged a wrong job ID.

TIMER, from time to time (exponential backoff with random error), for every acknowledged job in memory:

1. call `START-GC(job)`.

Limitations
===

* Disque is new code, not tested, and will require quite some time to reach production quality. It is likely very buggy and may contain wrong assumptions or tradeoffs.
* As long as the software is non stable, the API may change in random ways without prior notification.
* It is possible that Disque spends too much effort in approximating single delivery during failures. The **fast acknowledge** concept and command makes the user able to opt-out this efforts, but yet I may change the Disque implementation and internals in the future if I see the user base really not caring about multiple deliveries during partitions.
* There is yet a lot of Redis dead code inside probably that could be removed.
* Disque was designed a bit in *astronaut mode*, not triggered by an actual use case of mine, but more in response to what I was seeing people doing with Redis as a message queue and with other message queues. However I'm not an expert, if I succeeded to ship something useful for most users, this is kinda of an accomplishment. Otherwise it may just be that Disque is pretty useless.
* As Redis, Disque is single threaded. While in Redis there are stronger reasons to do so, in Disque there is no manipulation of complex data structures, so maybe in the future it should be moved into a threaded server. We need to see what happens in real use cases in order to understand if it's worth it or not.
* The number of jobs in a Disque process is limited to the amount of memory available. Again while this in Redis makes sense (IMHO), in Disque there are definitely simple ways in order to circumvent this limitation, like logging messages on disk when the server is out of memory and consuming back the messages when memory pressure is already acceptable. However in general, like in Redis, manipulating data structures in memory is a big advantage from the point of view of the implementation simplicity and the functionality we can provide to users.
* Disque is completely not optimized for speed, was never profiled so far. I'm currently not aware of the fact it's slow, fast, or average, compared to other messaging solutions. For sure it is not going to have Redis-alike numbers because it does a lot more work at each command. For example when a job is added, it is serialized and transmitted to other `N` servers. There is a lot more message passing between nodes involved, and so forth. The good news is that being totally unoptimized, there is room for improvements.
* Ability of federation to handle well low and high loads without incurring into congestion or high latency, was not tested well enough. The algorithm is reasonable but may fail short under many load patterns.
* Amount of tested code path and possible states is not enough.

FAQ
===

Is Disque part of Redis?
---

No, it is a standalone project, however a big part of the Redis networking source code, nodes message bus, libraries, and the client protocol, were reused in this new project. In theory it was possible to extract the common code and release it as a framework to write distributed systems in C. However this is not a perfect solution as well, since the projects are expected to diverge more and more in the future, and to rely on a common foundation was hard. Moreover the initial effort to turn Redis into two different layers: an abstract server, networking stack and cluster bus, and the actual Redis implementation, was a huge effort, ways bigger than writing Disque itself.

However while it is a separated project, conceptually Disque is related to Redis, since it tries to solve a Redis use case in a vertical, ad-hoc way.

Who created Disque?
---

Disque is a side project of Salvatore Sanfilippo, aka @antirez.

There are chances for this project to be actively developed?
---

Currently I consider this just a public alpha: If I see people happy to use it for the right reasons (i.e. it is better in some use cases compared to other message queues) I'll continue the development. Otherwise it was anyway cool to develop it, I had much fun, and I definitely learned new things.

What happens when a node runs out of memory?
---

1. Maxmemory setting is mandatory in Disque, and defaults to 1GB.
2. When 75% of maxmemory is reached, Disque starts to replicate the new jobs only to external nodes, without taking a local copy, so basically if there is free RAM into other nodes, adding still works.
3. When 95% of maxmemory is reached, Disque starts to evict data that does not violates the safety guarantees: For instance acknowledged jobs and inactive queues.
4. When 100% of maxmemory is reached, commands that may result into more memory used are not processed at all and the client is informed with an error.

Are there plans to add the ability to hold more jobs than the physical memory of a single node can handle?
---

Yes. In Disque it should be relatively simple to use the disk when memory is not
available, since jobs are immutable and don't need to necessarily exist in
memory at a given time.

There are multiple strategies available. The current idea is that
when an instance is out of memory, jobs are stored into a log file instead
of memory. As more free memory is available in the instance, on disk jobs
are loaded.

However in order to implement this, there is to observe strong evidence of its
general usefulness for the user base.

When I consume and produce from different nodes, sometimes there is a delay in order for the jobs to reach the consumer, why?
---

Disque routing is not static, the cluster automatically tries to provide
messages to nodes where consumers are attached. When there is an high
enough traffic (even one message per second is enough) nodes remember other
nodes that recently were sources for jobs in a given queue, so it is possible
to aggressively send messages asking for more jobs, every time there are
consumers waiting for more messages and the local queue is empty.

However when the traffic is very low, informations about recent sources of
messages are discarded, and nodes rely on a more generic mechanism in order to
discover other nodes that may have messages in the queues we need them (which
is also used in high traffic conditions as well, in order to discover new
sources of messages for a given queue).

For example imagine a setup with two nodes, A and B.

1. A client attaches to node A and asks for jobs in the queue `myqueue`. Node A has no jobs enqueued, so the client is blocked.
2. After a few seconds another client produces messages into `myqueue`, but sending them to node B.

During step `1` if there was no recent traffic of imported messages for this queue, node A has no idea about who may have messages for the queue `myqueue`. Every other node may have, or none may have. So it starts to broadcast `NEEDJOBS` messages to the whole cluster. However we can't spam the cluster with messages, so if no reply is received after the first broadcast, the next will be sent with a larger delay, and so foth. The delay is exponential, with a maximum value of 30 seconds (this parameters will be configurable in the future, likely).

When there is some traffic instead, nodes send `NEEDJOBS` messages ASAP to other nodes that were recent sources of messages. Even when no reply is received, the next `NEEDJOBS` messages will be sent more aggressively to the subset of nodes that had messages in the past, with a delay that starts at 25 milliseconds and has a maximum value of two seconds.

In order to minimize the latency, `NEEDJOBS` messages are not throttled at all when:

1. A client consumed the last message from a given queue. Source nodes are informed immediately in order to receive messages before the node asks for more.
2. Blocked clients are served the last message available in the queue.

For more information, please refer to the file `queue.c`, especially the function `needJobsForQueue` and its callers.

Are messages re-enqueued in the queue tail or head or what?
---

Messages are put into the queue according to their *creation time* attribute. This means that they are enqueued in a best effort order in the local node queue. Messages that need to be put back into the queue again because their delivery failed are usually (but not always) older than messages already in queue, so they'll likely be among the first to be delivered to workers.

What Disque means?
---

DIStributed QUEue but is also a joke with ""dis"" as negation (like in *dis*order) of the strict concept of queue, since Disque is not able to guarantee the strict ordering you expect from something called *queue*. And because of this tradeof it gains many other interesting things.

Community: how to get help and how to help
===

Get in touch with us in one of the following ways:

1. Post on [Stack Overflow](http://stackoverflow.com) using the `disque` tag. This is the preferred method to get general help about Disque: other users will easily find previous questions so we can incrementally build a knowledge base.
2. Join the `#disque` IRC channel at **irc.freenode.net**.
3. Create an Issue or Pull request if your question or issue is about the Disque implementation itself.

Thanks
===

I would like to say thank you to the following persons and companies.

* Pivotal, for allowing me to work on Disque, most in my spare time, but sometimes during work hours. Moreover Pivotal agreed to leave the copyright of the code to me. This is very generous. Thanks Pivotal!
* Michel Martens and Damian Janowski for providing early feedback about Disque while the project was still private.
* Everybody who is already writing client libraries, sending pull requests, creating issues in order to move this forward from alpha to something actually usable."
LILIDD/FFmpeg,131268,0,1,0,User,False,67391,2,0,619,False,,,0,7,0,,,,,0,0,0,0,7103,0,0,0,0,0,0,130,,30,,"FFmpeg README
=============

FFmpeg is a collection of libraries and tools to process multimedia content
such as audio, video, subtitles and related metadata.

## Libraries

* `libavcodec` provides implementation of a wider range of codecs.
* `libavformat` implements streaming protocols, container formats and basic I/O access.
* `libavutil` includes hashers, decompressors and miscellaneous utility functions.
* `libavfilter` provides a mean to alter decoded Audio and Video through chain of filters.
* `libavdevice` provides an abstraction to access capture and playback devices.
* `libswresample` implements audio mixing and resampling routines.
* `libswscale` implements color conversion and scaling routines.

## Tools

* [ffmpeg](http://ffmpeg.org/ffmpeg.html) is a command line toolbox to
  manipulate, convert and stream multimedia content.
* [ffplay](http://ffmpeg.org/ffplay.html) is a minimalistic multimedia player.
* [ffprobe](http://ffmpeg.org/ffprobe.html) is a simple analisys tool to inspect
  multimedia content.
* Additional small tools such as `aviocat`, `ismindex` and `qt-faststart`.

## Documentation

The offline documentation is available in the **doc/** directory.

The online documentation is available in the main [website](http://ffmpeg.org)
and in the [wiki](http://trac.ffmpeg.org).

### Examples

Coding examples are available in the **doc/examples** directory.

## License

FFmpeg codebase is mainly LGPL-licensed with optional components licensed under
GPL. Please refer to the LICENSE file for detailed information."
tizenorg/platform.upstream.libva-intel-driver,4565,0,2,0,Organization,False,1308,32,126,22,False,,,0,7,0,0,0,0,0,0,0,0,0,3974,0,0,0,0,0,0,1,0,,,
axxx007xxxz/android_hardware_qcom_audio,2169,0,1,0,User,False,1330,1,0,30,False,,,0,8,0,0,0,0,0,0,0,0,0,2903,0,0,0,0,0,0,107,,6,,
sdegutis/mjolnir,11473,5081,86,136,Organization,False,1502,2,4,32,False,Lightweight automation and productivity app for OS X,,0,7,1,0,493,0,0,0,82,0,0,2357,1,1,19,0,0,0,10,0,,mjolnirapp/mjolnir,"# Mjolnir

<img src=""https://raw.githubusercontent.com/mjolnirapp/mjolnir/master/Mjolnir/Images.xcassets/AppIcon.appiconset/icon_128x128.png"" alt=""Mjolnir logo"" title=""Mjolnir logo"" align=""right"" width=""64"" height=""64""/>

*Lightweight automation and productivity power-tool for OS X*

* Current version:  Mjolnir 1.0.1
* Requires:         OS X 10.12 or higher

## What is Mjolnir?

Mjolnir is an OS X app that lets you automate common tasks using the
language Lua. At its core, it doesn't actually do anything besides
load up a Lua environment; the real power lies in all the useful
modules that you can install.

You write a ""config"", which just means `~/.mjolnir/init.lua`. This
file, along with whatever modules it requires, have full access to the
built-in `mjolnir` module, and all Lua modules that you have installed
(e.g. from LuaRocks or any way you want to install them).

## Try it out

1. Download [the latest release](https://github.com/mjolnirapp/mjolnir/releases/latest), unzip, right-click `Mjolnir.app`, choose ""Open"". Or, install it with [Homebrew](https://brew.sh/):

   ~~~bash
   $ brew cask install mjolnir
   ~~~

2. Install Lua into /usr/local e.g. from [Homebrew](http://brew.sh/), and then install LuaRocks:

   ~~~bash
   $ brew update
   $ brew install lua
   $ brew install luarocks
   ~~~

3. Install some modules from this list: https://luarocks.org/search?q=mjolnir

   ~~~bash
   $ luarocks install mjolnir.hotkey
   $ luarocks install mjolnir.application
   ~~~

   Note: you don't need to install every module, since some of them have lower-level ones as dependencies, e.g. installing mjolnir-hotkey automatically installs mjolnir-keycodes, etc.

4. Create `~/.mjolnir/init.lua`, and at the top, require the modules you installed, e.g. like this:

   ~~~lua
   local application = require ""mjolnir.application""
   local hotkey = require ""mjolnir.hotkey""
   local window = require ""mjolnir.window""
   local fnutils = require ""mjolnir.fnutils""
   ~~~

   NOTE: The `mjolnir.window` module comes with `mjolnir.application`,
         so you don't need to (and can't) install it separately. Also,
         `mjolnir.fnutils` is already installed as a dependency of the
         other modules, so you don't need to explicitly install it.

5. Start writing some fun stuff!

   ~~~lua
   hotkey.bind({""cmd"", ""alt"", ""ctrl""}, ""D"", function()
      local win = window.focusedwindow()
      local f = win:frame()
      f.x = f.x + 10
      win:setframe(f)
   end)
   ~~~
   
6. Reload config using `mjolnir.reload()` in the console.

## Uninstalling

If for any reason you want to undo everything in the above steps, do:

~~~bash
$ luarocks purge --tree=/usr/local
$ brew uninstall lua luarocks
$ rm ~/.luarocks/config.lua
~~~

## Installing to $HOME

If you run `luarocks --local install ...` instead of `luarocks
install ...`, it will install to `~/.luarocks/` instead of
`/usr/local`. Update your `package.path` and `package.cpath`
accordingly, as noted in the FAQ.

## Finding modules

Check out https://luarocks.org/search?q=mjolnir for a list of
published Mjolnir modules.

Notable modules:

- `mjolnir.hotkey` for creating global hotkeys
- `mjolnir.application` for inspecting and manipulating running OS X applications and windows
- `mjolnir.alert` for showing on-screen messages

## Documentation

Mjolnir and mjolnir-modules use [Dash](http://kapeli.com/dash) for
documentation. You can install Mjolnir's docset from the User
Contributed section of the Downloads tab in Dash's Preferences
window. It should generally update on its own.

## Publishing modules

Wrote an awesome module, and want to share with the world? Check out
the `sample-plugin` subdirectory.

When it's published, please announce it on our mailing list :)

Avoid the temptation to reformat the Repos page in the wiki. It uses
a strict format that's needed by the documentation generator.

## Principles

Development of Mjolnir.app and the core Mjolnir modules follow these
principles:

1. They must be stable. The app should never crash. You should only
   ever have to launch it once, and it should stay running until you
   quit. Period.

2. They must be lightweight. They should never do anything that drains
   your computer's battery. They should never poll for anything. They
   should use as little RAM as possible. Everything they do should
   feel instant and snappy, never sluggish or delayed.

3. They should be completely transparent. There should be no surprises
   in how it's behaving, or what's being executed and when. Everything
   should be fully predictable.

4. They must not be bloated. The app and core modules must always
   adhere to the minimalist philosophy, no excuses. Everything else
   can be a separate Lua module.

## FAQ

1. **Add Spaces support!**

   Not a question. But anyway, there are no public *or* private APIs
   to manage Spaces in OS X 10.9 or 10.10, there are only private APIs
   that work in 10.8. No open source project has been able to crack
   this problem yet, not just us.

2. **Does LuaRocks have a way to upgrade modules automatically?**

   Sadly no. But it can be done manually by removing a module and
   re-installing it. I'm hoping maybe one day some enthusiastic
   Mjolnir users can jump in and improve the tooling around this. ;)

3. **I'm getting an error like this: ""attempt to index field 'win' (a nil value)""**

   Disable and re-enable accessibility. It may look enabled, but do it
   anyway. (This is an OS X bug, not a Mjolnir bug.)

4. **I don't have things in /usr/local, so I can't load modules!**

   Add the path to `package.path` and `package.cpath` in your
   init-file. For example, if you're using Boxen, add this:

   ~~~lua
   package.path = package.path .. ';/opt/boxen/homebrew/share/lua/5.3/?.lua'
   package.cpath = package.cpath .. ';/opt/boxen/homebrew/lib/lua/5.3/?.so'
   ~~~

## Mjolnir vs. other apps

1. **Hydra, Phoenix, or Zephyros?**

   Those are my old apps. Mjolnir is their spiritual successor.

2. **Slate**

   They're both programmer-centric with somewhat similar goals but
   different approaches. Mjolnir is more modularized, Slate is more
   all-in-one. Try them both and see which one suits you better.

3. **Spectacle, Moom, SizeUp, Divvy**

   Mjolnir is intended for programmers who want to write programs that
   customize their environment. It's not intended to be a drag-n-drop
   solution; it's meant to allow you to write your own personalized
   productivity enhancement suite to keep and to use long-term.

4. **Hammerspoon**

   [Hammerspoon](https://github.com/hammerspoon) is a fork of Mjolnir (get it? a ""fork and/or spoon"" of Mjolnir aka. Thor's ""hammer""? :) ). It was created to turn Mjolnir back into an all-in-one application, for those who prefer that over a completely decentralized module system with a bare-bones core (kind of like the debate of monolithic kernel vs microkernel). It's actively maintained, like literally there's commit activit every week.

## Community

Our [mailing list](https://groups.google.com/forum/#!forum/mjolnir-io)
is a fine place to share ideas, and follow releases and announcements.

We also have a shrinking IRC channel on freenode, #mjolnir.

## Credits and Thanks

Mjolnir is developed by Steven Degutis with the help of
[various contributors](https://github.com/sdegutis/mjolnir/graphs/contributors).

Special thanks, in no special order:

- @Habbie for his constant help and support ever since the moment I
  first jumped into #lua and said ""anyone wanna try out an OS X window
  manager scriptable in Lua?"" and being the first person to join our
  IRC channel and for helping with nearly every Lua question I had

- @cmsj, @Keithbsmiley, @BrianGilbert, @muescha, @chdiza, @asmagill,
  @splintax, @arxanas, and @DomT4 for all their help and support

- @jasonm23 for writing, not one, not two, but *three* app icons for
  this project

- @jhgg for contributing so many awesome modules to the project

- @kapeli for his patience with my constant Dash questions and PRs

- Everyone else who has helped who I've probably forgotten: thanks for
  all your help!

- Everyone who has donated: thank you so much for your support!

See the in-app About panel for the open source licenses for the
software Mjolnir uses internally (basically just Lua's license).

## Changes

**NOTE:** When upgrading, System Preferences will *pretend* like
  Mjolnir's accessibility is enabled, showing a checked checkbox. But
  in fact, you'll still need to be disable and re-enable it. This is a
  bug in OS X.

### 0.4.3

- Removed donation requests

### 0.4.{0,1,2}

- Default implementation of `mjolnir.showerror(err)` now opens the console and focuses Mjolnir
- There's a new variable, `mjolnir.configdir = ""~/.mjolnir/""` for users and modules to coordinate
- New `mjolnir.focus()` function to make Mjolnir the focused app
- The original `print` function is now stored in `mjolnir.rawprint` (rather than `mjolnir.print`, to disambiguate it)
- New `mjolnir.openconsole()` function to open console (and bring Mjolnir to front)

### 0.3.1

- Renamed global `mj` to `mjolnir`

### 0.3

- The UI has changed drastically. Expect nothing to be in the same
  place or look the same. Pretend it's a brand new app.
- Modules are now handled by LuaRocks instead of by the app itself.
- The ""core"" namespace has been renamed to ""mj"".
- The 'mj.window' module now ships with the 'mj.application' LuaRocks
  package since they depend on each other.
- `mj.screen:frame_without_dock_or_menu()` is now called `mj.screen:frame()`
- `mj.screen:frame_including_dock_and_menu()` is now called `mj.screen:fullframe()`

## License

> Released under MIT license.
>
> Copyright (c) 2014 Steven Degutis
>
> Permission is hereby granted, free of charge, to any person obtaining a copy
> of this software and associated documentation files (the ""Software""), to deal
> in the Software without restriction, including without limitation the rights
> to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
> copies of the Software, and to permit persons to whom the Software is
> furnished to do so, subject to the following conditions:
>
> The above copyright notice and this permission notice shall be included in
> all copies or substantial portions of the Software.
>
> THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
> THE SOFTWARE."
nexjhealth/x264,4847,0,1,0,User,False,2762,1,0,93,False,,,0,7,0,0,0,0,0,0,0,0,0,5192,0,0,0,0,0,0,24,,3,,
rootmelo92118/SoftEtherVPN_Stable,356640,0,0,0,User,False,187,1,5,30,False,,,0,8,0,0,0,0,0,0,0,0,0,2357,0,0,0,0,0,0,75,,80,,
MZO9400/android_device_oneplus_bacon-Minimal,8513,0,1,0,User,False,391,6,0,36,False,MinimalOS bacon device tree,,0,7,0,0,0,0,0,0,0,0,0,2294,0,0,0,0,0,0,100,,32,,
ZdrowyGosciu/caf_device,994,0,0,0,User,False,1612,1,0,14,False,MSM8974 CAF,,0,7,0,,,,,0,0,0,0,3449,0,0,0,0,0,0,27,,19,,
michaelarmstrong/mongoose,692,0,0,0,User,False,279,1,0,3,False,Automatically exported from code.google.com/p/mongoose,,0,17,0,32,383,0,0,0,0,0,0,3694,0,0,0,0,0,0,30,,28,,
fogleman/Craft,14647,7928,371,1015,User,False,751,16,1,9,False,A simple Minecraft clone written in C using modern OpenGL (shaders).,http://www.michaelfogleman.com/craft/,7,6,0,65,75,5,1,38,84,9,10,2623,1,1,2,2,0,0,131,,4,,"## Craft

Minecraft clone for Windows, Mac OS X and Linux. Just a few thousand lines of C using modern OpenGL (shaders). Online multiplayer support is included using a Python-based server.

http://www.michaelfogleman.com/craft/

![Screenshot](https://i.imgur.com/SH7wcas.png)

### Features

* Simple but nice looking terrain generation using perlin / simplex noise.
* More than 10 types of blocks and more can be added easily.
* Supports plants (grass, flowers, trees, etc.) and transparency (glass).
* Simple clouds in the sky (they don't move).
* Day / night cycles and a textured sky dome.
* World changes persisted in a sqlite3 database.
* Multiplayer support!

### Download

Mac and Windows binaries are available on the website.

http://www.michaelfogleman.com/craft/

See below to run from source.

### Install Dependencies

#### Mac OS X

Download and install [CMake](http://www.cmake.org/cmake/resources/software.html)
if you don't already have it. You may use [Homebrew](http://brew.sh) to simplify
the installation:

    brew install cmake

#### Linux (Ubuntu)

    sudo apt-get install cmake libglew-dev xorg-dev libcurl4-openssl-dev
    sudo apt-get build-dep glfw

#### Windows

Download and install [CMake](http://www.cmake.org/cmake/resources/software.html)
and [MinGW](http://www.mingw.org/). Add `C:\MinGW\bin` to your `PATH`.

Download and install [cURL](http://curl.haxx.se/download.html) so that
CURL/lib and CURL/include are in your Program Files directory.

Use the following commands in place of the ones described in the next section.

    cmake -G ""MinGW Makefiles""
    mingw32-make

### Compile and Run

Once you have the dependencies (see above), run the following commands in your
terminal.

    git clone https://github.com/fogleman/Craft.git
    cd Craft
    cmake .
    make
    ./craft

### Multiplayer

Register for an account!

https://craft.michaelfogleman.com/

#### Client

You can connect to a server with command line arguments...

    ./craft craft.michaelfogleman.com

Or, with the ""/online"" command in the game itself.

    /online craft.michaelfogleman.com

#### Server

You can run your own server or connect to mine. The server is written in Python
but requires a compiled DLL so it can perform the terrain generation just like
the client.

    gcc -std=c99 -O3 -fPIC -shared -o world -I src -I deps/noise deps/noise/noise.c src/world.c
    python server.py [HOST [PORT]]

### Controls

- WASD to move forward, left, backward, right.
- Space to jump.
- Left Click to destroy a block.
- Right Click or Cmd + Left Click to create a block.
- Ctrl + Right Click to toggle a block as a light source.
- 1-9 to select the block type to create.
- E to cycle through the block types.
- Tab to toggle between walking and flying.
- ZXCVBN to move in exact directions along the XYZ axes.
- Left shift to zoom.
- F to show the scene in orthographic mode.
- O to observe players in the main view.
- P to observe players in the picture-in-picture view.
- T to type text into chat.
- Forward slash (/) to enter a command.
- Backquote (`) to write text on any block (signs).
- Arrow keys emulate mouse movement.
- Enter emulates mouse click.

### Chat Commands

    /goto [NAME]

Teleport to another user.
If NAME is unspecified, a random user is chosen.

    /list

Display a list of connected users.

    /login NAME

Switch to another registered username.
The login server will be re-contacted. The username is case-sensitive.

    /logout

Unauthenticate and become a guest user.
Automatic logins will not occur again until the /login command is re-issued.

    /offline [FILE]

Switch to offline mode.
FILE specifies the save file to use and defaults to ""craft"".

    /online HOST [PORT]

Connect to the specified server.

    /pq P Q

Teleport to the specified chunk.

    /spawn

Teleport back to the spawn point.

### Screenshot

![Screenshot](https://i.imgur.com/foYz3aN.png)

### Implementation Details

#### Terrain Generation

The terrain is generated using Simplex noise - a deterministic noise function seeded based on position. So the world will always be generated the same way in a given location.

The world is split up into 32x32 block chunks in the XZ plane (Y is up). This allows the world to be “infinite” (floating point precision is currently a problem at large X or Z values) and also makes it easier to manage the data. Only visible chunks need to be queried from the database.

#### Rendering

Only exposed faces are rendered. This is an important optimization as the vast majority of blocks are either completely hidden or are only exposing one or two faces. Each chunk records a one-block width overlap for each neighboring chunk so it knows which blocks along its perimeter are exposed.

Only visible chunks are rendered. A naive frustum-culling approach is used to test if a chunk is in the camera’s view. If it is not, it is not rendered. This results in a pretty decent performance improvement as well.

Chunk buffers are completely regenerated when a block is changed in that chunk, instead of trying to update the VBO.

Text is rendered using a bitmap atlas. Each character is rendered onto two triangles forming a 2D rectangle.

“Modern” OpenGL is used - no deprecated, fixed-function pipeline functions are used. Vertex buffer objects are used for position, normal and texture coordinates. Vertex and fragment shaders are used for rendering. Matrix manipulation functions are in matrix.c for translation, rotation, perspective, orthographic, etc. matrices. The 3D models are made up of very simple primitives - mostly cubes and rectangles. These models are generated in code in cube.c.

Transparency in glass blocks and plants (plants don’t take up the full rectangular shape of their triangle primitives) is implemented by discarding magenta-colored pixels in the fragment shader.

#### Database

User changes to the world are stored in a sqlite database. Only the delta is stored, so the default world is generated and then the user changes are applied on top when loading.

The main database table is named “block” and has columns p, q, x, y, z, w. (p, q) identifies the chunk, (x, y, z) identifies the block position and (w) identifies the block type. 0 represents an empty block (air).

In game, the chunks store their blocks in a hash map. An (x, y, z) key maps to a (w) value.

The y-position of blocks are limited to 0 <= y < 256. The upper limit is mainly an artificial limitation to prevent users from building unnecessarily tall structures. Users are not allowed to destroy blocks at y = 0 to avoid falling underneath the world.

#### Multiplayer

Multiplayer mode is implemented using plain-old sockets. A simple, ASCII, line-based protocol is used. Each line is made up of a command code and zero or more comma-separated arguments. The client requests chunks from the server with a simple command: C,p,q,key. “C” means “Chunk” and (p, q) identifies the chunk. The key is used for caching - the server will only send block updates that have been performed since the client last asked for that chunk. Block updates (in realtime or as part of a chunk request) are sent to the client in the format: B,p,q,x,y,z,w. After sending all of the blocks for a requested chunk, the server will send an updated cache key in the format: K,p,q,key. The client will store this key and use it the next time it needs to ask for that chunk. Player positions are sent in the format: P,pid,x,y,z,rx,ry. The pid is the player ID and the rx and ry values indicate the player’s rotation in two different axes. The client interpolates player positions from the past two position updates for smoother animation. The client sends its position to the server at most every 0.1 seconds (less if not moving).

Client-side caching to the sqlite database can be performance intensive when connecting to a server for the first time. For this reason, sqlite writes are performed on a background thread. All writes occur in a transaction for performance. The transaction is committed every 5 seconds as opposed to some logical amount of work completed. A ring / circular buffer is used as a queue for what data is to be written to the database.

In multiplayer mode, players can observe one another in the main view or in a picture-in-picture view. Implementation of the PnP was surprisingly simple - just change the viewport and render the scene again from the other player’s point of view.

#### Collision Testing

Hit testing (what block the user is pointing at) is implemented by scanning a ray from the player’s position outward, following their sight vector. This is not a precise method, so the step rate can be made smaller to be more accurate.

Collision testing simply adjusts the player’s position to remain a certain distance away from any adjacent blocks that are obstacles. (Clouds and plants are not marked as obstacles, so you pass right through them.)

#### Sky Dome

A textured sky dome is used for the sky. The X-coordinate of the texture represents time of day. The Y-values map from the bottom of the sky sphere to the top of the sky sphere. The player is always in the center of the sphere. The fragment shaders for the blocks also sample the sky texture to determine the appropriate fog color to blend with based on the block’s position relative to the backing sky.

#### Ambient Occlusion

Ambient occlusion is implemented as described on this page:

http://0fps.wordpress.com/2013/07/03/ambient-occlusion-for-minecraft-like-worlds/

#### Dependencies

* GLEW is used for managing OpenGL extensions across platforms.
* GLFW is used for cross-platform window management.
* CURL is used for HTTPS / SSL POST for the authentication process.
* lodepng is used for loading PNG textures.
* sqlite3 is used for saving the blocks added / removed by the user.
* tinycthread is used for cross-platform threading."
SamyPesse/How-to-Make-a-Computer-Operating-System,5986,18796,1671,3269,User,False,243,1,12,34,False,How to Make a Computer Operating System in C++,https://samypesse.gitbook.io/how-to-c…,0,6,1,38,25,1,0,37,54,1,0,2392,0,0,0,0,0,0,86,,2,,"How to Make a Computer Operating System
=======================================

Online book about how to write a computer operating system in C/C++ from scratch.

**Caution**: This repository is a remake of my old course. It was written several years ago [as one of my first projects when I was in High School](https://github.com/SamyPesse/devos), I'm still refactoring some parts. The original course was in French and I'm not an English native. I'm going to continue and improve this course in my free-time.

**Book**: An online version is available at [http://samypesse.gitbooks.io/how-to-create-an-operating-system/](http://samypesse.gitbooks.io/how-to-create-an-operating-system/) (PDF, Mobi and ePub). It was generated using [GitBook](https://www.gitbook.com/).

**Source Code**: All the system source code will be stored in the [src](https://github.com/SamyPesse/How-to-Make-a-Computer-Operating-System/tree/master/src) directory. Each step will contain links to the different related files.

**Contributions**: This course is open to contributions, feel free to signal errors with issues or directly correct the errors with pull-requests.

**Questions**: Feel free to ask any questions by adding issues or commenting sections.

You can follow me on Twitter [@SamyPesse](https://twitter.com/SamyPesse) or [GitHub](https://github.com/SamyPesse).

### What kind of OS are we building?

The goal is to build a very simple UNIX-based operating system in C++, not just a ""proof-of-concept"". The OS should be able to boot, start a userland shell, and be extensible.

![Screen](./preview.png)"
lyzsh/apv,64458,0,0,0,User,False,300,2,1,5,False,Automatically exported from code.google.com/p/apv,,0,16,0,116,55,0,0,0,0,0,0,4415,0,0,0,0,0,0,2,,0,,
furthestworld/-pecl-php-memcache,298,0,1,0,Organization,False,221,1,0,10,False,pecl-php-memcache,,0,7,0,0,0,0,0,0,0,0,0,5969,0,0,0,0,0,0,14,0,,,"memcached module for PHP
------------------------
This module requires zlib library, used for on-the-fly data (de)compression.
Also, you'll need memcached to use it =)

The memcached website is here:
    http://www.danga.com/memcached/

You will probably need libevent to install memcached:
You can download it here: http://www.monkey.org/~provos/libevent/

How to run tests:
1. sh tests/memcache.sh
2. TEST_PHP_EXECUTABLE=/usr/local/bin/php php -dextension=modules/memcache.so run-tests.php -d extension=modules/memcache.so

Maintainers:
Herman J. Radtke III hradtke at php dot net


--------
## 用官方[memcache扩展](https://git.php.net/repository/pecl/caching/memcache.git)代码为例子，研究下PHP扩展开发的一些常规套路

> 有兴趣的童鞋一起哈~

* 参考书籍
    * [PHP扩展开发及内核应用](http://www.cunmou.com/phpbook/index.md)
    请自行阅读一百遍~ :)"
NurKeinNeid/dharma_kernel_nextbit_msm8992,647254,0,1,0,User,False,435328,3,4,4751,False,A Dysfunctional CAF-based custom kernel for ether @ReaperRoms,,0,8,0,,,,,0,0,0,0,6641,0,0,0,0,0,0,60,,23,,
TurokMods/T4Editor,3449,2,2,0,Organization,False,113,5,0,2,False,,https://www.turokforums.com/index.php…,0,8,0,27,1,0,0,0,5,0,0,985,0,0,0,0,0,0,2,0,,,
bluelinkgaming/nintendon-t,58570,0,0,0,User,False,329,2,0,3,False,Automatically exported from code.google.com/p/nintendon-t,,0,12,0,133,64,0,0,0,0,0,0,2238,0,0,0,0,0,0,1,,0,,
dasfoo/rpi-gpio,6,1,2,0,Organization,False,16,1,1,2,False,Very simple GPIO library in CGo using /dev/gpiomem (bonus: DHT11 sensor),,0,7,0,0,0,0,0,0,7,0,0,1629,0,0,0,0,0,0,17,2,,,"# rpi-gpio
Very simple GPIO library in CGo using /dev/gpiomem

[![GoDoc](https://godoc.org/github.com/dasfoo/rpi-gpio?status.svg)](http://godoc.org/github.com/dasfoo/rpi-gpio)
[![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org)
[![Build Status](https://travis-ci.org/dasfoo/rpi-gpio.svg?branch=master)](https://travis-ci.org/dasfoo/rpi-gpio)
[![Coverage Status](https://coveralls.io/repos/dasfoo/rpi-gpio/badge.svg?branch=master&service=github)](https://coveralls.io/github/dasfoo/rpi-gpio?branch=master)"
michaelsvit/FIAR,48,0,2,0,User,False,82,1,0,2,False,Four In a Row CLI game,,0,7,0,0,0,0,0,0,7,0,0,1118,0,0,0,0,0,0,13,,1,,
macks22/comdetect,480,1,3,1,User,False,97,2,0,2,False,Community detection algorithm implementations.,,0,7,0,4,8,0,0,0,0,0,0,1958,0,0,0,0,0,0,41,,28,,"# comdetect
Community detection algorithm implementations."
JamesH65/GertMatrix,120,0,2,2,User,False,5,1,0,2,False,Source code to drive the GertMatrix LED add on for the Raspberry PI.,,0,6,0,0,0,0,0,0,1,0,0,2378,0,0,0,0,0,0,21,,39,,GertMatrix README.md
ahmadfahmip/syntax-validator-tbfo,56,3,1,3,User,False,31,1,0,2,False,Project for 2nd Task in Formal Language and Automata Theory class Institut Teknologi Bandung. CYK algorithms are used in this project.,,0,8,0,0,0,0,0,0,2,0,0,936,0,0,0,0,0,0,35,,27,,"# syntax-validator-tbfo
Project for 2nd Task in Formal Language and Automata Theory class Institut Teknologi Bandung. CYK algorithms are used in this project."
dlopez890/skipfish,352,0,0,0,User,False,9,2,0,1,False,Automatically exported from code.google.com/p/skipfish,,0,18,0,39,178,0,0,0,0,0,0,2952,0,0,0,0,0,0,2,,0,,
zishuiym/vim,43812,0,0,0,User,False,6781,4,3435,1,False,Automatically exported from code.google.com/p/vim,,0,15,0,77,188,0,0,0,0,0,0,3687,0,0,0,0,0,0,22,,0,,
XPila/HW,80,0,1,0,User,False,44,1,0,1,False,Hardware Library,,0,7,0,3,1,0,0,0,0,0,0,1664,0,0,0,0,0,0,20,,1,,"# HW
Hardware Library
test
test2"
tluyben/php-for-android,1672,0,0,0,User,False,8,1,1,1,False,Automatically exported from code.google.com/p/php-for-android,,0,12,0,14,4,0,0,0,0,0,0,3610,0,0,0,0,0,0,148,,26,,
j4cobgarby/raytracer,12521,0,1,1,User,False,29,1,0,1,False,"My second renderer - works similar to my last one but can render anything (as opposed to AABBs), and the camera can rotate better.",,0,9,0,4,0,0,0,0,0,0,0,824,0,0,0,0,0,0,86,,13,,"# Raytracer

I'm making a raytracer which is cooler than my previous ray _caster_, because the camera can rotate
along a pitch and yaw, and also can render anything! As opposed to the last one just being able to
render AABBs. To be fair, the last one would be reasonable to use for a game, whereas this - like any
other raytracter - wouldn't be.

## Contributors

... would be very useful. Read `CONTRIBUTING.md` for an overview of the different files, and also some
guidelines to contributing.

## Examples

![Example 1](examples/suzanne.bmp)
![Example 2](examples/out.bmp)"
stefanesser/opensource_taig,168,129,44,33,User,False,20,1,0,1,False,Lets create an open source version of the latest TaiG jailbreak.,,0,7,0,1,0,0,0,0,2,0,0,1818,0,0,0,0,0,0,13,,979,,"# opensource_taig
Lets create an open source version of the latest TaiG jailbreak.

## Status

Currently decompilation of the taig untether binary in progress.
Binary in question has the following properties:

* Filename: taig
* Size: 312672 bytes
* SHA1: 065783b31dd2016cc3d48e6b174fcc12e2dff337
* MD5: 38dd260a09690465adaf872268def218
* SHA256: df9c72d2f7be90847affdeec6e18483e985e093ead3264c962883d6ae103758b


taig binary contains a number of obfuscated strings:

* IOPMrootDomain
* IOHIDResource
* IOHIDLibUserClient
* IOHIDEventService
* IOUserClientClass
* ReportDescriptor
* ReportInterval

Components found in the untether so far:

* planetbeing's patchfinder - https://github.com/planetbeing/ios-jailbreak-patchfinder/blob/master/patchfinder.c
* libtar
* google-toolbox-for-mac - https://code.google.com/p/google-toolbox-for-mac/"
Cisteaux/gimp-dds,1662,0,0,0,User,False,190,5,16,1,False,Automatically exported from code.google.com/p/gimp-dds,,0,13,0,19,24,0,0,0,0,0,0,3694,0,0,0,0,0,0,2,,0,,
DeXP/xkb-switch-win,196,15,1,1,User,False,16,1,1,1,False,xkb-switch-lib API port to Win32/Win64,,0,6,0,0,1,0,0,0,0,0,0,2630,0,0,0,0,0,0,30,,26,,"xkb-switch-win
==============

xkb-switch-lib API port to Win32/Win64. Needed for [vim-xkbswitch plugin](http://www.vim.org/scripts/script.php?script_id=4503) (GitHub: https://github.com/lyokha/vim-xkbswitch ) .


Installation and configuring
----------------------------

In windows you need not only the plugin, but DLL-files from [latest release](https://github.com/DeXP/xkb-switch-win/releases). If you have 64-bit Vim, you need libxkbswitch64.dll. For 32-bit version use libxkbswitch32.dll. If you do not know - you can get both, plugin detects version automatically.

By default you need to put DLL-file into top directory of Vim (where vim.exe is located). Or you need to set g:XkbSwitchLib variable:

```vim
let g:XkbSwitchLib = 'c:\path\to\dll\libxkbswitch32.dll' 
```


Character maps
--------------

Currently only Russian winkeys layout translation map ('ru') is supported out of the box for vim-xkbswitch. But you can create your own layout-file, and 'charmapgen' can help you.

Charmap generator outputs current installed in system charmaps to console. The easiest way to do that - download 'charmapgen32.exe' and 'charmapgen.bat' from [/charmap](https://github.com/DeXP/xkb-switch-win/tree/master/charmap) directory. Than double click on bat-file - a new file will be appeared in current directory.

Than put 'charmap.txt' to your top directory of Vim (where vim.exe is located). You can see languages in this file. Than add needed languages to vimrc :

```vim
let g:XkbSwitchIMappingsTrData = 'charmap.txt'
let g:XkbSwitchIMappings = ['ru', 'by', 'ua']
```

You can see samples of charmap files in [/charmap](https://github.com/DeXP/xkb-switch-win/tree/master/charmap) directory.


Authors
-------

Author: Dmitry Hrabrov a.k.a. DeXPeriX


Original xkb-switch for Linux: https://github.com/ierton/xkb-switch "
Estwald/PSDK3v2,333582,27,7,20,User,False,17,1,0,1,False,"Homebrew PS3 SDK under MinGW/Win32 based in the marioga compilation, psl1ght, Tiny3D, and others libs",,0,6,0,1,0,1,0,0,0,0,0,2679,0,0,0,0,0,0,5,,30,,"PSDK3 v2
========

Basado en el trabajo de Estwald, Marioga y otros sceners y en librer�as como PSL1GHT, Tiny3D y PS3 Soundlib, PSDK3 pretende ser un entorno
estable de programaci�n de homebrew en PS3 bajo Windows, sin influencias externas que amenacen la integridad de las librer�as, ni de las 
aplicaciones que se construyen con ellas, cuando alguien decide cambiarlo todo, rompiendo la compatibilidad con las aplicaciones creadas 
con esas librer�as y provocando un claro perjuicio. Aqu� el lema es ""si algo funciona, no lo toques"" y se pueden a�adir cosas, pero no
restar, ni toquetear pijoteramente el c�digo.

Que hacer:

- Baja el ZIP (en el boton).

- Crea en raiz de C: la carpeta ""PSDK3v2"" y descomprime dentro el contenido (puede instalarse en otras unidades o directorios cambiando 
los ficheros Make*.bat, si se prefiere, a posteriori)

- Extrae el fichero MinGW.7z (yo uso IZArc) y tendr�s la carpeta ""MinGW"" con el entorno para hacer Make, etc.

- En PSDK3v2\MinGW\msys\1.0\etc edita el fichero ""profile"" y al final, donde pone ""export PS3SDK=""/c/PSDK3v2""""
cambia la ruta por la que vayas a utilizar, si quieres lanzar la consola (msys.bat) con las variables de entorno
necesarias.

- Extrae ps3dev.7z y tendr�s la carpeta ""ps3dev"" con los compiladores de PS3, las librer�as necesarias ya compiladas
y las utilidades 

Lo que contiene el proyecto:

- MinGW: entorno MinGW/MSYS montado espec�ficamente para trabajar con las herramientas de PS3.

- ps3dev: contiene los compiladores, las librer�as externas compiladas y otras herramientas de apoyo, como la scetool 
para firmar aplicaciones (en sustitucion de las de geohot), crear paquetes, as� como algunas DLL necesarias. 

- psl1ght: contiene la libreria psl1ght ya compilada

- libraries-src: contiene los c�digos fuentes de PSL1GHT, Tiny3D y PS3 Soundlib

- project: contiene los c�digos fuentes de los ejemplos y los ficheros .bat para crear los ejecutables

Cambiando la ruta de instalaci�n desde C: a otra
-------------------------------------------------

Aparte de los cambios mencionados antes en /etc/profile (solopara la consola), si editas los Make*.bat y cambias:

set PS3SDK=/F/PSDK3v2
set WIN_PS3SDK=F:/PSDK3v2

por la ruta correspondiente, bastar�.

Compilando
----------

Make_clean.bat -> Borra todos los ficheros compilados, excepto los .pkg

Make_SELF.bat -> Crea un .self firmado con las keys 3.40 (mediante scetool)

Make_EBOOT.BIN.bat -> Crea el self NPDRM EBOOT.BIN de la aplicaci�n. Esto es interesante, si se actualiza v�a FTP un .pkg 
                      ya instalado

Make_PKG.bat -> Crea el .pkg para instalar la aplicaci�n.

La ""scetool"" requiere un fichero con keys que usa una ruta relativa. Por ese motivo se ha incluido la utilidad ""fake_scetool""
para apoyarla.

ppu_rules ha sido modificado convenientemente para usar esa aplicaci�n. Por defecto, contiene las keys en ""SCETOOL_FLAGS"". Se puede
sobrecargar definiendo con SCETOOL_FLAGS := y a�adir m�s par�metros con SCETOOL_FLAGS += . Las keys 3.40 funcionan en los CFW que usamos
y por eso las he escogido.

Los Makefile de referencia los tienes en los ejemplos de project/sample. En concreto, ""fireworks3D"" es un ejemplo de utilizaci�n conjunta
de Tiny3D y la PS3 Soundlib y las librer�as necesarias

Espero que estas herramientas os sean �tiles en vuestros proyectos.

Saludos"
satya1993/jenkins,2,0,0,0,User,False,6,2,0,1,False,,,0,8,0,0,0,0,0,0,1,0,0,,0,0,0,0,0,0,4,,0,,
id-Software/Quake-III-Arena,3576,4846,307,1393,Organization,False,1,1,0,1,False,Quake III Arena GPL Source Release,,0,0,0,,,,,1,1,0,0,3057,0,0,0,0,0,0,18,0,,,
janiex/TheRQMProject,127,0,1,0,User,False,2,1,0,1,False,,,0,7,0,1,0,0,0,0,0,0,0,1132,0,0,0,0,0,0,9,,6,,"# TheRQMProject


The project contains the complete set of attributes like OSEK, ACC ADXL345 drivers, to achieve the required functionality for Ride Quality Measurement.

Create workspace, repository, project from scratch
1) Create new workspace: File--> Switch worksapce --> Other . Type in or browse the directory name of the new workspace.
2) Create new git: File-->New-->Other-->Git-->Git Repository. Browse to the same directory as above workspace.
3) Create a new project: File --> New--> CCS project:
3.1) Select the target (I use Tiva TM4C123GH6PM)
3.2) Select the connection (I use Stellaris In-Circuit Debug Interface).
3.3) Select a project template and project name (default will be Empty Project with main.c) --> finish
4) Create and push to github: 
4.1) open github for windows --> click on ""+"" --> Add tab --> the directory path of the workspace above
4.2) Commit
4.3) Publish Repository: if you're already log-in before. Then just type name and description. Check private repository if you want to make it private ($7/per month).
5) Restart CCS"
ColinGilbert/stringencoders,1456,0,0,0,User,False,414,2,0,0,False,Automatically exported from code.google.com/p/stringencoders,,0,12,0,22,20,0,0,0,0,0,0,,0,0,0,0,0,0,139,,7,,
c0untd0wn/pintos,492,0,1,0,User,False,2,2,0,1,False,Pintos Project,,0,7,0,1,0,0,0,0,0,0,0,2147,0,0,0,0,0,0,25,,19,,"pintos
======

Pintos Project"
enchev/ios-ng2-tns,24996,5,0,0,User,False,2,1,0,1,False,,,0,7,0,1,0,0,0,0,0,0,0,1293,0,0,0,0,0,0,10,,59,,"Read more about this app here: https://medium.com/@enchev/extend-your-existing-ios-app-with-angular-2-and-nativescript-c2225c9bf616#.tj7r7lez4

Read more about NativeScript and Angular2 here: https://github.com/NativeScript/NativeScript https://github.com/NativeScript/nativescript-angular"
wenshuai-xi/Document,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
mareku322/diosmios,208,0,0,0,User,False,22,2,0,0,False,Automatically exported from code.google.com/p/diosmios,,0,10,0,1,14,0,0,0,0,0,0,,0,0,0,0,0,0,1,,0,,
neutrak/accirc,393,2,3,0,User,False,225,2,2,0,False,"accidental irc, a multi-server ncurses irc client",,0,7,0,0,1,0,0,0,0,0,0,,0,0,0,0,0,0,12,,3,,
LSayhi/book-paper-note,591903,50,6,19,User,False,27,1,0,1,False,books papers notes,,0,8,0,0,0,0,0,0,0,0,0,866,0,0,0,0,0,0,18,,28,,
Jesse-V/RLAGS-USU,343053,2,3,0,User,False,104,1,0,2,False,RLAG Sensor - Space Dynamics Laboratory,,0,7,0,0,0,0,0,0,0,0,0,2210,0,0,0,0,0,0,35,,56,,
stijnmeul/thesis,358681,1,2,0,User,False,157,1,0,1,False,Identity-based encryption for online social networks. Thesis text + implementation for applying identity-based encryption on Facebook by relying on the existing Scramble interface.,,0,6,0,0,0,0,0,0,0,0,0,2308,0,0,0,0,0,0,4,,1,,"Practical Identity-based encryption for Online Social Networks
======

This is my github working directory containing all my thesis work on identity-based encryption for Online Social Networks. My thesis builds on Scramble ( https://www.cosic.esat.kuleuven.be/scramble/ ) a tool developed at COSIC, KU Leuven to broadcast encrypted status updates on Online Social Networks (OSNs).

Scramble originally relies on OpenPGP for key management such that any user with a keypair uploaded to the OpenPGP infrastructure can start receiving encrypted messages. However, since an average user is not familiar with asymmetric cryptography and all the additional steps required for uploading and generating keys to the OpenPGP infrastructure, the existing Scramble architecture is often considered complex by average social network users.

Instead of relying on OpenPGP, my thesis designs a new system relying on identity-based encryption for key management. To show the feasibility of our solution, Scramble is further extended to apply our architecture to Facebook. Facebook URLs are being used as a public key. Private keys are initialsed based on Distributed Key Generation (DKG) in order to keep them secure as long as a threshold number of Public Key Generators (PKGs) are not colluding. For more information please check out the ""thesistext"" folder containing my full thesis text or the ""pets"" folder containing an article that was submitted to HotPETS 2014.

This thesis was promoted by prof. dr. ir. Vincent Rijmen and prof. dr. ir. Bart Preneel and supervised by Filipe Beato. Thesis results were evaluated by assessors prof. dr. ir. Claudia Diaz and prof. dr. ir. Frank Piessens. The thesis was graded with 18/20 (90%) and received the Vasco Data Security thesis prize ( http://eng.kuleuven.be/evenementen/masterproefprijzen2013-2014/criteriamasterproefprijzen2014 ).

Directory structure
=====
anotherScramble:          Firefox files required for the Scramble Firefox extension.

code:                     C and C++ files required for the identity-based encryption mechanism

 code/client:             C and C++ files for clientside encryption and decryption
 
 code/cppmiracl:          Old C and C++ MIRACL files that can not be used for multithreading
 
 code/dkg:                C and C++ files to support the DKG mechanism
 
 code/htdocs-thesis:      PHP files and C++ binaries for the server side of the DKG mechanism
 
 code/miraclthread:       C and C++ MIRACL files used to support the identity-based encryption mechanism
 
 code/setupPkg:           C and C++ files to build, install and copy the correct serverside binaries to the PHP server path
 
 code/socketDemo:         A simple socket demo
 
finalpresentation:        Keynote of my final thesis presentation in June 2014

guidelines:               files describing the goals of this thesis and how the text should look like

intermediatepresentation: Keynote of my intermediate thesis presentation in December 2013

pets:                     short article that was submitted to HotPETS 2014 (although not accepted)

planning:                 Ghant chart with deadlines

scramble:                 old Firefox files for an outdated Scramble extension

securitymodel:            files describing how the attacker model looks like. See thesistext for updated version."
yubaoliu/Library,1623614,0,1,0,User,False,3,1,0,1,False,"book, ppt, pdf resources for Software Development",,0,8,0,0,0,0,0,0,0,0,0,873,0,0,0,0,0,0,102,,10,,"# Library
book, ppt, pdf resources for Software Development"
AdnanHaiderAD/MSc-Dissertation,2070000,0,1,0,User,False,258,1,0,1,False,,,0,6,0,0,0,0,0,0,0,0,0,2630,0,0,0,0,0,0,19,,2,,"MSc-Dissertation
================"
lisider/my_book,536042,22,5,21,User,False,8,1,0,1,False,喜欢的技术书籍,,0,8,0,0,0,0,0,0,0,0,0,775,0,0,0,0,0,0,50,,4,,
clhne/mybook,696858,1,1,0,User,False,22,1,0,2,False,,,0,8,0,0,0,0,0,0,0,0,0,943,0,0,0,0,0,0,805,,40,,# mybook
Gloader/Laboratory,455256,0,1,0,User,False,18,1,0,1,False,,,0,6,0,0,0,0,0,0,0,0,0,2343,0,0,0,0,0,0,4,,0,,"Laboratory
=========="
yash98/Semester3-StudyMaterial,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
olinrobotics/irl,1896713,8,15,1,Organization,False,2,9,0,2,False,"ST-R17 and UR5 Interactive Robotics Laboratory, formerly known as Edwin",,5,7,0,0,0,0,0,0,88,0,0,621,0,0,0,0,0,0,57,6,,,"### This repository is deprecated! Please check out the current repository, [hiro](https://github.com/olinrobotics/hiro).
---
# Interactive Robotics Laboratory
ST-R17 and UR5 interactive co-working dinosaur (and dragon) robotic arms
Website: https://olinrobotics.github.io/irl/
Arm Manual: http://strobotics.com/manuals/R17%20manual.pdf

## Troubleshooting

Grouped by error message

*Encoder-stepper mismatch*
* Is the area around the K11R control box and robot arm clear?
* Turn the controller on/off. (Power-cycle the robot)
* The arm generally makes a pseudo circle using its waist at the beginning of the calibration, done counterclockwise. Try starting it closer to its end point for that rotation, and then execute the startup code. The same thing could be tried for any of the joints (or axes, which is what Roboforth refers to them by).

---

*rosrun irl arm_node.py is stuck at ""in block_on_result""*
* Is the turn-key at the front of the controller set to warm?
* Is the light on the Tripp-Lite serial-usb converter blinking?
* Open a new terminal window and run the command again

---

*Controller refuses to turn on*
* Check the fuses. Two are located in the back of the controller, one is located on the power supply itself. (Where the power cord is plugged in)"
lianhongHou/books,1521857,0,1,0,User,False,26,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1335,0,0,0,0,0,0,12,,0,,
yjjing/ebooks,1567469,0,1,0,User,False,40,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1629,0,0,0,0,0,0,4,,0,,
Leon555/Books-Programming,761708,5,2,5,User,False,1,1,0,1,False,"repo for OS-related books like linux, network, system",,0,7,0,0,0,0,0,0,0,0,0,1881,0,0,0,0,0,0,40,,9,,
piaoshuai/DVD_BOOK,1917856,2,1,1,User,False,2,1,0,2,False,基于Xilinx ZYNQ 嵌入式软件硬件协同设计奖实战指南 光盘资源,,0,6,0,,,,,0,0,0,0,2448,0,0,0,0,0,0,2,,1,,"DVD_BOOK
========

基于Xilinx ZYNQ 嵌入式软件硬件协同设计奖实战指南 光盘资源
</>
git clone https://github.com/Zrobot/DVD_BOOK.git"
VhJoren/CompPhys-Exercise-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
isislovecruft/library--,1886220,391,37,57,User,False,1974,1,0,2,False,The papers and books I've read or am about to read.,,0,7,0,0,0,0,0,0,1,0,0,2308,0,0,0,0,0,0,160,,1,,
K0-0K/book,3508136,8,3,68,Organization,False,25,1,0,1,False,book,,0,7,0,,,,,0,0,0,0,2553,0,0,0,0,0,0,9,0,,,"book
====

book"
Larzdk/Learning,0,0,1,0,User,False,4,1,0,1,False,Example Code from Learning C,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,2,,0,,
klmr92/uguu,0,0,0,1,User,False,778,11,0,1,False,Automatically exported from code.google.com/p/uguu,,0,31,0,27,56,0,0,0,0,0,0,3813,0,0,0,0,0,0,2,,0,,
HanaAsakura/Juan_Aguilar_hw6,0,0,1,0,User,False,2,1,0,1,False,:),,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,12,,1,asakura09/Juan_Aguilar_hw6,"# Juan_Aguilar_hw6
:)"
yenng/RNG,0,0,1,0,User,False,3,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1699,0,0,0,0,0,0,31,,7,,"Project {#index}
===================


This is an example project."
cbyu/hello-program,0,0,1,0,User,False,3,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,4,,1,,"# hello-program
Sending my first file"
maxmiru/HelloWorld,0,0,1,0,User,False,8,1,0,1,False,This is my first github repository,,0,7,0,0,1,0,0,0,3,0,0,1678,0,0,0,0,0,0,3,,0,,"# HelloWorld
This is my first github repository

The purpose of this repository is to introduce me to the world!

Hello World
Hi Oceans

Ok i hope now everybody knows what this project is for.

This is where Master comes in, let's see if it causes any conflicts

BB

P/S: i'll add more stuffs into this project later ."
prachi-shahi/HelloWorld,0,0,1,0,User,False,2,1,0,1,False,First Repo,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,11,,2,,
reksi0/helloworld,0,0,1,0,User,False,2,1,0,1,False,program that turns other programs into helloworld apps,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,3,,0,,"# helloworld
program that turns other programs into helloworld apps"
tgut/books,586483,1,1,2,User,False,11,1,0,1,False,useful book on programming,,0,8,0,0,0,0,0,0,0,0,0,467,1,3,0,0,0,0,10,,0,,"# books
some books on computer programming
the file below are deleted for +100MB size.

./android/深入理解Android内核设计思想（带书签完整高清版）.林学森.pdf
./android/Android：卷II 邓凡平著 PDF扫描版.pdf
./android/Android系统源代码情景分析 [罗升阳著][电子工业出版社][2012.10][840页].pdf"
pietrobarbiero/hello-world,0,0,0,0,User,False,22,2,0,0,False,My first repository,,0,7,0,0,1,0,0,0,1,0,0,,0,0,0,0,0,0,17,,3,,"hello-world

This is my first repository.
I'm new on GitHub, but it's very interesting and I think I'll use it for long."
chengyihe/kernel-module-debugfs_hello,0,1,0,0,User,False,1,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,22,,5,,
wayzyaw/first_hello,0,0,1,0,User,False,1,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,1,,0,,this is the first time to use git.
nchronas/Hello-C,0,0,0,2,User,False,4,1,0,1,False,A hello world project using C with Resin.io,,0,7,0,0,0,0,0,1,0,0,0,1678,0,0,0,0,0,0,94,,11,,"# Hello C in Resin.io

This is a very simple project that is an example of how to run C code on a device that is supported by [Resin.io](http://resin.io).

### Note for Raspberry pi 1
If the device you are planning to use is a raspberry pi 1 you will have to modify Dockerfile.template in order to use the application.
```
FROM resin/%%RESIN_MACHINE_NAME%%-debian
```
To
```
FROM resin/%%RESIN_MACHINE_NAME%%-raspbian
```"
wdzhu82/npapi-chrome-plugin-helloworld-example,0,0,0,0,User,False,3,2,0,0,False,Automatically exported from code.google.com/p/npapi-chrome-plugin-helloworld-example,,0,11,0,2,0,0,0,0,0,0,0,,0,0,0,0,0,0,1,,0,,
Martyn-MSys/Hello-git,0,0,1,1,User,False,6,1,0,1,False,example project,,0,7,0,0,1,0,0,0,2,0,0,1678,0,0,0,0,0,0,2,,0,,"# Hello-git
example 

please always do the following when pushing code:

pull commit push"
yuqi1129/helloworld,0,0,1,0,User,False,2,1,0,1,False,My first project,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,126,,15,,"# helloworld
My first project"
yuzhigangcn/helloworld-test,0,0,1,0,User,False,2,1,2,1,False,a helloworld demo test,,0,7,0,0,0,0,0,0,0,0,0,1671,0,0,0,0,0,0,5,,0,,
MoMoWan/hello1,0,0,2,0,User,False,3,4,0,0,False,第一个项目,,0,7,0,1,0,0,0,1,0,0,0,,0,0,0,0,0,0,128,,0,,
VictaminC/helloworld,0,0,1,0,User,False,15,2,0,1,False,我的第一个git repository,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,3,,0,,
LimboLee/HelloWorld,0,0,1,0,User,False,2,1,0,1,False,Hello Github,,0,7,0,0,0,0,0,0,0,0,0,1671,0,0,0,0,0,0,1,,0,,"# HelloWorld
Hello Github"
yujacha/command_hello,0,0,1,0,User,False,12,2,0,1,False,test_for_learning,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,4,,0,,remote repository of test
takano32/CodeIQ-Challenge-2518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
chienhua7243/TestFile,0,0,1,0,User,False,2,1,0,1,False,test,,0,7,0,0,0,0,0,0,0,0,0,1664,0,0,0,0,0,0,1,,0,,
anontruck/Mytest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
tsubone/hello-world,0,0,1,0,User,False,2,1,0,1,False,test repository,,0,7,0,0,0,0,0,0,0,0,0,1664,0,0,0,0,0,0,12,,0,,"# hello-world
test repository"
akshit-sharma/ODM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
TEAM-RAZOR-DEVICES/device_oneplus_oneplus2_old,0,0,6,0,Organization,False,202,1,0,16,False,,,0,7,0,0,0,0,0,0,0,0,0,2196,0,0,0,0,0,0,54,1,,,
huangjiehua/xiaohua,0,0,1,0,User,False,3,1,0,1,False,我的第一个repository,,0,7,0,0,0,0,0,0,0,0,0,1790,0,0,0,0,0,0,23,,1,,"# xiaohua
我的第一个repository，吼吼"
xaviedga/Mytest,0,0,1,0,Organization,False,2,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1657,0,0,0,0,0,0,1,0,,,
HuyGemini/Array,0,1,1,2,User,False,2,1,0,1,False,G2-1. Array project in program technique class,,0,7,0,0,0,0,0,2,0,0,0,1573,0,0,0,0,0,0,5,,0,,"# Array
G2-1. Array project in program technique class"
vania-mendonca/BookFinder,758552,0,3,1,User,False,70,2,0,3,False,Project in the context of 3D Simulation and Game Programming,,0,7,0,0,0,0,0,0,0,0,0,2224,0,0,0,0,0,0,11,,16,,"PSJ_Unity
=========

INSTITUTO SUPERIOR TECNICO
Master Degree in Information Systems and Computer Engineering

Course: Three-Dimensional Simulation and Game Programming
57690 - Soraia Meneses Alarcao
64052 - Catia Dias
68202 - Vania Mendonca"
yoichiyaguchi/miru-wakate,0,0,1,0,User,False,2,2,0,1,False,Test repository for MIRU 2016 wakate-no-kai.,,0,7,0,0,0,0,0,0,0,0,0,1419,0,0,0,0,0,0,1,,0,,"miru-wakate
===

Test repository for MIRU wakate."
mohanasundari/sample_repo_1,0,0,1,0,User,False,1,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1545,0,0,0,0,0,0,1,,0,,
HyOsori/git_example,0,0,0,0,Organization,False,3,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1538,0,0,0,0,0,0,33,0,,,# example
wanksta/FirstCodeOnGitHub,0,0,1,0,User,False,2,1,0,1,False,A test project,,0,7,0,0,0,0,0,0,0,0,0,1587,0,0,0,0,0,0,3,,0,,"# FirstCodeOnGitHub
A test project"
michaelfeinberg/ListHW,0,0,1,0,User,False,2,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1342,0,0,0,0,0,0,22,,0,,# List
mju-oss-class/sample_proj1,2,2,1,50,User,False,6,1,0,1,False,,,0,9,0,0,0,0,0,1,0,0,0,201,0,0,0,0,0,0,18,,15,,# sample_proj1
yaejin91/the-c-programming-language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
CPTS224/HW4,0,0,0,3,Organization,False,1,1,0,1,False,,,0,8,0,0,0,0,0,0,0,0,0,957,0,0,0,0,0,0,9,0,,,
DevTLBB/Launch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
nandujkishor/open-web-build,1,0,1,10,User,False,2,2,0,1,False,,,0,7,0,0,0,0,0,16,0,0,0,1804,0,0,0,0,0,0,24,,9,,# open-web-build
MidweekGames/gmBASS,0,0,2,0,Organization,False,1,1,0,0,False,BASS for GameMaker,,0,7,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,4,0,,,
brunomatheusc/Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
gandarela/LLRedBlack,0,0,0,0,User,False,1,1,0,1,False,,,0,8,0,0,0,0,0,0,0,0,0,936,0,0,0,0,0,0,7,,0,JoaoPedroD/LLRedBlack,
adarsh3010/example.c,0,0,0,0,User,False,1,1,0,1,False,example.c,,0,8,0,0,0,0,0,0,0,0,0,922,0,0,0,0,0,0,4,,0,,
Git-Math/malloc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
concretecloud/chirp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
ArenMarkBoghozian/Mathematics,0,0,0,0,User,False,1,1,0,1,False,,,0,8,0,0,0,0,0,0,0,0,0,789,0,0,0,0,0,0,11,,3,ArenBoghozian8/Mathematics,
Unsonghero/home-exercise,0,0,0,0,User,False,1,1,0,0,False,c language,,0,8,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,2,,0,,
GMECE/PROGRAMS,0,0,0,0,User,False,1,1,0,1,False,C PROGRAMS,,0,8,0,0,0,0,0,0,0,0,0,894,0,0,0,0,0,0,2,,0,,
openspaceaarhus/ZABLET,22648,5,8,0,Organization,False,33,1,0,3,False,OSAAs TV-B-Gone + miniPOV clone,http://zablet.osaa.dk,0,0,0,0,0,0,0,0,0,0,0,3414,0,0,0,0,0,0,24,11,,,
zpfd3d/MyNote2,13,0,1,0,User,False,15,2,0,1,False,Just some note.,,0,8,0,,,,,0,0,0,0,1125,0,0,0,0,0,0,29,,0,,"111 2017-05-15

git remote add origin https://github.com/a379039233/MyNote2.git
git push -u origin master"
ArchSirius/inf2610,1417,0,1,2,User,False,24,1,0,1,False,Noyau d'un système d'exploitation,http://www.polymtl.ca/etudes/cours/de…,0,7,0,0,0,0,0,0,0,0,0,1713,0,0,0,0,0,0,22,,12,,
iNavFlight/inav,249989,1290,149,742,Organization,False,9689,60,66,261,False,INAV: Navigation-enabled flight control software,https://inavflight.github.io,6,22,2,301,2766,104,221,69,2697,31,215,2630,20,291,10825,7978,0,0,14,3,,,"# INAV - navigation capable flight controller

## F3 based flight controllers

> STM32 F3 flight controllers like Omnibus F3 or SP Racing F3 are deprecated and soon they will reach the end of support in INAV. If you are still using F3 boards, please migrate to F4 or F7.

![INAV](http://static.rcgroups.net/forums/attachments/6/1/0/3/7/6/a9088858-102-inav.png)
![Travis CI status](https://travis-ci.org/iNavFlight/inav.svg?branch=master)

## Features

* Runs on the most popular F4 and F7 flight controllers
* Outstanding performance out of the box
* Position Hold, Altitude Hold, Return To Home and Missions
* Excellent support for fixed wing UAVs: airplanes, flying wings 
* Fully configurable mixer that allows to run any hardware you want: multirotor, fixed wing, rovers, boats and other experimental devices
* Multiple sensor support: GPS, Pitot tube, sonar, lidar, temperature, ESC with BlHeli_32 telemetry
* SmartAudio and IRC Tramp VTX support
* DSHOT and Multishot ESCs
* Blackbox flight recorder logging
* On Screen Display (OSD) - both character and pixel style
* Telemetry: SmartPort, FPort, MAVlink, LTM
* Multi-color RGB LED Strip support
* Advanced gyro filtering: Matrix Filter and RPM filter
* Logic Conditions, Global Functions and Global Variables: you can program INAV with a GUI
* And many more!

For a list of features, changes and some discussion please review consult the releases [page](https://github.com/iNavFlight/inav/releases) and the documentation.

## Tools

### INAV Configurator

Official tool for INAV can be downloaded [here](https://github.com/iNavFlight/inav-configurator/releases). It can be run on Windows, MacOS and Linux machines and standalone application.  

### INAV Blackbox Explorer

Tool for Blackbox logs analysis is available [here](https://github.com/iNavFlight/blackbox-log-viewer/releases)

### Telemetry screen for OpenTX

Users of FrSky Taranis X9 and Q X7 can use INAV Lua Telemetry screen created by @teckel12 . Software and installation instruction are available here: [https://github.com/iNavFlight/LuaTelemetry](https://github.com/iNavFlight/LuaTelemetry)

## Installation

See: https://github.com/iNavFlight/inav/blob/master/docs/Installation.md

## Documentation, support and learning resources
* [Fixed Wing Guide](docs/INAV_Fixed_Wing_Setup_Guide.pdf)
* [Autolaunch Guide](docs/INAV_Autolaunch.pdf)
* [Modes Guide](docs/INAV_Modes.pdf)
* [Wing Tuning Masterclass](docs/INAV_Wing_Tuning_Masterclass.pdf)
* [Official documentation](https://github.com/iNavFlight/inav/tree/master/docs)
* [Official Wiki](https://github.com/iNavFlight/inav/wiki)
* [INAV Official on Telegram](https://t.me/INAVFlight)
* [INAV Official on Facebook](https://www.facebook.com/groups/INAVOfficial)
* [RC Groups Support](https://www.rcgroups.com/forums/showthread.php?2495732-Cleanflight-iNav-(navigation-rewrite)-project)
* [Video series by Painless360](https://www.youtube.com/playlist?list=PLYsWjANuAm4qdXEGFSeUhOZ10-H8YTSnH)
* [Video series by Paweł Spychalski](https://www.youtube.com/playlist?list=PLOUQ8o2_nCLloACrA6f1_daCjhqY2x0fB)

## Contributing

Contributions are welcome and encouraged.  You can contribute in many ways:

* Documentation updates and corrections.
* How-To guides - received help?  help others!
* Bug fixes.
* New features.
* Telling us your ideas and suggestions.
* Buying your hardware from this [link](https://inavflight.com/shop/u/bg/)

A good place to start is Telegram channel or Facebook group. Drop in, say hi.

Github issue tracker is a good place to search for existing issues or report a new bug/feature request:

https://github.com/iNavFlight/inav/issues

https://github.com/iNavFlight/inav-configurator/issues

Before creating new issues please check to see if there is an existing one, search first otherwise you waste peoples time when they could be coding instead!

## Developers

Please refer to the development section in the [docs/development](https://github.com/iNavFlight/inav/tree/master/docs/development) folder.


## INAV Releases
https://github.com/iNavFlight/inav/releases"
nodemcu/nodemcu-firmware,111218,6083,563,2760,Organization,False,2251,5,27,152,False,"Lua based interactive firmware for ESP8266, ESP8285 and ESP32",https://nodemcu.readthedocs.io,8,20,1,142,1744,57,51,33,1239,21,50,2049,18,61,75195,13360,0,0,5,6,,,"# NodeMCU 3.0.0

[![Join the chat at https://gitter.im/nodemcu/nodemcu-firmware](https://img.shields.io/gitter/room/badges/shields.svg)](https://gitter.im/nodemcu/nodemcu-firmware?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
[![Build Status](https://travis-ci.org/nodemcu/nodemcu-firmware.svg)](https://travis-ci.org/nodemcu/nodemcu-firmware)
[![Documentation Status](https://img.shields.io/badge/docs-master-yellow.svg?style=flat)](http://nodemcu.readthedocs.io/en/master/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE)

### A Lua based firmware for ESP8266 WiFi SOC

NodeMCU is an open source [Lua](https://www.lua.org/) based firmware for the [ESP8266 WiFi SOC from Espressif](http://espressif.com/en/products/esp8266/) and uses an on-module flash-based [SPIFFS](https://github.com/pellepl/spiffs) file system. NodeMCU is implemented in C and is layered on the [Espressif NON-OS SDK](https://github.com/espressif/ESP8266_NONOS_SDK).

The firmware was initially developed as is a companion project to the popular ESP8266-based [NodeMCU development modules]((https://github.com/nodemcu/nodemcu-devkit-v1.0)), but the project is now community-supported, and the firmware can now be run on _any_ ESP module.

# Summary

- Easy to program wireless node and/or access point
- Based on Lua 5.1.4 but without `debug`, `io`, `os` and (most of the) `math` modules
- Asynchronous event-driven programming model
- more than **65 built-in modules**
- Firmware available with or without floating point support (integer-only uses less memory)
- Up-to-date documentation at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io)

### LFS support
In July 2018 support for a Lua Flash Store (LFS) was introduced. LFS  allows Lua code and its associated constant data to be executed directly out of flash-memory; just as the firmware itself is executed. This now enables NodeMCU developers to create **Lua applications with up to 256Kb** Lua code and read-only constants executing out of flash. All of the RAM is available for read-write data!

# Programming Model

The NodeMCU programming model is similar to that of [Node.js](https://en.wikipedia.org/wiki/Node.js), only in Lua. It is asynchronous and event-driven. Many functions, therefore, have parameters for callback functions. To give you an idea what a NodeMCU program looks like study the short snippets below. For more extensive examples have a look at the [`/lua_examples`](lua_examples) folder in the repository on GitHub.

```lua
-- a simple HTTP server
srv = net.createServer(net.TCP)
srv:listen(80, function(conn)
 conn:on(""receive"", function(sck, payload)
  print(payload)
  sck:send(""HTTP/1.0 200 OK\r\nContent-Type: text/html\r\n\r\n<h1> Hello, NodeMCU.</h1>"")
 end)
 conn:on(""sent"", function(sck) sck:close() end)
end)
```
```lua
-- connect to WiFi access point
wifi.setmode(wifi.STATION)
wifi.sta.config{ssid=""SSID"", pwd=""password""}
```

# Documentation

The entire [NodeMCU documentation](https://nodemcu.readthedocs.io) is maintained right in this repository at [/docs](docs). The fact that the API documentation is maintained in the same repository as the code that *provides* the API ensures consistency between the two. With every commit the documentation is rebuilt by Read the Docs and thus transformed from terse Markdown into a nicely browsable HTML site at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io).

- How to [build the firmware](https://nodemcu.readthedocs.io/en/master/en/build/)
- How to [flash the firmware](https://nodemcu.readthedocs.io/en/master/en/flash/)
- How to [upload code and NodeMCU IDEs](https://nodemcu.readthedocs.io/en/master/en/upload/)
- API documentation for every module

# Releases

Due to the ever-growing number of modules available within NodeMCU, pre-built binaries are no longer made available. Use the automated [custom firmware build service](http://nodemcu-build.com/) to get the specific firmware configuration you need, or consult the [documentation](http://nodemcu.readthedocs.io/en/master/en/build/) for other options to build your own firmware.

This project uses two main branches, `master` and `dev`. `dev` is actively worked on and it's also where PRs should be created against. `master` thus can be considered ""stable"" even though there are no automated regression tests. The goal is to merge back to `master` roughly every 2 months. Depending on the current ""heat"" (issues, PRs) we accept changes to `dev` for 5-6 weeks and then hold back for 2-3 weeks before the next snap is completed.

A new tag is created every time `dev` is merged back to `master`. They are listed in the [releases section here on GitHub](https://github.com/nodemcu/nodemcu-firmware/releases). Tag names follow the \<SDK-version\>-master_yyyymmdd pattern.

# Support

See [https://nodemcu.readthedocs.io/en/master/en/support/](https://nodemcu.readthedocs.io/en/master/en/support/).

# License

[MIT](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE) © [zeroday](https://github.com/NodeMCU)/[nodemcu.com](http://nodemcu.com/index_en.html)"
lecram/gifenc,14,143,9,16,User,False,18,1,0,1,False,small C GIF encoder,,4,7,0,1,4,0,0,1,0,0,0,1776,0,0,0,0,0,0,27,,45,,
TadaNoButa/GTK_Glade_C_Treeview_Test,4,0,0,0,User,False,2,1,0,1,False,A test of Treeview that should update EntryBoxes (GTK/Glade/C),,0,8,0,0,0,0,0,0,0,0,0,908,0,0,0,0,0,0,2,,0,,"# GTK_Glade_C_Treeview_Test
A test of Treeview that should update EntryBoxes (GTK/Glade/C)"
alzomor/Test-Project,2,0,0,0,User,False,4,2,0,1,False,Just a test Project to pratice GitHub,,0,8,0,0,0,0,0,0,0,0,0,901,0,0,0,0,0,0,1,,0,,"# Test-Project
Just a test Project to pratice GitHub"
vasisht/fastq_splitter,5,2,1,1,User,False,5,1,0,1,False,Fast split of fastq files,,0,7,0,0,0,0,0,0,0,0,0,1363,0,0,0,0,0,0,4,,0,,
argyi2378/hello-world,2,0,0,0,User,False,4,1,0,1,False,argyi's first repository,,0,8,0,0,0,0,0,0,1,0,0,810,0,0,0,0,0,0,1,,0,,"# hello-world
argyi's first repository
you have to make changes."
fcr--/lua-matrix,130,0,1,0,User,False,34,1,0,1,False,yet another matrix library for lua without external dependencies,,0,8,0,7,10,0,0,0,0,0,0,726,0,0,0,0,0,0,45,,8,,"# lua-matrix
Yet another matrix library for lua without external dependencies

## Usage

Importing the library:
```lua
matrix = require 'matrix'
```

#### Creation

| method | description |
|--------|-------------|
| `m = matrix.new(h, w)` or<br> `m = matrix.new{h, w}` | creates a new h\*w matrix with zeros |
| `m = matrix.new{h, w, value=v}` | creates a new h\*w matrix with value v in every cell |
| `m = matrix.random(h, w)` | creates a random h\*w matrix (values between 0 and 1) |
| `m = matrix.id(n)` | creates an identity n\*n matrix |
| `m = matrix.fromtable{v1, ..., vn, rows=h, cols=w}` | creates a matrix from a table |

#### Conversion

To convert a matrix to a table: `m:totable()` which returns a lua table with indexes 1 to m.rows\*m.cols
containing the values for each cell plus the dimensions with the keys rows and cols.
Matrices are stored in column major order, which means that any h\*w matrix will be stored linearly
with the first *1* .. *h* entries containing the first column; *h+1* to *2\*h* the second one, and so on.

For instance, the following table, will be converted to `{1,2, 3,4, 5,6, cols=3, rows=2}`:

| 1 | 3 | 5 |
|---|---|---|
| 2 | 4 | 6 |

A simple way to initialize a matrix (when the values are known) is to convert a table back to a matrix
with `matrix.fromtable(t)`, where the argument must have the same format as returned by `m:totable()`.
Even though the `cols` and `rows` keys may be optional (default 1), the number of numeric items in the
table `#t` must be equal to `(t.cols or 1)*(t.rows or 1)`.

#### Slicing

In matrix jargon, slicing refers to the process of extracting a subset made of contiguous rows and
contiguous columns.  In this library both reading and writing slices is supported.

Let m be a h\*w matrix:
* `m[{}]` ← returns a copy of the array.
* `m[{1}]` ← returns a 1\*w matrix with the contents of m's first row
* `m[{{2,4}}]` ← returns a 3\*w matrix with the contents of m's second to fourth rows
* `m[{nil,2}]` ← returns a h\*1 matrix with a copy of m's second column.
* `m[{{10,19},{20,39}]` ← returns a 10\*20 matrix with the rows 10 to 19 of the columns 20 to 39 of m.

Note that returned slices are not ""views"" but full fledged matrices, copying the content of the
slice instead of sharing memory.

Slices can also be used for writing operations using a similar syntax. Let m be a h\*w matrix:
* `m[{1}] = 42` ← fills the matrix first row with fourty-twos
* `m[{nil,{2,3}}] = p` ← copies p into m's second and third columns, p must be a h\*2 matrix.

#### Operations

| element to element operation | description |
|--------|-------------|
| `m+N`, `N+m`, `m+m`, `rv+m`, `m+rw`, `cv+m`, `m+cv` | add matrix with number, matrix, row vector or column vector |
| `m-N`, `N-m`, `m-m`, `rv-m`, `m-rw`, `cv+m`, `m-cv` | substract matrix to/from number, matrix, row vector or column vector |
| `m*N`, `N*m`, `m*m`, `rv*m`, `m*rw`, `cv*m`, `m*cv` | element to element multiplication |
| `m/N`, `N/m`, `m/m`, `rv/m`, `m/rw`, `cv/m`, `m/cv` | element to element division |
| `m%N`, `N%m`, `m%m`, `rv%m`, `m%rw`, `cv%m`, `m%cv` | element to element remainder of division |
| `m^N`, `N^m`, `m^m`, `rv^m`, `m^rw`, `cv^m`, `m^cv` | element to element exponentiation |
| `-m` | negation (equivalent to `0-m`) |

| matrix operation | description |
|------------------|-------------|
| `m:t()`          | transposed matrix |
| `m:lup()`        | LU decomposition (with permutation) and determinant |
| `m:rref()`       | reduced row echelon form |
| `m:inv()`        | matrix inversion using rref |

Transpose: `m:t()`
> Returns the transposed matrix.

LUp decomposition: `lu = m:lup()`
> Returns a table containing the arrays for the LU decomposition, where:
> * `lu.L` is a lower triangular matrix with ones on its diagonal,
> * `lu.U` is an upper triangular matrix, and
> * `lu.P` is a permutation matrix (also saved as a table in lu.p, such that all lu.P's non-zero elements are located at `{i, lu.p[i]}`).
> * `lu.det` m's determinant.
>
> Note that lu.P:t():dot(lu.L:dot(lu.U)) should be approximately equal to lu ± computation errors.

Reduced Row Echelon Form: `rref, inv = m:rref()`
> This function returns the RREF of m. If m is square it also returns m's inverse. At each column this implementation chooses the row with biggest absolute value as the pivot (swapping the rows if needed) for best stability.

Matrix inversion: `inv = m:inv()`
> The m matrix must be square or an error is returned. Since the implementation uses `select(2, m:rref())` a result is returned even if it's not invertible, so (in that case) don't expect m:inv():inv() ≈ m.
> To check for invertibility use `m.cols == m.rows and m:rref()[m.cols*m.rows] == 1`.

#### Mutable Operations

The operations documented in this section change in some or other way the content of the matrices
used, and are only provided for performance reasons. Be careful if you choose to use them.

Row swap: `m:rswap(i1, i2)`
> Interchanges the contents of rows *i1* and *i2*. The arguments must be integers between 1 and `m.rows`.

Column swap: `m:cswap(j1, j2)`
> Similar to row swap, but this time the contents of the columns *j1* and *j2* get swapped. The arguments must be integers between 1 and `m.cols`.

Reshape: `m:reshape(h, w)`
> This changes the `rows` and `cols` attribute of the underlaying matrix without changing the content.
> However, the total size of the matrix must be kept the same, thus h\*w must be equal to m.rows\*m.cols.
> This can be used as a fast way to transpose a row vector to a column vector and vice versa."
medivhna/template_adaptation,2252,3,3,1,User,False,187,1,0,12,False,"N. Crosswhite, J. Byrne, O. M. Parkhi, et al. Template adaptation for face verification and identification. arXiv preprint arXiv:1603.03958, 2016.",https://arxiv.org/abs/1603.03958,0,7,0,1,0,0,0,0,0,0,0,2679,0,0,0,0,0,0,49,,19,,
Integreight/1Sheeld-Firmware,403,44,18,19,Organization,False,346,4,7,4,False,This is the source code of the firmware shipped with your 1Sheeld board. It is an implementation of a custom version of the Firmata protocol ported to ATmega162.,,0,7,0,0,0,0,0,0,2,0,0,2378,0,0,0,0,0,0,7,6,,,"# 1Sheeld Firmware [![Build Status](https://travis-ci.org/Integreight/1Sheeld-Firmware.svg?branch=master)](https://travis-ci.org/Integreight/1Sheeld-Firmware)#

## Overview ##

This is the source code of the firmware shipped with your 1Sheeld board. It is an implementation of a custom version of the Firmata protocal ported to ATmega162. It supports some of Firmata's messages like digital messages but lacks capability query, analog, I2C and servos messages.

## Building ##

The project is an generic C project, and can be built using any port of *avr-gcc* for either Microsoft Windows, Linux, or Mac OSX. Just make sure you have both *GNU Make* and *avr-gcc* tool chain installed on your platform then run ``` make ``` on the repo's root directory and both the classic and plus boards variations will be built in a new subdirectory called *build*.

If you are not going to use the *make* command, make sure you that either *PLUS_BOARD* or *CLASSIC_BOARD* is defined. (You can use the config.h file for that)

## Fuse Bits ##

- Low Value: 0xFD
- High Value: 0xD8
- Extended Value: 0xFB

Click [here](http://eleccelerator.com/fusecalc/fusecalc.php?chip=atmega162&LOW=FD&HIGH=D8&EXTENDED=FB&LOCKBIT=CC) for a description of the enabled fuse bits.

## Uploading ##

The ICSP pins are exposed with a 6-pin header on the bottom of your 1Sheeld board. You can easily connect any ATmega programmer and upload your own version of the firmware.

If you are using any [USBasp programmer](http://www.fischl.de/usbasp/) and have *avrdude* installed, after building using ``` make ``` you can flash your board by running either ``` make flashdebug ``` or ``` make flashrelease ``` on the repo's root directory. Make sure you erase the chip first by running ``` make erase ```.

Note: 1Sheeld is equipped with a bootloader that enables us to push updates to the firmware from the app. If you attempt to upload this firmware and erased the chip, you will lose this functionality.

## Documentation ##

You can generate the documentation by installing and running ``` doxygen ``` on the repo's root directory.

## Contribution ##

Contributions are welcomed, please follow this pattern:
- Fork the repo.
- Open an issue with your proposed feature or bug fix.
- Commit and push code to a new branch in your forked repo.
- Submit a pull request to our *development* branch.

Don't forget to drop us an email, post on our forum, or mention us on Twitter or Facebook about what you have built using 1Sheeld, we would love to hear about it.

## Changelog ##

To see what has changed in recent versions of 1Sheeld Firmware, see the [Change Log](CHANGELOG.md).

## License and Copyright ##

```
This code is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License version 3 only, as
published by the Free Software Foundation.

This code is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
version 3 for more details (a copy is included in the LICENSE file that
accompanied this code).

Please contact Integreight, Inc. at info@integreight.com or post on our
support forums www.1sheeld.com/forum if you need additional information
or have any questions.
```"
JamesCrook/audacity-from-svn-5GB,162042,1,0,2,User,False,13946,2,0,1,False,Automatically exported from code.google.com/p/audacity,,0,20,0,45,3,0,0,0,0,0,0,1909,1,1,3,3,0,0,12,,40,,"# Audacity
Audacity(R): A Free, Cross-Platform Digital Audio Editor. http://audacity.sourceforge.net/

This git repository is the FULL version of https://code.google.com/p/audacity/ SVN converted to git by the standard conversion tools as of 24th March 2015.  It's a snapshot of SVN and all its history as used for release of Audacity 2.1.0.  If you clone this repository you will end up with over 5GB of content.  All the branches and all the tags from SVN will be expanded.  It is  much more likely that you will want the subtree created by following this stackoverflow answer http://stackoverflow.com/questions/359424/detach-subdirectory-into-separate-git-repository/17864475#17864475 just for audacity-src/trunk which is at https://github.com/audacity/audacity.

This repository is kept here for people who (for example) want to see the old website, or mezzo, or who want to view detailed contributor history.  

We welcome feedback on Audacity, suggestions for new or improved features, 
bug reports and patches at:
  feedback@audacityteam.org .

Personal support with Audacity is not provided by e-mail, but on our Forum:
  http://audacityteam.org/forum/ .

Audacity is copyright (c) 1999-2020 by Audacity Team. This copyright notice
applies to all documents in the Audacity source code archive, except as
otherwise noted (mostly in the lib-src subdirectories).

The documentation for Audacity is licensed under the Creative Commons
Attribution 3.0 license:
http://creativecommons.org/licenses/by/3.0/legalcode .


""Audacity"" is a registered trademark of Dominic Mazzoni.

Version 2.1.0 

--------------------------------------------------------------------------------

## Licensing

This program is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the
Free Software Foundation; either version 2 of the License, or (at your
option) any later version. The program source code is also freely
available as per Section 4 of this README.

This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
License for more details.

You should have received a copy of the GNU General Public License
along with this program (in a file called LICENSE.txt); if not, go
to http://www.gnu.org/licenses/old-licenses/gpl-2.0.html or write to:

```
Free Software Foundation, Inc.
59 Temple Place - Suite 330
Boston, MA 02111-1307 USA
```

-------------------------------------------------------------------------------"
HybridAlpaca/Raptor,140439,1,2,0,Organization,False,192,2,0,1,False,Scary-Efficient Game Engine for PC,https://github.com/HybridAlpaca/Raptor,2,8,0,0,0,0,0,1,2,0,0,789,0,0,0,0,0,0,10,4,,,"![Raptor Logo](./Engine/Assets/Icons/raptor-logo.png ""Raptor Game Engine"")

# Raptor

Scary-efficient game engine for VR platforms, targeting PC and console.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

### Prerequisites

In order to begin hacking on the engine, you'll first need to set up your development enviroment with all the shiny new features of the modern digital world.  On linux systems, some quick `apt` magic should do the trick:

```
$ sudo apt-get update
$ sudo apt-get install build-essential software-properties-common -y
$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y
$ sudo apt-get update
$ sudo apt-get install g++-8 cmake
```

### Installing

Once you have your build environment set up, all the rest of the engine software and dependencies will be automagically installed once the engine is built. Just run:

```
$ ./Engine/Build/build.sh
```

And that's it!  If you find that you've messed something up, or your binaries have turned into doc-eating monsters of doom, don't panic.  Removing all bins, libs and temporaries are as simple as cleaning for a fresh build, like so:

```
$ ./Engine/Build/clean.sh
```

**Be warned**: this will clear all your build caches (for the engine source *and* all dependancies!), so running `clean` will require a full rebuild.

## Deployment

To get this working on your system, just navigate to Raptor's root directory and enter:

```
$ ./Engine/Binaries/Raptor
```

This will launch the engine in its entirety (i.e. editor, build tools, project management, etc).

## Built With

* [CMake](https://cmake.org/) - Automated build tool
* [GLFW](http://www.glfw.org/) - Dependency Management
* [Love](http://spongebob.wikia.com/wiki/Neptune%27s_Spatula) - Perfect patties are made with love, not magic

## Contributing

There currently is no code of conduct, and there isn't an official contributing markdown page either :P If you still consider your soul worthy of helping out on the project, don't hesitate to email us at `hybridalpaca[at]gmail[dot]com`.  Please, use common sense when copying that email address.

## Versioning

We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/HybridAlpaca/Raptor/tags).

## Authors

* **Seth Traman** - *Lead Developer* - [cellman123](https://github.com/cellman123)

See also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details

## Acknowledgments

* Thanks to all the folks over at GitHub who made open-source code sharing possible

* Special thanks to [PurpleBooth](https://github.com/PurpleBooth) for this snazzy Markdown template"
neftons-poker-lab/Poker_FSM,3332,0,1,1,Organization,False,2,2,1,1,False,Universal poker FSM C++,,0,6,0,5,0,0,0,0,0,0,0,2385,0,0,0,0,0,0,3,1,,,
renkun/u8glib,13393,0,0,1,User,False,951,2,78,1,False,Automatically exported from code.google.com/p/u8glib,,0,38,0,79,271,0,0,0,0,0,0,2091,0,0,0,0,0,0,15,,5,,
christophbeatty/gnome-mplayer,9445,0,0,1,User,False,2418,3,38,1,False,Automatically exported from code.google.com/p/gnome-mplayer,,0,17,0,87,648,0,0,0,0,0,0,2917,0,0,0,0,0,0,1,,0,,
PierreBougon/RayTracer,84459,1,4,0,User,False,237,1,0,4,False,RayTracer implementation in c.,http://raytracer.strikingly.com/,0,7,0,0,0,0,0,0,0,0,0,1524,0,0,0,0,0,0,24,,14,,"# RayTracer
#### 3D Engine with RayTracing technique
![Logo](assets/screenshots/logo2_0.png?raw=true ""Logo"")

RayTracing implementation in c. Use our own ini format for the scenes.

This is a school project we realized in 1 month.

100% HandMade using LibLapin 1.8 which is minimal school grafical library.

Check out the [site](http://raytracer.strikingly.com/) to see more informations and images.


## Build & Usage

* Run `make install` to install dependecies and the program.
* Run `make` if you want to compile the progrman alone.

* To run the program :
   - `./raytracer [scene.ini]`


## Engine implementation

### Features

* Objects
  - Sphere
  - Plan
  - Cube
  - Cone
  - Cylinder
  - Hyperboloid
  - Paraboloid
  - Helicoid
  - Tore (Coming soon)
  - Hole Cube (Coming soon)
* Constructive Solid Geometry
* Soft Shadows
* Anti-Aliasing
* Texture Mapping
* Reflection
* Phong
* Perlin Noise
* Brightness
* Limited Objects

### Reflection
![Engine](assets/screenshots/reflect.png?raw=true ""Reflection view"")

### Perlin noise
![Engine](assets/screenshots/perlin_noise.png?raw=true ""Perlin view"")

### Soft Shadows
![Engine](assets/screenshots/soft_shadows.png?raw=true ""Perlin view"")


## GUI Interface

* Check out the [Documentation on the site](http://raytracer.strikingly.com/) if you want to learn how to use the interface. 

### Use the interface to create your own scenes
![Interface](assets/screenshots/itfc.png?raw=true ""Interface view"")


## Rendered final scenes
![scene](assets/screenshots/glass1.png?raw=true ""Basic view"")
![scene](assets/screenshots/csg.png?raw=true ""Csg view"")



##### Project realized with Marc Brout, Benjamin Duhieu and Romain Samuel."
Helioform/d3dRTCW,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
panaC/libft,156,0,1,0,User,False,98,1,0,1,False,libft 42,,0,8,0,1,0,0,0,0,0,0,0,950,0,0,0,0,0,0,59,,12,,"# 42 - LibFT

Full C library from scratch for coding the Ansi C original library.
This is the first big project at 42 school.

The aims of this project is the construction of own's C library as i could see

the libft is sourounded by many .h header files like :

- libft.h : the main header contains about 60-70 fct.
- get_next_line : an another 42 project : only one fct.
- arg.h : my personnal arg cli handle, a fewer fct.
- matrix.h : my personal matrix manipulation fct.
- vector.h : my personal vector manipulation fct.

## Utilisation

```c
#include ""libft.h""
```

> Compile with libft.a

## Documentation

> Cf each header files for whenever i have doesn't create documentation for each

All of this repository contains more than 100 fct."
ananjaser1211/RefinedNougat,222839,8,9,4,User,False,312,6,0,18,False,Samsung Galaxy Note 4 Exynos - Nougat Kernel,,0,8,0,0,0,0,0,0,1,0,0,2441,0,0,0,0,0,0,35,,115,,
radiganm/sandbox,13020,1,1,0,User,False,104,1,0,1,False,"Short, Self Contained, Correct (Compilable), Examples",http://sscce.org,0,7,0,0,0,0,0,0,0,0,0,1097,0,0,0,0,0,0,161,,2,,
barryo/attila-libss7,220,2,1,1,User,False,57,1,0,1,False,A patched libss7 stack supporting SS7 ISUP SAM messages from Domjan Attila. Added to GitHub for a howto I wrote and for prosperity! The original SVN repository is linked in the homepage URL. See http://www.barryodonovan.com/index.php/2012/01/12/asterisk-ss7-sam-support for my tutorial.,https://observer.router.hu/repos_pub/…,0,0,0,0,0,0,0,0,0,0,0,3078,0,0,0,0,0,0,30,,32,,
facchinm/Arduino_avrusb_firmware,1867,3,6,4,User,False,13,2,0,2,False,firmwares for Arduino boards based on LUFA151115,,0,7,0,3,0,0,0,0,0,0,0,1622,0,0,0,0,0,0,133,,200,,
msikma/liballeg.4.4.2-osx,1997,2,1,0,User,False,5,1,1,1,False,Mac OS X build for Allegro (liballeg.4.4.2.dylib) plus tools and other binaries,,0,7,0,1,0,0,0,0,0,0,0,1356,0,0,0,0,0,0,124,,48,,
JvanKatwijk/swradio-8,35074,13,6,8,User,False,177,1,3,1,False,"shortwave receiver for use with sdrplay, hackrf, dabsticks and pmsdr",,0,8,0,0,1,0,0,0,0,0,0,845,1,2,34424,15067,0,0,29,,90,,"# swradio-8 [![Build Status](https://travis-ci.org/JvanKatwijk/swradio-8.svg?branch=master)](https://travis-ci.org/JvanKatwijk/swradio-8)

------------------------------------------------------------------
Introduction
------------------------------------------------------------------

swradio is  Software for Linux and windows for listening to short wave radio.
It is a rewrite and simplification of sdr-j-sw.
The software supports hackrf devices, rtlsdr devices
(using the rt820 tuner chip) and the SDRplay devices.
(For the rtlsdr based devices, use was made of a special version of the
library, the one by Oliver Jowett. Sources are included in the source tree of
this program.)

![swradio-8](/swradio-picture-1.png?raw=true)

One of the less common decoders is a *drm-decoder*, the picture shows
the reception of Nigeria. There are not that many drm transmissions,
but Kuwait, Tiganesti (i.e. Romenia), and Nigeria are receoved very well here.
The current version is limited to drm transmissions with a spectrum
equal to or less than 10 KHz.

Classical decoders are - obviously - available as well. Early evening there
is always - at least here - the ""Nederlandstalig amateurnet"" is well received.

![swradio-8](/swradio-picture-2.png?raw=true)

------------------------------------------------------------------------------
Implemented decoders
-----------------------------------------------------------------------------

*Decoders** are:
* am
* ssb, with selection for usb or lsb;
* psk, with a wide selection of modes and settings and with a visual tuning aid,
* mfsk, with a visual tuning aid,
* rtty, with a wide selection of modes and settings;
* cw, with (almost) automatic selection of speed and a visual tuning aid,
* drm, limited to 10 k bandwidth;
* amtor, with a wide selection of options;
* weatherfax decoder, with selection of a variety of settings.

![swradio-8](/swradio-picture-3.png?raw=true)

The ""main screen"" shows - next to the spectrum (or, if the widget
is touched with the right mouse button a waterfall) -the selected
frequency (in Hz), the assumed signal strength at the selected
frequency, and - if installed - a brief description of the band to
which the selected frequency belongs.

Touching a screen with the right mouse button will change the view on
the spectrum from ""classical"" spectrum view to ""waterfall"" (and back).

![swradio-8](/swradio-picture-4.png?raw=true)

This feature applies to both the display showing the full spectrum as
the display showing the ""decoder"" spectrum.

--------------------------------------------------------------------------
Using the swradio
---------------------------------------------------------------------------

If a configured device is connected to the computer where the program runs,
the device will be connected and opened. If no device is detected,
it is assumed that file input is requested and a file selection
menu will appear (input files in PCM format, with a 2 channel,
96000 samples/second configuration will be accepted.)

Most controls are on the main widget. Touching the frequency select
button will cause a keypad to be shown where the frequency can be
types in (in KHz or MHz).

One may select among a number of different filterings:
* wide, used for e.g. DRM decoding, uses the full 12 k bandwidth;
* am, used - as the name suggests - for am decoding, uses 9 k;
* usb, used for smallband decoding in the upper side band, has a width of 2500 Hz;
* lsb, used for ssb decoding is the lower sideband, has a width of 2500 Hz

The input can be written to a file, that file can be processed later on.

Frequency presets
can be stored, together with a user defined label (a program name).
A table of preferred frequencies (programs) is
maintained between program invocations.
A selected frequency can be stored by pressing the save frequency button.
If touched, one is asked to specify a name to be used to label that frequency.
The pair (name, frequency) then is added to the list.

Selecting such a ""preferred program"" is just by clicking the mouse on 
the programname or the associated field with the frequency.

Buttons and slider are equipped with a *tooltip*, touching the button or
slider will show a brief description of the function.

----------------------------------------------------------------------------
A bandplan
----------------------------------------------------------------------------

On program startup the program reads in a file "".sw-bandplan.xml""
with an xml encoded bandplan from the home directory (folder).
An example bandplan file ""sw-bandplan.xml"" is part of the source
distribution.

-------------------------------------------------------------------------------
Windows
--------------------------------------------------------------------------------

For Windows there is an installer, to be found in the releases section, setup-swradio.exe, for the swradio.
The installer will install an executable as well as the required dll's. The installer
will call the official installer for the dll implementing the api to get access to the
SDRplay device.

-------------------------------------------------------------------------------
Linux
-------------------------------------------------------------------------------
For Linux there is a description of how to create an executable, it is written
for Ubuntu, it is, however, simply to translate to scripts to be used with other
distros. 

Furthermore there is an ""appImage"", to be found in the releases section
(while the indicator at the top mentions ""failure"", the appImage is correct).
The appImage was created - using the travis service - in an Ubuntu 14 environment,
it contains the required libraries and should run on any more or less recent Linux
environment. (Note that a passwd is asked form since the software tries to install
the udev rules for the devices).


-------------------------------------------------------------------------
Using a pmSDR device and a soundcard
-------------------------------------------------------------------------

I dusted off my old pmSDR, dating from the era that the data was entering the
computer through the soundcard. Now, in that time I bought an HP Pavilion
since - according to the specs - it could deliver samples with a rate
of 192K, sufficient to do experiments with FM demodulation, which is
what I wanted to do at that time.

And indeed, samples could be read with a speed of 192k, however,
some internal filtering in the stupid computer halved the signal in bandwidth,
so receiving a signal sampled at 192k gave me a signal with a bandwidth of
less than 96k, completely useless for FM decoding (it is a time ago
but still pretty frustrated about it).

(*frustration section:*)
Even further, when sampling on 96k, the band was halved as well,
so the effective bandwidth then would only be 48k. 
Of course a solution to get 96K was to decimate (in software)
a signal sampled at 192k with a factor 2, but it was not why I bought the
stupid thing.

I'll not report on the discussion I had with the HP service desk
(bad for my health), I cannot remember to have met such unwilling (ignorant?)
people (and I can assure you that I met lots of them).
They, the HP people, (c/w)ould not confirm or falsify that the band was halved. They
claimed that they did not have a program to verify my claim, so my claim
was (""by definition"") false. Seems a little silly for a large
organization like HP. 
Of course, the programs that I wrote to show the bandwidth/samplerates were mine, not from HP, so results
I sent them (nice spectrum pictures) were ""not admissable as evidence"", so after
some talking they decided that there was no problem whatsoever, so no need to communicate
further. Needless to say that I'll never buy an HP laptop again. (*end of frustration section*)

Anyway, for using a soundcard, I had to buy an external card, an EMU-202 with
which I did all kinds of FM decoding at the time in combination with the pmSDR.
Here we need ""only"" 96k, it works well under Linux and at the time it worked on W7.
However, it does not like Windows-10, using it under W10 leads to a crash.

![swradio-8](/swradio-pmsdr-drm.png?raw=true)

------------------------------------------------------------------
Ubuntu Linux
------------------------------------------------------------------

For generating an executable under Ubuntu (16.04 or newer) one may take the
following steps.

1. Download the source tree (it is assumed that you have a git client and cmake installed.
   ```
   git clone https://github.com/JvanKatwijk/swradio-8
   ```

2. Fetch needed components
   ```
   sudo apt-get update
   sudo apt-get install qt5-qmake build-essential g++
   sudo apt-get install libsndfile1-dev qt5-default libfftw3-dev portaudio19-dev 
   sudo apt-get install zlib1g-dev libusb-1.0-0-dev mesa-common-dev
   sudo apt-get install libgl1-mesa-dev libqt5opengl5-dev libsamplerate0-dev libqwt-qt5-dev
   sudo apt-get install qtbase5-dev

   ```

3. Create the faad_drm library if you want to use the drm decoder.
   To make life easy, the sources for the faad library are included
   in the source tree (packed).

   ```
   cd ./swradio-8
   tar zxvf faad2-2.8.8.tar.gz
   cd faad2-2.8.8
   ./configure
   make
   sudo make install
   cd ..
   rm -rf faad2-2.8.8
   ```

4. Device support

  a) if you have an SDRplay device, I assume you already have installed
the library, otherwise visit ""www.sdrplay.com""  and follow the instructions.
Make sure to uncomment in swradio-8.pro the line

 CONFIG += sdrplay

  b) the sources for using the pmSDR device are part of the sourcetree. Note
that for pmSDR the cardread functions are installed. The idea is to use
either the pmSDR or the ""fast"" devices, reflected in the name. A configuration
with (only) pmSDR will be named ""swradio-pmsdr"", a configuration with (only)
fast input devices will be named ""swradio-8"".

For selecting the pmSDR, make sure to uncomment in swradio-8.pro the line

 CONFIG += pmsdr

and to comment out the lines

 #CONFIG += sdrplay
 #CONFIG += rtlsdr
 #CONFIG += hackrf

   Create a file /etc/udev/rules.d/96-pmsdr.rules with as content

 #
 # udev rules file for Microchip 18F4455 USB Micro (PMSDR)
 #
 SUBSYSTEM==""usb"", ATTRS{idVendor}==""04d8"", ATTRS{idProduct}==""000c"", MODE:=""0666""

   to ensure non-root access to the device through usb.

   c) To make life easy, the sources for the required -non-standard - rtlsdr library used are included in the source tree,
   again as a packed file.

  ```
   tar zxvf rtl-sdr.tgz
   cd rtl-sdr/
   mkdir build
   cd build
   cmake ../ -DINSTALL_UDEV_RULES=ON -DDETACH_KERNEL_DRIVER=ON
   make
   sudo make install
   rm -rf rtl-sdr
   sudo ldconfig
   cd ..
   rm -rf build
   cd ..
  ```

   Make sure that a file exists in the `/etc/udev/rules.d` directory
   describing the device, allowing ""ordinary"" users to access the device.

   d) Create a library for the hackrf device by downloading the sources and compiling

   ```
   git clone https://github.com/mossmann/hackrf
   cd hackrf
   cd host
   mkdir build
   cd build
   cmake .. -DINSTALL_UDEV_RULES=ON
   sudo make install
  ```

   Make sure that a file exists in the `/etc/udev/rules.d` directory
   describing the device, allowing ""ordinary"" users to access the device. I.e.
   add yourself to the ""plugdev"" group.
   
5. Edit the `swradio-8.pro` file for configuring the supported devices and decoders.
   For the devices

 * CONFIG += sdrplay
 * CONFIG += hackrf
 * CONFIG += rtlsdr
   or
 * CONFIG += pmsdr

   Select - or deselect - decoders:

 * CONFIG          += am-decoder
 * CONFIG          += ssb-decoder
 * CONFIG          += cw-decoder
 * CONFIG          += amtor-decoder
 * CONFIG          += psk-decoder
 * CONFIG          += rtty-decoder
 * CONFIG          += fax-decoder
 * CONFIG          += drm-decoder
 * CONFIG          += mfsk-decoder

Note that the ""faad_drm"" library is (only) needed for the drm decoder.

The ""DESTDIR"" parameter in the unix section in the "".pro"" file tells where the result is to be put.

6. Check the installation path to qwt. If you were downloading it from http://qwt.sourceforge.net/qwtinstall.html please mention the correct path in `qt-dab.pro` file (for other installation change it accordingly): 
  ```
  INCLUDEPATH += /usr/local/include  /usr/local/qwt-6.1.3
  ````

7. Build and make
  ```
  qmake qt-dab.pro
  make
  ```
Alternatively, you could use the ""cmake"" route. The file CMakeLists.txt-qt5 can be used for qt-5,
the file CMakeLists.txt-qt4 is merely used for the construction of the appImage.
The configurations here include the three mentioned ""fast"" devices.

-------------------------------------------------------------------------
--------------------------------------------------------------------------

# Copyright

 Copyright (C)  2013, 2014, 2015, 2016, 2017, 2018
 Jan van Katwijk (J.vanKatwijk@gmail.com)
 Lazy Chair Computing

 The swradio software is made available under the GPL-2.0.
 The SDR-J software, of which the sw-radio software is a part, 
 is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

# swradio-8"
escalade/LEDE,150815,14,4,2,User,False,47558,3,8,594,False,Fork of the LEDE Project (“Linux Embedded Development Environment”),,0,8,0,0,0,0,0,0,1,0,0,5920,58,1040,1318771,1235469,0,0,12,,11,,
sylcrq/libevent,348,4,2,2,User,False,24,2,2,1,False,Libevent源码学习,,0,7,0,0,0,0,0,0,0,0,0,2098,0,0,0,0,0,0,64,,37,,"libevent
========

*[libevent](http://libevent.org/)源码学习*

* [libevent 0.1](http://libevent.org/old-releases.html#0.x)

* [libevent 1.0](http://libevent.org/old-releases.html#1.0)

*开发环境*

* Ubuntu 14.04 LTS

* gcc version 4.8.2

By syl"
rryqszq4/php7-ext-jsonrpc,75,7,2,4,User,False,13,1,0,1,False,JsonRPC 2.0 client/server for PHP 7 extension,,0,7,0,0,0,0,0,0,0,0,0,1657,0,0,0,0,0,0,48,,119,,"php7-ext-jsonrpc
================

[![Build Status](https://travis-ci.org/rryqszq4/php7-ext-jsonrpc.svg?branch=master)](https://travis-ci.org/rryqszq4/php7-ext-jsonrpc)

Json-RPC 2.0 client/server for php-7 extension.

[php5.3~php5.6](https://github.com/rryqszq4/JsonRPC)

Features
--------
* JSON-RPC 2.0 protocol
* Base on curl and epoll of the multi client
* Persistent epoll in php-fpm
* Persistent curl_multi queue in php-fpm
* Support message and notifi notification
* Linux only(need to epoll)

Requirement
-----------
- PHP 7.0

Install
-------
```
$/path/to/phpize
$./configure --with-php-config=/path/to/php-config
$make && make install
```

Server
-----------
**Interface**
- Jsonrpc_Server::__construct
- Jsonrpc_Server::register
- Jsonrpc_Server::bind
- Jsonrpc_Server::jsonformat
- Jsonrpc_Server::rpcformat
- Jsonrpc_Server::executeprocedure
- Jsonrpc_Server::executecallback
- Jsonrpc_Server::executemethod
- Jsonrpc_Server::getresponse
- Jsonrpc_Server::execute

**Register Function**
```php
<?php

$server = new Jsonrpc_Server();

// style one function variable
$add1 = function($a, $b){
  return $a + $b;
};
$server->register('addition1', $add1);

// style two function string
function add2($a, $b){
  return $a + $b;
}
$server->register('addition2', 'add2');

// style three function closure
$server->register('addition3', function ($a, $b) {
    return $a + $b;
});

//style four class method string
class Api 
{
  static public function add($a, $b)
  {
    return $a + $b;
  }
}
$server->register('addition4', 'Api::add');

echo $server->execute();

//output >>>
//{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32700,""message"":""Parse error""}}

?>
```

**Bind Method**
```php
<?php

$server = new Jsonrpc_Server();

class Api
{
  static public function add($a, $b)
  {
    return $a + $b;
  }

  public function newadd($a,$b){
    return $a + $b;
  }
}

$server->bind('addition5', 'Api', 'add');

$server->bind('addition6', $a=new Api, 'newadd');

echo $server->execute();

//output >>>
//{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32700,""message"":""Parse error""}}

?>

```

Client
------------
**Interface**
- Jsonrpc_Client::__construct
- Jsonrpc_Client::call
- Jsonrpc_Client::execute

**Persistent**
> Jsonrpc_client(1) 
> When two resource epoll and curl_multi queue persist, the parame is 1. The default use of non-persistent.

**Multi Call**
```php
<?php

$client = new Jsonrpc_Client(1);
$client->call('http://localhost/server.php', 'addition1', array(3,5));
$client->call('http://localhost/server.php', 'addition2', array(10,20));

/* ... */
$result = $client->execute();

var_dump($result);

//output >>>
/*
array(2) {
  [0]=>
  array(3) {
    [""jsonrpc""]=>
    string(3) ""2.0""
    [""id""]=>
    int(110507766)
    [""result""]=>
    int(8)
  }
  [1]=>
  array(3) {
    [""jsonrpc""]=>
    string(3) ""2.0""
    [""id""]=>
    int(1559316299)
    [""result""]=>
    int(30)
  }
  ...
}
*/
?>
```
**Custom ID**
```php
<?php

$client = new Jsonrpc_client(1);
$client->call('http://localhost/server.php', 'addition', array(3,5),""custom_id_001"");
$result = $client->execute();
var_dump($result);

//output >>>
/*
array(1) {
  [0]=>
  array(3) {
    [""jsonrpc""]=>
    string(3) ""2.0""
    [""id""]=>
    string(13) ""custom_id_001""
    [""result""]=>
    int(8)
  }
}
*/
?>
```

Error Info
--------------
**jsonrpc 2.0 Error**
```javascript
// Parse error
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32700,""message"":""Parse error""}}

// Invalid Request
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32600,""message"":""Invalid Request""}}

// Method not found
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32601,""message"":""Method not found""}}

// Invalid params
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32602,""message"":""Invalid params""}}

//
```

**HTTP Error**
```javascript
// 400
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32400,""message"":""Bad Request""}}
// 401
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32401,""message"":""Unauthorized""}}
// 403
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32403,""message"":""Forbidden""}}
// 404
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32404,""message"":""Not Found""}}

// 500
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32500,""message"":""Internal Server Error""}}
// 502
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32502,""message"":""Bad Gateway""}}
...

// unknow
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32599,""message"":""HTTP Unknow""}}
```

**Curl Error**
```javascript
// 1 CURLE_UNSUPPORTED_PROTOCOL
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32001,""message"":""Curl Unsupported Protocol""}}

// 2 CURLE_FAILED_INIT
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32002,""message"":""Curl Failed Init""}}

// 3 CURLE_URL_MALFORMAT
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32003,""message"":""Curl Url Malformat""}}

// 4
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32004,""message"":""Curl Not Built In""}}

// 5 CURLE_COULDNT_RESOLVE_PROXY
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32005,""message"":""Curl Couldnt Resolve Proxy""}}

// 6 CURLE_COULDNT_RESOLVE_HOST
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32006,""message"":""Curl Couldnt Resolve Host""}}

// 7 CURLE_COULDNT_CONNECT
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32007,""message"":""Curl Couldnt Connect""}}
...

// CURL ERROR UNKNOW
{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32099,""message"":""Curl Error Unknow""}}
```"
f0rb1dd3n/cve-2010-4221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
meepingsnesroms/libretro-palmm515emu,28895,93,13,13,User,False,1014,9,1,4,False,"A new Palm OS emulator targeting compatibility, speed and accuracy in that order.",https://meepingsnesroms.github.io/,3,8,0,6,24,0,1,0,68,0,0,817,0,0,0,0,0,0,61,,31,meepingsnesroms/Mu,
essteban/ofxImageTS,10172,2,1,0,User,False,8,1,0,1,False,openFrameworks image/pixel fx library,,0,7,0,0,0,0,0,0,0,0,0,1468,0,0,0,0,0,0,37,,33,,"# ofxImageTS
![photo](ideas.png)"
svn2github/ccl,6984,2,1,0,User,False,283,1,0,0,False,"This is a clone of an SVN repository at http://ccl.googlecode.com/svn. It had been cloned by http://svn2github.com/ , but the service was since closed. Please read a closing note on my blog post: http://piotr.gabryjeluk.pl/blog:closing-svn2github . If you want to continue synchronizing this repo, look at https://github.com/gabrys/svn2github",,0,7,0,2,0,0,0,0,0,0,0,,0,0,0,0,0,0,1,,302,,
david-wang-1/ghost-mouse,6895,1,2,0,User,False,8,1,0,1,False,Final Project for Microcontrollers & Embedded Systems: Bluetooth Connected Gesture Recognition mouse,,0,7,0,0,0,0,0,0,0,0,0,1146,0,0,0,0,0,0,10,,4,,"# ghost-mouse
Finger gesture operated Bluetooth mouse. Devpost link with more details can be found here: https://devpost.com/software/ghost-mouse. "
lldavuull/PWM,3433,1,0,2,User,False,15,2,0,1,False,DMX512 for PIC16F1574,,0,7,0,1,0,0,0,0,0,0,0,1125,0,0,0,0,0,0,7,,6,,"# RDM
DMX512 for PIC16F1574

1. See video : https://www.youtube.com/watch?v=7dGFsjU8-uQ  (0:02~1:54)
Sorry for Chinese speech, this video used for self introduction to professor.

2. 0:27~0:43 is MPLAB X IDE, 'xc.h' is in the IDE. I don't use makefile, because I compile on this IDE.
MPLAB X is free, but the compiler 'xc8' isn't free. You would find xc8 crack software to install it.

3. The code extended from PIC demonstration code (DMX512 only), code is at bottom.
https://www.microchip.com/design-centers/intelligent-lighting-control/tools/lighting-communications-development-platform

4. The code used for PIC16F1575, the space is twice to pic16f1574.
circuit diagram:
![](circuit%20diagram.png)"
eggfly/LinkItProjects,2222,4,1,6,User,False,59,1,0,1,False,my linkit one and linkit assist 2502 projects,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,682,,55,,"# LinkItProjects
my linkit one and linkit assist 2502 projects
TODO: a good gitignore file."
keith-epidev/deltabot,65276,3,2,1,User,False,34,1,0,1,False,Open CNC / 3D Printer Deltabot,http://printer.epidev.com,0,6,0,0,0,0,0,0,0,0,0,2518,0,0,0,0,0,0,3,,5,,"Deltabot Project
================

This 'DeltaBot' is the product of Keith Brown's final year electronics project at [La Trobe University](http://www.latrobe.edu.au/). It falls under the umbrella project called [RepRap](http://reprap.org/). 

This repository serves as a place to store and track progress at a source level. Forcing software, hardware and designs into a single repository might not be the most practical use of git but it does allow me to easily share updates. Once the components come close to a stable release I will break this up into individual repos.

If you are interested, you can follow a brief blog at [http://printer.epidev.com](http://printer.epidev.com).

##Goals
- Cheap
- Modular
- CNC etching + 3D printing
- Produce alternative hardware, electonics and software. (Simply to learn)
- Automated Calibration

##Versions
There are three variants being produced by this project:  
1. Large robot  
2. Small, slow but strong platform  
3. Small, fast robot  

##Inspiration
Although I have not forked any current designs, this project is still based upon exiting work.
This project is primarily a derivative of the RepRap [Kossel](https://github.com/jcrocholl/kossel) by [jcrocholl](https://github.com/jcrocholl)
Of course I have also been inspired by other insane delta robots:
- [http://www.youtube.com/watch?v=Gv5B63HeF1E](http://www.youtube.com/watch?v=Gv5B63HeF1E)
- [http://www.youtube.com/watch?v=0-Kpv-ZOcKY](http://www.youtube.com/watch?v=0-Kpv-ZOcKY)
- [http://www.youtube.com/watch?v=NDzUiZsbQtw](http://www.youtube.com/watch?v=NDzUiZsbQtw)

And every other RepRap printer. But mostly The RepRap Project founder, Adrian Bowyer.
- [http://en.wikipedia.org/wiki/File:Adrian_Bowyer_-_PopTech_2007.webm](http://en.wikipedia.org/wiki/File:Adrian_Bowyer_-_PopTech_2007.webm)"
ghosoft/FlashLedSW,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
smurfjack/MJOY16-CN,10153,5,3,3,User,False,19,1,0,1,False,Joystick With Mega16,,0,7,0,0,0,0,0,0,0,0,0,1573,0,0,0,0,0,0,4,,0,,"# MJOY16-CN
����DIY�������վҡ�ˣ��Ӵ���MJOY16
MJOY16��Mindaugas Milasauskas������Ӳ����Դ����������Դ�����񻹺����о���ֵ�����߿�����MJOY����Դ����mega8Ϊ���أ����Լ������Mega16Ϊ���ص�MJOY16����Ƶúܹ淶��֧��8�����������64��������16���������أ�4������ñ�����ʺ�DIY
���������Ѿ��Ҳ����ټ��ˣ�����˹���ڴ˻����Ͽ�����MMJOY������arduino��Դ��arduino��Ȼ�����ã���������mega32�����ʺ��ֹ�DIY����������ԭ����C�������Ľ�MJOY16��������˸�������MJOY16-CN��֧��ԭ��̼�������ҵ��ʱ������Դ������Դ�������ħ���о�������������ԭ���ߵ�ԭ���·���Լ�ʹ��40pin IDE�ӿ���չ������ӵ��뷨�� ��mega16��4��IO����ȫ������������JTAG���Խӿڣ�Ӧ����˵�����Ժ�ǿ�ˣ���ȫ������Ϊavr�Ŀ���������������;��"
RockySong/micropython-rocky,82287,142,16,70,User,False,201,5,8,5,False,micropython and OpenMV port to NXP MCUs,,0,8,0,10,1,0,0,0,12,0,5,880,2,42,2521844,3975872,0,0,14,,63,,"[![Build Status](https://travis-ci.org/micropython/micropython.png?branch=master)](https://travis-ci.org/micropython/micropython) [![Coverage Status](https://coveralls.io/repos/micropython/micropython/badge.png?branch=master)](https://coveralls.io/r/micropython/micropython?branch=master)

The MicroPython and OpenMV port to NXP MCUs
=======================
<p align=""center"">
  <img src=""https://raw.githubusercontent.com/micropython/micropython/master/logo/upython-with-micro.jpg"" alt=""MicroPython Logo""/>
  <img src=""https://github.com/RockySong/micropython-rocky/blob/master/omvrt_banner.jpg"" alt=""project banner"" />
</p>

This is project is cloned from MicroPython project, which aims to put an implementation
of Python 3.x on microcontrollers and small embedded systems.
You can find the official website at [micropython.org](http://www.micropython.org).

WARNING: this project is in beta stage and is subject to changes of the
code-base, including project-wide name changes and API changes.

MicroPython implements the entire Python 3.4 syntax (including exceptions,
`with`, `yield from`, etc., and additionally `async`/`await` keywords from
Python 3.5). The following core datatypes are provided: `str` (including
basic Unicode support), `bytes`, `bytearray`, `tuple`, `list`, `dict`, `set`,
`frozenset`, `array.array`, `collections.namedtuple`, classes and instances.
Builtin modules include `sys`, `time`, and `struct`, etc. Select ports have
support for `_thread` module (multithreading). Note that only a subset of
Python 3 functionality is implemented for the data types and modules.

MicroPython can execute scripts in textual source form or from precompiled
bytecode, in both cases either from an on-device filesystem or ""frozen"" into
the MicroPython executable.

See the repository http://github.com/micropython/pyboard for the MicroPython
board (PyBoard), the officially supported reference electronic circuit board.

Major components in this repository:
- py/ -- the core Python implementation, including compiler, runtime, and
  core library.
- mpy-cross/ -- the MicroPython cross-compiler which is used to turn scripts
  into precompiled bytecode.
- ports/unix/ -- a version of MicroPython that runs on Unix.
- ports/nxp_rt1050_60/ -- a version of MicroPython that runs on the imxrtevk105x and similar
  i.mx rt1050/60 boards (using NXP's MCUXpresso SDK drivers). OpenMV is also ported to i.MX RT1050/60 devices.
- ports/nxp_lpc546/ -- a version of MicroPython that runs on the LPCXPresso54608 board and similar
  lpc54608 boards (using NXP's MCUXpresso SDK drivers).
- tests/ -- test framework and test scripts.
- docs/ -- user documentation in Sphinx reStructuredText format. Rendered
  HTML documentation is available at http://docs.micropython.org (be sure
  to select needed board/port at the bottom left corner).

Additional components:
- ports/bare-arm/ -- a bare minimum version of MicroPython for ARM MCUs. Used
  mostly to control code size.
- ports/teensy/ -- a version of MicroPython that runs on the Teensy 3.1
  (preliminary but functional).
- ports/pic16bit/ -- a version of MicroPython for 16-bit PIC microcontrollers.
- ports/cc3200/ -- a version of MicroPython that runs on the CC3200 from TI.
- ports/esp8266/ -- an experimental port for ESP8266 WiFi modules.
- extmod/ -- additional (non-core) modules implemented in C.
- tools/ -- various tools, including the pyboard.py module.
- examples/ -- a few example Python scripts.

The subdirectories above may include READMEs with additional info.

""make"" is used to build the components, or ""gmake"" on BSD-based systems.
You will also need bash, gcc, and Python (at least 2.7 or 3.3).

The Unix version
----------------

The ""unix"" port requires a standard Unix environment with gcc and GNU make.
x86 and x64 architectures are supported (i.e. x86 32- and 64-bit), as well
as ARM and MIPS. Making full-featured port to another architecture requires
writing some assembly code for the exception handling and garbage collection.
Alternatively, fallback implementation based on setjmp/longjmp can be used.

To build (see section below for required dependencies):

    $ git submodule update --init
    $ cd ports/unix
    $ make axtls
    $ make

Then to give it a try:

    $ ./micropython
    >>> list(5 * x + y for x in range(10) for y in [4, 2, 1])

Use `CTRL-D` (i.e. EOF) to exit the shell.
Learn about command-line options (in particular, how to increase heap size
which may be needed for larger applications):

    $ ./micropython --help

Run complete testsuite:

    $ make test

Unix version comes with a builtin package manager called upip, e.g.:

    $ ./micropython -m upip install micropython-pystone
    $ ./micropython -m pystone

Browse available modules on
[PyPI](https://pypi.python.org/pypi?%3Aaction=search&term=micropython).
Standard library modules come from
[micropython-lib](https://github.com/micropython/micropython-lib) project.

External dependencies
---------------------

Building MicroPython ports may require some dependencies installed.

For Unix port, `libffi` library and `pkg-config` tool are required. On
Debian/Ubuntu/Mint derivative Linux distros, install `build-essential`
(includes toolchain and make), `libffi-dev`, and `pkg-config` packages.

Other dependencies can be built together with MicroPython. This may
be required to enable extra features or capabilities, and in recent
versions of MicroPython, these may be enabled by default. To build
these additional dependencies, first fetch git submodules for them:

    $ git submodule update --init

Use the same command to get the latest versions of dependencies, as
they are updated from time to time. After that, in the port directory
(e.g. `ports/unix/`), execute:

    $ make deplibs

This will build all available dependencies (regardless whether they
are used or not). If you intend to build MicroPython with additional
options (like cross-compiling), the same set of options should be passed
to `make deplibs`. To actually enable/disable use of dependencies, edit
`ports/unix/mpconfigport.mk` file, which has inline descriptions of the options.
For example, to build SSL module (required for `upip` tool described above,
and so enabled by dfeault), `MICROPY_PY_USSL` should be set to 1.

For some ports, building required dependences is transparent, and happens
automatically. They still need to be fetched with the git submodule command
above.

The i.mx RT105x and LPC54608 version
-----------------

The ""rt1050_60"" port supports GCC toolchain (as mpy official) and KEIL.
To build under keil, just open KEIL project in ""prj_keil_<MCU series>"" folder.
To build under GCC, requires an ARM compiler, arm-none-eabi-gcc, and associated
bin-utils.  For those using Arch Linux, you need arm-none-eabi-binutils,
arm-none-eabi-gcc and arm-none-eabi-newlib packages.  Otherwise, try here:
https://launchpad.net/gcc-arm-embedded

To build:

    $ git submodule update --init
    $ cd ports/nxp_rt105
    $ make



Contributing
------------

MicroPython and OpenMV are open-source projects and welcome contributions. To be
productive, please be sure to follow the
[Contributors' Guidelines](https://github.com/micropython/micropython/wiki/ContributorGuidelines)
and the [Code Conventions](https://github.com/micropython/micropython/blob/master/CODECONVENTIONS.md).
Note that MicroPython is licenced under the MIT license, and all contributions
should follow this license."
bcomnes/fpga-lab,43404,3,1,0,User,False,27,1,1,1,False,Repo for educational labs using a Digilent Nexys 3 FPGA card,,0,6,0,0,0,0,0,0,0,0,0,2749,0,0,0,0,0,0,407,,450,,"fpga-lab
========

Repo for educational labs using a Digilent Nexys 3 FPGA card.

I'll write a real readme soon, but here is some useful information to work with in the meantime.

## Some useful Links

*   [Cosmiac Website](http://www.cosmiac.org/development.html)
    *   [Example Labs](http://www.cosmiac.org/Projects_FPGA.html#Lab1) - A bunch of labs having to do with a Nexys 2 board, but many of the labs have been updated for the Nexys 3.
*   [Xilinx University Program](http://www.xilinx.com/university/index.htm) - A program associated with the Nexys 3 board, providing educational licenses to the IDE used to program their FPGAs.
*   [Digilent Tutorials](http://www.digilentinc.com/classroom/Tutorials/) - Tutorials provided to accompany the Nexys 3 board."
Nikesh001/android_kernel_xiaomi_msm8937,809431,4,0,8,User,False,532432,3,0,5947,False,,,0,8,0,0,0,0,0,0,0,0,0,6641,0,0,0,0,0,0,37,,53,,
satanthedoge/Arduinomodule1,6794,0,0,0,User,False,27,1,0,3,False,Premier module projet,,0,8,0,0,0,0,0,0,0,0,0,943,0,0,0,0,0,0,7,,0,,
pdroaugust/waytickets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
lalalaring/AdbWide,4460,3,3,3,User,False,6,1,0,1,False,Unicode Support Adb ;Chinese path support;中文支持的adb,http://yanyuhongchen.net,0,7,0,0,0,0,0,0,0,0,0,2203,0,0,0,0,0,0,446,,24,,"AdbWide
=======

Unicode Support Adb ;Chinese path support
中文路径支持的ADB
shell也支持中文

[![Build status](https://ci.appveyor.com/api/projects/status/dpdsmak9r7q9se5y?svg=true)](https://ci.appveyor.com/project/lalalaring/adbwide)"
arii/FollowTheYellowBrickRoad,1134,0,1,2,User,False,20,1,0,2,False,16.31 class project :-),,0,7,0,0,1,0,0,0,1,0,0,1671,0,0,0,0,0,0,35,,54,,"# FollowTheYellowBrickRoad
16.31 class project :-)"
atrisovic/yadageexamples,2,0,0,0,User,False,4,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1377,0,0,0,0,0,0,42,,22,,# yadageexamples
sgfowler98/TCPServer,4,0,0,0,User,False,3,1,0,1,False,TCP Server and Client for Networks & Network Programming class,,0,8,0,0,0,0,0,0,0,0,0,831,0,0,0,0,0,0,7,,0,,"# TCPServer
TCP Server and Client for Networks &amp; Network Programming class
sgfowle - Sam Fowler

This program is a TCP server and client. The client creates a socket, connects,
and then sends data line-by-line from an input file, then closes the connection.
The server creates a socket, binds the address, listens, then when it accepts a
client it receives the data, reverses the strings, and prints the reversed strings
to stdout, then it closes the connection.

Files:
tcpserver.c
This file provides the implementation of the TCP server

tcpclient.c
This file provides the implementation of the TCP client

Makefile
This file provides compilation rules for the program

Other information:
When the total file length is greater than the server's buffer, ntohl(len) was returning
huge numbers which would make the program exit on the first line length check even though
the length was well below the max length.
When trying to investigate that, sometimes my program was appending 'time' to my first
line of output, and then later it was adding vdso_get_ to my output. I couldn't find
any relevant information about this problem and my only guess is something to do with
the OS."
SpicedEggs/decimal_turn,2,0,1,0,User,False,4,1,0,1,False,decimal turn to hex octal,,0,7,0,0,0,0,0,0,0,0,0,1328,0,0,0,0,0,0,3,,0,,
az333/31-10-17-systems,2,0,1,0,User,False,3,1,0,0,False,,,0,8,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,40,,0,,# 31-10-17-systems
5icode/mydpp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
gitpan/Net-Sharktools,220,0,4,0,Organization,False,7,1,23,0,False,Read-only release history for Net-Sharktools,http://metacpan.org/release/Net-Shark…,0,7,0,,,,,0,0,0,0,,0,0,0,0,0,0,36,2,,,
mingzhou-yang/LeetCode,196,0,1,0,User,False,5,1,0,1,False,LeetCode programming problems,,0,7,0,0,0,0,0,0,0,0,0,2035,0,0,0,0,0,0,4,,1,,
Swainstha/GPIB_To_DRAW,16009,0,0,0,User,False,4,1,0,1,False,The coordinates of waveform can be taken from oscilloscope 5400B and it was transferred to computer via a GPIB to UART converter which we built ourselves and using opengl we plotted the points in the computer screen.,,0,8,0,0,0,0,0,0,0,0,0,978,0,0,0,0,0,0,43,,9,,
kitsune-denshi/sysmon,449,1,1,0,User,False,5,1,0,1,False,USB system load monitor,,0,7,0,0,0,0,0,0,0,0,0,1601,0,0,0,0,0,0,2,,0,,"# sysmon
USB system load monitor"
shun-yo/cethping,160,0,0,0,User,False,14,1,0,1,False,L2 packet sender,,0,8,0,0,0,0,0,0,0,0,0,719,0,0,0,0,0,0,6,,3,,"# cethping

Raw socketで通信。[ethping](https://github.com/y-sira/pyng)を参考。パケットを作って文字列""Hi""を送信する。


Environment
-------

- Ubuntu14.04


Compile
-------
makeでコンパイル
```bash
make
```


Usage
-------

### Server
receiver側のインターフェース名を指定

```bash
sudo cethpingd $INTERFACE_NAME
```

### Client
接続先のMACアドレスとsender側のインターフェースを指定
```bash
sudo cethping $DESTINATION_MAC_ADDRESS $SOURCE_INTERFACE_NAME
```


Screen shot
-------
![screen shot](images/image1.png)"
JIAQIA/static_power,360,0,1,0,User,False,6,1,0,1,False,static power control,,0,6,0,0,0,0,0,0,0,0,0,2630,0,0,0,0,0,0,9,,0,,"static_power
============

static power control"
noitojp/nutil,92,1,1,1,User,False,5,1,0,1,False,,,0,6,0,0,0,0,0,0,0,0,0,2987,0,0,0,0,0,0,9,,5,,
onkar37/ROD,1422,0,0,0,User,False,10,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1356,0,0,0,0,0,0,4,,0,,
sdu-hpcl/BGSA,1192,1,9,3,Organization,False,20,2,1,2,False,A Bit-Parallel Global Sequence Alignment Toolkit for Multi-core and Many-core Architectures.,https://sdu-hpcl.github.io/BGSA/,0,7,0,0,0,0,0,0,1,0,0,1048,0,0,0,0,0,0,6,1,,,"# BGSA
BGSA is a bit-parallel global sequence alignment toolkit for multi-core and many-core architectures. BGSA
supports both Myers algorithm (including the banded version) and BitPAl in order to gain fast speed of calculating unit-cost global alignments as well as global alignments with general integer scoring schemes.

## Contents

<!-- toc -->

- [Features](#features)
- [Usage](#usage)
- [Generator](#generator)
- [Performance](#performance)
- [Use BGSA in your project](#use-bgsa-in-your-project)
- [Support or Contact](#support-or-contact)

<!-- tocstop -->

## Features
* Supports both **unit-cost global aligment([Myers algorithm](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.9395&rep=rep1&type=pdf))** and **general integer scoring global aligment([BitPAl algorithm](https://www.ncbi.nlm.nih.gov/pubmed/25075119))**.
* Supports both **global and semi-global aligment**.
* It also implements the **banded Myers algorithm** for fast verification under a given error threshold _e_;
* **Supports multiple SIMD intrinsics**: SSE, AVX2, KNC and AVX512. It also supports **heterogeneous platform** that contains KNC and CPU.
* The parallel framework can **extend to other similar algorithm easily** with a little change.
* **Super faster** than other similar softwares.

## Usage
* Step 1: Use the _generator_ program to  generate the kernel source with your specified score and architecture. And you need the Java runtime 1.7 or higher.
 ```
 cd generator
 java -jar generator.jar -M 2 -I -3 -G -5 -a sse
 ```

* Step 2: Move the generated file to the BGSA source folder accroding to your selected architecture, and then run `make` command. 
 ```
 mv generated/align_core.c ../original/BGSA_SSE/
 cd ../original/BGSA_SSE/
 make
 ```
 The default compiler is icc and CPU platform. You can pass `arch=KNL` for KNL platform and `cc=gcc` for other compilers.
 ```
 # use gcc compiler on KNL platform
 make arch=KNL cc=gcc
 ```

* Step 3: Run a test.
 ```
 ./aligner -q sample-data/query.txt -d sample-data/subject.txt -f data/result.txt
 ```

* Step 4: Convert the result file to readable format.
 ```
 ./convert -r data/result.txt
 ```

## Generator
We provide a code generator written in Java to quickly generate algorithm kernel source for a wide variety of devices as well as different scoring schemes. The following are the parameters of the generator module:

```
-M,--match <arg>:             Specify the match score which should be positive or zero. Default is 2.

-I,--mismatch <arg>:          Specify the mismatch score which should be negative. Default is -3.

-G,--gap <arg>:               Specify the gap score which should be negative. Default is -5.

-d,--directory <arg>:         Specify the directory where to place generated source files.

-n,--name <arg>:              Set the name of generated source file.

-t,--type <arg>:              Set BitPAl algorithm type. Valid values are: non-packed, packed. Default is packed.

-m,--myers:                   Using Myers' algorithm. Valid values are: 0, 1. 0 presents weights(0, -1, -1) and 1 presents weights(0, 1, 1). Default is 0.

-a,--arch:                    Specify the SIMD architecture. Valid values are: none, sse, avx2, knc, avx512. Default is sse. If you want generate kernel source for multiple architectures, you can use '-' to join them as none-sse-avx2

-e,--element:                 Specify vector element size. Valid value is 64, 32, 16, 8. Default is 32.

-s,--semi:                    Using semi-global algorithm.

-b,--banded:                  Using banded myers algorithm.

-h,--help:                    Display help Information.
```

## Performance

The following figures show the performance comparison of BGSA, Parasail and SeqAn on CPU and Xeon Phi platforms for unit scoring scheme. From the figures, we can see that the performance of BGSA is much faster than that of the other two algorithms. For more performance evluations, you can see our paper(under submission). And you can download test data from [this link](https://pan.baidu.com/s/1JFmfIYzOBH_TK9V_4IFFiw).

![](images/cpu.png)
> Comparison on E5-2620 and W-2123

![](images/knl.png)
> Comparison on Phi-7110 and Phi-7210

## Use BGSA in your project
You can use BGSA in your project by directly copying header and source files. For simplicity, you can first save the sequences to be compared into temporary files, and then call BGSA to read sequences from the temporary files, calculate alignment scores and save them to a result file, and finally your program can read the scores from the result file. If you want to use sequences already stored in an array, you can modify the logic in calling `get_read_from_file` and `get_ref_from_file` and change the pointer to point the existing sequence array. 

If you just want to use the kernel alignment method, you need to process your data into the format required by this method. The following is a complete demo using SSE intrinsics:

> demo.c

```c
#include <stdio.h>
#include <string.h>
#include ""align_core.h""

int cpu_threads = 1;

void align(char *query, char **subjects, int subject_count) {
    int query_len = strlen(query);
    int subject_len = strlen(subjects[0]);
    int real_subject_count = subject_count;
    int total_subject_count = real_subject_count;
    if (real_subject_count % SSE_V_NUM != 0) {
        total_subject_count = (real_subject_count / SSE_V_NUM + 1) * SSE_V_NUM;
    }
    seq_t subject_seq;
    subject_seq.content = malloc_mem(sizeof(char) * total_subject_count * (subject_len + 1));
    subject_seq.count = real_subject_count;
    subject_seq.len = subject_len;
    for (int i = 0; i < total_subject_count; i++) {
        for (int j = 0; j < subject_len + 1; j++) {
            subject_seq.content[i * (subject_len + 1) + j] = subjects[i][j];
        }
    }

    int word_num;
    if (full_bits) {
        word_num = (subject_len + SSE_WORD_SIZE) / (SSE_WORD_SIZE - 1);
    } else {
        word_num = (subject_len + SSE_WORD_SIZE - 1) / (SSE_WORD_SIZE - 2);
    }
    int pre_process_size = sizeof(sse_read_t) * word_num * CHAR_NUM * real_subject_count;
    sse_read_t *pre_precess_subjects = malloc_mem(pre_process_size);
    init_mapping_table();
    sse_handle_reads(&subject_seq, pre_precess_subjects, word_num, 0, total_subject_count);

    char *pre_precess_query = malloc_mem(sizeof(char) * query_len);
    for (int i = 0; i < query_len; i++) {
        pre_precess_query[i] = mapping_table[query[i]];
    }

    sse_write_t *results = malloc_mem(sizeof(sse_write_t) * total_subject_count);
    __m128i *bit_mem = malloc_mem(sizeof(__m128i) * word_num * dvdh_len);
    align_sse(pre_precess_query, pre_precess_subjects, query_len, subject_len, word_num, 1, 0, results, bit_mem);
    for (int i = 0; i < real_subject_count; i++) {
        printf(""%d\n"", results[i]);
    }
    free_mem(subject_seq.content);
    free_mem(pre_precess_subjects);
    free_mem(pre_precess_query);
    free_mem(results);
    free_mem(bit_mem);
}

int main() {
    char *query = ""AAAA"";
    char *subjects[4] = {""AAAA"", ""AACA"", ""CAAC"", ""AGGG""};
    align(query, subjects, 4);
    return 0;
}
```
You can use the following command to compile demo.c .
```bash
icc -o demo demo.c global.c align_core.c -qopenmp
```

## Support or Contact

If you have any questions, please contact: Weiguo,Liu ( weiguo.liu@sdu.edu.cn)."
aignacio/esp_rgb,620,0,0,0,User,False,1,1,0,0,False,esp code for rgb lights,,0,7,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,77,,25,,"**esp_mqtt**
==========
![](https://travis-ci.org/tuanpmt/esp_mqtt.svg?branch=master)

This is MQTT client library for ESP8266, port from: [MQTT client library for Contiki](https://github.com/esar/contiki-mqtt) (thanks)



**Features:**

 * Support subscribing, publishing, authentication, will messages, keep alive pings and all 3 QoS levels (it should be a fully functional client).
 * Support multiple connection (to multiple hosts).
 * Support SSL connection (sdk 1.3 with path)
 * Easy to setup and use
 * Update support SDK 1.3

**Compile:**

Make sure to add PYTHON PATH and compile PATH to Eclipse environment variable if using Eclipse

for Windows:

```bash
git clone --recursive https://github.com/tuanpmt/esp_mqtt
cd esp_mqtt
#clean
mingw32-make clean
#make
mingw32-make SDK_BASE=""c:/Espressif/ESP8266_SDK"" FLAVOR=""release"" all
#flash
mingw32-make ESPPORT=""COM1"" flash
```

for Mac or Linux:

```bash
git clone --recursive https://github.com/tuanpmt/esp_mqtt
cd esp_mqtt
#clean
make clean
#make
make SDK_BASE=""/opt/Espressif/ESP8266_SDK"" FLAVOR=""release"" all
#flash
make ESPPORT=""/dev/ttyUSB0"" flash
```

**Usage**
```c
#include ""ets_sys.h""
#include ""driver/uart.h""
#include ""osapi.h""
#include ""mqtt.h""
#include ""wifi.h""
#include ""config.h""
#include ""debug.h""
#include ""gpio.h""
#include ""user_interface.h""
#include ""mem.h""

MQTT_Client mqttClient;

void wifiConnectCb(uint8_t status)
{
 if(status == STATION_GOT_IP){
  MQTT_Connect(&mqttClient);
 } else {
  MQTT_Disconnect(&mqttClient);
 }
}
void mqttConnectedCb(uint32_t *args)
{
 MQTT_Client* client = (MQTT_Client*)args;
 INFO(""MQTT: Connected\r\n"");
 MQTT_Subscribe(client, ""/mqtt/topic/0"", 0);
 MQTT_Subscribe(client, ""/mqtt/topic/1"", 1);
 MQTT_Subscribe(client, ""/mqtt/topic/2"", 2);

 MQTT_Publish(client, ""/mqtt/topic/0"", ""hello0"", 6, 0, 0);
 MQTT_Publish(client, ""/mqtt/topic/1"", ""hello1"", 6, 1, 0);
 MQTT_Publish(client, ""/mqtt/topic/2"", ""hello2"", 6, 2, 0);

}

void mqttDisconnectedCb(uint32_t *args)
{
 MQTT_Client* client = (MQTT_Client*)args;
 INFO(""MQTT: Disconnected\r\n"");
}

void mqttPublishedCb(uint32_t *args)
{
 MQTT_Client* client = (MQTT_Client*)args;
 INFO(""MQTT: Published\r\n"");
}

void mqttDataCb(uint32_t *args, const char* topic, uint32_t topic_len, const char *data, uint32_t data_len)
{
 char *topicBuf = (char*)os_zalloc(topic_len+1),
   *dataBuf = (char*)os_zalloc(data_len+1);

 MQTT_Client* client = (MQTT_Client*)args;

 os_memcpy(topicBuf, topic, topic_len);
 topicBuf[topic_len] = 0;

 os_memcpy(dataBuf, data, data_len);
 dataBuf[data_len] = 0;

 INFO(""Receive topic: %s, data: %s \r\n"", topicBuf, dataBuf);
 os_free(topicBuf);
 os_free(dataBuf);
}


void user_init(void)
{
 uart_init(BIT_RATE_115200, BIT_RATE_115200);
 os_delay_us(1000000);

 CFG_Load();

 MQTT_InitConnection(&mqttClient, sysCfg.mqtt_host, sysCfg.mqtt_port, sysCfg.security);
 //MQTT_InitConnection(&mqttClient, ""192.168.11.122"", 1880, 0);

 MQTT_InitClient(&mqttClient, sysCfg.device_id, sysCfg.mqtt_user, sysCfg.mqtt_pass, sysCfg.mqtt_keepalive, 1);
 //MQTT_InitClient(&mqttClient, ""client_id"", ""user"", ""pass"", 120, 1);

 MQTT_InitLWT(&mqttClient, ""/lwt"", ""offline"", 0, 0);
 MQTT_OnConnected(&mqttClient, mqttConnectedCb);
 MQTT_OnDisconnected(&mqttClient, mqttDisconnectedCb);
 MQTT_OnPublished(&mqttClient, mqttPublishedCb);
 MQTT_OnData(&mqttClient, mqttDataCb);

 WIFI_Connect(sysCfg.sta_ssid, sysCfg.sta_pwd, wifiConnectCb);

 INFO(""\r\nSystem started ...\r\n"");
}

```

**Publish message and Subscribe**

```c
/* TRUE if success */
BOOL MQTT_Subscribe(MQTT_Client *client, char* topic, uint8_t qos);

BOOL MQTT_Publish(MQTT_Client *client, const char* topic, const char* data, int data_length, int qos, int retain);

```

**Already support LWT: (Last Will and Testament)**

```c

/* Broker will publish a message with qos = 0, retain = 0, data = ""offline"" to topic ""/lwt"" if client don't send keepalive packet */
MQTT_InitLWT(&mqttClient, ""/lwt"", ""offline"", 0, 0);

```

#Default configuration

See: **include/user_config.h**

If you want to load new default configurations, just change the value of CFG_HOLDER in **include/user_config.h**

**Define protocol name in include/user_config.h**

```c
#define PROTOCOL_NAMEv31 /*MQTT version 3.1 compatible with Mosquitto v0.15*/
//PROTOCOL_NAMEv311   /*MQTT version 3.11 compatible with https://eclipse.org/paho/clients/testing/*/
```

In the Makefile, it will erase section hold the user configuration at 0x3C000

```bash
flash: firmware/0x00000.bin firmware/0x40000.bin
 $(PYTHON) $(ESPTOOL) -p $(ESPPORT) write_flash 0x00000 firmware/0x00000.bin 0x3C000 $(BLANKER) 0x40000 firmware/0x40000.bin 
```
The BLANKER is the blank.bin file you find in your SDKs bin folder.

**Create SSL Self sign**

```
openssl req -x509 -newkey rsa:1024 -keyout key.pem -out cert.pem -days XXX
```

**SSL Mqtt broker for test**

```javascript
var mosca = require('mosca')
var SECURE_KEY = __dirname + '/key.pem';
var SECURE_CERT = __dirname + '/cert.pem';
var ascoltatore = {
  //using ascoltatore
  type: 'mongo',
  url: 'mongodb://localhost:27017/mqtt',
  pubsubCollection: 'ascoltatori',
  mongo: {}
};

var moscaSettings = {
  port: 1880,
  stats: false,
  backend: ascoltatore,
  persistence: {
    factory: mosca.persistence.Mongo,
    url: 'mongodb://localhost:27017/mqtt'
  },
  secure : {
    keyPath: SECURE_KEY,
    certPath: SECURE_CERT,
    port: 1883
  }
};

var server = new mosca.Server(moscaSettings);
server.on('ready', setup);

server.on('clientConnected', function(client) {
    console.log('client connected', client.id);
});

// fired when a message is received
server.on('published', function(packet, client) {
  console.log('Published', packet.payload);
});

// fired when the mqtt server is ready
function setup() {
  console.log('Mosca server is up and running')
}
```

**Example projects using esp_mqtt:**<br/>
- [https://github.com/eadf/esp_mqtt_lcd](https://github.com/eadf/esp_mqtt_lcd)

**Limited:**<br/>
- Not fully supported retransmit for QoS1 and QoS2

**Status:** *Pre release.*

[https://github.com/tuanpmt/esp_mqtt/releases](https://github.com/tuanpmt/esp_mqtt/releases)

[MQTT Broker for test](https://github.com/mcollina/mosca)

[MQTT Client for test](https://chrome.google.com/webstore/detail/mqttlens/hemojaaeigabkbcookmlgmdigohjobjm?hl=en)

**Contributing:**

***Feel free to contribute to the project in any way you like!***

**Requried:**

SDK esp_iot_sdk_v0.9.4_14_12_19 or higher

**Authors:**
[Tuan PM](https://twitter.com/TuanPMT)



**LICENSE - ""MIT License""**

Copyright (c) 2014-2015 Tuan PM, https://twitter.com/TuanPMT

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
x42/convoLV2,182,16,8,4,User,False,159,3,25,3,False,LV2 convolution plugin,http://x42-plugins.com/x42/x42-convolver,0,6,0,0,5,0,1,0,2,0,0,2854,1,3,23,14,0,0,109,,277,,"convoLV2
========

convoLV2 is a [LV2](http://lv2plug.in) plugin to convolve audio signals with
zero latency.

It is a very basic plugin: Currently the only parameter is the Impulse-Response file
and hence it is robust and efficient convolver.


The plugin comes in three variants:
*   Mono:  1 channel in, 1 channel out. Mono IR file
*   Mono To Stereo:  1 channel in, 2 channel out. Stereo IR file. (L, R)
*   True Stereo: 2 in, 2 out.  4 channel IR file (L -> L, R -> R, L -> R, R -> L)

Excess channels in an IR file are ignored. If an IR file has insufficient channels
for the required configuration, channel-assignment wraps around (modulo file channel count).

convoLV2's main use-case is cabinet-emulation and generic signal processing where latency matters.

For fancy reverb applications, see also [IR.lv2](https://tomszilagyi.github.io/plugins/ir.lv2/)

Install
-------

```bash
make
sudo make install PREFIX=/usr

# Test in jalv LV2 host
jalv.gtk http://gareus.org/oss/lv2/convoLV2#Mono
# or
jalv.gtk http://gareus.org/oss/lv2/convoLV2#MonoToStereo
# or
jalv.gtk http://gareus.org/oss/lv2/convoLV2#Stereo
```


Note to packagers: The Makefile honors `PREFIX` and `DESTDIR` variables as well
as `CFLAGS`, `LDFLAGS` and `OPTIMIZATIONS` (additions to `CFLAGS`), also
see the first 10 lines of the Makefile.
You really want to package the superset of [x42-plugins](https://github.com/x42/x42-plugins).


Under the hood
--------------

[libzita-convolver](http://kokkinizita.linuxaudio.org/linuxaudio/downloads/) is used to
perform the convolution, [libsndfile](http://www.mega-nerd.com/libsndfile/) to read
the impulse-response and [libsamplerate](http://www.mega-nerd.com/SRC/) to resample
the IR if necessary.

convoLV2 was written to demonstrate new features of LV2 1.2.0 (back in 2012):

*   http://lv2plug.in/ns/ext/buf-size/#powerOf2BlockLength - the plugin requires a blocksize that is a power of two.
*   http://lv2plug.in/ns/ext/buf-size/#maxBlockLength - the plugin only works with blocksizes between 64 and 8192 samples per period.
*   http://lv2plug.in/ns/ext/patch/ - allow a host to pass filenames to a plugin.
*   http://lv2plug.in/ns/ext/worker/ - on/offline instances. Re-loading an IR file is performed in the background, making the plugin realtime safe.

It since serves as example code for those LV2 extensions.


While the convolution engine supports pre-delay, channel-mapping and per-channel gain settings, these parameters
are currently not exposed in the LV2 interface (hack tip: they are supported in the LV2 DSP and saved as
text in the plugin-state which can be directly edited)."
FluidSynth/fluidsynth,6890,606,40,111,Organization,False,2108,8,29,38,False,Software synthesizer based on the SoundFont 2 specifications,http://www.fluidsynth.org,12,10,2,16,337,5,23,1,299,1,25,6305,7,82,2378,1868,0,0,1,2,,,"# FluidSynth

| | Build Status |
|---|---|
| <img src=""https://www.kernel.org/theme/images/logos/tux.png"" height=""30"" alt=""""> **Linux** | [![Build Status Travis](https://travis-ci.org/FluidSynth/fluidsynth.svg?branch=master)](https://travis-ci.org/FluidSynth/fluidsynth/branches) |
| <img src=""https://cdn.pling.com/img//hive/content-pre1/112422-1.png"" height=""25"" alt=""""> **FreeBSD** | [![Build Status](https://api.cirrus-ci.com/github/FluidSynth/fluidsynth.svg?branch=master)](https://cirrus-ci.com/github/FluidSynth/fluidsynth) |
| <img src=""https://www.microsoft.com/windows/favicon.ico"" height=""25"" alt=""""> **Windows** | [![Build Status](https://dev.azure.com/tommbrt/tommbrt/_apis/build/status/FluidSynth.fluidsynth.Win?branchName=master)](https://dev.azure.com/tommbrt/tommbrt/_build/latest?definitionId=3&branchName=master) |
| <img src=""https://www.microsoft.com/windows/favicon.ico"" height=""25"" alt=""""> **Windows (vcpkg)** | [![Build status](https://ci.appveyor.com/api/projects/status/anbmtebt5uk4q1it/branch/master?svg=true)](https://ci.appveyor.com/project/derselbst/fluidsynth-g2ouw/branch/master) |
| <img src=""https://www.apple.com/favicon.ico"" height=""30"" alt=""""> **MacOSX** | [![Build Status](https://dev.azure.com/tommbrt/tommbrt/_apis/build/status/FluidSynth.fluidsynth.macOS?branchName=master)](https://dev.azure.com/tommbrt/tommbrt/_build/latest?definitionId=5&branchName=master) |
| <img src=""https://www.android.com/favicon.ico"" height=""30"" alt=""""> **Android** | [![CircleCI](https://circleci.com/gh/FluidSynth/fluidsynth/tree/master.svg?style=shield)](https://circleci.com/gh/FluidSynth/fluidsynth) |



#### FluidSynth is a cross-platform, real-time software synthesizer based on the Soundfont 2 specification.

FluidSynth generates audio by reading and handling MIDI events from MIDI input devices by using a [SoundFont](https://github.com/FluidSynth/fluidsynth/wiki/SoundFont). It is the software analogue of a MIDI synthesizer. FluidSynth can also play MIDI files.

[![OHLOH Project Stats](https://www.openhub.net/p/fluidsynth/widgets/project_thin_badge?format=gif)](https://www.openhub.net/p/fluidsynth)

## Documentation

The central place for documentation and further links is our **wiki** here at GitHub:

#### https://github.com/FluidSynth/fluidsynth/wiki

If you are missing parts of the documentation, let us know by writing to our mailing list.
Of course, you are welcome to edit and improve the wiki yourself. All you need is an account at GitHub. Alternatively, you may send an EMail to our mailing list along with your suggested changes. Further information about the mailing list is available in the wiki as well.

Latest information about FluidSynth is also available on the web site at http://www.fluidsynth.org/.

## License

The source code for FluidSynth is distributed under the terms of the [GNU Lesser General Public License](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html), see the [LICENSE](https://github.com/FluidSynth/fluidsynth/blob/master/LICENSE) file. To better understand the conditions how FluidSynth can be used in e.g. commercial or closed-source projects, please refer to the [LicensingFAQ in our wiki](https://github.com/FluidSynth/fluidsynth/wiki/LicensingFAQ).

## Building from source

For information on how to build FluidSynth from source, please [refer to our wiki](https://github.com/FluidSynth/fluidsynth/wiki/BuildingWithCMake).

## Links

- FluidSynth's Home Page, http://www.fluidsynth.org

- FluidSynth's wiki, https://github.com/FluidSynth/fluidsynth/wiki

- FluidSynth's API documentation, http://www.fluidsynth.org/api/

---

## Historical background

### Why did we do it

The synthesizer grew out of a project, started by Samuel Bianchini and
Peter Hanappe, and later joined by Johnathan Lee, that aimed at
developing a networked multi-user game.

Sound (and music) was considered a very important part of the game. In
addition, users had to be able to extend the game with their own
sounds and images. Johnathan Lee proposed to use the Soundfont
standard combined with intelligent use of midifiles. The arguments
were:

- Wavetable synthesis is low on CPU usage, it is intuitive and it can
  produce rich sounds

- Hardware acceleration is possible if the user owns a Soundfont
  compatible soundcard (important for games!)

- MIDI files are small and Soundfont2 files can be made small thru the
  intelligent use of loops and wavetables. Together, they are easier to
  downloaded than MP3 or audio files.

- Graphical editors are available for both file format: various
  Soundfont editors are available on PC and on Linux (Smurf!), and
  MIDI sequencers are available on all platforms.

It seemed like a good combination to use for an (online) game. 

In order to make Soundfonts available on all platforms (Linux, Mac,
and Windows) and for all sound cards, we needed a software Soundfont
synthesizer. That is why we developed FluidSynth.

### Design decisions

The synthesizer was designed to be as self-contained as possible for
several reasons:

- It had to be multi-platform (Linux, macOS, Win32). It was therefore
  important that the code didn't rely on any platform-specific
  library.

- It had to be easy to integrate the synthesizer modules in various
  environments, as a plugin or as a dynamically loadable object. I
  wanted to make the synthesizer available as a plugin (jMax, LADSPA,
  Xmms, WinAmp, Director, ...); develop language bindings (Python,
  Java, Perl, ...); and integrate it into (game) frameworks (Crystal
  Space, SDL, ...). For these reasons I've decided it would be easiest
  if the project stayed very focused on its goal (a Soundfont
  synthesizer), stayed small (ideally one file) and didn't dependent
  on external code."
vitasdk/samples,535,137,22,50,Organization,False,75,3,0,23,False,Sample code.,,0,7,0,2,10,1,1,2,41,0,1,1727,1,1,3,0,0,0,13,3,,,"# vitasdk code samples

## Prerequisites

In order to build a vita sample, you need to add the toolchain `bin/` directory to your `$PATH`.

## Building

Every samples directory should include a CMake list.
To build a sample, place yourself into this directory and use the `cmake . && make` command to build it.

## Running

To run a sample:
- Send the generated `.vpk` to your vita:
 - Start an FTP server on your vita (for example, with VitaShell - by pressing the select button).
 - Upload the `.vpk` to the Vita using your FTP client (for example, with Curl - with `curl -T *.vpk ftp://YOUR_VITA_IP:1337/ux0:/`)
 - If Curl returns `fatal: No names found, cannot describe anything` it mean that you are trying to overwrite a folder with a file. Add a `/` to the end of your url to explain that you want to upload IN this folder.
- Install the `.vpk` on your vita using a vpk installer (for example, with VitaShell - by pressing the X button on the `.vpk`)
- This will create a new folder in the `ux0:/app/` for the sample.

## Building everything

Use the following command to build every samples:

```
mkdir build && cd build
cmake ..
make
```

## List of samples

* `audio`: Simple audio wave generator.
* `camera`: Demonstration of camera features.
* `common`: Common functions for samples.
* `ctrl`: A minimal controller (button) sample.
* `debug_print`: A minimal debug print sample.
* `debugscreen`: Debug text printing sample.
* `hello_cpp_world`: A minimal hello world sample in C++.
* `hello_world`: A minimal hello world sample.
* `ime`: Graphical dialog sample.
* `microphone`: Demonstration of microphone features.
* `motion`: Prints accelerometer data.
* `net_http`: A minimal HTTP download sample.
* `net_http_bsd`: A minimal HTTP download sample using BSD sockets.
* `net_libcurl`: A libcurl download sample.
* `power`: A minimal power sample.
* `pretty_livearea`: A minimal hello world sample with example livearea styling and features.
* `prx_loader`: Load/list prx modules.
* `prx_simple`: Minimal sample prx module.
* `redrectangle`: Example SDL rendering.
* `rtc`: A minimal RTC sample.
* `socket_ping`: ICMP ping using raw sockets.
* `soloud`: Plays an audio file and Text To Speech.
* `touch`: A minimal touch sample.

## Notes on images
- Images shall use indexed palettes (PNG-8 128 Dithered).
- The size of an image shall not exceed 420KB.
- For some reasons, some PNG files created by GIMP makes the .vpk installation crash.
- You can further minimize overhead by running your images through [pngquant](https://pngquant.org/).

## Notes on supporting files and folders
- File names shall not exceed 32B.
- Directory names shall not exceed 16B.
- Folder creation shall not exceed one level.

## Notes on XML
- UTF-8 character encoding, CRLF line termination.
- File size shall not exceed 32KB.
- Different visual styles are available, check the sample `pretty_livearea` for an example.

## License

All code and build scripts in this repo is licensed under the terms of [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)."
drhelius/Gearboy,56093,437,34,101,User,False,1399,2,24,21,False,"Game Boy / Gameboy Color emulator for iOS, macOS, Raspberry Pi, Windows, Linux and RetroArch.",http://twitter.com/drhelius,11,5,0,6,72,4,16,0,39,0,2,2875,1,234,243506,176853,0,0,42,,107,,"# Gearboy

[![Build Status](https://travis-ci.org/drhelius/Gearboy.svg?branch=master)](https://travis-ci.org/drhelius/Gearboy)

Gearboy is a cross-platform Game Boy / GameBoy Color emulator written in C++ that runs on Windows, macOS, Linux, iOS, Raspberry Pi and RetroArch.

This is an open source project with its ongoing development made possible thanks to the support by these awesome [backers](backers.md).

Please, consider [sponsoring](https://github.com/sponsors/drhelius) and following me on [Twitter](https://twitter.com/drhelius) for updates.

----------

## Downloads

- **Windows**: [Gearboy-3.1.1-Windows.zip](https://github.com/drhelius/Gearboy/releases/download/gearboy-3.1.1/Gearboy-3.1.1-Windows.zip)
  + NOTE: You may need to install the [Microsoft Visual C++ Redistributable](https://go.microsoft.com/fwlink/?LinkId=746572)
- **macOS**:
  + `brew cask install gearboy`
  + Or install manually: [Gearboy-3.1.1-macOS.zip](https://github.com/drhelius/Gearboy/releases/download/gearboy-3.1.1/Gearboy-3.1.1-macOS.zip)
- **Linux**: [Gearboy-3.1.1-Linux.tar.xz](https://github.com/drhelius/Gearboy/releases/download/gearboy-3.1.1/Gearboy-3.1.1-Linux.tar.xz)
  + NOTE: You may need to install `libsdl2` and `libglew`
- **iOS**: Build Gearboy with Xcode and transfer it to your device. You can open rom files from other apps like Safari or Dropbox, or use your iCloud Drive.
- **RetroArch**: [Libretro core documentation](https://libretro.readthedocs.io/en/latest/library/gearboy/).
- **Raspberry Pi**: Build Gearboy from sources. Optimized projects are provided for Raspberry Pi 1, 2, 3 and 4.

## Features

- Accurate CPU emulation, passes cpu_instrs.gb from blargg's tests.
- Accurate instruction and memory timing, passes instr_timing.gb and mem_timing.gb from blargg's tests.
- Supported cartridges: ROM, ROM + RAM, MBC1, MBC2, MBC3 + RTC, MBC5, HuC-1 and MBC1M (multicart).
- Accurate LCD controller emulation with correct timings and priorities including mid-scanline effects.
- Game Boy Color support.
- LCD screen ghosting effect as seen in the original Game Boy.
- LCD dot matrix effect.
- Sound emulation using SDL Audio and [Gb_Snd_Emu library](http://blargg.8bitalley.com/libs/audio.html#Gb_Snd_Emu).
- Save battery powered RAM cartridges to file.
- Save states.
- Compressed rom support (ZIP).
- Game Genie and GameShark cheat support.
- Supported platforms: Windows, Linux, macOS, Raspberry Pi, iOS and RetroArch (libretro).
- Full debugger with disassembler, breakpoints, debug symbols, memory editor, IO inspector and and VRAM viewer including tiles, sprites, backgrounds and palettes.

<img src=""http://www.geardome.com/files/gearboy/gearboy_debug_01.png"" width=""880"" height=""455"">

## Build Instructions

### Windows

- Install Microsoft Visual Studio Community 2019 or later.
- Open the Gearboy Visual Studio solution `platforms/windows/Gearboy.sln` and build.
- You may want to use the `platforms/windows/Makefile` to build the application using MinGW.

### macOS

- Install Xcode and run `xcode-select --install` in the terminal for the compiler to be available on the command line.
- Run this commands to generate a Mac *app* bundle:

``` shell
brew install sdl2
cd platforms/macos
make dist
```

### Linux

- Ubuntu / Debian:

``` shell
sudo apt-get install build-essential libsdl2-dev libglew-dev
cd platforms/linux
make
```

- Fedora:

``` shell
sudo dnf install @development-tools gcc-c++ SDL2-devel glew-devel
cd platforms/linux
make
```

### iOS

- Install Xcode for macOS. You need iOS 13 SDK or later.
- Build the project `platforms/ios/Gearboy.xcodeproj`
- Run it on real hardware using your iOS developer certificate. Make sure it builds on *Release* for better performance.

### Libretro

- Ubuntu / Debian:

``` shell
sudo apt-get install build-essential
cd platforms/libretro
make
```

- Fedora:

``` shell
sudo dnf install @development-tools gcc-c++
cd platforms/libretro
make
```

### Raspberry Pi 4 - Raspbian (Desktop)

``` shell
sudo apt install build-essential libsdl2-dev libglew-dev
cd platforms/raspberrypi4
make
```

### Raspberry Pi 2 & 3 - Raspbian (CLI)

- Install and configure [SDL 2](http://www.libsdl.org/download-2.0.php) for development:

``` shell
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install build-essential libfreeimage-dev libopenal-dev libpango1.0-dev libsndfile-dev libudev-dev libasound2-dev libjpeg-dev libtiff5-dev libwebp-dev automake
cd ~
wget https://www.libsdl.org/release/SDL2-2.0.12.tar.gz
tar zxvf SDL2-2.0.12.tar.gz
cd SDL2-2.0.12 && mkdir build && cd build
../configure --disable-pulseaudio --disable-esd --disable-video-mir --disable-video-wayland --disable-video-x11 --disable-video-opengl --host=armv7l-raspberry-linux-gnueabihf
make -j 4
sudo make install
```

- Install libconfig library dependencies for development: `sudo apt-get install libconfig++-dev`
- Use `make -j 4` in the `platforms/raspberrypi3/x64/` folder to build the project.
- Use `export SDL_AUDIODRIVER=ALSA` before running the emulator for the best performance.
- Gearboy generates a `gearboy.cfg` configuration file where you can customize keyboard and gamepads. Key codes are from [SDL](https://wiki.libsdl.org/SDL_Keycode).

## Accuracy Tests

Compared to other emulators: [see here](http://tasvideos.org/EmulatorResources/GBAccuracyTests.html).

Tests from [blargg's test roms](https://github.com/retrio/gb-test-roms):

![cpu_instrs.gb](http://www.geardome.com/files/gearboy/gearboy_001.png)![insrt_timing.gb](http://www.geardome.com/files/gearboy/gearboy_002.png)![lcd_sync.gb](http://www.geardome.com/files/gearboy/gearboy_003.png)![dmg_sound.gb](http://www.geardome.com/files/gearboy/gearboy_032.png)![cgb_sound.gb](http://www.geardome.com/files/gearboy/gearboy_033.png)![mem_timing.gb](http://www.geardome.com/files/gearboy/gearboy_memtiming2.png)

## Screenshots

![Screenshot](http://www.geardome.com/files/gearboy/gearboy_004.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_006.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_008.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_022.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_013.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_023.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_015.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_029.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_011.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_024.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_017.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_016.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_034.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_026.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_018.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_025.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_021.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_027.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_019.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_020.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_031.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_028.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_007.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_009.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_010.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_005.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_012.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_014.png)

## License

Gearboy is licensed under the GNU General Public License v3.0 License, see [LICENSE](LICENSE) for more information."
uccs/librte,952,2,9,0,Organization,False,156,3,0,3,False,The RTE project,,0,6,0,10,8,0,0,0,2,0,0,2497,0,0,0,0,0,0,2,0,,,"librte
======

The RTE project"
kuro68k/Retro-Adapter,16381,0,1,0,User,False,3,1,0,1,False,Retro Adapter V2,,0,7,0,0,0,0,0,0,0,0,0,1230,0,0,0,0,0,0,49,,3,,"# Retro-Adapter
Retro Adapter V2

Connect various old computer and game console controllers to USB.

http://denki.world3.net/retro_v2.html"
hachque-Emscripten/libpng-1.2.49,528,3,1,1,Organization,False,1,1,0,1,False,libpng port for Emscripten,,0,6,0,2,0,0,0,0,0,0,0,2924,0,0,0,0,0,0,4,1,,,
