full_name,size,stars,watches,forks,owner_type,if_fork,commits,branches,releases,contributors,license,description,website,topics,labels,milestones,open_issues,closed_issues,open_issues_recent,closed_issues_recent,open_prs,closed_prs,open_prs_recent,closed_prs_recent,age,recent_contributors,recent_commits,recent_added,recent_deleted,dependent_repositories,dependent_packages,repositories,people,followers,info,readme
ggreer/the_silver_searcher,2396,20377,430,1193,User,False,2021,55,49,189,False,"A code-searching tool similar to ack, but faster.",http://geoff.greer.fm/ag/,6,2,1,354,390,18,8,112,531,12,2,3120,3,3,28,4,0,0,66,,1,,"# The Silver SearcherþþA code searching tool similar to `ack`, with a focus on speed.þþ[![Build Status](https://travis-ci.org/ggreer/the_silver_searcher.svg?branch=master)](https://travis-ci.org/ggreer/the_silver_searcher)þþ[![Floobits Status](https://floobits.com/ggreer/ag.svg)](https://floobits.com/ggreer/ag/redirect)þþ[![#ag on Freenode](https://img.shields.io/badge/Freenode-%23ag-brightgreen.svg)](https://webchat.freenode.net/?channels=ag)þþDo you know C? Want to improve ag? [I invite you to pair with me](http://geoff.greer.fm/2014/10/13/help-me-get-to-ag-10/).þþþ## What's so great about Ag?þþ* It is an order of magnitude faster than `ack`.þ* It ignores file patterns from your `.gitignore` and `.hgignore`.þ* If there are files in your source repo you don't want to search, just add their patterns to a `.ignore` file. (\*cough\* `*.min.js` \*cough\*)þ* The command name is 33% shorter than `ack`, and all keys are on the home row!þþAg is quite stable now. Most changes are new features, minor bug fixes, or performance improvements. It's much faster than Ack in my benchmarks:þþ    ack test_blah ~/code/  104.66s user 4.82s system 99% cpu 1:50.03 totalþþ    ag test_blah ~/code/  4.67s user 4.58s system 286% cpu 3.227 totalþþAck and Ag found the same results, but Ag was 34x faster (3.2 seconds vs 110 seconds). My `~/code` directory is about 8GB. Thanks to git/hg/ignore, Ag only searched 700MB of that.þþThere are also [graphs of performance across releases](http://geoff.greer.fm/ag/speed/).þþ## How is it so fast?þþ* Ag uses [Pthreads](https://en.wikipedia.org/wiki/POSIX_Threads) to take advantage of multiple CPU cores and search files in parallel.þ* Files are `mmap()`ed instead of read into a buffer.þ* Literal string searching uses [Boyer-Moore strstr](https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm).þ* Regex searching uses [PCRE's JIT compiler](http://sljit.sourceforge.net/pcre.html) (if Ag is built with PCRE >=8.21).þ* Ag calls `pcre_study()` before executing the same regex on every file.þ* Instead of calling `fnmatch()` on every pattern in your ignore files, non-regex patterns are loaded into arrays and binary searched.þþI've written several blog posts showing how I've improved performance. These include how I [added pthreads](http://geoff.greer.fm/2012/09/07/the-silver-searcher-adding-pthreads/), [wrote my own `scandir()`](http://geoff.greer.fm/2012/09/03/profiling-ag-writing-my-own-scandir/), [benchmarked every revision to find performance regressions](http://geoff.greer.fm/2012/08/25/the-silver-searcher-benchmarking-revisions/), and profiled with [gprof](http://geoff.greer.fm/2012/02/08/profiling-with-gprof/) and [Valgrind](http://geoff.greer.fm/2012/01/23/making-programs-faster-profiling/).þþþ## Installingþþ### macOSþþ    brew install the_silver_searcherþþorþþ    port install the_silver_searcherþþþ### Linuxþþ* Ubuntu >= 13.10 (Saucy) or Debian >= 8 (Jessie)þþ        apt-get install silversearcher-agþ* Fedora 21 and lowerþþ        yum install the_silver_searcherþ* Fedora 22+þþ        dnf install the_silver_searcherþ* RHEL7+þþ        yum install epel-release.noarch the_silver_searcherþ* Gentooþþ        emerge -a sys-apps/the_silver_searcherþ* Archþþ        pacman -S the_silver_searcherþþ* Slackwareþþ        sbopkg -i the_silver_searcherþþ* openSUSE:þþ        zypper install the_silver_searcherþþ* CentOS:þ        þ        yum install the_silver_searcherþþ* SUSE Linux Enterprise: Follow [these simple instructions](https://software.opensuse.org/download.html?project=utilities&package=the_silver_searcher).þþþ### BSDþþ* FreeBSDþþ        pkg install the_silver_searcherþ* OpenBSD/NetBSDþþ        pkg_add the_silver_searcherþþ### Windowsþþ* Win32/64þþ  Unofficial daily builds are [available](https://github.com/k-takata/the_silver_searcher-win32).þ  þ* Chocolateyþþ        choco install agþ* MSYS2þþ        pacman -S mingw-w64-{i686,x86_64}-agþ* Cygwinþþ  Run the relevant [`setup-*.exe`](https://cygwin.com/install.html), and select ""the\_silver\_searcher"" in the ""Utils"" category.þþ## Building from sourceþþ### Building masterþþ1. Install dependencies (Automake, pkg-config, PCRE, LZMA):þ    * macOS:þþ            brew install automake pkg-config pcre xzþ        orþþ            port install automake pkgconfig pcre xzþ    * Ubuntu/Debian:þþ            apt-get install -y automake pkg-config libpcre3-dev zlib1g-dev liblzma-devþ    * Fedora:þþ            yum -y install pkgconfig automake gcc zlib-devel pcre-devel xz-develþ    * CentOS:þþ            yum -y groupinstall ""Development Tools""þ            yum -y install pcre-devel xz-devel zlib-develþ    * openSUSE:þþ            zypper source-install --build-deps-only the_silver_searcherþþ    * Windows: It's complicated. See [this wiki page](https://github.com/ggreer/the_silver_searcher/wiki/Windows).þ2. Run the build script (which just runs aclocal, automake, etc):þþ        ./build.shþþ   On Windows (inside an msys/MinGW shell):þþ        make -f Makefile.w32þ3. Make install:þþ        sudo make installþþþ### Building a release tarballþþGPG-signed releases are available [here](http://geoff.greer.fm/ag).þþBuilding release tarballs requires the same dependencies, except for automake and pkg-config. Once you've installed the dependencies, just run:þþ    ./configureþ    makeþ    make installþþYou may need to use `sudo` or run as root for the make install.þþþ## Editor Integrationþþ### VimþþYou can use Ag with [ack.vim](https://github.com/mileszs/ack.vim) by adding the following line to your `.vimrc`:þþ    let g:ackprg = 'ag --nogroup --nocolor --column'þþor:þþ    let g:ackprg = 'ag --vimgrep'þþWhich has the same effect but will report every match on the line.þþ### EmacsþþYou can use [ag.el][] as an Emacs front-end to Ag. See also: [helm-ag].þþ[ag.el]: https://github.com/Wilfred/ag.elþ[helm-ag]: https://github.com/syohex/emacs-helm-agþþ### TextMateþþTextMate users can use Ag with [my fork](https://github.com/ggreer/AckMate) of the popular AckMate plugin, which lets you use both Ack and Ag for searching. If you already have AckMate you just want to replace Ack with Ag, move or delete `""~/Library/Application Support/TextMate/PlugIns/AckMate.tmplugin/Contents/Resources/ackmate_ack""` and run `ln -s /usr/local/bin/ag ""~/Library/Application Support/TextMate/PlugIns/AckMate.tmplugin/Contents/Resources/ackmate_ack""`þþ## Other stuff you might likeþþ* [Ack](https://github.com/petdance/ack2) - Better than grep. Without Ack, Ag would not exist.þ* [ack.vim](https://github.com/mileszs/ack.vim)þ* [Exuberant Ctags](http://ctags.sourceforge.net/) - Faster than Ag, but it builds an index beforehand. Good for *really* big codebases.þ* [Git-grep](http://git-scm.com/docs/git-grep) - As fast as Ag but only works on git repos.þ* [ripgrep](https://github.com/BurntSushi/ripgrep)þ* [Sack](https://github.com/sampson-chen/sack) - A utility that wraps Ack and Ag. It removes a lot of repetition from searching and opening matching files."
julycoding/The-Art-Of-Programming-By-July,27451,19188,1906,7002,User,False,3633,1,0,99,False,本项目曾冲到全球第一，干货集锦见本页面最底部，另完整精致的纸质版《编程之法：面试和算法心得》已在京东/当当上销售,,0,6,0,45,82,0,0,22,320,2,0,2378,0,0,0,0,0,0,4,,4,,
memcached/memcached,8496,10155,737,2796,Organization,False,1697,5,96,156,False,memcached development tree,https://memcached.org,0,9,1,36,277,13,32,10,362,7,43,6228,11,88,2864,1405,0,0,5,5,,,"# MemcachedþþMemcached is a high performance multithreaded event-based key/value cacheþstore intended to be used in a distributed system.þþSee: https://memcached.org/aboutþþA fun story explaining usage: https://memcached.org/tutorialþþIf you're having trouble, try the wiki: https://memcached.org/wikiþþIf you're trying to troubleshoot odd behavior or timeouts, see:þhttps://memcached.org/timeoutsþþhttps://memcached.org/ is a good resource in general. Please use the mailingþlist to ask questions, github issues aren't seen by everyone!þþ## Dependenciesþþ* libevent, https://www.monkey.org/~provos/libevent/ (libevent-dev)þ* libseccomp, (optional, experimental, linux) - enables process restrictions forþ  better security. Tested only on x86-64 architectures.þ* openssl, (optional) - enables TLS support. need relatively up to dateþ  version.þþ## EnvironmentþþBe warned that the -k (mlockall) option to memcached might beþdangerous when using a large cache.  Just make sure the memcached machinesþdon't swap.  memcached does non-blocking network I/O, but not disk.  (itþshould never go to disk, or you've lost the whole point of it)þþ## Build statusþþSee https://build.memcached.org/ for multi-platform regression testing status.þþ## Bug reportsþþFeel free to use the issue tracker on github.þþ**If you are reporting a security bug** please contact a maintainer privately.þWe follow responsible disclosure: we handle reports privately, prepare aþpatch, allow notifications to vendor lists. Then we push a fix release and yourþbug can be posted publicly with credit in our release notes and commitþhistory.þþ## Websiteþþ* https://www.memcached.orgþþ## ContributingþþSee https://github.com/memcached/memcached/wiki/DevelopmentRepos"
stedolan/jq,6848,17128,305,1020,User,False,1323,16,11,112,False,Command-line JSON processor,http://stedolan.github.io/jq/,0,34,5,499,1169,60,22,89,385,19,12,2889,7,13,69,57,0,0,45,,575,,"jqþ==þþjq is a lightweight and flexible command-line JSON processor.þþ[![Coverage Status](https://coveralls.io/repos/stedolan/jq/badge.svg?branch=master&service=github)](https://coveralls.io/github/stedolan/jq?branch=master),þUnix: [![Build Status](https://travis-ci.org/stedolan/jq.svg?branch=master)](https://travis-ci.org/stedolan/jq),þWindows: [![Windows build status](https://ci.appveyor.com/api/projects/status/mi816811c9e9mx29?svg=true)](https://ci.appveyor.com/project/stedolan/jq)þþþIf you want to learn to use jq, read the documentation atþ[https://stedolan.github.io/jq](https://stedolan.github.io/jq).  Thisþdocumentation is generated from the docs/ folder of this repository.þYou can also try it online at [jqplay.org](https://jqplay.org).þþIf you want to hack on jq, feel free, but be warned that its internalsþare not well-documented at the moment. Bring a hard hat and aþshovel.  Also, read the wiki: https://github.com/stedolan/jq/wiki, whereþyou will find cookbooks, discussion of advanced topics, internals,þrelease engineering, and more.þþSource tarball and built executable releases can be found on theþhomepage and on the github release page, https://github.com/stedolan/jq/releasesþþIf you're building directly from the latest git, you'll need flex,þbison (3.0 or newer), libtool, make, automake, and autoconf installed.þTo get regexp support you'll also need to install Oniguruma or clone it as aþgit submodule as per the instructions below.þ(note that jq's tests require regexp support to pass).  To build, run:þþ    git submodule update --init # if building from git to get onigurumaþ    autoreconf -fi              # if building from gitþ    ./configure --with-oniguruma=builtinþ    make -j8þ    make checkþþTo build without bison or flex, add `--disable-maintainer-mode` to theþ./configure invocation:þþ    ./configure --with-oniguruma=builtin --disable-maintainer-modeþþ(Developers must not use `--disable-maintainer-mode`, not when makingþchanges to the jq parser and/or lexer.)þþTo build a statically linked version of jq, run:þþ    make LDFLAGS=-all-staticþþAfter make finishes, you'll be able to use `./jq`.  You can alsoþinstall it using:þþ    sudo make installþþIf you're not using the latest git version but instead building aþreleased tarball (available on the website), then you won't need toþrun `autoreconf` (and shouldn't), and you won't need flex or bison.þþTo cross-compile for OS X and Windows, see docs/Rakefile's build taskþand scripts/crosscompile.  You'll need a cross-compilation environment,þsuch as Mingw for cross-compiling for Windows.þþCross-compilation requires a clean workspace, then:þþ    # git clean ...þ    autoreconf -iþ    ./configureþ    make distcleanþ    scripts/crosscompile <name-of-build> <configure-options>þþUse the `--host=` and `--target=` ./configure options to select aþcross-compilation environment.  See also þ[""Cross compilation""](https://github.com/stedolan/jq/wiki/Cross-compilation) onþthe wiki.þþSend questions to https://stackoverflow.com/questions/tagged/jq or to the #jq channel (http://irc.lc/freenode/%23jq/) on Freenode (https://webchat.freenode.net/)."
bilibili/ijkplayer,8034,26694,1218,6964,Organization,False,2584,6,78,40,False,"Android/iOS video player based on FFmpeg n3.4, with MediaCodec, VideoToolbox support.",,6,27,0,2436,2418,97,20,49,140,2,0,2567,0,0,0,0,0,0,58,25,,,"# ijkplayerþþ Platform | Build Statusþ -------- | ------------þ Android | [![Build Status](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-android.svg?branch=master)](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-android)þ iOS | [![Build Status](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-ios.svg?branch=master)](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-ios)þþVideo player based on [ffplay](http://ffmpeg.org)þþ### Downloadþþ- Android:þ - Gradleþ```þ# requiredþallprojects {þ    repositories {þ        jcenter()þ    }þ}þþdependencies {þ    # required, enough for most devices.þ    compile 'tv.danmaku.ijk.media:ijkplayer-java:0.8.8'þ    compile 'tv.danmaku.ijk.media:ijkplayer-armv7a:0.8.8'þþ    # Other ABIs: optionalþ    compile 'tv.danmaku.ijk.media:ijkplayer-armv5:0.8.8'þ    compile 'tv.danmaku.ijk.media:ijkplayer-arm64:0.8.8'þ    compile 'tv.danmaku.ijk.media:ijkplayer-x86:0.8.8'þ    compile 'tv.danmaku.ijk.media:ijkplayer-x86_64:0.8.8'þþ    # ExoPlayer as IMediaPlayer: optional, experimentalþ    compile 'tv.danmaku.ijk.media:ijkplayer-exo:0.8.8'þ}þ```þ- iOSþ - in coming...þþ### My Build Environmentþ- Commonþ - Mac OS X 10.11.5þ- Androidþ - [NDK r10e](http://developer.android.com/tools/sdk/ndk/index.html)þ - Android Studio 2.1.3þ - Gradle 2.14.1þ- iOSþ - Xcode 7.3 (7D175)þ- [HomeBrew](http://brew.sh)þ - ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""þ - brew install gitþþ### Latest Changesþ- [NEWS.md](NEWS.md)þþ### Featuresþ- Commonþ - remove rarely used ffmpeg components to reduce binary size [config/module-lite.sh](config/module-lite.sh)þ - workaround for some buggy online video.þ- Androidþ - platform: API 9~23þ - cpu: ARMv7a, ARM64v8a, x86 (ARMv5 is not tested on real devices)þ - api: [MediaPlayer-like](android/ijkplayer/ijkplayer-java/src/main/java/tv/danmaku/ijk/media/player/IMediaPlayer.java)þ - video-output: NativeWindow, OpenGL ES 2.0þ - audio-output: AudioTrack, OpenSL ESþ - hw-decoder: MediaCodec (API 16+, Android 4.1+)þ - alternative-backend: android.media.MediaPlayer, ExoPlayerþ- iOSþ - platform: iOS 7.0~10.2.xþ - cpu: armv7, arm64, i386, x86_64, (armv7s is obselete)þ - api: [MediaPlayer.framework-like](ios/IJKMediaPlayer/IJKMediaPlayer/IJKMediaPlayback.h)þ - video-output: OpenGL ES 2.0þ - audio-output: AudioQueue, AudioUnitþ - hw-decoder: VideoToolbox (iOS 8+)þ - alternative-backend: AVFoundation.Framework.AVPlayer, MediaPlayer.Framework.MPMoviePlayerControlelr (obselete since iOS 8)þþ### NOT-ON-PLANþ- obsolete platforms (Android: API-8 and below; iOS: pre-6.0)þ- obsolete cpu: ARMv5, ARMv6, MIPS (I don't even have these types of devices…)þ- native subtitle renderþ- avfilter supportþþ### Before Buildþ```þ# install homebrew, git, yasmþruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""þbrew install gitþbrew install yasmþþ# add these lines to your ~/.bash_profile or ~/.profileþ# export ANDROID_SDK=<your sdk path>þ# export ANDROID_NDK=<your ndk path>þþ# on Cygwin (unmaintained)þ# install git, make, yasmþ```þþ- If you prefer more codec/formatþ```þcd configþrm module.shþln -s module-default.sh module.shþcd android/contribþ# cd iosþsh compile-ffmpeg.sh cleanþ```þþ- If you prefer less codec/format for smaller binary size (include hevc function)þ```þcd configþrm module.shþln -s module-lite-hevc.sh module.shþcd android/contribþ# cd iosþsh compile-ffmpeg.sh cleanþ```þþ- If you prefer less codec/format for smaller binary size (by default)þ```þcd configþrm module.shþln -s module-lite.sh module.shþcd android/contribþ# cd iosþsh compile-ffmpeg.sh cleanþ```þþ- For Ubuntu/Debian users.þ```þ# choose [No] to use bashþsudo dpkg-reconfigure dashþ```þþ- If you'd like to share your config, pull request is welcome.þþ### Build Androidþ```þgit clone https://github.com/Bilibili/ijkplayer.git ijkplayer-androidþcd ijkplayer-androidþgit checkout -B latest k0.8.8þþ./init-android.shþþcd android/contribþ./compile-ffmpeg.sh cleanþ./compile-ffmpeg.sh allþþcd ..þ./compile-ijk.sh allþþ# Android Studio:þ#     Open an existing Android Studio projectþ#     Select android/ijkplayer/ and importþ#þ#     define ext block in your root build.gradleþ#     ext {þ#       compileSdkVersion = 23       // depending on your sdk versionþ#       buildToolsVersion = ""23.0.0"" // depending on your build tools versionþ#þ#       targetSdkVersion = 23        // depending on your sdk versionþ#     }þ#þ# If you want to enable debugging ijkplayer(native modules) on Android Studio 2.2+: (experimental)þ#     sh android/patch-debugging-with-lldb.sh armv7aþ#     Install Android Studio 2.2(+)þ#     Preference -> Android SDK -> SDK Toolsþ#     Select (LLDB, NDK, Android SDK Build-tools,Cmake) and installþ#     Open an existing Android Studio projectþ#     Select android/ijkplayerþ#     Sync Project with Gradle Filesþ#     Run -> Edit Configurations -> Debugger -> Symbol Directoriesþ#     Add ""ijkplayer-armv7a/.externalNativeBuild/ndkBuild/release/obj/local/armeabi-v7a"" to Symbol Directoriesþ#     Run -> Debug 'ijkplayer-example'þ#     if you want to reverse patches:þ#     sh patch-debugging-with-lldb.sh reverse armv7aþ#þ# Eclipse: (obselete)þ#     File -> New -> Project -> Android Project from Existing Codeþ#     Select android/ and import all projectþ#     Import appcompat-v7þ#     Import preference-v7þ#þ# Gradleþ#     cd ijkplayerþ#     gradleþþ```þþþ### Build iOSþ```þgit clone https://github.com/Bilibili/ijkplayer.git ijkplayer-iosþcd ijkplayer-iosþgit checkout -B latest k0.8.8þþ./init-ios.shþþcd iosþ./compile-ffmpeg.sh cleanþ./compile-ffmpeg.sh allþþ# Demoþ#     open ios/IJKMediaDemo/IJKMediaDemo.xcodeproj with Xcodeþ# þ# Import into Your own Applicationþ#     Select your project in Xcode.þ#     File -> Add Files to ... -> Select ios/IJKMediaPlayer/IJKMediaPlayer.xcodeprojþ#     Select your Application's target.þ#     Build Phases -> Target Dependencies -> Select IJKMediaFrameworkþ#     Build Phases -> Link Binary with Libraries -> Add:þ#         IJKMediaFramework.frameworkþ#þ#         AudioToolbox.frameworkþ#         AVFoundation.frameworkþ#         CoreGraphics.frameworkþ#         CoreMedia.frameworkþ#         CoreVideo.frameworkþ#         libbz2.tbdþ#         libz.tbdþ#         MediaPlayer.frameworkþ#         MobileCoreServices.frameworkþ#         OpenGLES.frameworkþ#         QuartzCore.frameworkþ#         UIKit.frameworkþ#         VideoToolbox.frameworkþ#þ#         ... (Maybe something else, if you get any link error)þ# þ```þþþ### Support (支持) ###þ- Please do not send e-mail to me. Public technical discussion on github is preferred.þ- 请尽量在 github 上公开讨论[技术问题](https://github.com/bilibili/ijkplayer/issues)，不要以邮件方式私下询问，恕不一一回复。þþþ### Licenseþþ```þCopyright (c) 2017 BilibiliþLicensed under LGPLv2.1 or laterþ```þþijkplayer required features are based on or derives from projects below:þ- LGPLþ  - [FFmpeg](http://git.videolan.org/?p=ffmpeg.git)þ  - [libVLC](http://git.videolan.org/?p=vlc.git)þ  - [kxmovie](https://github.com/kolyvan/kxmovie)þ  - [soundtouch](http://www.surina.net/soundtouch/sourcecode.html)þ- zlib licenseþ  - [SDL](http://www.libsdl.org)þ- BSD-style licenseþ  - [libyuv](https://code.google.com/p/libyuv/)þ- ISC licenseþ  - [libyuv/source/x86inc.asm](https://code.google.com/p/libyuv/source/browse/trunk/source/x86inc.asm)þþandroid/ijkplayer-exo is based on or derives from projects below:þ- Apache License 2.0þ  - [ExoPlayer](https://github.com/google/ExoPlayer)þþandroid/example is based on or derives from projects below:þ- GPLþ  - [android-ndk-profiler](https://github.com/richq/android-ndk-profiler) (not included by default)þþios/IJKMediaDemo is based on or derives from projects below:þ- Unknown licenseþ  - [iOS7-BarcodeScanner](https://github.com/jpwiddy/iOS7-BarcodeScanner)þþijkplayer's build scripts are based on or derives from projects below:þ- [gas-preprocessor](http://git.libav.org/?p=gas-preprocessor.git)þ- [VideoLAN](http://git.videolan.org)þ- [yixia/FFmpeg-Android](https://github.com/yixia/FFmpeg-Android)þ- [kewlbear/FFmpeg-iOS-build-script](https://github.com/kewlbear/FFmpeg-iOS-build-script) þþ### Commercial Useþijkplayer is licensed under LGPLv2.1 or later, so itself is free for commercial use under LGPLv2.1 or laterþþBut ijkplayer is also based on other different projects under various licenses, which I have no idea whether they are compatible to each other or to your product.þþ[IANAL](https://en.wikipedia.org/wiki/IANAL), you should always ask your lawyer for these stuffs before use it in your product."
numpy/numpy,80836,13969,513,4610,Organization,False,23203,23,172,931,False,The fundamental package for scientific computing with Python.,https://www.numpy.org/,2,82,3,1929,6670,322,353,257,7713,100,699,6753,33,668,36593,21682,394120,23590,19,28,,,"# <img alt=""NumPy"" src=""https://cdn.rawgit.com/numpy/numpy/master/branding/icons/numpylogo.svg"" height=""60"">þþ[![Travis](https://img.shields.io/travis/numpy/numpy/master.svg?label=Travis%20CI)](þ    https://travis-ci.org/numpy/numpy)þ[![Azure](https://dev.azure.com/numpy/numpy/_apis/build/status/azure-pipeline%20numpy.numpy)](þ    https://dev.azure.com/numpy/numpy/_build/latest?definitionId=5)þ[![codecov](https://codecov.io/gh/numpy/numpy/branch/master/graph/badge.svg)](þ    https://codecov.io/gh/numpy/numpy)þþNumPy is the fundamental package needed for scientific computing with Python.þþ- **Website:** https://www.numpy.orgþ- **Documentation:** https://numpy.org/docþ- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussionþ- **Source code:** https://github.com/numpy/numpyþ- **Contributing:** https://www.numpy.org/devdocs/dev/index.htmlþ- **Bug reports:** https://github.com/numpy/numpy/issuesþ- **Report a security vulnerability:** https://tidelift.com/docs/securityþþIt provides:þþ- a powerful N-dimensional array objectþ- sophisticated (broadcasting) functionsþ- tools for integrating C/C++ and Fortran codeþ- useful linear algebra, Fourier transform, and random number capabilitiesþþTesting:þþ- NumPy versions &ge; 1.15 require `pytest`þ- NumPy versions &lt; 1.15 require `nose`þþTests can then be run after installation with:þþ    python -c 'import numpy; numpy.test()'þþþCall for Contributionsþ----------------------þþNumPy appreciates help from a wide range of different backgrounds.þWork such as high level documentation or website improvements are valuableþand we would like to grow our team with people filling these roles.þSmall improvements or fixes are always appreciated and issues labeled as easyþmay be a good starting point.þIf you are considering larger contributions outside the traditional coding work,þplease contact us through the mailing list.þþþ[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org)"
vurtun/nuklear,12127,13220,576,1081,User,False,1712,1,0,100,False,A single-header ANSI C gui library,,6,9,0,208,451,0,0,31,249,0,0,1930,0,0,0,0,0,0,4,,510,,
git/git,141137,32929,2190,19259,Organization,False,59611,5,772,1373,False,Git Source Code Mirror - This is a publish-only repository and all pull requests are ignored. Please follow Documentation/SubmittingPatches procedure for any of your improvements.,,2,4,0,,,,,32,681,25,81,5542,33,853,84103,63199,0,0,7,17,,,"[![Build status](https://github.com/git/git/workflows/CI/PR/badge.svg)](https://github.com/git/git/actions?query=branch%3Amaster+event%3Apush)þþGit - fast, scalable, distributed revision control systemþ=========================================================þþGit is a fast, scalable, distributed revision control system with anþunusually rich command set that provides both high-level operationsþand full access to internals.þþGit is an Open Source project covered by the GNU General PublicþLicense version 2 (some parts of it are under different licenses,þcompatible with the GPLv2). It was originally written by LinusþTorvalds with help of a group of hackers around the net.þþPlease read the file [INSTALL][] for installation instructions.þþMany Git online resources are accessible from <https://git-scm.com/>þincluding full documentation and Git related tools.þþSee [Documentation/gittutorial.txt][] to get started, then seeþ[Documentation/giteveryday.txt][] for a useful minimum set of commands, andþ`Documentation/git-<commandname>.txt` for documentation of each command.þIf git has been correctly installed, then the tutorial can also beþread with `man gittutorial` or `git help tutorial`, and theþdocumentation of each command with `man git-<commandname>` or `git helpþ<commandname>`.þþCVS users may also want to read [Documentation/gitcvs-migration.txt][]þ(`man gitcvs-migration` or `git help cvs-migration` if git isþinstalled).þþThe user discussion and development of Git take place on the Gitþmailing list -- everyone is welcome to post bug reports, featureþrequests, comments and patches to git@vger.kernel.org (readþ[Documentation/SubmittingPatches][] for instructions on patch submission).þTo subscribe to the list, send an email with just ""subscribe git"" inþthe body to majordomo@vger.kernel.org. The mailing list archives areþavailable at <https://lore.kernel.org/git/>,þ<http://marc.info/?l=git> and other archival sites.þþIssues which are security relevant should be disclosed privately toþthe Git Security mailing list <git-security@googlegroups.com>.þþThe maintainer frequently sends the ""What's cooking"" reports thatþlist the current status of various development topics to the mailingþlist.  The discussion following them give a good reference forþproject status, development direction and remaining tasks.þþThe name ""git"" was given by Linus Torvalds when he wrote the veryþfirst version. He described the tool as ""the stupid content tracker""þand the name as (depending on your mood):þþ - random three-letter combination that is pronounceable, and notþ   actually used by any common UNIX command.  The fact that it is aþ   mispronunciation of ""get"" may or may not be relevant.þ - stupid. contemptible and despicable. simple. Take your pick from theþ   dictionary of slang.þ - ""global information tracker"": you're in a good mood, and it actuallyþ   works for you. Angels sing, and a light suddenly fills the room.þ - ""goddamn idiotic truckload of sh*t"": when it breaksþþ[INSTALL]: INSTALLþ[Documentation/gittutorial.txt]: Documentation/gittutorial.txtþ[Documentation/giteveryday.txt]: Documentation/giteveryday.txtþ[Documentation/gitcvs-migration.txt]: Documentation/gitcvs-migration.txtþ[Documentation/SubmittingPatches]: Documentation/SubmittingPatches"
irungentoo/toxcore,10353,8601,598,1246,User,False,3771,2,1,155,False,The future of online communications.,https://tox.chat/,2,25,2,94,551,0,0,7,931,0,1,2546,0,0,0,0,0,0,13,,438,,"![Project Tox](https://raw.github.com/irungentoo/toxcore/master/other/tox.png ""Project Tox"")þ***þþWith the rise of government surveillance programs, Tox, a FOSS initiative, aims to be an easy to use, all-in-one communication platform that ensures full privacy and secure message delivery.<br /> <br />þþ[**Website**](https://tox.chat) **|** [**Wiki**](https://wiki.tox.chat/) **|** [**Blog**](https://blog.tox.chat/) **|** [**FAQ**](https://wiki.tox.chat/doku.php?id=users:faq) **|** [**Binaries/Downloads**](https://wiki.tox.chat/Binaries) **|** [**Clients**](https://wiki.tox.chat/doku.php?id=clients) **|** [**Compiling**](/INSTALL.md)þþ**IRC Channels:** [#tox@freenode](https://webchat.freenode.net/?channels=tox), [#tox-dev@freenode](https://webchat.freenode.net/?channels=tox-dev)þþþ## The Complex Stuff:þ### UDP vs. TCPþTox must use UDP simply because [hole punching](https://en.wikipedia.org/wiki/UDP_hole_punching) with TCP is not as reliable.þHowever, Tox does use [TCP relays](/docs/TCP_Network.txt) as a fallback if it encounters a firewall that prevents UDP hole punching.þþ### Connecting & CommunicatingþEvery peer is represented as a [byte string](https://en.wikipedia.org/wiki/String_(computer_science)) (the public key [Tox ID] of the peer). By using torrent-style DHT, peers can find the IP of other peers by using their Tox ID. Once the IP is obtained, peers can initiate a [secure](/docs/updates/Crypto.md) connection with each other. Once the connection is made, peers can exchange messages, send files, start video chats, etc. using encrypted communications.þþþ**Current build status:** [![Build Status](https://travis-ci.org/irungentoo/toxcore.png?branch=master)](https://travis-ci.org/irungentoo/toxcore)þþþ## Q&A:þþ### What are your goals with Tox?þþWe want Tox to be as simple as possible while remaining as secure as possible.þþ### Why are you doing this? There are already a bunch of free Skype alternatives.þThe goal of this project is to create a configuration-free P2P Skype replacement. “Configuration-free” means that the user will simply have to open the program and will be capable of adding people and communicating with them without having to set up an account. There are many so-called Skype replacements, but all of them are either hard to configure for the normal user or suffer from being way too centralized.þþ## TODO:þ- [TODO](/docs/TODO.md)þþþ## Documentation:þþ- [Compiling](/INSTALL.md)þ- [DHT Protocol](/docs/updates/DHT.md)<br />þ- [Crypto](/docs/updates/Crypto.md)<br />"
git-up/GitUp,50051,9058,175,617,Organization,False,378,5,48,40,False,The Git interface you've been missing all your life has finally arrived.,http://gitup.co,0,8,0,245,293,34,17,3,139,2,10,1769,7,19,2603,3730,0,0,12,0,,,"[![Build Status](https://travis-ci.org/git-up/GitUp.svg?branch=master)](https://travis-ci.org/git-up/GitUp)þþGitUpþ=====þþ**Work quickly, safely, and without headaches. The Git interface you've been missing all your life has finally arrived.**þþ<p align=""center"">þ<img src=""https://i.imgur.com/JuQIxJu.png"" width=""50%"" height=""50%""><img src=""https://i.imgur.com/9rgXktz.png"" width=""50%"" height=""50%"">þ</p>þþGit recently celebrated its 10 years anniversary, but most engineers are still confused by its intricacy (3 of the [top 5 questions of all time](http://stackoverflow.com/questions?sort=votes) on Stack Overflow are Git related). Since Git turns even simple actions into mystifying commands (“git add” to stage versus “git reset HEAD” to unstage anyone?), it’s no surprise users waste time, get frustrated, distract the rest of their team for help, or worse, screw up their repo!þþGitUp is a bet to invent a new Git interaction model that lets engineers of all levels work quickly, safely, and without headaches. It's unlike any other Git client out there from the way it’s built (it interacts directly with the Git database on disk), to the way it works (you manipulate the repository graph instead of manipulating commits).þþWith GitUp, you get a truly efficient Git client for Mac:þ- A **live and interactive repo graph** (edit, reorder, fixup, merge commits…),þ- **Unlimited undo / redo** of almost all operations (even rebases and merges),þ- Time Machine like **snapshots for 1-click rollbacks** to previous repo states,þ- Features that don’t even exist natively in Git like a **visual commit splitter** or a **unified reflog browser**,þ- **Instant search across the entire repo** including diff contents, þ- A **ridiculously fast UI**, often faster than the command line.þþ*GitUp was created by [@swisspol](https://github.com/swisspol) in late 2014 as a bet to reinvent the way developers interact with Git. After several months of work, it was made available in pre-release early 2015 and reached the [top of Hacker News](https://news.ycombinator.com/item?id=9653978) along with being [featured by Product Hunt](http://www.producthunt.com/tech/gitup-1) and [Daring Fireball](http://daringfireball.net/linked/2015/06/04/gitup). 30,000 lines of code later, GitUp reached 1.0 mid-August 2015 and was released open source as a gift to the developer community.*þþGetting Startedþ===============þþ**Learn all about GitUp and download the latest release from http://gitup.co.**þþ**Read the [docs](https://github.com/git-up/GitUp/wiki) and use [GitHub Issues](https://github.com/git-up/GitUp/issues) for support & feedback.**þþReleases notes are available at https://github.com/git-up/GitUp/releases. Builds tagged with a `v` (e.g. `v1.2.3`) are released on the ""Stable"" channel, while builds tagged with a `b` (e.g. `b1234`) are only released on the ""Continuous"" channel. You can change the update channel used by GitUp in the app preferences.þþTo build GitUp yourself, simply run the command `git clone --recursive https://github.com/git-up/GitUp.git` in Terminal, then open the `GitUp/GitUp.xcodeproj` Xcode project and hit Run.þþ**IMPORTANT:** If you do not have an Apple ID with a developer account for code signing Mac apps, the build  will fail with a code signing error. Simply delete the ""Code Signing Identity"" build setting of the ""Application"" target to work around the issue:þþ<p align=""center"">þ<img src=""http://i.imgur.com/dWpJExk.png"">þ</p>þþ**Alternatively**, if you do have a developer account, you can create the file ""Xcode-Configurations/DEVELOPMENT_TEAM.xcconfig"" with the following build setting as its content:þ> DEVELOPMENT_TEAM = [Your TeamID]þþFor a more detailed description of this, you can have a look at the comments at the end of the file ""Xcode-Configurations/Base.xcconfig"". þþGitUpKitþ========þþ**GitUp is built as a thin layer on top of a reusable generic Git toolkit called ""GitUpKit"". This means that you can use that same GitUpKit framework to build your very own Git UI!**þþ*GitUpKit has a very different goal than [ObjectiveGit](https://github.com/libgit2/objective-git). Instead of offering extensive raw bindings to [libgit2](https://github.com/libgit2/libgit2), GitUpKit only uses a minimal subset of libgit2 and reimplements everything else on top of it (it has its own ""rebase engine"" for instance).þThis allows it to expose a very tight and consistent API, that completely follows Obj-C conventions and hides away the libgit2 complexity and sometimes inconsistencies. GitUpKit adds on top of that a number of exclusive and powerful features, from undo/redo and Time Machine like snapshots, to entire drop-in UI components.*þþArchitectureþ------------þþThe GitUpKit source code is organized as 2 independent layers communicating only through the use of public APIs:þþ**Base Layer (depends on Foundation only and is compatible with OS X and iOS)**þ- `Core/`: wrapper around the required minimal functionality of [libgit2](https://github.com/libgit2/libgit2), on top of which is then implemented all the Git functionality required by GitUp (note that GitUp uses a [slightly customized fork](https://github.com/git-up/libgit2/tree/gitup) of libgit2)þ- `Extensions/`: categories on the `Core` classes to add convenience features implemented only using the public APIsþþ**UI Layer (depends on AppKit and is compatible with OS X only)**þ- `Interface/`: low-level view classes e.g. `GIGraphView` to render the GitUp Map viewþ- `Utilities/`: interface utility classes e.g. the base view controller class `GIViewController`þ- `Components/`: reusable single-view view controllers e.g. `GIDiffContentsViewController` to render a diffþ- `Views/`: high-level reusable multi-views view controllers e.g. `GIAdvancedCommitViewController` to implement the entire GitUp Advanced Commit viewþþ**IMPORTANT**: If the preprocessor constant `DEBUG` is defined to a non-zero value when building GitUpKit (this is the default when building in ""Debug"" configuration), a number of extra consistency checks are enabled at run time as well as extra logging. Be aware that this overhead can significantly affect performance.þþGitUpKit APIþ------------þþUsing the GitUpKit API should be pretty straightforward since it is organized by functionality (e.g. repository, branches, commits, interface components, etc...) and a best effort has been made to name functions clearly.þþRegarding the ""Core"" APIs, the best way to learn them is to peruse the associated unit tests - for instance see [the branch tests](GitUpKit/Core/GCBranch-Tests.m) for the branch API.þþHere is some sample code to get you started (error handling is left as an exercise to the reader):þþ**Opening and browsing a repository:**þ```objcþ// Open repoþGCRepository* repo = [[GCRepository alloc] initWithExistingLocalRepository:<PATH> error:NULL];þþ// Make sure repo is cleanþassert([repo checkClean:kGCCleanCheckOption_IgnoreUntrackedFiles error:NULL]);þþ// List all branchesþNSArray* branches = [repo listAllBranches:NULL];þNSLog(@""%@"", branches);þþ// Lookup HEADþGCLocalBranch* headBranch;  // This would be nil if the HEAD is detachedþGCCommit* headCommit;þ[repo lookupHEADCurrentCommit:&headCommit branch:&headBranch error:NULL];þNSLog(@""%@ = %@"", headBranch, headCommit);þþ// Load the *entire* repo history in memory for fast access, including all commits, branches and tagsþGCHistory* history = [repo loadHistoryUsingSorting:kGCHistorySorting_ReverseChronological error:NULL];þassert(history);þNSLog(@""%lu commits total"", history.allCommits.count);þNSLog(@""%@\n%@"", history.rootCommits, history.leafCommits);þ```þþ**Modifying a repository:**þ```objcþ// Take a snapshot of the repoþGCSnapshot* snapshot = [repo takeSnapshot:NULL];þþ// Create a new branch and check it outþGCLocalBranch* newBranch = [repo createLocalBranchFromCommit:headCommit withName:@""temp"" force:NO error:NULL];þNSLog(@""%@"", newBranch);þassert([repo checkoutLocalBranch:newBranch options:0 error:NULL]);þþ// Add a file to the indexþ[[NSData data] writeToFile:[repo.workingDirectoryPath stringByAppendingPathComponent:@""empty.data""] atomically:YES];þassert([repo addFileToIndex:@""empty.data"" error:NULL]);þþ// Check index statusþGCDiff* diff = [repo diffRepositoryIndexWithHEAD:nil options:0 maxInterHunkLines:0 maxContextLines:0 error:NULL];þassert(diff.deltas.count == 1);þNSLog(@""%@"", diff);þþ// Create a commitþGCCommit* newCommit = [repo createCommitFromHEADWithMessage:@""Added file"" error:NULL];þassert(newCommit);þNSLog(@""%@"", newCommit);þþ// Restore repo to saved snapshot before topic branch and commit were createdþBOOL success = [repo restoreSnapshot:snapshot withOptions:kGCSnapshotOption_IncludeAll reflogMessage:@""Rolled back"" didUpdateReferences:NULL error:NULL];þassert(success);þ  þ// Make sure topic branch is goneþassert([repo findLocalBranchWithName:@""temp"" error:NULL] == nil);þ  þ// Update workdir and index to match HEADþassert([repo resetToHEAD:kGCResetMode_Hard error:NULL]);þ```þþComplete Example #1: GitDownþ----------------------------þþ[GitDown](Examples/GitDown) is a very basic app that prompts the user for a repo and displays an interactive and live-updating list of its stashes (all with ~20 lines of code in `-[AppDelegate applicationDidFinishLaunching:]`):þþ<p align=""center"">þ<img src=""http://i.imgur.com/ZfxM7su.png"">þ</p>þþThrough GitUpKit, this basic app also gets for free unlimited undo/redo, unified and side-by-side diffs, text selection and copy, keyboard shortcuts, etc...þþThis source code also demonstrates how to use some other GitUpKit view controllers as well as building a customized one.þþComplete Example #2: GitDiffþ----------------------------þþ[GitDiff](Examples/GitDiff) demonstrates how to create a view controller that displays a live updating diff between `HEAD` and the workdir à la `git diff HEAD`:þþ<p align=""center"">þ<img src=""http://i.imgur.com/29hxDcJ.png"">þ</p>þþComplete Example #3: GitYþ-------------------------þþ[GitY](Examples/GitY) is a [GitX](http://gitx.frim.nl/) clone built using GitUpKit and less than 200 lines of code:þþ<p align=""center"">þ<img src=""http://i.imgur.com/6cuPcT4.png"">þ</p>þþComplete Example #4: iGitþ-------------------------þþ[iGit](Examples/iGit) is a test iOS app that simply uses GitUpKit to clone a GitHub repo and perform a commit.þþContributingþ============þþSee [CONTRIBUTING.md](CONTRIBUTING.md).þþCreditsþ=======þþ- [@swisspol](https://github.com/swisspol): concept and codeþ- [@wwayneee](https://github.com/wwayneee): UI designþ- [@jayeb](https://github.com/jayeb): websiteþþ*Also a big thanks to the fine [libgit2](https://libgit2.github.com/) contributors without whom GitUp would have never existed!*þþLicenseþ=======þþGitUp is copyright 2015-2018 Pierre-Olivier Latour and available under [GPL v3 license](http://www.gnu.org/licenses/gpl-3.0.txt). See the [LICENSE](LICENSE) file in the project for more information.þþ**IMPORTANT:** GitUp includes some other open-source projects and such projects remain under their own license."
nodemcu/nodemcu-firmware,111218,6083,563,2760,Organization,False,2251,5,27,152,False,"Lua based interactive firmware for ESP8266, ESP8285 and ESP32",https://nodemcu.readthedocs.io,8,20,1,142,1744,57,51,33,1239,21,50,2049,18,61,75195,13360,0,0,5,6,,,"# NodeMCU 3.0.0þþ[![Join the chat at https://gitter.im/nodemcu/nodemcu-firmware](https://img.shields.io/gitter/room/badges/shields.svg)](https://gitter.im/nodemcu/nodemcu-firmware?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)þ[![Build Status](https://travis-ci.org/nodemcu/nodemcu-firmware.svg)](https://travis-ci.org/nodemcu/nodemcu-firmware)þ[![Documentation Status](https://img.shields.io/badge/docs-master-yellow.svg?style=flat)](http://nodemcu.readthedocs.io/en/master/)þ[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE)þþ### A Lua based firmware for ESP8266 WiFi SOCþþNodeMCU is an open source [Lua](https://www.lua.org/) based firmware for the [ESP8266 WiFi SOC from Espressif](http://espressif.com/en/products/esp8266/) and uses an on-module flash-based [SPIFFS](https://github.com/pellepl/spiffs) file system. NodeMCU is implemented in C and is layered on the [Espressif NON-OS SDK](https://github.com/espressif/ESP8266_NONOS_SDK).þþThe firmware was initially developed as is a companion project to the popular ESP8266-based [NodeMCU development modules]((https://github.com/nodemcu/nodemcu-devkit-v1.0)), but the project is now community-supported, and the firmware can now be run on _any_ ESP module.þþ# Summaryþþ- Easy to program wireless node and/or access pointþ- Based on Lua 5.1.4 but without `debug`, `io`, `os` and (most of the) `math` modulesþ- Asynchronous event-driven programming modelþ- more than **65 built-in modules**þ- Firmware available with or without floating point support (integer-only uses less memory)þ- Up-to-date documentation at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io)þþ### LFS supportþIn July 2018 support for a Lua Flash Store (LFS) was introduced. LFS  allows Lua code and its associated constant data to be executed directly out of flash-memory; just as the firmware itself is executed. This now enables NodeMCU developers to create **Lua applications with up to 256Kb** Lua code and read-only constants executing out of flash. All of the RAM is available for read-write data!þþ# Programming ModelþþThe NodeMCU programming model is similar to that of [Node.js](https://en.wikipedia.org/wiki/Node.js), only in Lua. It is asynchronous and event-driven. Many functions, therefore, have parameters for callback functions. To give you an idea what a NodeMCU program looks like study the short snippets below. For more extensive examples have a look at the [`/lua_examples`](lua_examples) folder in the repository on GitHub.þþ```luaþ-- a simple HTTP serverþsrv = net.createServer(net.TCP)þsrv:listen(80, function(conn)þ conn:on(""receive"", function(sck, payload)þ  print(payload)þ  sck:send(""HTTP/1.0 200 OK\r\nContent-Type: text/html\r\n\r\n<h1> Hello, NodeMCU.</h1>"")þ end)þ conn:on(""sent"", function(sck) sck:close() end)þend)þ```þ```luaþ-- connect to WiFi access pointþwifi.setmode(wifi.STATION)þwifi.sta.config{ssid=""SSID"", pwd=""password""}þ```þþ# DocumentationþþThe entire [NodeMCU documentation](https://nodemcu.readthedocs.io) is maintained right in this repository at [/docs](docs). The fact that the API documentation is maintained in the same repository as the code that *provides* the API ensures consistency between the two. With every commit the documentation is rebuilt by Read the Docs and thus transformed from terse Markdown into a nicely browsable HTML site at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io).þþ- How to [build the firmware](https://nodemcu.readthedocs.io/en/master/en/build/)þ- How to [flash the firmware](https://nodemcu.readthedocs.io/en/master/en/flash/)þ- How to [upload code and NodeMCU IDEs](https://nodemcu.readthedocs.io/en/master/en/upload/)þ- API documentation for every moduleþþ# ReleasesþþDue to the ever-growing number of modules available within NodeMCU, pre-built binaries are no longer made available. Use the automated [custom firmware build service](http://nodemcu-build.com/) to get the specific firmware configuration you need, or consult the [documentation](http://nodemcu.readthedocs.io/en/master/en/build/) for other options to build your own firmware.þþThis project uses two main branches, `master` and `dev`. `dev` is actively worked on and it's also where PRs should be created against. `master` thus can be considered ""stable"" even though there are no automated regression tests. The goal is to merge back to `master` roughly every 2 months. Depending on the current ""heat"" (issues, PRs) we accept changes to `dev` for 5-6 weeks and then hold back for 2-3 weeks before the next snap is completed.þþA new tag is created every time `dev` is merged back to `master`. They are listed in the [releases section here on GitHub](https://github.com/nodemcu/nodemcu-firmware/releases). Tag names follow the \<SDK-version\>-master_yyyymmdd pattern.þþ# SupportþþSee [https://nodemcu.readthedocs.io/en/master/en/support/](https://nodemcu.readthedocs.io/en/master/en/support/).þþ# Licenseþþ[MIT](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE) © [zeroday](https://github.com/NodeMCU)/[nodemcu.com](http://nodemcu.com/index_en.html)"
libgit2/libgit2,53375,7462,412,1935,Organization,False,13256,101,83,395,False,"A cross-platform, linkable library implementation of Git that you can use in your application.",https://libgit2.org/,3,16,2,236,1422,45,39,89,3793,29,101,4247,9,239,7059,5864,0,0,21,37,,,"libgit2 - the Git linkable libraryþ==================================þþ| Build Status | |þ| ------------ | - |þ| **master** branch CI builds | [![Azure Pipelines Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/libgit2?branchName=master)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=7&branchName=master)   |þ| **v1.0 branch** CI builds | [![Azure Pipelines Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/libgit2?branchName=maint/v1.0)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=7&branchName=maint/v1.0) |þ| **v0.28 branch** CI builds | [![Azure Pipelines Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/libgit2?branchName=maint/v0.28)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=7&branchName=maint/v0.28) |þ| **Nightly** builds | [![Azure Pipelines Build Status](https://libgit2.visualstudio.com/libgit2/_apis/build/status/nightly?branchName=master&label=Full+Build)](https://libgit2.visualstudio.com/libgit2/_build/latest?definitionId=9&branchName=master) [![Coverity Build Status](https://dev.azure.com/libgit2/libgit2/_apis/build/status/coverity?branchName=master&label=Coverity+Build)](https://dev.azure.com/libgit2/libgit2/_build/latest?definitionId=21?branchName=master) [![Coverity Scan Build Status](https://scan.coverity.com/projects/639/badge.svg)](https://scan.coverity.com/projects/639) |þþ`libgit2` is a portable, pure C implementation of the Git core methodsþprovided as a linkable library with a solid API, allowing to build Gitþfunctionality into your application.  Language bindings likeþ[Rugged](https://github.com/libgit2/rugged) (Ruby),þ[LibGit2Sharp](https://github.com/libgit2/libgit2sharp) (.NET),þ[pygit2](http://www.pygit2.org/) (Python) andþ[NodeGit](http://nodegit.org) (Node) allow you to build Git toolingþin your favorite language.þþ`libgit2` is used to power Git GUI clients likeþ[GitKraken](https://gitkraken.com/) and [gmaster](https://gmaster.io/)þand on Git hosting providers like [GitHub](https://github.com/),þ[GitLab](https://gitlab.com/) andþ[Azure DevOps](https://azure.com/devops).þWe perform the merge every time you click ""merge pull request"".þþ`libgit2` is licensed under a **very permissive license** (GPLv2 with a specialþLinking Exception).  This basically means that you can link it (unmodified)þwith any kind of software without having to release its source code.þAdditionally, the example code has been released to the public domain (see theþ[separate license](examples/COPYING) for more information).þþTable of Contentsþ=================þþ* [Quick Start](#quick-start)þ* [Getting Help](#getting-help)þ* [What It Can Do](#what-it-can-do)þ* [Optional dependencies](#optional-dependencies)þ* [Initialization](#initialization)þ* [Threading](#threading)þ* [Conventions](#conventions)þ* [Building libgit2 - Using CMake](#building-libgit2---using-cmake)þ    * [Building](#building)þ    * [Installation](#installation)þ    * [Advanced Usage](#advanced-usage)þ    * [Compiler and linker options](#compiler-and-linker-options)þ    * [MacOS X](#macos-x)þ    * [Android](#android)þ    * [MinGW](#mingw)þ* [Language Bindings](#language-bindings)þ* [How Can I Contribute?](#how-can-i-contribute)þ* [License](#license)þþQuick Startþ===========þþ**Prerequisites** for building libgit2:þþ1. [CMake](https://cmake.org/), and is recommended to be installed intoþ   your `PATH`.þ2. [Python](https://www.python.org) is used by our test framework, andþ   should be installed into your `PATH`.þ3. C compiler: libgit2 is C90 and should compile on most compilers.þ   * Windows: Visual Studio is recommendedþ   * Mac: Xcode is recommendedþ   * Unix: gcc or clang is recommended.þþ**Build**þþ1. Create a build directory beneath the libgit2 source directory, and changeþ   into it: `mkdir build && cd build`þ2. Create the cmake build environment: `cmake ..`þ3. Build libgit2: `cmake --build .`þþTrouble with these steps?  Read our [troubleshooting guide](docs/troubleshooting.md).þMore detailed build guidance is available below.þþGetting Helpþ============þþ**Chat with us**þþ- via IRC: join [#libgit2](https://webchat.freenode.net/#libgit2) on Freenodeþ- via Slack: visit [slack.libgit2.org](http://slack.libgit2.org/) to sign up,þ  then join us in `#libgit2`þþ**Getting Help**þþIf you have questions about the library, please be sure to check out theþ[API documentation](http://libgit2.github.com/libgit2/).  If you still haveþquestions, reach out to us on Slack or post a question on þ[StackOverflow](http://stackoverflow.com/questions/tagged/libgit2) (with the `libgit2` tag).þþ**Reporting Bugs**þþPlease open a [GitHub Issue](https://github.com/libgit2/libgit2/issues) andþinclude as much information as possible.  If possible, provide sample codeþthat illustrates the problem you're seeing.  If you're seeing a bug onlyþon a specific repository, please provide a link to it if possible.þþWe ask that you not open a GitHub Issue for help, only for bug reports.þþ**Reporting Security Issues**þþPlease have a look at SECURITY.md.þþWhat It Can Doþ==============þþlibgit2 provides you with the ability to manage Git repositories in theþprogramming language of your choice.  It's used in production to power manyþapplications including GitHub.com, Plastic SCM and Azure DevOps.þþIt does not aim to replace the git tool or its user-facing commands. Some APIsþresemble the plumbing commands as those align closely with the concepts of theþGit system, but most commands a user would type are out of scope for thisþlibrary to implement directly.þþThe library provides:þþ* SHA conversions, formatting and shorteningþ* abstracted ODB backend systemþ* commit, tag, tree and blob parsing, editing, and write-backþ* tree traversalþ* revision walkingþ* index file (staging area) manipulationþ* reference management (including packed references)þ* config file managementþ* high level repository managementþ* thread safety and reentrancyþ* descriptive and detailed error messagesþ* ...and more (over 175 different API calls)þþAs libgit2 is purely a consumer of the Git system, we have toþadjust to changes made upstream. This has two major consequences:þþ* Some changes may require us to change provided interfaces. While we try toþ  implement functions in a generic way so that no future changes are required,þ  we cannot promise a completely stable API.þ* As we have to keep up with changes in behavior made upstream, we may lagþ  behind in some areas. We usually to document these incompatibilities in ourþ  issue tracker with the label ""git change"".þþOptional dependenciesþ=====================þþWhile the library provides git functionality without the need forþdependencies, it can make use of a few libraries to add to it:þþ- pthreads (non-Windows) to enable threadsafe access as well as multi-threaded pack generationþ- OpenSSL (non-Windows) to talk over HTTPS and provide the SHA-1 functionsþ- LibSSH2 to enable the SSH transportþ- iconv (OSX) to handle the HFS+ path encoding peculiaritiesþþInitializationþ===============þþThe library needs to keep track of some global state. Callþþ    git_libgit2_init();þþbefore calling any other libgit2 functions. You can call this function many times. A matching number of calls toþþ    git_libgit2_shutdown();þþwill free the resources.  Note that if you have worker threads, you shouldþcall `git_libgit2_shutdown` *after* those threads have exited.  If youþrequire assistance coordinating this, simply have the worker threads callþ`git_libgit2_init` at startup and `git_libgit2_shutdown` at shutdown.þþThreadingþ=========þþSee [threading](docs/threading.md) for informationþþConventionsþ===========þþSee [conventions](docs/conventions.md) for an overview of the externalþand internal API/coding conventions we use.þþBuilding libgit2 - Using CMakeþ==============================þþBuildingþ--------þþ`libgit2` builds cleanly on most platforms without any external dependencies.þUnder Unix-like systems, like Linux, \*BSD and Mac OS X, libgit2 expects `pthreads` to be available;þthey should be installed by default on all systems. Under Windows, libgit2 uses the native Windows APIþfor threading.þþThe `libgit2` library is built using [CMake](<https://cmake.org/>) (version 2.8 or newer) on all platforms.þþOn most systems you can build the library using the following commandsþþ $ mkdir build && cd buildþ $ cmake ..þ $ cmake --build .þþAlternatively you can point the CMake GUI tool to the CMakeLists.txt file and generate platform specific build project or IDE workspace.þþRunning Testsþ-------------þþOnce built, you can run the tests from the `build` directory with the commandþþ $ ctest -VþþAlternatively you can run the test suite directly using,þþ $ ./libgit2_clarþþInvoking the test suite directly is useful because it allows you to executeþindividual tests, or groups of tests using the `-s` flag.  For example, toþrun the index tests:þþ    $ ./libgit2_clar -sindexþþTo run a single test named `index::racy::diff`, which corresponds to the testþfunction [`test_index_racy__diff`](https://github.com/libgit2/libgit2/blob/master/tests/index/racy.c#L23):þþ    $ ./libgit2_clar -sindex::racy::diffþþThe test suite will print a `.` for every passing test, and an `F` for anyþfailing test.  An `S` indicates that a test was skipped because it is notþapplicable to your platform or is particularly expensive.þþ**Note:** There should be _no_ failing tests when you build an unmodifiedþsource tree from a [release](https://github.com/libgit2/libgit2/releases),þor from the [master branch](https://github.com/libgit2/libgit2/tree/master).þPlease contact us or [open an issue](https://github.com/libgit2/libgit2/issues)þif you see test failures.þþInstallationþ------------þþTo install the library you can specify the install prefix by setting:þþ $ cmake .. -DCMAKE_INSTALL_PREFIX=/install/prefixþ $ cmake --build . --target installþþAdvanced Usageþ--------------þþFor more advanced use or questions about CMake please read <https://cmake.org/Wiki/CMake_FAQ>.þþThe following CMake variables are declared:þþ- `CMAKE_INSTALL_BINDIR`: Where to install binaries to.þ- `CMAKE_INSTALL_LIBDIR`: Where to install libraries to.þ- `CMAKE_INSTALL_INCLUDEDIR`: Where to install headers to.þ- `BUILD_SHARED_LIBS`: Build libgit2 as a Shared Library (defaults to ON)þ- `BUILD_CLAR`: Build [Clar](https://github.com/vmg/clar)-based test suite (defaults to ON)þ- `THREADSAFE`: Build libgit2 with threading support (defaults to ON)þþTo list all build options and their current value, you can do theþfollowing:þþ # Create and set up a build directoryþ $ mkdir buildþ $ cmake ..þ # List all build options and their valuesþ $ cmake -LþþCompiler and linker optionsþ---------------------------þþCMake lets you specify a few variables to control the behavior of theþcompiler and linker. These flags are rarely used but can be useful forþ64-bit to 32-bit cross-compilation.þþ- `CMAKE_C_FLAGS`: Set your own compiler flagsþ- `CMAKE_FIND_ROOT_PATH`: Override the search path for librariesþ- `ZLIB_LIBRARY`, `OPENSSL_SSL_LIBRARY` AND `OPENSSL_CRYPTO_LIBRARY`:þTell CMake where to find those specific librariesþþMacOS Xþ-------þþIf you want to build a universal binary for Mac OS X, CMake sets itþall up for you if you use `-DCMAKE_OSX_ARCHITECTURES=""i386;x86_64""`þwhen configuring.þþAndroidþ-------þþExtract toolchain from NDK using, `make-standalone-toolchain.sh` script.þOptionally, crosscompile and install OpenSSL inside of it. Then create CMakeþtoolchain file that configures paths to your crosscompiler (substitute `{PATH}`þwith full path to the toolchain):þþ SET(CMAKE_SYSTEM_NAME Linux)þ SET(CMAKE_SYSTEM_VERSION Android)þþ SET(CMAKE_C_COMPILER   {PATH}/bin/arm-linux-androideabi-gcc)þ SET(CMAKE_CXX_COMPILER {PATH}/bin/arm-linux-androideabi-g++)þ SET(CMAKE_FIND_ROOT_PATH {PATH}/sysroot/)þþ SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)þ SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)þ SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)þþAdd `-DCMAKE_TOOLCHAIN_FILE={pathToToolchainFile}` to cmake commandþwhen configuring.þþMinGWþ-----þþIf you want to build the library in MinGW environment with SSH support enabled,þyou may need to pass `-DCMAKE_LIBRARY_PATH=""${MINGW_PREFIX}/${MINGW_CHOST}/lib/""` flagþto CMake when configuring. This is because CMake cannot find the Win32 libraries inþMinGW folders by default and you might see an error message stating that CMakeþcould not resolve `ws2_32` library during configuration.þþAnother option would be to install `msys2-w32api-runtime` package before configuring.þThis package installs the Win32 libraries into `/usr/lib` folder which is by defaultþrecognized as the library path by CMake. Please note though that this package is meantþfor MSYS subsystem which is different from MinGW.þþLanguage Bindingsþ==================================þþHere are the bindings to libgit2 that are currently available:þþ* C++þ    * libqgit2, Qt bindings <https://projects.kde.org/projects/playground/libs/libqgit2/repository/>þ* Chicken Schemeþ    * chicken-git <https://wiki.call-cc.org/egg/git>þ* Dþ    * dlibgit <https://github.com/s-ludwig/dlibgit>þ* Delphiþ    * GitForDelphi <https://github.com/libgit2/GitForDelphi>þ* Erlangþ    * Geef <https://github.com/carlosmn/geef>þ* Goþ    * git2go <https://github.com/libgit2/git2go>þ* GObjectþ    * libgit2-glib <https://wiki.gnome.org/Projects/Libgit2-glib>þ* Guileþ * Guile-Git <https://gitlab.com/guile-git/guile-git>þ* Haskellþ    * hgit2 <https://github.com/jwiegley/gitlib>þ* Javaþ    * Jagged <https://github.com/ethomson/jagged>þ* Javascript / WebAssembly ( browser and nodejs )þ    * WASM-git <https://github.com/petersalomonsen/wasm-git>þ* Juliaþ    * LibGit2.jl <https://github.com/JuliaLang/julia/tree/master/stdlib/LibGit2>þ* Luaþ    * luagit2 <https://github.com/libgit2/luagit2>þ* .NETþ    * libgit2sharp <https://github.com/libgit2/libgit2sharp>þ* Node.jsþ    * nodegit <https://github.com/nodegit/nodegit>þ* Objective-Cþ    * objective-git <https://github.com/libgit2/objective-git>þ* OCamlþ    * ocaml-libgit2 <https://github.com/fxfactorial/ocaml-libgit2>þ* Parrot Virtual Machineþ    * parrot-libgit2 <https://github.com/letolabs/parrot-libgit2>þ* Perlþ    * Git-Raw <https://github.com/jacquesg/p5-Git-Raw>þ* PHPþ    * php-git <https://github.com/libgit2/php-git>þ* PowerShellþ    * PSGit <https://github.com/PoshCode/PSGit>þ* Pythonþ    * pygit2 <https://github.com/libgit2/pygit2>þ* Rþ    * git2r <https://github.com/ropensci/git2r>þ* Rubyþ    * Rugged <https://github.com/libgit2/rugged>þ* Rustþ    * git2-rs <https://github.com/rust-lang/git2-rs>þ* Swiftþ    * SwiftGit2 <https://github.com/SwiftGit2/SwiftGit2>þ* Valaþ    * libgit2.vapi <https://github.com/apmasell/vapis/blob/master/libgit2.vapi>þþIf you start another language binding to libgit2, please let us know soþwe can add it to the list.þþHow Can I Contribute?þ==================================þþWe welcome new contributors!  We have a number of issues marked asþ[""up for grabs""](https://github.com/libgit2/libgit2/issues?q=is%3Aissue+is%3Aopen+label%3A%22up+for+grabs%22)þandþ[""easy fix""](https://github.com/libgit2/libgit2/issues?utf8=✓&q=is%3Aissue+is%3Aopen+label%3A%22easy+fix%22)þthat are good places to jump in and get started.  There's much more detailedþinformation in our list of [outstanding projects](docs/projects.md).þþPlease be sure to check the [contribution guidelines](docs/contributing.md) toþunderstand our workflow, and the libgit2 [coding conventions](docs/conventions.md).þþLicenseþ==================================þþ`libgit2` is under GPL2 **with linking exception**. This means you can link toþand use the library from any program, proprietary or open source; paid orþgratis.  However, if you modify libgit2 itself, you must distribute theþsource to your modified version of libgit2.þþSee the [COPYING file](COPYING) for the full license text."
facebook/zstd,23803,12241,392,1143,Organization,False,7718,39,59,152,False,Zstandard - Fast real-time compression algorithm,http://www.zstd.net,0,25,0,41,673,28,72,10,1471,8,166,1972,17,291,18972,14362,0,0,125,168,,,"<p align=""center""><img src=""https://raw.githubusercontent.com/facebook/zstd/dev/doc/images/zstd_logo86.png"" alt=""Zstandard""></p>þþ__Zstandard__, or `zstd` as short version, is a fast lossless compression algorithm,þtargeting real-time compression scenarios at zlib-level and better compression ratios.þIt's backed by a very fast entropy stage, provided by [Huff0 and FSE library](https://github.com/Cyan4973/FiniteStateEntropy).þþThe project is provided as an open-source dual [BSD](LICENSE) and [GPLv2](COPYING) licensed **C** library,þand a command line utility producing and decoding `.zst`, `.gz`, `.xz` and `.lz4` files.þShould your project require another programming language,þa list of known ports and bindings is provided on [Zstandard homepage](http://www.zstd.net/#other-languages).þþ**Development branch status:**þþ[![Build Status][travisDevBadge]][travisLink]þ[![Build status][AppveyorDevBadge]][AppveyorLink]þ[![Build status][CircleDevBadge]][CircleLink]þ[![Build status][CirrusDevBadge]][CirrusLink]þ[![Fuzzing Status][OSSFuzzBadge]][OSSFuzzLink]þþ[travisDevBadge]: https://travis-ci.org/facebook/zstd.svg?branch=dev ""Continuous Integration test suite""þ[travisLink]: https://travis-ci.org/facebook/zstdþ[AppveyorDevBadge]: https://ci.appveyor.com/api/projects/status/xt38wbdxjk5mrbem/branch/dev?svg=true ""Windows test suite""þ[AppveyorLink]: https://ci.appveyor.com/project/YannCollet/zstd-p0yf0þ[CircleDevBadge]: https://circleci.com/gh/facebook/zstd/tree/dev.svg?style=shield ""Short test suite""þ[CircleLink]: https://circleci.com/gh/facebook/zstdþ[CirrusDevBadge]: https://api.cirrus-ci.com/github/facebook/zstd.svg?branch=devþ[CirrusLink]: https://cirrus-ci.com/github/facebook/zstdþ[OSSFuzzBadge]: https://oss-fuzz-build-logs.storage.googleapis.com/badges/zstd.svgþ[OSSFuzzLink]: https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:zstdþþ## BenchmarksþþFor reference, several fast compression algorithms were tested and comparedþon a server running Arch Linux (`Linux version 5.5.11-arch1-1`),þwith a Core i9-9900K CPU @ 5.0GHz,þusing [lzbench], an open-source in-memory benchmark by @inikepþcompiled with [gcc] 9.3.0,þon the [Silesia compression corpus].þþ[lzbench]: https://github.com/inikep/lzbenchþ[Silesia compression corpus]: http://sun.aei.polsl.pl/~sdeor/index.php?page=silesiaþ[gcc]: https://gcc.gnu.org/þþ| Compressor name         | Ratio | Compression| Decompress.|þ| ---------------         | ------| -----------| ---------- |þ| **zstd 1.4.5 -1**       | 2.884 |   500 MB/s |  1660 MB/s |þ| zlib 1.2.11 -1          | 2.743 |    90 MB/s |   400 MB/s |þ| brotli 1.0.7 -0         | 2.703 |   400 MB/s |   450 MB/s |þ| **zstd 1.4.5 --fast=1** | 2.434 |   570 MB/s |  2200 MB/s |þ| **zstd 1.4.5 --fast=3** | 2.312 |   640 MB/s |  2300 MB/s |þ| quicklz 1.5.0 -1        | 2.238 |   560 MB/s |   710 MB/s |þ| **zstd 1.4.5 --fast=5** | 2.178 |   700 MB/s |  2420 MB/s |þ| lzo1x 2.10 -1           | 2.106 |   690 MB/s |   820 MB/s |þ| lz4 1.9.2               | 2.101 |   740 MB/s |  4530 MB/s |þ| **zstd 1.4.5 --fast=7** | 2.096 |   750 MB/s |  2480 MB/s |þ| lzf 3.6 -1              | 2.077 |   410 MB/s |   860 MB/s |þ| snappy 1.1.8            | 2.073 |   560 MB/s |  1790 MB/s |þþ[zlib]: http://www.zlib.net/þ[LZ4]: http://www.lz4.org/þþThe negative compression levels, specified with `--fast=#`,þoffer faster compression and decompression speed in exchange for some loss inþcompression ratio compared to level 1, as seen in the table above.þþZstd can also offer stronger compression ratios at the cost of compression speed.þSpeed vs Compression trade-off is configurable by small increments.þDecompression speed is preserved and remains roughly the same at all settings,þa property shared by most LZ compression algorithms, such as [zlib] or lzma.þþThe following tests were runþon a server running Linux Debian (`Linux version 4.14.0-3-amd64`)þwith a Core i7-6700K CPU @ 4.0GHz,þusing [lzbench], an open-source in-memory benchmark by @inikepþcompiled with [gcc] 7.3.0,þon the [Silesia compression corpus].þþCompression Speed vs Ratio | Decompression Speedþ---------------------------|--------------------þ![Compression Speed vs Ratio](doc/images/CSpeed2.png ""Compression Speed vs Ratio"") | ![Decompression Speed](doc/images/DSpeed3.png ""Decompression Speed"")þþA few other algorithms can produce higher compression ratios at slower speeds, falling outside of the graph.þFor a larger picture including slow modes, [click on this link](doc/images/DCspeed5.png).þþþ## The case for Small Data compressionþþPrevious charts provide results applicable to typical file and stream scenarios (several MB). Small data comes with different perspectives.þþThe smaller the amount of data to compress, the more difficult it is to compress. This problem is common to all compression algorithms, and reason is, compression algorithms learn from past data how to compress future data. But at the beginning of a new data set, there is no ""past"" to build upon.þþTo solve this situation, Zstd offers a __training mode__, which can be used to tune the algorithm for a selected type of data.þTraining Zstandard is achieved by providing it with a few samples (one file per sample). The result of this training is stored in a file called ""dictionary"", which must be loaded before compression and decompression.þUsing this dictionary, the compression ratio achievable on small data improves dramatically.þþThe following example uses the `github-users` [sample set](https://github.com/facebook/zstd/releases/tag/v1.1.3), created from [github public API](https://developer.github.com/v3/users/#get-all-users).þIt consists of roughly 10K records weighing about 1KB each.þþCompression Ratio | Compression Speed | Decompression Speedþ------------------|-------------------|--------------------þ![Compression Ratio](doc/images/dict-cr.png ""Compression Ratio"") | ![Compression Speed](doc/images/dict-cs.png ""Compression Speed"") | ![Decompression Speed](doc/images/dict-ds.png ""Decompression Speed"")þþþThese compression gains are achieved while simultaneously providing _faster_ compression and decompression speeds.þþTraining works if there is some correlation in a family of small data samples. The more data-specific a dictionary is, the more efficient it is (there is no _universal dictionary_).þHence, deploying one dictionary per type of data will provide the greatest benefits.þDictionary gains are mostly effective in the first few KB. Then, the compression algorithm will gradually use previously decoded content to better compress the rest of the file.þþ### Dictionary compression How To:þþ1. Create the dictionaryþþ   `zstd --train FullPathToTrainingSet/* -o dictionaryName`þþ2. Compress with dictionaryþþ   `zstd -D dictionaryName FILE`þþ3. Decompress with dictionaryþþ   `zstd -D dictionaryName --decompress FILE.zst`þþþ## Build instructionsþþ### MakefileþþIf your system is compatible with standard `make` (or `gmake`),þinvoking `make` in root directory will generate `zstd` cli in root directory.þþOther available options include:þ- `make install` : create and install zstd cli, library and man pagesþ- `make check` : create and run `zstd`, tests its behavior on local platformþþ### cmakeþþA `cmake` project generator is provided within `build/cmake`.þIt can generate Makefiles or other build scriptsþto create `zstd` binary, and `libzstd` dynamic and static libraries.þþBy default, `CMAKE_BUILD_TYPE` is set to `Release`.þþ### MesonþþA Meson project is provided within [`build/meson`](build/meson). Followþbuild instructions in that directory.þþYou can also take a look at [`.travis.yml`](.travis.yml) file for anþexample about how Meson is used to build this project.þþNote that default build type is **release**.þþ### VCPKGþYou can build and install zstd [vcpkg](https://github.com/Microsoft/vcpkg/) dependency manager:þþ    git clone https://github.com/Microsoft/vcpkg.gitþ    cd vcpkgþ    ./bootstrap-vcpkg.shþ    ./vcpkg integrate installþ    ./vcpkg install zstdþþThe zstd port in vcpkg is kept up to date by Microsoft team members and community contributors.þIf the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.þþ### Visual Studio (Windows)þþGoing into `build` directory, you will find additional possibilities:þ- Projects for Visual Studio 2005, 2008 and 2010.þ  + VS2010 project is compatible with VS2012, VS2013, VS2015 and VS2017.þ- Automated build scripts for Visual compiler by [@KrzysFR](https://github.com/KrzysFR), in `build/VS_scripts`,þ  which will build `zstd` cli and `libzstd` library without any need to open Visual Studio solution.þþ### BuckþþYou can build the zstd binary via buck by executing: `buck build programs:zstd` from the root of the repo.þThe output binary will be in `buck-out/gen/programs/`.þþ## StatusþþZstandard is currently deployed within Facebook. It is used continuously to compress large amounts of data in multiple formats and use cases.þZstandard is considered safe for production environments.þþ## LicenseþþZstandard is dual-licensed under [BSD](LICENSE) and [GPLv2](COPYING).þþ## ContributingþþThe ""dev"" branch is the one where all contributions are merged before reaching ""master"".þIf you plan to propose a patch, please commit into the ""dev"" branch, or its own feature branch.þDirect commit to ""master"" are not permitted.þFor more information, please read [CONTRIBUTING](CONTRIBUTING.md)."
raspberrypi/linux,2405550,6807,745,3324,Organization,False,798503,66,53,9730,False,Kernel source tree for Raspberry Pi Foundation-provided kernel builds. Issues unrelated to the linux kernel should be posted on the community forum at https://www.raspberrypi.org/forum,,0,11,0,311,2133,106,79,27,1193,9,103,6641,52,374,3149,1651,0,0,25,1,,,
videolan/vlc,379481,6226,516,2331,Organization,False,85467,1,50,540,False,"VLC media player - All pull requests are ignored, please follow https://wiki.videolan.org/Sending_Patches_VLC/",http://www.videolan.org/vlc,7,0,0,,,,,1,98,1,5,6809,20,1351,42093,27455,0,0,18,6,,,
postgres/postgres,489062,6824,441,2281,Organization,False,49354,33,509,33,False,"Mirror of the official PostgreSQL GIT repository. Note that this is just a *mirror* - we don't work with pull requests on github. To contribute, please see https://wiki.postgresql.org/wiki/Submitting_a_Patch",https://www.postgresql.org/,0,0,0,,,,,0,52,0,8,8741,21,1052,342000,238238,0,0,9,5,,,
jonas/tig,6511,9101,173,519,User,False,2616,10,47,141,False,Text-mode interface for git,https://jonas.github.io/tig/,4,21,1,147,481,16,16,18,365,3,4,5178,5,23,6000,5688,0,0,110,,276,,
contiki-os/contiki,73109,3220,457,2465,Organization,False,12329,4,16,152,False,"The official git repository for Contiki, the open source OS for the Internet of Things",http://www.contiki-os.org/,0,30,2,430,521,15,3,168,1528,1,1,4093,0,0,0,0,0,0,4,9,,,"The Contiki Operating Systemþ============================þþ[![Build Status](https://travis-ci.org/contiki-os/contiki.svg?branch=master)](https://travis-ci.org/contiki-os/contiki/branches)þþContiki is an open source operating system that runs on tiny low-powerþmicrocontrollers and makes it possible to develop applications thatþmake efficient use of the hardware while providing standardizedþlow-power wireless communication for a range of hardware platforms.þþContiki is used in numerous commercial and non-commercial systems,þsuch as city sound monitoring, street lights, networked electricalþpower meters, industrial monitoring, radiation monitoring,þconstruction site monitoring, alarm systems, remote house monitoring,þand so on.þþFor more information, see the Contiki website:þþ[http://contiki-os.org](http://contiki-os.org)"
lpereira/lwan,4896,5196,309,542,User,False,2775,5,3,38,False,"Experimental, scalable, high performance HTTP server",https://lwan.ws,5,11,0,48,145,1,0,3,84,1,4,3064,4,173,3774,2245,0,0,70,,532,,"Lwan Web Serverþ===============þþLwan is a **high-performance** & **scalable** web server.þþThe [project web site](https://lwan.ws/) contains more details.þþBuild statusþ------------þþ| OS          | Arch   | Release | Debug | Static Analysis | Tests |þ|-------------|--------|---------|-------|-----------------|------------|þ| Linux       | x86_64 | ![release](https://shield.lwan.ws/img/gycKbr/release ""Release"")  | ![debug](https://shield.lwan.ws/img/gycKbr/debug ""Debug"")     | ![static-analysis](https://shield.lwan.ws/img/gycKbr/clang-analyze ""Static Analysis"") ![coverity](https://scan.coverity.com/projects/375/badge.svg) [Report history](https://buildbot.lwan.ws/sa/) | ![tests](https://shield.lwan.ws/img/gycKbr/unit-tests ""Test"") [![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/lwan.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:lwan)           |þ| Linux       | armv7  | ![release-arm](https://shield.lwan.ws/img/gycKbr/release-arm ""Release"")  | ![debug-arm](https://shield.lwan.ws/img/gycKbr/debug-arm ""Debug"")     |        |           |þ| FreeBSD     | x86_64 | ![freebsd-release](https://shield.lwan.ws/img/gycKbr/release-freebsd ""Release FreeBSD"") | ![freebsd-debug](https://shield.lwan.ws/img/gycKbr/debug-freebsd ""Debug FreeBSD"")     |                |           |þ| macOS       | x86_64 | ![osx-release](https://shield.lwan.ws/img/gycKbr/release-sierra ""Release macOS"")       | ![osx-debug](https://shield.lwan.ws/img/gycKbr/debug-sierra ""Debug macOS"")     |               |          |þ| OpenBSD 6.6 | x86_64 | ![openbsd-release](https://shield.lwan.ws/img/gycKbr/release-openbsd ""Release OpenBSD"")       | ![openbsd-debug](https://shield.lwan.ws/img/gycKbr/debug-openbsd ""Debug OpenBSD"")     |               | ![openbsd-tests](https://shield.lwan.ws/img/gycKbr/openbsd-unit-tests ""OpenBSD Tests"")         |þþBuildingþ--------þþBefore installing Lwan, ensure all dependencies are installed. All of themþare common dependencies found in any GNU/Linux distribution; package namesþwill be different, but it shouldn't be difficult to search using whateverþpackage management tool that's used by your distribution.þþ### Required dependenciesþþ - [CMake](https://cmake.org/), at least version 2.8þ - [ZLib](http://zlib.net)þþ### Optional dependenciesþþThe build system will look for these libraries and enable/link if available.þþ - [Lua 5.1](http://www.lua.org) or [LuaJIT 2.0](http://luajit.org)þ - [Valgrind](http://valgrind.org)þ - [Brotli](https://github.com/google/brotli)þ - [ZSTD](https://github.com/facebook/zstd)þ - Alternative memory allocators can be used by passing `-DUSE_ALTERNATIVE_MALLOC` to CMake with the following values:þ    - [""mimalloc""](https://github.com/microsoft/mimalloc)þ    - [""jemalloc""](http://jemalloc.net/)þ    - [""tcmalloc""](https://github.com/gperftools/gperftools)þ    - ""auto"": Autodetect from the list above, falling back to libc malloc if none foundþ - To run test suite:þ    - [Python](https://www.python.org/) (2.6+) with Requestsþ    - [Lua 5.1](http://www.lua.org)þ - To run benchmark:þ    - Special version of [Weighttp](https://github.com/lpereira/weighttp)þ    - [Matplotlib](https://github.com/matplotlib/matplotlib)þ - To build TechEmpower benchmark suite:þ    - Client libraries for either [MySQL](https://dev.mysql.com) or [MariaDB](https://mariadb.org)þ    - [SQLite 3](http://sqlite.org)þþþ### Common operating system package namesþþ#### Minimum to buildþ - ArchLinux: `pacman -S cmake zlib`þ - FreeBSD: `pkg install cmake pkgconf`þ - Ubuntu 14+: `apt-get update && apt-get install git cmake zlib1g-dev pkg-config`þ - macOS: `brew install cmake`þþ#### Build all examplesþ - ArchLinux: `pacman -S cmake zlib sqlite luajit libmariadbclient gperftools valgrind`þ - FreeBSD: `pkg install cmake pkgconf sqlite3 lua51`þ - Ubuntu 14+: `apt-get update && apt-get install git cmake zlib1g-dev pkg-config lua5.1-dev libsqlite3-dev libmysqlclient-dev`þ - macOS: `brew install cmake mysql-connector-c sqlite lua@5.1 pkg-config`þþ### Build commandsþþ#### Clone the repositoryþþ    ~$ git clone git://github.com/lpereira/lwanþ    ~$ cd lwanþþ#### Create the build directoryþþ    ~/lwan$ mkdir buildþ    ~/lwan$ cd buildþþ#### Select build typeþþSelecting a *release* version (no debugging symbols, messages, enable someþoptimizations, etc):þþ    ~/lwan/build$ cmake .. -DCMAKE_BUILD_TYPE=ReleaseþþIf you'd like to enable optimizations but still use a debugger, use this instead:þþ    ~/lwan/build$ cmake .. -DCMAKE_BUILD_TYPE=RelWithDebInfoþþTo disable optimizations and build a more debugging-friendly version:þþ    ~/lwan/build$ cmake .. -DCMAKE_BUILD_TYPE=Debugþþ#### Build Lwanþþ    ~/lwan/build$ makeþþThis will generate a few binaries:þþ - `src/bin/lwan/lwan`: The main Lwan executable. May be executed with `--help` for guidance.þ - `src/bin/testrunner/testrunner`: Contains code to execute the test suite.þ - `src/samples/freegeoip/freegeoip`: [FreeGeoIP sample implementation](https://freegeoip.lwan.ws). Requires SQLite.þ - `src/samples/techempower/techempower`: Code for the TechEmpower Web Framework benchmark. Requires SQLite and MySQL libraries.þ - `src/samples/clock/clock`: [Clock sample](https://time.lwan.ws). Generates a GIF file that always shows the local time.þ - `src/bin/tools/mimegen`: Builds the extension-MIME type table. Used during build process.þ - `src/bin/tools/bin2hex`: Generates a C file from a binary file, suitable for use with #include.þ - `src/bin/tools/configdump`: Dumps a configuration file using the configuration reader API.þþ#### RemarksþþPassing `-DCMAKE_BUILD_TYPE=Release` will enable some compilerþoptimizations (such as [LTO](http://gcc.gnu.org/wiki/LinkTimeOptimization))þand tune the code for current architecture. *Please use this versionþwhen benchmarking*, as the default is the Debug build, which not onlyþlogs all requests to the standard output, but does so while holding aþmutex.þþThe default build (i.e. not passing `-DCMAKE_BUILD_TYPE=Release`) will buildþa version suitable for debugging purposes.  This version can be used underþValgrind *(if its headers are present)* and includes debugging messages thatþare stripped in the release version.  Debugging messages are printed forþeach and every request.þþOn debug builds, sanitizers can be enabled.  To select which one to build Lwanþwith, specify one of the following options to the CMake invocation line:þþ - `-DSANITIZER=ubsan` selects the Undefined Behavior Sanitizer.þ - `-DSANITIZER=address` selects the Address Sanitizer.þ - `-DSANITIZER=thread` selects the Thread Sanitizer.þþAlternative memory allocators can be selected as well.  Lwan currentlyþsupports [TCMalloc](https://github.com/gperftools/gperftools),þ[mimalloc](https://github.com/microsoft/mimalloc), andþ[jemalloc](http://jemalloc.net/) out of the box.  To use either one of them,þpass `-DALTERNATIVE_MALLOC=ON` to the CMake invocation line.þþ### Testsþþ    ~/lwan/build$ make testsuiteþþThis will compile the `testrunner` program and execute regression test suiteþin `src/scripts/testsuite.py`.þþ### Benchmarkþþ    ~/lwan/build$ make benchmarkþþThis will compile `testrunner` and execute benchmark scriptþ`src/scripts/benchmark.py`.þþ### CoverageþþLwan can also be built with the Coverage build type by specifyingþ`-DCMAKE_BUILD_TYPE=Coverage`.  This enables the `generate-coverage` makeþtarget, which will run `testrunner` to prepare a test coverage report withþ[lcov](http://ltp.sourceforge.net/coverage/lcov.php).þþEvery commit in this repository triggers the generation of this report,þand results are [publicly available](https://buildbot.lwan.ws/lcov/).þþRunningþ-------þþSet up the server by editing the provided `lwan.conf`; the format isþexplained in details below.  (Lwan will try to find a configuration fileþbased in the executable name in the current directory; `testrunner.conf`þwill be used for the `testrunner` binary, `lwan.conf` for the `lwan` binary,þand so on.)þþConfiguration files are loaded from the current directory. If no changesþare made to this file, running Lwan will serve static files located inþthe `./wwwroot` directory. Lwan will listen on port 8080 on all interfaces.þþLwan will detect the number of CPUs, will increase the maximum number ofþopen file descriptors and generally try its best to autodetect reasonableþsettings for the environment it's running on.  Many of these settings canþbe tweaked in the configuration file, but it's usually a good idea to notþmess with them.þþOptionally, the `lwan` binary can be used for one-shot static file servingþwithout any configuration file. Run it with `--help` for help on that.þþConfiguration Fileþ----------------þþ### FormatþþLwan uses a familiar `key = value` configuration file syntax.  Comments areþsupported with the `#` character (similar to e.g.  shell scripts, Python,þand Perl).  Nested sections can be created with curly brackets.  Sectionsþcan be empty; in this case, curly brackets are optional.þþ`some_key_name` is equivalent to `some key name` in configuration files (asþan implementation detail, code reading configuration options will only beþgiven the version with underscores).þþValues can contain environment variables. Use the syntax `${VARIABLE_NAME}`.þDefault values can be specified with a colon (e.g.  `${VARIABLE_NAME:foo}`,þwhich evaluates to `${VARIABLE_NAME}` if it's set, or `foo` otherwise).þþ```þsound volume = 11 # This one is 1 louderþþplaylist metal {þ   files = '''þ /multi/line/strings/are/supported.mp3þ /anything/inside/these/are/stored/verbatim.mp3þ   '''þ}þþplaylist chiptune {þ   files = """"""þ /if/it/starts/with/single/quotes/it/ends/with/single/quotes.modþ /but/it/can/use/double/quotes.s3mþ   """"""þ}þ```þþSome examples can be found in `lwan.conf` and `techempower.conf`.þþ#### Value typesþþ| Type   | Description |þ|--------|-------------|þ| `str`  | Any kind of free-form text, usually application specific |þ| `int`  | Integer number. Range is application specific |þ| `time` | Time interval.  See table below for units |þ| `bool` | Boolean value. See table below for valid values |þþ#### Time IntervalsþþTime fields can be specified using multipliers. Multiple can be specified, they'reþjust added together; for instance, ""1M 1w"" specifies ""1 month and 1 week"".  The followingþtable lists all known multipliers:þþ| Multiplier | Description |þ|------------|-------------|þ| `s`        | Seconds |þ| `m`        | Minutes |þ| `h`        | Hours |þ| `d`        | Days |þ| `w`        | Weeks |þ| `M`        | Months |þ| `y`        | Years |þþA number with a multiplier not in this table is ignored; a warning is issued whileþreading the configuration file.  No spaces must exist between the number and itsþmultiplier.þþ#### Boolean Valuesþþ| True Values | False Values |þ|-------------|--------------|þ| Any integer number different than 0 | 0 |þ| `on` | `off` |þ| `true` | `false` |þ| `yes` | `no` |þþ### Global SettingsþþIt's generally a good idea to let Lwan decide the best settings for yourþenvironment.  However, not every environment is the same, and not all usesþcan be decided automatically, so some configuration options are provided.þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `keep_alive_timeout` | `time`  | `15` | Timeout to keep a connection alive |þ| `quiet` | `bool` | `false` | Set to true to not print any debugging messages. Only effective in release builds. |þ| `reuse_port` | `bool` | `false` | Sets `SO_REUSEPORT` to `1` in the master socket |þ| `expires` | `time` | `1M 1w` | Value of the ""Expires"" header. Default is 1 month and 1 week |þ| `threads` | `int` | `0` | Number of I/O threads. Default (0) is the number of online CPUs |þ| `proxy_protocol` | `bool` | `false` | Enables the [PROXY protocol](https://www.haproxy.com/blog/haproxy/proxy-protocol/). Versions 1 and 2 are supported. Only enable this setting if using Lwan behind a proxy, and the proxy supports this protocol; otherwise, this allows anybody to spoof origin IP addresses |þ| `max_post_data_size` | `int` | `40960` | Sets the maximum number of data size for POST requests, in bytes |þþ### StraitjacketþþLwan can drop its privileges to a user in the system, and limit itsþfilesystem view with a chroot.  While not bulletproof, this provides aþfirst layer of security in the case there's a bug in Lwan.þþIn order to use this feature, declare a `straitjacket` section, and setþsome options.  This requires Lwan to be executed as `root`.þþAlthough this section can be written anywhere in the file (as long asþit is a top level declaration), if any directories are open, due toþe.g.  instantiating the `serve_files` module, Lwan will refuse toþstart.  (This check is only performed on Linux as a safeguard forþmalconfiguration.)þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `user` | `str`  | `NULL` | Drop privileges to this user name |þ| `chroot` | `str` | `NULL` | Path to `chroot()` |þ| `drop_capabilities` | `bool` | `true` | Drop all capabilities with capset(2) (under Linux), or pledge(2) (under OpenBSD). |þþ### ListenersþþIn order to specify which interfaces Lwan should listen on, a `listener` sectionþmust be specified.  Only one listener per Lwan process is accepted at the moment.þThe only parameter to a listener block is the interface address and the port toþlisten on; anything inside a listener section are instances of modules.þþThe syntax for the listener parameter is `${ADDRESS}:${PORT}`, where `${ADDRESS}`þcan either be `*` (binding to all interfaces), an IPv6 address (if surrounded byþsquare brackets), an IPv4 address, or a hostname.  If systemd's socket activationþis used, `systemd` can be specified as a parameter.þþ### Routing URLs Using Modules or HandlersþþIn order to route URLs, Lwan matches the largest common prefix from the requestþURI with a set of prefixes specified in the listener section.  How a request toþa particular prefix will be handled depends on which handler or module has beenþdeclared in the listener section.  Handlers and modules are similar internally;þhandlers are merely functions and hold no state, and modules holds state (namedþinstance).  Multiple instances of a module can appear in a listener section.þþThere is no special syntax to attach a prefix to a handler or module; all theþconfiguration parser rules apply here.  Use `${NAME} ${PREFIX}` to link theþ`${PREFIX}` prefix path to either a handler named `${NAME}` (if `${NAME}`þbegins with `&`, as with C's ""address of"" operator), or a module namedþ`${NAME}`.  Empty sections can be used here.þþEach module will have its specific set of options, and they're listed in theþnext sections.  In addition to configuration options, a special `authorization`þsection can be present in the declaration of a module instance.  Handlers doþnot take any configuration options, but may include the `authorization`þsection.þþA list of built-in modules can be obtained by executing Lwan with the `-m`þcommand-line argument.  The following is some basic documentation for theþmodules shipped with Lwan.þþ#### File ServingþþThe `serve_files` module will serve static files, and automatically createþdirectory indices or serve pre-compressed files.  It'll generally try itsþbest to serve files in the fastest way possible according to some heuristics.þþþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `path`                     | `str`  | `NULL`       | Path to a directory containing files to be served |þ| `index_path`               | `str`  | `index.html` | File name to serve as an index for a directory |þ| `serve_precompressed_path` | `bool` | `true`       | If $FILE.gz exists, is smaller and newer than $FILE, and the client accepts `gzip` encoding, transfer it |þ| `auto_index`               | `bool` | `true`       | Generate a directory list automatically if no `index_path` file present.  Otherwise, yields 404 |þ| `auto_index_readme`        | `bool` | `true`       | Includes the contents of README files as part of the automatically generated directory index |þ| `directory_list_template`  | `str`  | `NULL`       | Path to a Mustache template for the directory list; by default, use an internal template |þ| `read_ahead`               | `int`  | `131702`     | Maximum amount of bytes to read ahead when caching open files.  A value of `0` disables readahead.  Readahead is performed by a low priority thread to not block the I/O threads while file extents are being read from the filesystem. |þ| `cache_for`                | `time` | `5s`         | Time to keep file metadata (size, compressed contents, open file descriptor, etc.) in cache |þþ#### LuaþþThe `lua` module will allow requests to be serviced by scripts written inþthe [Lua](https://www.lua.org/) programming language.  Although theþfunctionality provided by this module is quite spartan, it's able to runþframeworks such as [Sailor](https://github.com/lpereira/sailor-hello-lwan).þþScripts can be served from files or embedded in the configuration file, andþthe results of loading them, the standard Lua modules, and (optionally, ifþusing LuaJIT) optimizing the code will be cached for a while.  Each I/Oþthread in Lwan will create an instance of a Lua VM (i.e.  one `lua_State`þstruct for every I/O thread), and each Lwan coroutine will spawn a Luaþthread (with `lua_newthread()`) per request.  Because of this, Lua scriptsþcan't use global variables, as they may be not only serviced by differentþthreads, but the state will be available only for the amount of timeþspecified in the `cache_period` configuration option.þþThere's no need to have one instance of the Lua module for each endpoint; aþsingle script, embedded in the configuration file or otherwise, can serviceþmany different endpoints.  Scripts are supposed to implement functions withþthe following signature: `handle_${METHOD}_${ENDPOINT}(req)`, whereþ`${METHOD}` can be a HTTP method (i.e.  `get`, `post`, `head`, etc.), andþ`${ENDPOINT}` is the desired endpoint to be handled by that function.  Theþspecial `${ENDPOINT}` `root` can be specified to act as a catchall.  Theþ`req` parameter points to a metatable that contains methods to obtainþinformation from the request, or to set the response, as seen below:þþ   - `req:query_param(param)` returns the query parameter (from the query string) with the key `param`, or `nil` if not foundþ   - `req:post_param(param)` returns the post parameter (only for `${POST}` handlers) with the key `param`, or `nil` if not foundþ   - `req:set_response(str)` sets the response to the string `str`þ   - `req:say(str)` sends a response chunk (using chunked encoding in HTTP)þ   - `req:send_event(event, str)` sends an event (using server-sent events)þ   - `req:cookie(param)` returns the cookie named `param`, or `nil` is not foundþ   - `req:set_headers(tbl)` sets the response headers from the table `tbl`; a header may be specified multiple times by using a table, rather than a string, in the table value (`{'foo'={'bar', 'baz'}}`); must be called before sending any response with `say()` or `send_event()`þ   - `req:sleep(ms)` pauses the current handler for the specified amount of millisecondsþ   - `req:ws_upgrade()` returns `1` if the connection could be upgraded to a WebSocket; `0` otherwiseþ   - `req:ws_write(str)` sends `str` through the WebSocket-upgraded connectionþ   - `req:ws_read()` returns a string obtained from the WebSocket, or `nil` on errorþþHandler functions may return either `nil` (in which case, a `200 OK` responseþis generated), or a number matching an HTTP status code.  Attempting to returnþan invalid HTTP status code or anything other than a number or `nil` will resultþin a `500 Internal Server Error` response being thrown.þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `default_type` | `str` | `text/plain` | Default MIME-Type for responses |þ| `script_file` | `str` | `NULL` | Path to Lua script|þ| `cache_period` | `time` | `15s` | Time to keep Lua state loaded in memory |þ| `script` | `str` | `NULL` | Inline lua script |þþ#### RewriteþþThe `rewrite` module will matchþ[patterns](https://man.openbsd.org/patterns.7) in URLs and give the optionþto either redirect to another URL, or rewrite the request in a way that Lwanþwill handle the request as if it were made in that way originally.  Theþpatterns are a special kind of regular expressions, forked from Lua 5.3.1,þthat do not contain backreferences and other features that could createþdenial-of-service issues in Lwan.  The new URL can be specified using aþsimple text substitution syntax, or use Lua scripts; Lua scripts willþcontain the same metamethods available in the `req` metatable provided byþthe Lua module, so it can be quite powerful.þþEach instance of the rewrite module will require a `pattern` and the actionþto execute when such pattern is matched.  Patterns are evaluated in theþorder they appear in the configuration file, and are specified using nestedþsections in the configuration file.  For instance, consider the followingþexample, where two patterns are specified:þþ```þrewrite /some/base/endpoint {þ    pattern posts/(%d+) {þ        # Matches /some/base/endpointposts/2600 and /some/base/endpoint/posts/2600þ        rewrite_as = /cms/view-post?id=%1þ    }þ    pattern imgur/(%a+)/(%g+) {þ        # Matches /some/base/endpointimgur/gif/mpT94Ld and /some/base/endpoint/imgur/gif/mpT94Ldþ        redirect_to = https://i.imgur.com/%2.%1þ    }þ}þ```þþThis example defines two patterns, one providing a nicer URL that's hiddenþfrom the user, and another providing a different way to obtain a direct linkþto an image hosted on a popular image hosting service (i.e.  requestingþ`/some/base/endpoint/imgur/mp4/4kOZNYX` will redirect directly to a resourceþin the Imgur service).þþThe value of `rewrite_as` or `redirect_to` can be Lua scripts as well; inþwhich case, the option `expand_with_lua` must be set to `true`, and, insteadþof using the simple text substitution syntax as the example above, aþfunction named `handle_rewrite(req, captures)` has to be defined instead.þThe `req` parameter is documented in the Lua module section; the `captures`þparameter is a table containing all the captures, in order.  This functionþreturns the new URL to redirect to.þþThis module has no options by itself.  Options are specified in each andþevery pattern.þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `rewrite_as` | `str` | `NULL` | Rewrite the URL following this pattern |þ| `redirect_to` | `str` | `NULL` | Redirect to a new URL following this pattern |þ| `expand_with_lua` | `bool` | `false` | Use Lua scripts to redirect to or rewrite a request |þþ`redirect_to` and `rewrite_as` options are mutually exclusive, and one ofþthem must be specified at least.þþ#### RedirectþþThe `redirect` module will, as it says in the tin, generate a `301þMoved permanently` (by default; the code can be changed, see below)þresponse, according to the options specified in its configuration.þGenerally, the `rewrite` module should be used instead as it packs moreþfeatures; however, this module serves also as an example of how toþwrite Lwan modules (less than 100 lines of code).þþIf the `to` option is not specified, it always generates a `500þInternal Server Error` response.  Specifying an invalid HTTP code, or aþcode that Lwan doesn't know about (see `enum lwan_http_status`), willþproduce a `301 Moved Permanently` response.þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `to` | `str` | `NULL` | The location to redirect to |þ| `code` | `int` | `301` | The HTTP code to perform a redirect |þþ#### ResponseþþThe `response` module will generate an artificial response of any HTTP code.þIn addition to also serving as an example of how to write a Lwan module,þit can be used to carve out voids from other modules (e.g. generating aþ`405 Not Allowed` response for files in `/.git`, if `/` is served withþthe `serve_files` module).þþIf the supplied `code` falls outside the response codes known by Lwan,þa `404 Not Found` error will be sent instead.þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `code` | `int` | `999` | A HTTP response code |þþ### Authorization SectionþþAuthorization sections can be declared in any module instance or handler,þand provides a way to authorize the fulfillment of that request throughþthe standard HTTP authorization mechanism.  In order to require authorizationþto access a certain module instance or handler, declare an `authorization`þsection with a `basic` parameter, and set one of its options.þþ| Option | Type | Default | Description |þ|--------|------|---------|-------------|þ| `realm` | `str` | `Lwan` | Realm for authorization. This is usually shown in the user/password UI in browsers |þ| `password_file` | `str` | `NULL` | Path for a file containing username and passwords (in clear text).  The file format is the same as the configuration file format used by Lwan |þþHackingþ-------þþPlease read this section (and follow it) if you're planning on contributingþto Lwan.  There's nothing unexpected here; this mostly follows the rules andþexpectations of many other FOSS projects, but every one expects things aþlittle bit different from one another.þþ### Coding StyleþþLwan tries to follow a consistent coding style throughout the project.  If you'reþconsidering contributing a patch to the project, please respect this style by tryingþto match the style of the surrounding code.  In general:þþ - `global_variables_are_named_like_this`, even though they tend to be rare and should be marked as `static` (with rare exceptions)þ - Local variables are usually shorter, e.g. `local_var`, `i`, `conn`þ - Struct names are often as short as they're descriptive.  `typedef` for structs are rarely used in Lwanþ - Header files should use `#pragma once` instead of the usual include guard hackeryþ - Functions that are used between .c files but are not APIs to be exposed to liblwan should have their prototype added to `lwan-private.h`þ - Functions should be short and sweet.  Exceptions may applyþ - Public functions should be prefixed with `lwan_`þ - Public types should be prefixed with `lwan_`þ - Private functions must be static, and can be named without the `lwan_` prefixþ - Code is indented with 4 spaces; don't use tabsþ - There's a suggested line break at column 80, but it's not enforcedþ - `/* Old C-style comments are preferred */`þ - `clang-format` can be used to format the source code in an acceptable way; a `.clang-format` file is providedþþ### TestsþþIf modifying well-tested areas of the code (e.g. the event loop, HTTP parser,þetc.), please add a new integration test and make sure that, before you send aþpull request, all tests (including the new ones you've sent) are working.þTests can be added by modifying `src/scripts/testsuite.py`, and executed byþeither invoking that script directly from the source root, or executing theþ`testsuite` build target.þþSome tests will only work on Linux, and won't be executed on other platforms.þþ### Fuzz-testingþþLwan is automatically fuzz-tested byþ[OSS-Fuzz](https://github.com/google/oss-fuzz/).  To fuzz-test locally,þthough, one can [follow the instructions to testþlocally](https://github.com/google/oss-fuzz/blob/master/docs/new_project_guide.md#testing-locally).þþThis fuzzes only the request parsing code.  There are plans to add fuzzingþdrivers for other parts of the code, including the rewriting engine,þconfiguration file reader, template parser, and URL routing.þþ### Exporting APIsþþThe shared object version of `liblwan` on ELF targets (e.g. Linux) will useþa symbol filter script to hide symbols that are considered private to theþlibrary.  Please edit `src/lib/liblwan.sym` to add new symbols that shouldþbe exported to `liblwan.so`.þþ### Using Git and Pull RequestsþþLwan tries to maintain a source history that's as flat as possible, devoid ofþmerge commits.  This means that pull requests should be rebased on top of theþcurrent master before they can be merged; sometimes this can be doneþautomatically by the GitHub interface, sometimes they need some manual work toþfix conflicts.  It is appreciated if the contributor fixes these conflicts whenþasked.þþIt is advisable to push your changes to your fork on a branch-per-pull request,þrather than pushing to the `master` branch; the reason is explained below.þþPlease ensure that Git is configured properly with your name (it doesn't reallyþmatter if it is your legal name or a nickname, but it should be enough to creditþyou) and a valid email address.  There's no need to add `Signed-off-by` lines,þeven though it's fine to send commits with them.þþIf a change is requested in a pull request, you have two choices:þþ - *Reply asking for clarification.*  Maybe the intentions were not clear enough,þand whoever asked for changes didn't fully understand what you were trying toþachieveþ - *Fix the issue.*  When fixing issues found in pull requests, *please* useþ[interactive rebases](https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History) toþsquash or fixup commits; don't add your fixes on top of your tree.  Do not createþanother pull request just to accomodate the changes. After rewritingþthe history locally, force-push to your PR branch; the PR will update automaticallyþwith your changes.  Rewriting the history of development branches is fine, andþforce-pushing them is normal and expectedþþIt is not enforced, but it is recommended to create smaller commits. Howþcommits are split in Lwan is pretty much arbitrary, so please take a look atþthe commit history to get an idea on how the division should be made.  Gitþoffers a plethora of commands to achieve this result: the already mentionedþinteractive rebase, the `-p` option to `git add`, and `git commit --amend`þare good examples.þþCommit messages should have one line of summary (~72 chars), followed by anþempty line, followed by paragraphs of 80-char lines explaining the change.  Theþparagraphs explaining the changes are usually not necessary if the summaryþis good enough.  Try to [write good commit messages](https://chris.beams.io/posts/git-commit/).þþ### LicensingþþLwan is licensed under the GNU General Public License, version 2, or (at your option),þany later version.  Therefore:þþ - Code must be either LGPLv2.1, GPLv2, a permissive ""copyfree"" license that is compatibleþwith GPLv2 (e.g. MIT, BSD 3-clause), or public domain code (e.g. CC0)þ - Although the program can be distributed and used as if it were licensed as GPLv3,þits code must be compatible with GPLv2 as well; no new code can be licensed under versionsþof GPL newer than 2þ - Likewise, code licensed under licenses compatible with GPLv3 butþincompatible with GPLv2 (e.g.  Apache 2) are not suitable for inclusion inþLwanþ - Even if the license does not specify that credit should be given (e.g. CC0-licensed code),þplease give credit to the original author for that piece of codeþ - Contrary to popular belief, it is possible to use a GPL'd piece of code on a server withoutþhaving to share the code for your application.  It is only when the binary of that server isþshared that source must be available to whoever has that binary.  Merely accessing a Lwanþserver through HTTP does not qualify as having access to the binary program that's runningþon the serverþ - When in doubt, don't take legal advice from a README file: please consultþa lawyer that understands free software licensingþþPortabilityþ-----------þþWhile Lwan was written originally for Linux, it has been ported to BSDþsystems as well.  The build system will detect the supported featuresþand build support library functions as appropriate.þþFor instance, [epoll](https://en.wikipedia.org/wiki/Epoll) has beenþimplemented on top of [kqueue](https://en.wikipedia.org/wiki/Kqueue), andþLinux-only syscalls and GNU extensions have been implemented for theþsupported systems.  [This blog post](https://tia.mat.br/posts/2018/06/28/include_next_and_portability.html)þexplains the details and how `#include_next` is used.þþPerformanceþ-----------þþIt can achieve good performance, yielding about **320000 requests/second**þon a Core i7 laptop for requests without disk access, and without pipelining.þþWhen disk I/O is required, for files up to 16KiB, it yields aboutþ**290000 requests/second**; for larger files, this drops to **185000þrequests/second**, which isn't too shabby either.þþThese results, of course, with keep-alive connections, and with weighttpþrunning on the same machine (and thus using resources that could be usedþfor the webserver itself).þþWithout keep-alive, these numbers drop around 6-fold.þþIRC Channelþ-----------þþThere is an IRC channel (`#lwan`) on [Freenode](http://freenode.net). Aþstandard IRC client can be used.  A [web IRC gateway](http://webchat.freenode.net?channels=%23lwan&uio=d4)þis also available.þþLwan in the wildþ----------------þþHere's a non-definitive list of third-party stuff that uses Lwan and haveþbeen seen in the wild.  *Help build this list!*þþ* [This project uses Cython and Lwan](https://www.erp5.com/NXD-Blog.Multicore.Python.HTTP.Server) to make it possible to write handlers in Python.þ* [An experimental version of Node.js using Lwan](https://github.com/raadad/node-lwan) as its HTTP server is maintained by [@raadad](https://github.com/raadad).þ* The beginnings of a C++11 [web framework](https://github.com/vileda/wfpp) based on Lwan written by [@vileda](https://github.com/vileda).þ* A more complete C++14 [web framework](https://github.com/matt-42/silicon) by [@matt-42](https://github.com/matt-42) offers Lwan as one of its backends.þ* A [word ladder sample program](https://github.com/sjnam/lwan-sgb-ladders) by [@sjnam](https://github.com/sjnam). [Demo](http://tbcoe.ddns.net/sgb/ladders?start=chaos&goal=order).þ* A [Shodan search](https://www.shodan.io/search?query=server%3A+lwan) listing some brave souls that expose Lwan to the public internet.þþSome other distribution channels were made available as well:þþ* A `Dockerfile` is maintained by [@jaxgeller](https://github.com/jaxgeller), and is [available from the Docker registry](https://hub.docker.com/r/jaxgeller/lwan/).þ* A buildpack for Heroku is maintained by [@bherrera](https://github.com/bherrera), and is [available from its repo](https://github.com/bherrera/heroku-buildpack-lwan).þ* Lwan is also available as a package in [Biicode](http://docs.biicode.com/c++/examples/lwan.html).þ* It's also available in some GNU/Linux distributions:þ    * [Arch Linux](https://aur.archlinux.org/packages/lwan-git/)þ    * [Ubuntu](https://launchpad.net/lwan-unofficial)þ    * [Alpine Linux](https://pkgs.alpinelinux.org/package/edge/testing/x86_64/lwan)þ    * [NixOS](https://nixos.org/nixos/packages.html#lwan)þ* It's also available as a package for the [Nanos unikernel](https://github.com/nanovms/nanos).þþLwan has been also used as a benchmark:þþ* [Raphael Javaux's master thesis](https://github.com/RaphaelJ/master-thesis) cites Lwan in chapter 5 (""Performance Analysis"").þ* Lwan is used as a benchmark by the [PyParallel](http://pyparallel.org/) [author](https://www.reddit.com/r/programming/comments/3jhv80/pyparallel_an_experimental_proofofconcept_fork_of/cur4tut).þ* [Kong](https://getkong.org/about/benchmark/) uses Lwan as the [backend API](https://gist.github.com/montanaflynn/01376991f0a3ad07059c) in its benchmark.þ* [TechEmpower Framework benchmarks](https://www.techempower.com/benchmarks/#section=data-r10&hw=peak&test=json) feature Lwan since round 10.þ* [KrakenD](http://www.krakend.io) used Lwan for the REST API in all official [benchmarks](http://www.krakend.io/docs/benchmarks/aws/)þþMentions in academic journals:þþ* [A dynamic predictive race detector for C/C++ programs](https://link.springer.com/article/10.1007/s11227-017-1996-8) uses Lwan as a ""real world example"".þþSome talks mentioning Lwan:þþ* [Talk about Lwan](https://www.youtube.com/watch?v=cttY9FdCzUE) at Polyconf16, given by [@lpereira](https://github.com/lpereira).þ* This [talk about Iron](https://michaelsproul.github.io/iron-talk/), a framework for Rust, mentions Lwan as an *insane C thing*.þ* [University seminar presentation](https://github.com/cu-data-engineering-s15/syllabus/blob/master/student_lectures/LWAN.pdf) about Lwan.þ* This [presentation about Sailor web framework](http://www.slideshare.net/EtieneDalcol/web-development-with-lua-bulgaria-web-summit) mentions Lwan.þ* [Performance and Scale @ Istio Service Mesh](https://www.youtube.com/watch?v=G4F5aRFEXnU), at around 7:30min, presented at KubeCon Europe 2018, mentions that Lwan is used on the server side for testing due to its performance and robustness.þ* [A multi-core Python HTTP server (much) faster than Go (spoiler: Cython)](https://www.youtube.com/watch?v=mZ9cXOH6NYk) presented at PyConFR 2018 by J.-P. Smets mentions [Nexedi's work](https://www.nexedi.com/NXD-Blog.Multicore.Python.HTTP.Server) on using Lwan as a backend for Python services with Cython.þþNot really third-party, but alas:þþ* The [author's blog](http://tia.mat.br).þ* The [project's webpage](http://lwan.ws).þþLwan quotesþ-----------þþThese are some of the quotes found in the wild about Lwan.  They're presentedþin no particular order.  Contributions are appreciated:þþ> ""I read lwan's source code. Especially, the part of using coroutine wasþ> very impressive and it was more interesting than a good novel.  Thank youþ> for that."" --þ> [@patagonia](https://twitter.com/hakman314/status/996617563470680064)þþ> ""For the server side, we're using Lwan, which can handle 100k+ reqs/s.þ> It's supposed to be super robust and it's working well for us."" --þ> [@fawadkhaliq](https://twitter.com/fawadkhaliq)þþ> ""Insane C thing"" -- [Michaelþ> Sproul](https://michaelsproul.github.io/iron-talk/)þþ> ""I've never had a chance to thank you for Lwan.  It inspired me a lot toþ> develop [Zewo](https://github.com/Zewo/Zero)"" --þ> [@paulofariarl](https://twitter.com/paulofariarl/status/707926806373003265)þþ> ""Let me say that lwan is a thing of beauty.  I got sucked into reading theþ> source code for pure entertainment, it's so good.  *high five*"" --þ> [@kwilczynski](https://twitter.com/kwilczynski/status/692881117003644929)þþ> ""Nice work with Lwan! I haven't looked _that_ carefully yet but so far Iþ> like what I saw.  You definitely have the right ideas."" --þ> [@thinkingfish](https://twitter.com/thinkingfish/status/521574267612196864)þþ> ""Lwan is a work of art. Every time I read through it, I am almost alwaysþ> awe-struck."" --þ> [@neurodrone](https://twitter.com/neurodrone/status/359296080283840513)þþ> ""For Round 10, Lwan has taken the crown"" --þ> [TechEmpower](https://www.techempower.com/blog/2015/04/21/framework-benchmarks-round-10/)þþ> ""Jeez this is amazing. Just end to end, rock solid engineering. (...) But that sells this work short.""þ> [kjeetgill](https://news.ycombinator.com/item?id=17548983)þþ> ""I am only a spare time C coder myself and was surprised that I can follow the code. Nice!""þ> [cntlzw](https://news.ycombinator.com/item?id=17550319)þþ> ""Impressive all and all, even more for being written in (grokkable!) C. Nice work.""þ> [tpaschalis](https://news.ycombinator.com/item?id=17550961)"
nanomsg/nanomsg,8221,4946,457,839,Organization,False,1643,11,19,87,False,nanomsg library,,0,6,0,6,623,1,5,1,415,1,2,2805,1,1,11,0,0,0,16,2,,,"Welcome to nanomsgþ==================þþ[![Release](https://img.shields.io/github/release/nanomsg/nanomsg.svg)](https://github.com/nanomsg/nanomsg/releases/latest)þ[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/nanomsg/nanomsg/blob/master/COPYING)þ[![Linux Status](https://img.shields.io/circleci/project/github/nanomsg/nanomsg/master.svg?label=linux)](https://circleci.com/gh/nanomsg/nanomsg)þ[![Windows Status](https://img.shields.io/appveyor/ci/nanomsg/nanomsg/master.svg?label=windows)](https://ci.appveyor.com/project/nanomsg/nanomsg)þ[![Coverage](https://codecov.io/gh/nanomsg/nanomsg/branch/master/graph/badge.svg?label=coverage)](https://codecov.io/gh/nanomsg/nanomsg)þ[![Gitter](https://img.shields.io/badge/gitter-join-brightgreen.svg)](https://gitter.im/nanomsg/nanomsg)þþThe nanomsg library is a simple high-performance implementation of severalþ""scalability protocols"". These scalability protocols are light-weight messagingþprotocols which can be used to solve a number of very common messagingþpatterns, such as request/reply, publish/subscribe, surveyor/respondent,þand so forth.  These protocols can run over a variety of transports suchþas TCP, UNIX sockets, and even WebSocket.þþFor more information check the [website](http://nanomsg.org).þþPrerequisitesþ-------------þþ1. Windows.þ   * Windows Vista or newer (Windows XP and 2003 are *NOT* supported)þ   * Microsoft Visual Studio 2010 (including C++) or newer, or mingw-w64.þ     (Specifically mingw and older Microsoft compilers are *NOT supported,þ     and we do not test mingw-w64 at all, so YMMV.)þ   * CMake 2.8.7 or newer, available in $PATH as `cmake`þþ2. POSIX (Linux, MacOS X, UNIX)þ   * ANSI C compiler supporting C89þ   * POSIX pthreads (should be present on all modern POSIX systems)þ   * BSD sockets support for both TCP and UNIX domain socketsþ   * CMake (http://cmake.org) 2.8.7 or newer, available in $PATH as `cmake`þþ3. Documentation (optional)þ   * asciidoctor (http://asciidoctor.org/) available as `asciidoctor`þ   * If not present, docs are not formatted, but left in readable ASCIIþ   * Also available on-line at http://nanomsg.org/documentationþþQuick Build Instructionsþ------------------------þþThese steps here are the minimum steps to get a default Debugþbuild.  Using CMake you can do many other things, includingþsetting additional variables, setting up for static builds, orþgeneration project or solution files for different developmentþenvironments.  Please check the CMake website for all the variousþoptions that CMake supports.þþ## POSIXþþThis assumes you have a shell in the project directory, and haveþthe cmake and suitable compilers (and any required supporting toolsþlike linkers or archivers) on your path.þþ1.  `% mkdir build`þ2.  `% cd build`þ3.  `% cmake ..`þ4.  `% cmake --build .`þ5.  `% ctest .`þ6.  `% sudo cmake --build . --target install`þ7.  `% sudo ldconfig` (if on Linux)þþ## WindowsþþThis assumes you are in a command or powershell window and haveþthe appropriate variables setup to support Visual Studio, typicallyþby running `vcvarsall.bat` or similar with the appropriate argument(s).þIt also assumes you are in the project directory.þþ1.  `md build`þ2.  `cd build`þ3.  `cmake ..`þ4.  `cmake --build . --config Debug`þ5.  `ctest -C Debug .`þ6.  `cmake --build . --config Debug --target install`þ    *NB:* This may have to be done using an Administrator account.þþAlternatively, you can build and install nanomsg using [vcpkg](https://github.com/microsoft/vcpkg/) dependency manager:þþ1.  `git clone https://github.com/Microsoft/vcpkg.git`þ2.  `cd vcpkg`þ3.  `./bootstrap-vcpkg.bat`þ4.  `./vcpkg integrate install`þ5.  `./vcpkg install nanomsg`þþThe nanomsg port in vcpkg is kept up to date by microsoft team members and community contributors.þIf the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.þþStatic Libraryþ--------------þþWe normally build a dynamic library (.so or .DLL) by default.þþIf you want a static library (.a or .LIB), configure by passingþ`-DNN_STATIC_LIB=ON` to the first `cmake` command.þþ### POSIXþþPOSIX systems will need to link with the libraries normally used when buildingþnetwork applications.  For some systems this might mean -lnsl or -lsocket.þþ### WindowsþþYou will also need to define `NN_STATIC_LIB` in your compilation environmentþwhen building programs that use this library.  This is required because ofþthe way Windows changes symbol names depending on whether the symbols shouldþbe exported in a DLL or not.þþWhen using the .LIB on Windows, you will also need to link with theþws2_32, mswsock, and advapi32 libraries, as nanomsg depends on them.þþSupportþ-------þþThis library is considered to be in ""sustaining"" mode, which means that newþfeature development has ended, and bug fixes are made only when strictlyþnecessary for severe issues.þþNew development is now occurring in the [NNG](https://github.com/nanomsg/nng)þproject, which offers both protocol and API compatibility with this project.þPlease consider using NNG for new projects.þþPlease see the file SUPPORT for more details.þþResourcesþ---------þþWebsite: [http://nanomsg.org](http://nanomsg.org)þþSource code: [https://github.com/nanomsg/nanomsg](http://github.com/nanomsg/nanomsg)þþDocumentation: [http://nanomsg.org/documentation.html](http://nanomsg.org/documentation.html)þþBug tracker: [https://github.com/nanomsg/nanomsg/issues](http://github.com/nanomsg/nanomsg/issues)þþMailing list: [nanomsg@freelists.org](http://www.freelists.org/list/nanomsg)þþGitter Chat: [https://gitter.im/nanomsg/nanomsg](https://gitter.im/nanomsg/nanomsg)þþIRC chatroom: [#nanomsg at irc.freenode.net/8001](http://webchat.freenode.net?channels=%23nanomsg)"
mruby/mruby,14048,4609,268,692,Organization,False,11446,4,13,246,False,Lightweight Ruby,,0,7,0,117,1353,7,30,58,3495,5,105,2980,10,235,19286,5481,6,0,3,7,,,"[![Build Status][build-status-img]][travis-ci]þþ## What is mrubyþþmruby is the lightweight implementation of the Ruby language complying to (partþof) the [ISO standard][ISO-standard]. Its syntax is Ruby 2.x compatible.þþmruby can be linked and embedded within your application.  We provide theþinterpreter program ""mruby"" and the interactive mruby shell ""mirb"" as examples.þYou can also compile Ruby programs into compiled byte code using the mrubyþcompiler ""mrbc"".  All those tools reside in the ""bin"" directory.  ""mrbc"" isþalso able to generate compiled byte code in a C source file, see the ""mrbtest""þprogram under the ""test"" directory for an example.þþThis achievement was sponsored by the Regional Innovation Creation R&D Programsþof the Ministry of Economy, Trade and Industry of Japan.þþ## How to get mrubyþþThe stable version 2.1.1 of mruby can be downloaded via the following URL: [https://github.com/mruby/mruby/archive/2.1.1.zip](https://github.com/mruby/mruby/archive/2.1.1.zip)þþThe latest development version of mruby can be downloaded via the following URL: [https://github.com/mruby/mruby/zipball/master](https://github.com/mruby/mruby/zipball/master)þþThe trunk of the mruby source tree can be checked out with theþfollowing command:þþ    $ git clone https://github.com/mruby/mruby.gitþþYou can also install and compile mruby using [ruby-install](https://github.com/postmodern/ruby-install), [ruby-build](https://github.com/rbenv/ruby-build) or [rvm](https://github.com/rvm/rvm).þþ## mruby home-pageþþThe URL of the mruby home-page is: https://mruby.org.þþ## Mailing listþþWe don't have a mailing list, but you can use [GitHub issues](https://github.com/mruby/mruby/issues).þþ## How to compile and install (mruby and gems)þþSee the [compile.md](https://github.com/mruby/mruby/blob/master/doc/guides/compile.md) file.þþ## Running TestsþþTo run the tests, execute the following from the project's root directory.þþ    $ rake testþþNote: `bison` bundled with MacOS is too old to compile `mruby`.þTry `brew install bison` and follow the instuction shown to updateþthe `$PATH` to compile `mruby`.þþ## Building documentationþþThere are two sets of documentation in mruby: the mruby API (generated by yard) and C API (Doxygen)þþTo build both of them, simply goþþ    rake docþþYou can also view them in your browserþþ    rake view_apiþ    rake view_capiþþ## How to customize mruby (mrbgems)þþmruby contains a package manager called *mrbgems*. To create extensionsþin C and/or Ruby you should create a *GEM*. For a documentation of how toþuse mrbgems consult the file [mrbgems.md](https://github.com/mruby/mruby/blob/master/doc/guides/mrbgems.md).þFor example code of how to use mrbgems look into the folder *examples/mrbgems/*.þþ## Licenseþþmruby is released under the [MIT License](https://github.com/mruby/mruby/blob/master/LICENSE).þþ## Note for Licenseþþmruby has chosen a MIT License due to its permissive license allowingþdevelopers to target various environments such as embedded systems.þHowever, the license requires the display of the copyright notice and licenseþinformation in manuals for instance. Doing so for big projects can beþcomplicated or troublesome.  This is why mruby has decided to display ""mrubyþdevelopers"" as the copyright name to make it simple conventionally.þIn the future, mruby might ask you to distribute your new codeþ(that you will commit,) under the MIT License as a member ofþ""mruby developers"" but contributors will keep their copyright.þ(We did not intend for contributors to transfer or waive their copyrights,þActual copyright holder name (contributors) will be listed in the AUTHORSþfile.)þþPlease ask us if you want to distribute your code under another license.þþ## How to ContributeþþSee the [contribution guidelines][contribution-guidelines], and then send a pullþrequest to <http://github.com/mruby/mruby>.  We consider you have grantedþnon-exclusive right to your contributed code under MIT license.  If you want toþbe named as one of mruby developers, please include an update to the AUTHORSþfile in your pull request.þþ[ISO-standard]: http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=59579þ[build-status-img]: https://travis-ci.org/mruby/mruby.svg?branch=masterþ[contribution-guidelines]: https://github.com/mruby/mruby/blob/master/CONTRIBUTING.mdþ[travis-ci]: https://travis-ci.org/mruby/mruby"
borgbackup/borg,18678,6278,169,472,Organization,False,5853,7,83,185,False,Deduplicating archiver with compression and authenticated encryption.,https://www.borgbackup.org/,11,36,7,443,2303,79,106,15,2455,11,151,3771,18,98,13827,8446,19,6,7,3,,,
jonls/redshift,2116,4608,109,331,User,False,687,8,19,43,False,Redshift adjusts the color temperature of your screen according to your surroundings. This may help your eyes hurt less if you are working in front of the screen at night.,http://jonls.dk/redshift,6,11,1,167,367,6,3,31,190,4,0,3876,0,0,0,0,0,0,43,,242,,"Redshiftþ========þþRedshift adjusts the color temperature of your screen according toþyour surroundings. This may help your eyes hurt less if you areþworking in front of the screen at night.þþ![Redshift logo](http://jonls.dk/assets/redshift-icon-256.png)þþRun `redshift -h` for help on command line options. You can run the programþas `redshift-gtk` instead of `redshift` for a graphical status icon.þþ* Website: http://jonls.dk/redshift/þ* Project page: https://github.com/jonls/redshiftþþBuild statusþ------------þþ[![Build Status](https://travis-ci.org/jonls/redshift.svg?branch=master)](https://travis-ci.org/jonls/redshift)þ[![Build Status](https://ci.appveyor.com/api/projects/status/github/jonls/redshift?branch=master&svg=true)](https://ci.appveyor.com/project/jonls/redshift)þþFAQþ---þþ**How do I install Redshift?**þþUse the packages provided by your distribution, e.g. for Ubuntu:þ`apt-get install redshift` or `apt-get install redshift-gtk`. For developers,þplease see _Building from source_ and _Latest builds from master branch_ below.þþ**How do I setup a configuration file?**þþA configuration file is not required but is useful for saving customþconfigurations and manually defining the location in case of issues with theþautomatic location provider. An example configuration can be found inþ[redshift.conf.sample](redshift.conf.sample).þþThe configuration file should be saved in the following location depending onþthe platform:þþ- Linux/macOS: `~/.config/redshift/redshift.conf` (if the environment variable `XDG_CONFIG_HOME` is undefined) or `${XDG_CONFIG_HOME}/redshift/redshift.conf` (if `XDG_CONFIG_HOME` is defined).þ- Windows: Put `redshift.conf` in `%USERPROFILE%\AppData\Local\`þ    (aka `%localappdata%`).þþ**Where can I find my coordinates to put in the configuration file?**þþThere are multiple web sites that provide coordinates for map locations, forþexample clicking anywhere on Google Maps will bring up a box with theþcoordinates. Remember that longitudes in the western hemisphere (e.g. theþAmericas) must be provided to Redshift as negative numbers.þþ**Why does GeoClue fail with access denied error?**þþIt is possible that the location services have been disabled completely. Theþcheck for this case varies by desktop environment. For example, in GNOME theþlocation services can be toggled in Settings > Privacy > Location Services.þþIf this is not the case, it is possible that Redshift has been improperlyþinstalled or not been given the required permissions to obtain locationþupdates from a system administrator. Seeþhttps://github.com/jonls/redshift/issues/318 for further discussion on thisþissue.þþ**Why doesn't Redshift work on my Chromebook/Raspberry Pi?**þþCertain video drivers do not support adjustable gamma ramps. In some casesþRedshift will fail with an error message, but other drivers silently ignoreþadjustments to the gamma ramp.þþ**Why doesn't Redshift change the backlight when I use the brightness option?**þþRedshift has a brightness adjustment setting but it does not work the way mostþpeople might expect. In fact it is a fake brightness adjustment obtained byþmanipulating the gamma ramps which means that it does not reduce the backlightþof the screen. Preferably only use it if your normal backlight adjustment isþtoo coarse-grained.þþ**Why doesn't Redshift work on Wayland (e.g. Fedora 25)?**þþThe Wayland protocol does not support Redshift. There is currently no way forþRedshift to adjust the color temperature in Wayland.þþ**Why doesn't Redshift work on Ubuntu with Mir enabled?**þþMir does not support Redshift.þþ**The redness effect is applied during the day instead of at night. Why?**þþThis usually happens to users in America when the longitude has been set in theþconfiguration file to a positive number. Longitudes in the western hemisphereþshould be provided as negative numbers (e.g. New York City is at approximatelyþlatitude/longitude 41, -74).þþ**Why does the redness effect occasionally switch off for a few seconds?**þþRedshift uses the gamma ramps of the graphics driver to apply the rednessþeffect but Redshift cannot block other applications from also changing theþgamma ramps. Some applications (particularly games and video players) willþreset the gamma ramps. After a few seconds Redshift will kick in again. Thereþis no way for Redshift to prevent this from happening.þþ**Why does the redness effect continuously flicker?**þþYou may have multiple instances of Redshift running simultaneously. Make sureþthat only one instance is running for the display where you are seeing theþflicker.þþ**Why doesn't Redshift change the color of the mouse cursor?**þþMouse cursors are usually handled separately by the graphics hardware and isþnot affected by gamma ramps. Some graphics drivers can be configured to useþsoftware cursors instead.þþ**I have an issue with Redshift but it was not mentioned in this FAQ. Whatþdo I do?**þþPlease go to [the issue tracker](https://github.com/jonls/redshift/issues) andþcheck if your issue has already been reported. If not, please open a new issueþdescribing you problem.þþLatest builds from master branchþ--------------------------------þþ- [Ubuntu PPA](https://launchpad.net/~dobey/+archive/ubuntu/redshift-daily/+packages) (`sudo add-apt-repository ppa:dobey/redshift-daily`)þ- [Windows x86_64](https://ci.appveyor.com/api/projects/jonls/redshift/artifacts/redshift-windows-x86_64.zip?branch=master&job=Environment%3A+arch%3Dx86_64&pr=false)þ- [Windows x86](https://ci.appveyor.com/api/projects/jonls/redshift/artifacts/redshift-windows-i686.zip?branch=master&job=Environment%3A+arch%3Di686&pr=false)þþContributing / Building from sourceþ-----------------------------------þþSee the file [CONTRIBUTING](CONTRIBUTING.md) for more details."
sqlcipher/sqlcipher,40344,4082,243,960,Organization,False,735,13,40,18,False,SQLCipher is an SQLite extension that provides 256 bit AES encryption of database files.,https://www.zetetic.net/sqlcipher/,0,0,0,9,294,3,13,6,50,0,1,4338,1,11,11886,2993,0,0,10,4,,,"## SQLCipherþþSQLCipher extends the [SQLite](https://www.sqlite.org) database library to add security enhancements that make it more suitable for encrypted local data storage like:þþ- on-the-fly encryptionþ- tamper detectionþ- memory sanitizationþ- strong key derivationþþSQLCipher is based on SQLite and stable upstream release features are periodically integrated. þþSQLCipher is maintained by Zetetic, LLC, and additional information and documentation is available on the official [SQLCipher site](https://www.zetetic.net/sqlcipher/).þþ## Featuresþþ- Fast performance with as little as 5-15% overhead for encryption on many operationsþ- 100% of data in the database file is encryptedþ- Good security practices (CBC mode, HMAC, key derivation)þ- Zero-configuration and application level cryptographyþ- Algorithms provided by the peer reviewed OpenSSL crypto library.þ- Configurable crypto providersþþ## CompatibilityþþSQLCipher maintains database format compatibility within the same major version number so an application on any platform can open databases created by any other application provided the major version of SQLCipher is the same between them. However, major version updates (e.g. from 3.x to 4.x) often include changes to default settings. This means that newer major versions of SQLCipher will not open databases created by older versions without using special settings. For example, SQLCipher 4 introduces many new performance and security enhancements. The new default algorithms, increased KDF iterations, and larger page size mean that SQLCipher 4 will not open databases created by SQLCipher 1.x, 2.x, or 3.x by default. Instead, an application would either need to migrate the older databases to use the new format or enable a special backwards-compatibility mode. The available options are described in SQLCipher's [upgrade documentation](https://discuss.zetetic.net/t/upgrading-to-sqlcipher-4/3283). þþSQLCipher is also compatible with standard SQLite databases. When a key is not provided, SQLCipher will behave just like the standard SQLite library. It is also possible to convert from a plaintext database (standard SQLite) to an encrypted SQLCipher database using [ATTACH and the sqlcipher_export() convenience function](https://discuss.zetetic.net/t/how-to-encrypt-a-plaintext-sqlite-database-to-use-sqlcipher-and-avoid-file-is-encrypted-or-is-not-a-database-errors/868).þþ## ContributionsþþThe SQLCipher team welcomes contributions to the core library. All contributions including pull requests and patches should be based on the `prerelease` branch, and must be accompanied by a [contributor agreement](https://www.zetetic.net/contributions/). We strongly encourage [discussion](https://discuss.zetetic.net/c/sqlcipher) of the proposed change prior to development and submission.þþ## CompilingþþBuilding SQLCipher is similar to compiling a regular version of SQLite from source a couple small exceptions:  þþ 1. You *must* define `SQLITE_HAS_CODEC` and either `SQLITE_TEMP_STORE=2` or SQLITE_TEMP_STORE=3` þ 2. You will need to link against a support cryptograpic provider (OpenSSL, LibTomCrypt, CommonCrypto/Security.framework, or NSS)þ þThe following examples demonstrate linking against OpenSSL, which is a readily available provider on most Unix-like systems. þþExample 1. Static linking (replace /opt/local/lib with the path to libcrypto.a). Note in this þexample, `--enable-tempstore=yes` is setting `SQLITE_TEMP_STORE=2` for the build.þþ```þ $ ./configure --enable-tempstore=yes CFLAGS=""-DSQLITE_HAS_CODEC"" \þ  LDFLAGS=""/opt/local/lib/libcrypto.a""þ $ makeþ```þþExample 2. Dynamic linkingþþ```þ $ ./configure --enable-tempstore=yes CFLAGS=""-DSQLITE_HAS_CODEC"" \þ  LDFLAGS=""-lcrypto""þ $ makeþ```þþ## TestingþþThe full SQLite test suite will not complete successfully when using SQLCipher. In some cases encryption interferes with low-level tests that require access to database file data or features which are unsupported by SQLCipher. Those tests that are intended to support encryption are intended for non-SQLCipher implementations. In addition, because SQLite tests are not always isolated, if one test fails it can trigger a domino effect with other failures in later steps.þþAs a result, the SQLCipher package includes it's own independent tests that exercise and verify the core functionality of the SQLCipher extensions. This test suite is intended to provide an abbreviated verification of SQLCipher's internal logic; it does not perform an exhaustive test of the SQLite database system as a whole or verify functionality on specific platforms. Because SQLCipher is based on stable upstream builds of SQLite, it is consider a basic assumption that the core SQLite library code is operating properly (the SQLite core is almost untouched in SQLCipher). Thus, the additional SQLCipher-specific test provide the requisite verification that the library is operating as expected with SQLCipher's security features enabled.þþTo run SQLCipher specific tests, configure as described above and run the following to execute the tests and recieve a report of the results:þþ```þ  $ make testfixtureþ  $ ./testfixture test/sqlcipher.testþ```þþ## Encrypting a databaseþþTo specify an encryption passphrase for the database via the SQL interface you þuse a PRAGMA. The passphrase you enter is passed through PBKDF2 key derivation toþobtain the encryption key for the database þþ PRAGMA key = 'passphrase';þþAlternately, you can specify an exact byte sequence using a blob literal. If youþuse this method it is your responsibility to ensure that the data you provide is aþ64 character hex string, which will be converted directly to 32 bytes (256 bits) of þkey data without key derivation.þþ PRAGMA key = ""x'2DD29CA851E7B56E4697B0E1F08507293D761A05CE4D1B628663F411A8086D99'"";þþTo encrypt a database programmatically you can use the `sqlite3_key` function. þThe data provided in `pKey` is converted to an encryption key according to the þsame rules as `PRAGMA key`. þþ int sqlite3_key(sqlite3 *db, const void *pKey, int nKey);þþ`PRAGMA key` or `sqlite3_key` should be called as the first operation when a database is open.þþ## Changing a database keyþþTo change the encryption passphrase for an existing database you may use the rekey PRAGMAþafter you've supplied the correct database password;þþ PRAGMA key = 'passphrase'; -- start with the existing database passphraseþ PRAGMA rekey = 'new-passphrase'; -- rekey will reencrypt with the new passphraseþþThe hex rekey pragma may be used to rekey to a specific binary valueþþ PRAGMA rekey = ""x'2DD29CA851E7B56E4697B0E1F08507293D761A05CE4D1B628663F411A8086D99'"";þþThis can be accomplished programmatically by using sqlite3_rekey;þ  þ sqlite3_rekey(sqlite3 *db, const void *pKey, int nKey)þþ## SupportþþThe primary source for complete documentation (desing, API, platforms, usage) is the SQLCipher website:þþhttps://www.zetetic.net/sqlcipher/documentationþþThe primary avenue for support and discussions is the SQLCipher discuss site:þþhttps://discuss.zetetic.net/c/sqlcipherþþIssues or support questions on using SQLCipher should be entered into the þGitHub Issue tracker:þþhttps://github.com/sqlcipher/sqlcipher/issuesþþPlease DO NOT post issues, support questions, or other problems to blog þposts about SQLCipher as we do not monitor them frequently.þþIf you are using SQLCipher in your own software please let us know at þsupport@zetetic.net!þþ## Community Edition Open Source LicenseþþCopyright (c) 2020, ZETETIC LLCþAll rights reserved.þþRedistribution and use in source and binary forms, with or withoutþmodification, are permitted provided that the following conditions are met:þ    * Redistributions of source code must retain the above copyrightþ      notice, this list of conditions and the following disclaimer.þ    * Redistributions in binary form must reproduce the above copyrightþ      notice, this list of conditions and the following disclaimer in theþ      documentation and/or other materials provided with the distribution.þ    * Neither the name of the ZETETIC LLC nor theþ      names of its contributors may be used to endorse or promote productsþ      derived from this software without specific prior written permission.þþTHIS SOFTWARE IS PROVIDED BY ZETETIC LLC ''AS IS'' AND ANYþEXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIEDþWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AREþDISCLAIMED. IN NO EVENT SHALL ZETETIC LLC BE LIABLE FOR ANYþDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGESþ(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;þLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED ANDþON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORTþ(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THISþSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.þþ# Begin SQLite README.mdþþ<h1 align=""center"">SQLite Source Repository</h1>þþThis repository contains the complete source code for the þ[SQLite database engine](https://sqlite.org/).  Some test scripts þare also included.  However, many other test scriptsþand most of the documentation are managed separately.þþ## Version ControlþþSQLite sources are managed using theþ[Fossil](https://www.fossil-scm.org/), a distributed version control systemþthat was specifically designed and written to support SQLite development.þThe [Fossil repository](https://sqlite.org/src/timeline) contains the urtext.þþIf you are reading this on GitHub or some other Git repository or service,þthen you are looking at a mirror.  The names of check-ins andþother artifacts in a Git mirror are different from the officialþnames for those objects.  The offical names for check-ins areþfound in a footer on the check-in comment for authorized mirrors.þThe official check-in name can also be seen in the `manifest.uuid` fileþin the root of the tree.  Always use the official name, not  theþGit-name, when communicating about an SQLite check-in.þþIf you pulled your SQLite source code from a secondary source and want toþverify its integrity, there are hints on how to do that in theþ[Verifying Code Authenticity](#vauth) section below.þþ## Obtaining The CodeþþIf you do not want to use Fossil, you can download tarballs or ZIPþarchives or [SQLite archives](https://sqlite.org/cli.html#sqlar) as follows:þþ  *  Lastest trunk check-in asþ     [Tarball](https://www.sqlite.org/src/tarball/sqlite.tar.gz),þ     [ZIP-archive](https://www.sqlite.org/src/zip/sqlite.zip), orþ     [SQLite-archive](https://www.sqlite.org/src/sqlar/sqlite.sqlar).þþ  *  Latest release asþ     [Tarball](https://www.sqlite.org/src/tarball/sqlite.tar.gz?r=release),þ     [ZIP-archive](https://www.sqlite.org/src/zip/sqlite.zip?r=release), orþ     [SQLite-archive](https://www.sqlite.org/src/sqlar/sqlite.sqlar?r=release).þþ  *  For other check-ins, substitute an appropriate branch name orþ     tag or hash prefix in place of ""release"" in the URLs of the previousþ     bullet.  Or browse the [timeline](https://www.sqlite.org/src/timeline)þ     to locate the check-in desired, click on its information page link,þ     then click on the ""Tarball"" or ""ZIP Archive"" links on the informationþ     page.þþIf you do want to use Fossil to check out the source tree, þfirst install Fossil version 2.0 or later.þ(Source tarballs and precompiled binaries availableþ[here](https://www.fossil-scm.org/fossil/uv/download.html).  Fossil isþa stand-alone program.  To install, simply download or build the single þexecutable file and put that file someplace on your $PATH.)þThen run commands like this:þþ        mkdir ~/sqliteþ        cd ~/sqliteþ        fossil clone https://www.sqlite.org/src sqlite.fossilþ        fossil open sqlite.fossilþ    þAfter setting up a repository using the steps above, you can alwaysþupdate to the lastest version using:þþ        fossil update trunk   ;# latest trunk check-inþ        fossil update release ;# latest official releaseþþOr type ""fossil ui"" to get a web-based user interface.þþ## CompilingþþFirst create a directory in which to placeþthe build products.  It is recommended, but not required, that theþbuild directory be separate from the source directory.  Cd into theþbuild directory and then from the build directory run the configureþscript found at the root of the source tree.  Then run ""make"".þþFor example:þþ        tar xzf sqlite.tar.gz    ;#  Unpack the source tree into ""sqlite""þ        mkdir bld                ;#  Build will occur in a sibling directoryþ        cd bld                   ;#  Change to the build directoryþ        ../sqlite/configure      ;#  Run the configure scriptþ        make                     ;#  Run the makefile.þ        make sqlite3.c           ;#  Build the ""amalgamation"" source fileþ        make test                ;#  Run some tests (requires Tcl)þþSee the makefile for additional targets.þþThe configure script uses autoconf 2.61 and libtool.  If the configureþscript does not work out for you, there is a generic makefile namedþ""Makefile.linux-gcc"" in the top directory of the source tree that youþcan copy and edit to suit your needs.  Comments on the generic makefileþshow what changes are needed.þþ## Using MSVCþþOn Windows, all applicable build products can be compiled with MSVC.þFirst open the command prompt window associated with the desired compilerþversion (e.g. ""Developer Command Prompt for VS2013"").  Next, use NMAKEþwith the provided ""Makefile.msc"" to build one of the supported targets.þþFor example:þþ        mkdir bldþ        cd bldþ        nmake /f Makefile.msc TOP=..\sqliteþ        nmake /f Makefile.msc sqlite3.c TOP=..\sqliteþ        nmake /f Makefile.msc sqlite3.dll TOP=..\sqliteþ        nmake /f Makefile.msc sqlite3.exe TOP=..\sqliteþ        nmake /f Makefile.msc test TOP=..\sqliteþþThere are several build options that can be set via the NMAKE commandþline.  For example, to build for WinRT, simply add ""FOR_WINRT=1"" argumentþto the ""sqlite3.dll"" command line above.  When debugging into the SQLiteþcode, adding the ""DEBUG=1"" argument to one of the above command lines isþrecommended.þþSQLite does not require [Tcl](http://www.tcl.tk/) to run, but a Tcl installationþis required by the makefiles (including those for MSVC).  SQLite containsþa lot of generated code and Tcl is used to do much of that code generation.þþ## Source Code TourþþMost of the core source files are in the **src/** subdirectory.  Theþ**src/** folder also contains files used to build the ""testfixture"" testþharness. The names of the source files used by ""testfixture"" all beginþwith ""test"".þThe **src/** also contains the ""shell.c"" fileþwhich is the main program for the ""sqlite3.exe""þ[command-line shell](https://sqlite.org/cli.html) andþthe ""tclsqlite.c"" file which implements theþ[Tcl bindings](https://sqlite.org/tclsqlite.html) for SQLite.þ(Historical note:  SQLite began as a Tclþextension and only later escaped to the wild as an independent library.)þþTest scripts and programs are found in the **test/** subdirectory.þAddtional test code is found in other source repositories.þSee [How SQLite Is Tested](http://www.sqlite.org/testing.html) forþadditional information.þþThe **ext/** subdirectory contains code for extensions.  TheþFull-text search engine is in **ext/fts3**.  The R-Tree engine is inþ**ext/rtree**.  The **ext/misc** subdirectory contains a number ofþsmaller, single-file extensions, such as a REGEXP operator.þþThe **tool/** subdirectory contains various scripts and programs usedþfor building generated source code files or for testing or for generatingþaccessory programs such as ""sqlite3_analyzer(.exe)"".þþ### Generated Source Code FilesþþSeveral of the C-language source files used by SQLite are generated fromþother sources rather than being typed in manually by a programmer.  Thisþsection will summarize those automatically-generated files.  To create allþof the automatically-generated files, simply run ""make target&#95;source"".þThe ""target&#95;source"" make target will create a subdirectory ""tsrc/"" andþfill it with all the source files needed to build SQLite, bothþmanually-edited files and automatically-generated files.þþThe SQLite interface is defined by the **sqlite3.h** header file, which isþgenerated from src/sqlite.h.in, ./manifest.uuid, and ./VERSION.  Theþ[Tcl script](http://www.tcl.tk) at tool/mksqlite3h.tcl does the conversion.þThe manifest.uuid file contains the SHA3 hash of the particular check-inþand is used to generate the SQLITE\_SOURCE\_ID macro.  The VERSION fileþcontains the current SQLite version number.  The sqlite3.h header is reallyþjust a copy of src/sqlite.h.in with the source-id and version number insertedþat just the right spots. Note that comment text in the sqlite3.h file isþused to generate much of the SQLite API documentation.  The Tcl scriptsþused to generate that documentation are in a separate source repository.þþThe SQL language parser is **parse.c** which is generate from a grammar inþthe src/parse.y file.  The conversion of ""parse.y"" into ""parse.c"" is doneþby the [lemon](./doc/lemon.html) LALR(1) parser generator.  The source codeþfor lemon is at tool/lemon.c.  Lemon uses the tool/lempar.c file as aþtemplate for generating its parser.þLemon also generates the **parse.h** header file, at the same time itþgenerates parse.c.þþThe **opcodes.h** header file contains macros that define the numbersþcorresponding to opcodes in the ""VDBE"" virtual machine.  The opcodes.hþfile is generated by the scanning the src/vdbe.c source file.  TheþTcl script at ./mkopcodeh.tcl does this scan and generates opcodes.h.þA second Tcl script, ./mkopcodec.tcl, then scans opcodes.h to generateþthe **opcodes.c** source file, which contains a reverse mapping fromþopcode-number to opcode-name that is used for EXPLAIN output.þþThe **keywordhash.h** header file contains the definition of a hash tableþthat maps SQL language keywords (ex: ""CREATE"", ""SELECT"", ""INDEX"", etc.) intoþthe numeric codes used by the parse.c parser.  The keywordhash.h file isþgenerated by a C-language program at tool mkkeywordhash.c.þþThe **pragma.h** header file contains various definitions used to parseþand implement the PRAGMA statements.  The header is generated by aþscript **tool/mkpragmatab.tcl**. If you want to add a new PRAGMA, editþthe **tool/mkpragmatab.tcl** file to insert the information needed by theþparser for your new PRAGMA, then run the script to regenerate theþ**pragma.h** header file.þþ### The AmalgamationþþAll of the individual C source code and header files (both manually-editedþand automatically-generated) can be combined into a single big source fileþ**sqlite3.c** called ""the amalgamation"".  The amalgamation is the recommendedþway of using SQLite in a larger application.  Combining all individualþsource code files into a single big source code file allows the C compilerþto perform more cross-procedure analysis and generate better code.  SQLiteþruns about 5% faster when compiled from the amalgamation versus when compiledþfrom individual source files.þþThe amalgamation is generated from the tool/mksqlite3c.tcl Tcl script.þFirst, all of the individual source files must be gathered into the tsrc/þsubdirectory (using the equivalent of ""make target_source"") then theþtool/mksqlite3c.tcl script is run to copy them all together in just theþright order while resolving internal ""#include"" references.þþThe amalgamation source file is more than 200K lines long.  Some symbolicþdebuggers (most notably MSVC) are unable to deal with files longer than 64Kþlines.  To work around this, a separate Tcl script, tool/split-sqlite3c.tcl,þcan be run on the amalgamation to break it up into a single small C fileþcalled **sqlite3-all.c** that does #include on about seven other filesþnamed **sqlite3-1.c**, **sqlite3-2.c**, ..., **sqlite3-7.c**.  In this way,þall of the source code is contained within a single translation unit soþthat the compiler can do extra cross-procedure optimization, but noþindividual source file exceeds 32K lines in length.þþ## How It All Fits TogetherþþSQLite is modular in design.þSee the [architectural description](http://www.sqlite.org/arch.html)þfor details. Other documents that are useful inþ(helping to understand how SQLite works include theþ[file format](http://www.sqlite.org/fileformat2.html) description,þthe [virtual machine](http://www.sqlite.org/opcode.html) that runsþprepared statements, the description ofþ[how transactions work](http://www.sqlite.org/atomiccommit.html), andþthe [overview of the query planner](http://www.sqlite.org/optoverview.html).þþYears of effort have gone into optimizating SQLite, bothþfor small size and high performance.  And optimizations tend to result inþcomplex code.  So there is a lot of complexity in the current SQLiteþimplementation.  It will not be the easiest library in the world to hack.þþKey files:þþ  *  **sqlite.h.in** - This file defines the public interface to the SQLiteþ     library.  Readers will need to be familiar with this interface beforeþ     trying to understand how the library works internally.þþ  *  **sqliteInt.h** - this header file defines many of the data objectsþ     used internally by SQLite.  In addition to ""sqliteInt.h"", someþ     subsystems have their own header files.þþ  *  **parse.y** - This file describes the LALR(1) grammar that SQLite usesþ     to parse SQL statements, and the actions that are taken at each stepþ     in the parsing process.þþ  *  **vdbe.c** - This file implements the virtual machine that runsþ     prepared statements.  There are various helper files whose namesþ     begin with ""vdbe"".  The VDBE has access to the vdbeInt.h header fileþ     which defines internal data objects.  The rest of SQLite interactsþ     with the VDBE through an interface defined by vdbe.h.þþ  *  **where.c** - This file (together with its helper files namedþ     by ""where*.c"") analyzes the WHERE clause and generatesþ     virtual machine code to run queries efficiently.  This file isþ     sometimes called the ""query optimizer"".  It has its own privateþ     header file, whereInt.h, that defines data objects used internally.þþ  *  **btree.c** - This file contains the implementation of the B-Treeþ     storage engine used by SQLite.  The interface to the rest of the systemþ     is defined by ""btree.h"".  The ""btreeInt.h"" header defines objectsþ     used internally by btree.c and not published to the rest of the system.þþ  *  **pager.c** - This file contains the ""pager"" implementation, theþ     module that implements transactions.  The ""pager.h"" header fileþ     defines the interface between pager.c and the rest of the system.þþ  *  **os_unix.c** and **os_win.c** - These two files implement the interfaceþ     between SQLite and the underlying operating system using the run-timeþ     pluggable VFS interface.þþ  *  **shell.c.in** - This file is not part of the core SQLite library.  Thisþ     is the file that, when linked against sqlite3.a, generates theþ     ""sqlite3.exe"" command-line shell.  The ""shell.c.in"" file is transformedþ     into ""shell.c"" as part of the build process.þþ  *  **tclsqlite.c** - This file implements the Tcl bindings for SQLite.  Itþ     is not part of the core SQLite library.  But as most of the tests in thisþ     repository are written in Tcl, the Tcl language bindings are important.þþ  *  **test*.c** - Files in the src/ folder that begin with ""test"" go intoþ     building the ""testfixture.exe"" program.  The testfixture.exe program isþ     an enhanced Tcl shell.  The testfixture.exe program runs scripts in theþ     test/ folder to validate the core SQLite code.  The testfixture programþ     (and some other test programs too) is build and run when you typeþ     ""make test"".þþ  *  **ext/misc/json1.c** - This file implements the various JSON functionsþ     that are build into SQLite.þþThere are many other source files.  Each has a succinct header comment thatþdescribes its purpose and role within the larger system.þþ<a name=""vauth""></a>þ## Verifying Code AuthenticityþþThe `manifest` file at the root directory of the source treeþcontains either a SHA3-256 hash (for newer files) or a SHA1 hash (for þolder files) for every source file in the repository.þThe SHA3-256 hash of the `manifest`þfile itself is the official name of the version of the source tree that youþhave. The `manifest.uuid` file should contain the SHA3-256 hash of theþ`manifest` file. If all of the above hash comparisons are correct, thenþyou can be confident that your source tree is authentic and unadulterated.þþThe format of the `manifest` file should be mostly self-explanatory, butþif you want details, they are availableþ[here](https://fossil-scm.org/fossil/doc/trunk/www/fileformat.wiki#manifest).þþ## ContactsþþThe main SQLite website is [http://www.sqlite.org/](http://www.sqlite.org/)þwith geographically distributed backups atþ[http://www2.sqlite.org/](http://www2.sqlite.org) andþ[http://www3.sqlite.org/](http://www3.sqlite.org)."
cloudius-systems/osv,20445,3062,302,554,Organization,False,8016,11,45,100,False,"OSv, a new operating system for the cloud.",http://osv.io,0,35,5,310,703,7,12,0,67,0,3,2756,6,148,11591,4588,0,0,35,6,,,"***OSv was originally designed and implemented by Cloudius Systems (now ScyllaDB) howeverþ currently it is being maintained and enhanced by a small community of volunteers.þ If you are into systems programming or want to learn and help us improve OSv, then pleaseþ contact us on [OSv Google Group forum](https://groups.google.com/forum/#!forum/osv-dev).þ For details on how to format and send patches, please readþ [this wiki](https://github.com/cloudius-systems/osv/wiki/Formatting-and-sending-patches)þ (__we do NOT accept pull requests__).***þþ# OSvþþOSv is an open-source versatile modular **unikernel** designed to run single **unmodifiedþLinux application** securely as microVM on top of a hypervisor, when compared to traditionalþoperating systems which were designed for a vast range of physical machines. Built fromþthe ground up for effortless deployment and management of microservicesþand serverless apps, with superior performance.þþOSv has been designed to run unmodified x86-64 Linuxþbinaries **as is**, which effectively makes it a **Linux binary compatible unikernel**þ(for more details about Linux ABI compatibility please readþ[this doc](https://github.com/cloudius-systems/osv/wiki/OSv-Linux-ABI-Compatibility)).þIn particular OSv can run many managed language runtimes includingþ[**JVM**](https://github.com/cloudius-systems/osv-apps/tree/master/java-example),þ**Python** [**2**](https://github.com/cloudius-systems/osv-apps/tree/master/python2x) andþ[**3**](https://github.com/cloudius-systems/osv-apps/tree/master/python3x),þ[**Node.JS**](https://github.com/cloudius-systems/osv-apps/tree/master/node-from-host),þ[**Ruby**](https://github.com/cloudius-systems/osv-apps/tree/master/ruby-example), **Erlang**,þand applications built on top of those runtimes.þIt can also run applications written in languages compiling directly to native machine code likeþ**C**, **C++**,þ[**Golang**](https://github.com/cloudius-systems/osv-apps/tree/master/golang-httpserver)þand [**Rust**](https://github.com/cloudius-systems/osv-apps/tree/master/rust-httpserver)þas well as native images producedþby [**GraalVM**](https://github.com/cloudius-systems/osv-apps/tree/master/graalvm-example)þand [WebAssembly/Wasmer](https://github.com/cloudius-systems/osv-apps/tree/master/webassembly).þþOSv can boot as fast as **~5 ms** on Firecracker using as low as 15 MB of memory.þOSv can run on many hypervisors including QEMU/KVM,þ[Firecracker](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-Firecracker),þXen, [VMWare](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-VMware-ESXi),þ[VirtualBox](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-VirtualBox) andþHyperkit as well as open clouds like AWS EC2, GCE and OpenStack.þþFor more information about OSv, see the [main wiki page](https://github.com/cloudius-systems/osv/wiki)þand http://osv.io/.þþ## Building and Running Apps on OSvþþIn order to run an application on OSv, one needs to build an image by fusing OSv kernel, andþthe application files together. This, in high level can be achieved in two ways, either:þ- by using the shell script located at `./scripts/build`þ that builds the kernel from sources and fuses it with application files, orþ- by using the [capstan tool](https://github.com/cloudius-systems/capstan) that uses *pre-builtþ kernel* and combines it with application files to produce a final image.þþIf your intention is to try to run your app on OSv with the least effort possible, you should pursue the *capstan*þroute. For introduction please read this þ[crash course](https://github.com/cloudius-systems/osv/wiki/Build-and-run-apps-on-OSv-using-Capstan).þFor more details about *capstan* please read þthis more detailed [documentation](https://github.com/cloudius-systems/capstan#documentation). Pre-built OSv kernel filesþ(`ovs-loader.qemu`) can be automatically downloaded by *capstan* from þthe [OSv regular releases page](https://github.com/cloudius-systems/osv/releases) or manually from þthe [nightly releases repo](https://github.com/osvunikernel/osv-nightly-releases/releases/tag/ci-master-latest).þþIf you are comfortable with make and GCC toolchain and want to try the latest OSv code, then you shouldþread this [part of the readme](#setting-up-development-environment) to guide you how to set up yourþ development environment and build OSv kernel and application images.þþ## ReleasesþþWe aim to release OSv 2-3 times a year. You can find the [latest one on github](https://github.com/cloudius-systems/osv/releases)þalong with number of published artifacts including kernel and some modules.þþIn addition, we have set up [Travis-based CI/CD pipeline](https://travis-ci.org/github/cloudius-systems/osv) where eachþcommit to the master and ipv6 branches triggers full build of the latest kernel and publishes some artifacts to þthe [nightly releases repo](https://github.com/osvunikernel/osv-nightly-releases/releases). Each commit alsoþtriggers publishing of new Docker ""build tool chain"" images to the [Docker hub](https://hub.docker.com/u/osvunikernel).þþ## DesignþþGood bit of the design of OSv is pretty well explained in þthe [Components of OSv](https://github.com/cloudius-systems/osv/wiki/Components-of-OSv) wiki page. You þcan find even more information in the original þ[USENIX paper and its presentation](https://www.usenix.org/conference/atc14/technical-sessions/presentation/kivity).þþIn addition, you can find a lot of good information about design of specific OSv components onþthe [main wiki page](https://github.com/cloudius-systems/osv/wiki) and http://osv.io/ and http://blog.osv.io/.þUnfortunately, some of that information may be outdated (especially on http://osv.io/), so it is alwaysþbest to ask on the [mailing list](https://groups.google.com/forum/#!forum/osv-dev) if in doubt.þþ## Metrics and PerformanceþþThere are no official **up-to date** performance metrics comparing OSv to other unikernels or Linux.þIn general OSv lags behind Linux in disk-I/O-intensive workloads partially due to coarse-grained locking þin VFS around read/write operations as described in this [issue](https://github.com/cloudius-systems/osv/issues/450).þIn network-I/O-intensive workloads, OSv should fare better (or at least used to as Linux has advanced a lot since)þas shown with performance tests of Redis and [Memcached](https://github.com/cloudius-systems/osv/wiki/OSv-Case-Study:-Memcached).þYou can find some old ""numbers"" on the main wiki, http://osv.io/benchmarks and some papers listed at the bottom of this readme.þþSo OSv is probably not best suited to run MySQL or ElasticSearch, but should deliver pretty solid performance for generalþ stateless applications like microservices or serverless (at least as some papers show).þþ### Kernel SizeþþAt this moment (as of May 2020) the size of the uncompressed OSv kernel (`kernel.elf` artifact) is aroundþ6.7 MB (the compressed is ~ 2.7 MB). This is not that small comparing to Linux kernel and quite large comparingþto other unikernels. However, bear in mind that OSv kernel (being unikernel) provides **subset** of functionalityþ of the following Linux libraries (see their approximate size on Linux host):þ- `libresolv.so.2` (_100 K_)þ- `libc.so.6` (_2 MB_)þ- `libm.so.6` (_1.4 MB_)þ- `ld-linux-x86-64.so.2` (_184 K_)þ- `libpthread.so.0` (_156 K_)þ- `libdl.so.2` (_20 K_)þ- `librt.so.1` (_40 K_)þ- `libstdc++.so.6` (_2 MB_)þ- `libaio.so.1` (_16 K_)þ- `libxenstore.so.3.0` (_32 K_)þ- `libcrypt.so.1` (_44 K_)þþThe equivalent static version of `libstdc++.so.6` is actually linked `--whole-archive` so thatþany C++ apps can run without having to add `libstdc++.so.6` to the image (whether it needs it or not).þFinally, OSv kernel comes with ZFS implementation which in theory later can be extracted as a þ[separate library](https://github.com/cloudius-systems/osv/issues/1009). Theþpoint of this is to illustrate that comparing OSv kernel size to Linux kernel size does notþquite make sense.þþ### Boot TimeþþOSv with _Read-Only FS with networking off_ can boot as fast as **~5 ms** on Firecracker þand even faster around **~3 ms** on QEMU with the microvm machine. However, in general the boot timeþwill depend on many factors like hypervisor including settings of individual para-virtual devices, þfilesystem (ZFS, ROFS or RAMFS) and some boot parameters. Please note that by default OSv imagesþget built with ZFS filesystem.þþFor example, the boot time of ZFS image on Firecracker is around ~40 ms and regular QEMU around 200 ms these days. Also,þnewer versions of QEMU (>=4.0) are typically faster to boot. Booting on QEMU in PVH/HVM mode (aka direct kernel boot, enabled þby `-k` option of `run.py`) should always be faster as OSv is directly invoked in 64-bit long mode. Please seeþ[this Wiki](https://github.com/cloudius-systems/osv/wiki/OSv-boot-methods-overview) for the brief review of the bootþmethods OSv supports.þþFinally, some boot parameters passed to the kernel may affect the boot time:þ- `--console serial` - this disables VGA console that is [slow to initialize](https://github.com/cloudius-systems/osv/issues/987) and can shave off 60-70 ms on QEMUþ- `--nopci` - this disables enumeration of PCI devices especially if we know none are present (QEMU with microvm or Firecracker) and can shave off 10-20 ms þ- `--redirect=/tmp/out` - writing to the console can impact the performance quite severely (30-40%) if application logs þa lot, so redirecting standard output and error to a file might speed up performance quite a lotþþYou can always see boot time breakdown by adding `--bootchart` parameter:þ```þ./scripts/run.py -e '--bootchart /hello'þOSv v0.54.0-197-g1f0df4e4þeth0: 192.168.122.15þ disk read (real mode): 25.85ms, (+25.85ms)þ uncompress lzloader.elf: 45.11ms, (+19.26ms)þ TLS initialization: 45.72ms, (+0.61ms)þ .init functions: 47.61ms, (+1.89ms)þ SMP launched: 48.08ms, (+0.47ms)þ VFS initialized: 50.99ms, (+2.91ms)þ Network initialized: 51.12ms, (+0.14ms)þ pvpanic done: 51.25ms, (+0.13ms)þ pci enumerated: 61.55ms, (+10.29ms)þ drivers probe: 61.55ms, (+0.00ms)þ drivers loaded: 135.91ms, (+74.36ms)þ ROFS mounted: 136.98ms, (+1.07ms)þ Total time: 138.16ms, (+1.18ms)þCmdline: /helloþHello from C codeþ```þþ### Memory UtilizationþþOSv needs at least 15 M of memory to run a _hello world_ app. Even though it is half ofþwhat it was 2 years ago, it is still quite a lot comparing to other unikernels. We are planning to further lowerþthis number by reducing size of the kernel, adding [self-tuning logic to L1/L2 memory pools](https://github.com/cloudius-systems/osv/issues/1013) andþmaking application threads use [lazily allocated stacks](https://github.com/cloudius-systems/osv/issues/143).þþ## TestingþþOSv comes with around 130 unit tests that get executed upon every commit and run on ScyllaDB servers. There are also number of extraþtests located under `tests/` sub-tree that are not automated at this point.þþYou can run unit tests in number of ways:þ```þ./scripts/build check                  # Create ZFS test image and run all tests on QEMUþþ./scripts/build check fs=rofs          # Create ROFS test image and run all tests on QEMUþþ./scripts/build image=tests && \       # Create ZFS test image and run all tests on Firecrackerþ./scripts/test.py -p firecrackerþþ./scripts/build image=tests && \       # Create ZFS test image and run all tests on QEMUþ./scripts/test.py -p qemu_microvm      # with microvm machineþ```þþIn addition, there is an [Automated Testing Framework](https://github.com/cloudius-systems/osv/wiki/Automated-Testing-Framework)þthat can be used to run around 30 real apps, some of themþunder stress using `ab` or `wrk` tools. The intention is to catch any regressions that might be missedþby unit tests.þþFinally, one can use [Docker files](https://github.com/cloudius-systems/osv/tree/master/docker#docker-osv-builder) toþtest OSv on different Linux distribution.þþ## Setting up Development EnvironmentþþOSv can only be built on a 64-bit x86 Linux distribution. Please note thatþthis means the ""x86_64"" or ""amd64"" version, not the 32-bit ""i386"" version.þþIn order to build OSv kernel you need a physical or virtual machine with Linux distribution on it and GCC toolchain andþall necessary packages and libraries OSv build process depends on. The fastest way to set it up is to use theþ[Docker files](https://github.com/cloudius-systems/osv/tree/master/docker#docker-osv-builder) that OSv comes with.þYou can use them to build your own Docker image and then start it in order to build OSv kernel or run an app on OSv inside of it.þPlease note that the main docker file depends on pre-built base **Docker images** for þ[Ubuntu](https://hub.docker.com/repository/docker/osvunikernel/osv-ubuntu-19.10-builder-base) þor [Fedora](https://hub.docker.com/repository/docker/osvunikernel/osv-fedora-31-builder-base) þthat get published to DockerHub upon every commit. This should speed up building the final imagesþas all necessary packages should already be part of the base images.þþAlternatively, you can manually clone OSv repo and use [setup.py](https://github.com/cloudius-systems/osv/blob/master/scripts/setup.py)þto install all required packages and libraries, as long as it supports your Linux distribution, and you have both git þand python 3 installed on your machine:þ```bashþgit clone https://github.com/cloudius-systems/osv.gitþcd osv && git submodule update --init --recursiveþ./scripts/setup.pyþ```þþThe `setup.py` recognizes and installs packages for number of Linux distributions including Fedora, Ubuntu, þ[Debian](https://github.com/cloudius-systems/osv/wiki/Building-OSv-on-Debian-stable), LinuxMint and RedHat ones þ(Scientific Linux, NauLinux, CentOS Linux, Red Hat Enterprise Linux, Oracle Linux). Please note that we activelyþmaintain and test only Ubuntu and Fedora, so your mileage with other distributions may vary. The `setup.py`þis actually used by Docker files internally to achieve the same result. þþ### IDEsþþIf you like working in IDEs, we recommend either [Eclipse CDT](https://www.eclipse.org/cdt/) which can be setupþas described in this [wiki page](https://github.com/cloudius-systems/osv/wiki/Working-With-Eclipse-CDT) or þ[CLion from JetBrains](https://www.jetbrains.com/clion/) which can be setup to work with OSv makefile usingþso called compilation DB as described in this [guide](https://www.jetbrains.com/help/clion/managing-makefile-projects.html).þþ## Building OSv Kernel and Creating ImagesþþBuilding OSv is as easy as using the shell script `./scripts/build`þthat orchestrates the build process by delegating to the main [makefile](https://github.com/cloudius-systems/osv/blob/master/Makefile)þto build the kernel and by using number of Python scripts like `./scripts/module.py` þto build application and *fuse* it together with the kernelþinto a final image placed at `./build/release/usr.img` (or `./build/$(arch)/usr.img` in general).þPlease note that *building an application* does not necessarily mean building from sources as in many þcases the application binaries would be located on and copied from the Linux build machineþusing the shell script `./scripts/manifest_from_host.sh`þ(see [this Wiki page](https://github.com/cloudius-systems/osv/wiki/Running-unmodified-Linux-executables-on-OSv) for details).þþThe shell script `build` can be used as the examples below illustrate:þ```bashþ# Create default image that comes with command line and REST API serverþ./scripts/buildþþ# Create image with native-example appþ./scripts/build -j4 fs=rofs image=native-exampleþþ# Create image with spring boot app with Java 10 JREþ./scripts/build JAVA_VERSION=10 image=openjdk-zulu-9-and-above,spring-boot-exampleþþ # Create image with 'ls' executable taken from the hostþ./scripts/manifest_from_host.sh -w ls && ./scripts/build --append-manifestþþ# Create test image and run all tests in itþ./scripts/build checkþþ# Clean the build treeþ./scripts/build cleanþ```þþCommand nproc will calculate the number of jobs/threads for make and `./scripts/build` automatically.þAlternatively, the environment variable MAKEFLAGS can be exported as follows:þþ```þexport MAKEFLAGS=-j$(nproc)þ```þþIn that case, make and scripts/build do not need the parameter -j.þþFor details on how to use the build script, please run `./scripts/build --help`.þþThe `./scripts/build` creates the image `build/last/usr.img` in qcow2 format.þTo convert this image to other formats, use the `./scripts/convert`þtool, which can convert an image to the vmdk, vdi or raw formats.þFor example:þþ```þ./scripts/convert rawþ```þþ### Aarch64þþBy default, OSv kernel gets built for x86_64 architecture, but it is also possibleþ to build one for ARM by adding **arch** parameter like so:þ```bashþ./scripts/build arch=aarch64þ```þAt this point cross-compiling the **aarch64** version of OSv is only supportedþon Fedora and relevant aarch64 gcc and libraries' binaries can be downloaded usingþthe `./scripts/download_fedora_aarch64_packages.py` script.þPlease note that simple ""hello world"" app should work just fine, but overall the ARM part of OSv has not beenþ as well maintained and tested as x86_64 due to the lack of volunteers. In addition,þ the same simple example can successfully run on QEMU on Raspberry PI 4 with KVM acceleration enabled.þ For more information about the aarch64 port please read [this Wiki page](https://github.com/cloudius-systems/osv/wiki/AArch64).þþ### FilesystemsþþAt the end of the boot process, OSv dynamic linker loads an application ELF and any related librariesþ from the filesystem on a disk that is part of the image. By default, the images built by `./scripts/build`þ contain a disk formatted as ZFS, which you can read more about [here](https://github.com/cloudius-systems/osv/wiki/ZFS).þ ZFS is a great read-write file system and may be a perfect fit if you want to run MySQL on OSv. However, it may be an overkillþ if you want to run stateless apps in which case you may consider þ [Read-Only FS](https://github.com/cloudius-systems/osv/commit/cd449667b7f86721095ddf4f9f3f8b87c1c414c9). Finally,þ you can also have OSv read the application binary from RAMFS, in which case the filesystem get embedded as part ofþ the kernel ELF. You can specify which filesystem to build image disk asþ  by setting parameter `fs` of `./scripts/build` to one of the three values -`zfs`, `rofs` or `ramfs`.þþIn addtion, one can mount NFS filesystem, which had been recently transformed to be a shared library pluggable as a [module](https://github.com/cloudius-systems/osv/tree/master/modules/nfs), and newly implemented [Virtio-FS filesystem](https://stefanha.github.io/virtio/virtio-fs.html#x1-41500011). The NFS and Virtio-FS mounts can be setup by adding proper entry `/etc/fstab` or by passing a boot parameter as explained in this [commit comments](https://github.com/cloudius-systems/osv/commit/47c7e9268ff96f67f4649bb6c63685a5c2d74f00).þþ## Running OSvþþRunning an OSv image, built by `scripts/build`, is as easy as:þ```bashþ./scripts/run.pyþ```þþBy default, the `run.py` runs OSv under KVM, with 4 vCPUs and 2 GB of memory. þYou can control these and tens of other ones by passing relevant parameters to þthe `run.py`. For details, on how to use the script, please run `./scripts/run.py --help`.þþThe `run.py` can run OSv image on QEMU/KVM, Xen and VMware. If running under KVM you can terminate by hitting Ctrl+A X.þþAlternatively, you can use `./scripts/firecracker.py` to run OSv on [Firecracker](https://firecracker-microvm.github.io/). þThis script automatically downloads firecracker binary if missing, and accepts number of parameters like number ot vCPUs, memoryþnamed exactly like `run.py` does. You can learn more about running OSv on Firecracker þfrom this [wiki](https://github.com/cloudius-systems/osv/wiki/Running-OSv-on-Firecracker). þþPlease note that in order to run OSv with the best performance on Linux under QEMU or Firecracker you need KVM enabled þ(this is only possible on *physical* Linux machines, EC2 ""bare metal"" (i3) instances or VMs that support nested virtualization with KVM on). þThe easiest way to verify if KVM is enabled is to check if `/dev/kvm` is present, and your user account can read from and write to it. þAdding your user to the kvm group may be necessary like so:þ```bashþusermod -aG kvm <user name>þ```þþFor more information about building and running JVM, Node.JS, Python and other managed runtimes as well as Rust, Golang or C/C++ appsþ on OSv, please read this [wiki page](https://github.com/cloudius-systems/osv/wiki#running-your-application-on-osv). þ For more information about various example apps you can build and run on OSv, please read þ [the osv-apps repo README](https://github.com/cloudius-systems/osv-apps#osv-applications).þþ### NetworkingþþBy default, the `run.py`  starts OSv withþ [user networking/SLIRP](https://wiki.qemu.org/Documentation/Networking#User_Networking_.28SLIRP.29) on. þTo start OSv with more performant external networking, you need to enable `-n` and `-v` options like so:þþ```þsudo ./scripts/run.py -nvþ```þþThe -v is for KVM's vhost that provides better performanceþand its setup requires tap device and thus we use sudo.þþBy default, OSv spawns a `dhcpd`-like thread that automatically configures virtual NICs.þA static configuration can be done within OSv by configuring networking like so:þþ```þifconfig virtio-net0 192.168.122.100 netmask 255.255.255.0 upþroute add default gw 192.168.122.1þ```þþTo enable networking on Firecracker, you have to explicitly enable `-n` optionþto `firecracker.py`.þþFinally, please note that the master branch of OSv only implements IPV4 subset of networking stack.þIf you need IPV6, please build from [ipv6 branch](https://github.com/cloudius-systems/osv/tree/ipv6)þ or use IPV6 kernel published to [nightly releases repo](https://github.com/osvunikernel/osv-nightly-releases/releases/tag/ci-ipv6-latest). þþ## Debugging, Monitoring, Profiling OSvþþ- OSv can be debugged with gdb; for more details please read thisþ [wiki](https://github.com/cloudius-systems/osv/wiki/Debugging-OSv)þ- OSv kernel and application can be traced and profiled; for more details please read þthis [wiki](https://github.com/cloudius-systems/osv/wiki/Trace-analysis-using-trace.py)þ- OSv comes with the admin/monitoring REST API server; for more details please read þ[this](https://github.com/cloudius-systems/osv/wiki/Command-Line-Interface-(CLI)) andþ [that wiki page](https://github.com/cloudius-systems/osv/wiki/Using-OSv-REST-API). There is alsoþ lighter [monitoring REST API module](https://github.com/cloudius-systems/osv/commit/aa32614221254ce300f401bb99c506b528b85682) þ that is effectively a read-only subset of the former one. þ þ## FAQ and ContactþþIf you want to learn more about OSv or ask questions, þplease contact us on [OSv Google Group forum](https://groups.google.com/forum/#!forum/osv-dev).þYou can also follow us on [Twitter](https://twitter.com/osv_unikernel).þþ## Papers and Articles about OSvþþList of somewhat newer articles about OSv found on the Web:þ* [Unikernels vs Containers: An In-Depth Benchmarking Study in the context of Microservice Applications](https://biblio.ugent.be/publication/8582433/file/8582438)þ* [Towards a Practical Ecosystem of Specialized OS Kernels](http://cs.iit.edu/~khale/docs/diver-ross19.pdf)þ* [A Performance Evaluation of Unikernels](https://pdfs.semanticscholar.org/d956/f72dbc65301578dc95e0f751f4ae7c09d831.pdf)þ* [Security Perspective on Unikernels](https://arxiv.org/pdf/1911.06260.pdf)þ* [Performance Evaluation of OSv for Server Applications](http://www.cs.utah.edu/~peterm/prelim-osv-performance.pdf)þ* [Time provisioning Evaluation of KVM, Docker and Unikernels in a Cloud Platform](https://tiagoferreto.github.io/pubs/2016ccgrid_xavier.pdf)þ* [Unikernels - Beyond Containers to the Next Generation of the Cloud](https://theswissbay.ch/pdf/_to_sort/O'Reilly/unikernels.pdf)þþYou can find some older articles and presentations at http://osv.io/resources and http://blog.osv.io/."
bartobri/no-more-secrets,185,4662,123,231,User,False,252,1,7,14,False,A command line tool that recreates the famous data decryption effect seen in the 1992 movie Sneakers.,,6,7,0,2,38,0,1,0,34,0,0,1531,0,0,0,0,0,0,12,,175,,"![Version](https://img.shields.io/badge/Version-0.3.3-green.svg)þþNo More Secretsþ===============þþThis project provides a command line tool called `nms` that recreates theþfamous data decryption effect seen on screen in the 1992 hacker movie Sneakers.þFor reference, you can see this effect at 0:35 in [this movie clip](https://www.youtube.com/watch?v=F5bAa6gFvLs&t=35).þþThis command works on piped data. Pipe any ASCII or UTF-8 text to `nms`,þand it will apply the Hollywood effect, initially showing encrypted data,þthen starting a decryption sequence to reveal the original plain-text characters.þþ![Screenshot](http://www.brianbarto.info/extern/images/nms/nms.gif)þþAlso included in this project is a program called `sneakers` that recreatesþwhat we see in the above movie clip. Note that this program requires theþuser to select one of the menu options before it terminates.þþ![Screenshot](http://www.brianbarto.info/extern/images/nms/sneakers.gif)þþBy default, this project has no dependencies, but it does rely on ANSI/VT100þterminal escape sequences to recreate the effect. Most modern terminalþprograms support these sequences so this should not be an issue for mostþusers. If yours does not, this project also provides a ncurses implementationþwhich supports non-ANSI terminals, but at the expense of losing the inlineþfunctionality (ncurses will always clear the screen prior to displaying output).þþTable of Contentsþ-----------------þþ1. [Download and Install](#download-and-install)þ2. [Usage](#usage)þ3. [The NMS Library](#the-nms-library)þ4. [License](#license)þ5. [Tips](#tips)þþDownload and Installþ--------------------þþMore and more Unix/Linux platforms are including this project in theirþpackage manager. You may wish to search your package manager to see if itþis an installation option. If you install form a package manager, pleaseþcheck that you have the latest version (`nms -v`). If not, I suggestþinstalling from source by following the instructions below.þþTo install this project from source, you will need to have the tools `git`,þ`gcc`, and `make` to download and build it. Install them from your packageþmanager if they are not already installed.þþOnce you have the necessary tools installed, follow these instructions:þþ#### Install:þ```þ$ git clone https://github.com/bartobri/no-more-secrets.gitþ$ cd ./no-more-secretsþ$ make nmsþ$ make sneakers             ## Optionalþ$ sudo make installþ```þþ#### Uninstall:þþ```þ$ sudo make uninstallþ```þþ#### Install with Ncurses SupportþþIf your terminal does not support ANSI/VT100 escape sequences, the effectþmay not render properly. This project provides a ncurses implementationþfor such cases. You will need the ncurses library installed. [Install thisþlibrary from your package manager](NCURSES.md). Next, follow these instructions:þþ```þ$ git clone https://github.com/bartobri/no-more-secrets.gitþ$ cd ./no-more-secretsþ$ make nms-ncursesþ$ make sneakers-ncurses     ## Optionalþ$ sudo make installþ```þþUsageþ-----þþ`nms` works on piped data. Pipe any ASCII or UTF-8 characters to it andþenjoy the magic. In the below examples, I use a simple directory listing.þþ```þ$ ls -l | nmsþ$ ls -l | nms -a           // Set auto-decrypt flagþ$ ls -l | nms -s           // Set flag to mask space charactersþ$ ls -l | nms -f green     // Set foreground color to greenþ$ ls -l | nms -c           // Clear screenþ$ nms -v                   // Display versionþ```þþNote that by default, after the initial encrypted characters are displayed,þ`nms` will wait for the user to press a key before initiating the decryptionþsequence. This is how the it is depicted in the movie.þþ#### Command Line Optionsþþ`-a`þþSet the auto-decrypt flag. This will automatically start theþdecryption sequence without requiring a key press.þþ`-s`þþSet a flag to mask space characters. This will only mask single blank spaceþcharacters. Other space characters such as tabs and newlines will not be masked.þþ`-f <color>`þþSet the foreground color of the decrypted text to the colorþspecified. Valid options are white, yellow, black, magenta, blue, green,þor red. This is blue by default.þþ`-c`þþClear the screen prior to printing any output. Specifically,þit saves the state of the terminal (all current output), and restores itþonce the effect is completed. Note that when using this option, `nms` requiresþthe user to press a key before restoring the terminal.þþ`-v`þþDisplay version info.þþThe NMS Libraryþ---------------þþFor those who would like to use this effect in their own projects, I haveþcreated a C library that provides simple interface and can easily be usedþfor any program that runs from the command line.þþSee [LibNMS](https://github.com/bartobri/libnms) for more info.þþLicenseþ-------þþThis program is free software; you can redistribute it and/or modify itþunder the terms of the GNU General Public License. See [LICENSE](LICENSE) forþmore details.þþTipsþ----þþ[Tips are always appreciated!](https://github.com/bartobri/tips)"
cmus/cmus,5836,3786,117,412,Organization,False,2159,6,56,111,False,"Small, fast and powerful console music player for Unix-like operating systems.",https://cmus.github.io/,0,27,1,203,490,24,8,18,276,8,0,5395,2,2,19,14,0,0,2,2,,,"*Warning: cmus is not actively maintained. For details, please see [#856](https://github.com/cmus/cmus/issues/856)*þþcmus — C\* Music Playerþ=======================þþhttps://cmus.github.io/þþ[![Build Status](https://travis-ci.org/cmus/cmus.svg?branch=master)](https://travis-ci.org/cmus/cmus)þþCopyright © 2004-2008 Timo Hirvonen <tihirvon@gmail.com>þþCopyright © 2008-2017 Various AuthorsþþþConfigurationþ-------------þþList available optional featuresþþ    $ ./configure --helpþþAuto-detect everythingþþ    $ ./configureþþTo disable some feature, arts for example, and install to `$HOME` runþþ    $ ./configure prefix=$HOME CONFIG_ARTS=nþþAfter running configure you can see from the generated `config.mk` fileþwhat features have been configured in (see the `CONFIG_*` options).þþ*Note*: For some distributions you need to install development versionsþof the dependencies.  For example if you want to use 'mad' input pluginþ(mp3) you need to install `libmad0-dev` (Debian) or `libmad-devel` (RPM)þpackage. After installing dependencies you need to run `./configure`þagain, of course.þþIf you want to use the Tremor library as alternative for decodingþOgg/Vorbis files you have to pass `CONFIG_TREMOR=y` to the configureþscript:þþ    $ ./configure CONFIG_VORBIS=y CONFIG_TREMOR=yþþThe Tremor library is supposed to be used on hardware that has no FPU.þþþBuildingþ--------þþ    $ makeþþOr on some BSD systems you need to explicitly use GNU make:þþ    $ gmakeþþþInstallationþ------------þþ    $ make installþþOr to install to a temporary directory:þþ    $ make install DESTDIR=~/tmp/cmusþþThis is useful when creating binary packages.þþRemember to replace `make` with `gmake` if needed.þþþManualsþ-------þþ    $ man cmus-tutorialþþAndþþ    $ man cmusþþþMailing Listþ------------þþTo subscribe to cmus-devel@lists.sourceforge.net or view the archive visitþhttp://lists.sourceforge.net/lists/listinfo/cmus-devel.þþThe mailing list now serves as an archive for old releases and issues.þPlease use the github [issues](https://github.com/cmus/cmus/issues)þpage for any problems, suggestions, or bug reports.þþþReporting Bugsþ--------------þþBugs should be reported using the Github [issue tracker](https://github.com/cmus/cmus/issues).þWhen creating a new issue, a template will be shown containing instructions on how to collectþthe necessary information.þþAdditional debug information can be found in `~/cmus-debug.txt` if you configured cmus withþmaximum debug level (`./configure DEBUG=2`). In case of a crash the last lines may be helpful.þþþGit Repositoryþ--------------þþhttps://github.com/cmus/cmusþþ    $ git clone https://github.com/cmus/cmus.gitþþþHackingþ-------þþcmus uses the [Linux kernel coding style](https://www.kernel.org/doc/html/latest/process/coding-style.html).þUse hard tabs.  Tabs are _always_ 8 characters wide.  Keep the style consistent with rest of theþcode.þþBug fixes and implementations of new features should be suggested as aþ[pull request](https://github.com/cmus/cmus/pulls) directly on Github."
FreeRDP/FreeRDP,41435,4384,333,1561,Organization,False,14365,6,32,262,False,FreeRDP is a free remote desktop protocol library and clients,http://www.freerdp.com/,3,31,2,334,2688,70,174,30,3222,12,241,3274,15,463,25994,18500,0,0,8,17,,,"# FreeRDP: A Remote Desktop Protocol ImplementationþþFreeRDP is a free implementation of the Remote Desktop Protocol (RDP), released under the Apache license.þEnjoy the freedom of using your software wherever you want, the way you want it, in a world whereþinteroperability can finally liberate your computing experience.þþ## ResourcesþþProject website: https://www.freerdp.com/  þIssue tracker: https://github.com/FreeRDP/FreeRDP/issues  þSources: https://github.com/FreeRDP/FreeRDP/  þDownloads: https://pub.freerdp.com/releases/  þWiki: https://github.com/FreeRDP/FreeRDP/wiki  þAPI documentation: https://pub.freerdp.com/api/  þþIRC channel: #freerdp @ irc.freenode.net  þMailing list: https://lists.sourceforge.net/lists/listinfo/freerdp-develþþ## Microsoft Open SpecificationsþþInformation regarding the Microsoft Open Specifications can be found at:þhttp://www.microsoft.com/openspecifications/þþA list of reference documentation is maintained here:þhttps://github.com/FreeRDP/FreeRDP/wiki/Reference-Documentationþþ## CompilationþþInstructions on how to get started compiling FreeRDP can be found on the wiki:þhttps://github.com/FreeRDP/FreeRDP/wiki/Compilation"
tmk/tmk_keyboard,24764,3019,210,1528,User,False,1586,10,4,60,False,Keyboard firmwares for Atmel AVR and Cortex-M,,0,16,0,124,322,4,6,42,163,2,3,3596,2,97,32694,416405,0,0,37,,407,,"TMK Keyboard Firmware Collectionþ================================þThis repository includes keyboard and converter firmware projects built with [`tmk_core`][tmk_core] keyboard library.þþThe latest source code is available here: <http://github.com/tmk/tmk_keyboard>þþþUpdatesþ-------þ#### 2017/01/11þChanged action code for `ACTION_LAYER_MODS` and this may cause incompatibility with existent shared URL and downloaded firmwware of keymap editor. If you are using the action you just have to redefine it on keymap editor. Existent keymap code should not suffer.þþ#### 2016/06/26þKeymap framework was updated. `fn_actions[]` should be defined as `action_t` instead of `uint16_t`. And default code for keymap handling is now included in core you just need define `uint8_t keymaps[][MATRIX_ROWS][MATRIX_COLS]` and `action_t fn_actions[]`.þþþ#### 2016/06/22þSome projects were moved from `converter` and `keyboard` to `orphan` directory. Those might be removed in some future but you will be able to access them with `orphans` tag. See <https://github.com/tmk/tmk_keyboard/issues/173>þþ#### 2016/02/10þcore: flabbergast's Chibios protocol was merged from <https://github.com/flabbergast/tmk_keyboard/tree/chibios> (@72b1668). See [tmk_core/protocol/chibios/README.md](tmk_core/protocol/chibios/README.md). Chibios protocol supports Cortex-M such as STM32 and Kinetis.þþ#### 2015/04/22þCore library was separated to other branch `core`. <https://github.com/tmk/tmk_keyboard/tree/core>þþIn `Makefile` you need to set `TMK_DIR` to indicate core library location now.þþ    TMK_DIR = ../../tmk_coreþþþþProjectsþ--------þYou can find some keyboard specific projects under `converter` and `keyboard` directory.þþ### converterþ* [ps2_usb](converter/ps2_usb/)             - [PS/2 keyboard to USB][GH_ps2]þ* [adb_usb](converter/adb_usb/)             - [ADB keyboard to USB][GH_adb]þ* [m0110_usb](converter/m0110_usb)          - [Macintosh 128K/512K/Plus keyboard to USB][GH_m0110]þ* [terminal_usb](converter/terminal_usb/)   - [IBM Model M terminal keyboard(PS/2 scancode set3) to USB][GH_terminal]þ* [news_usb](converter/news_usb/)           - [Sony NEWS keyboard to USB][GH_news]þ* [x68k_usb](converter/x68k_usb/)           - [Sharp X68000 keyboard to USB][GH_x68k]þ* [sun_usb](converter/sun_usb/)             - [Sun] to USB(type4, 5 and 3?)þ* [pc98_usb](converter/pc98_usb/)           - [PC98] to USBþ* [usb_usb](converter/usb_usb/)             - [USB to USB][GH_usb]þ* [ibm4704_usb](converter/ibm4704_usb)      - [IBM 4704 keyboard to USB][GH_ibm4704]þ* [next_usb](converter/next_usb)            - NeXT(Non-ADB) to USB, contributed by [BCG](https://github.com/bgould) and based on [Adafruit's work](https://learn.adafruit.com/usb-next-keyboard-with-arduino-micro/overview)þþ### keyboardþ* [hhkb](keyboard/hhkb/)                    - [Happy Hacking Keyboard pro][GH_hhkb] **my main board**þ* [alps64](keyboard/alps64/)                - [Alps64 PCB](https://geekhack.org/index.php?topic=69740.0)þ* [hbkb](keyboard/hbkb/)                    - [Happy Buckling spring keyboard][GH_hbkb](IBM Model M 60% mod)þ* [Infinity](keyboard/infinity/)            - Massdrop [Infinity keyboard][Infinity]þ* [gh60](keyboard/gh60/)                    - [GH60] DIY 60% keyboard [prototype][GH60_proto] **my second board**þ* [onekey](keyboard/onekey/)                - Simple one key keyboard exampleþþþ### Projects based tmk_keyboard or tmk_coreþhttps://github.com/tmk/tmk_keyboard/wiki/TMK-Based-Projectsþþþ[GH_hhkb]:      http://geekhack.org/showwiki.php?title=Island:12047þ[GH_ps2]:       http://geekhack.org/showwiki.php?title=Island:14618þ[GH_adb]:       http://geekhack.org/showwiki.php?title=Island:14290þ[GH_hhkb_bt]:   http://geekhack.org/showwiki.php?title=Island:20851þ[GH_m0110]:     http://geekhack.org/showwiki.php?title=Island:24965þ[GH_news]:      http://geekhack.org/showwiki.php?title=Island:25759þ[GH_terminal]:  http://geekhack.org/showwiki.php?title=Island:27272þ[GH_x68k]:      http://geekhack.org/showwiki.php?title=Island:29060þ[GH_hbkb]:      http://geekhack.org/showwiki.php?title=Island:29483þ[GH_ibm4704]:   http://geekhack.org/index.php?topic=54706.0þ[GH60]:         http://geekhack.org/index.php?topic=34959þ[GH60_proto]:   http://geekhack.org/index.php?topic=37570.0þ[PC98]:         http://en.wikipedia.org/wiki/NEC_PC-9801þ[Sun]:          http://en.wikipedia.org/wiki/Sun-3þ[Infinity]:     https://www.massdrop.com/buy/infinity-keyboard-kitþ[tmk_core]:     https://github.com/tmk/tmk_coreþþþþLicenseþ-------þ**GPLv2** or later. Some protocol files are under **Modified BSD License**.þþThird party libraries like LUFA, PJRC and V-USB have their own license respectively.þþþþBuild Firmware and Program Controllerþ-------------------------------------þSee [tmk_core/doc/build.md](tmk_core/doc/build.md).þþþþChange your keymapþ------------------þSee [tmk_core/doc/keymap.md](tmk_core/doc/keymap.md).þþþþMagic Commandsþ--------------þTo see help press `Magic` + `H`.þþ`Magic` key combination is `LShift` + `RShift` in many projects, but `Power` key on ADB converter.þ`Magic` keybind can be vary on each project, check `config.h` in project directory.þþFollowing commands can be also executed with `Magic` + key. In console mode `Magic` keybind is not needed.þþ    ----- Command Help -----þ    c:      enter console modeþ    d:      toggle debug enableþ    x:      toggle matrix debugþ    k:      toggle keyboard debugþ    m:      toggle mouse debugþ    v:      print device version & infoþ    t:      print timer countþ    s:      print statusþ    e:     print eeprom configþ    n:     toggle NKROþ    0/F10:  switch to Layer0þ    1/F1:   switch to Layer1þ    2/F2:   switch to Layer2þ    3/F3:   switch to Layer3þ    4/F4:   switch to Layer4þ    PScr:   power down/remote wake-upþ    Caps:   Lock Keyboard(Child Proof)þ    Paus:   jump to bootloaderþþþþBoot Magic Configuration - Virtual DIP Switchþ---------------------------------------------þBoot Magic are executed during boot up time. Press Magic key below then plug in keyboard cable.þNote that you must use keys of **Layer 0** as Magic keys. These settings are stored in EEPROM so that retain your configure over power cycles.þþTo avoid configuring accidentally additive salt key `KC_SPACE` also needs to be pressed along with the following configuration keys. The salt key is configurable in `config.h`. See [tmk_core/common/bootmagic.h](tmk_core/common/bootmagic.h).þþ#### Generalþ- Skip reading EEPROM to start with default configuration(`ESC`)þ- Clear configuration stored in EEPROM to reset configuration(`Backspace`)þþ#### Bootloaderþ- Kick up Bootloader(`B`)þþ#### Debugþ- Debug enable(`D`)þ- Debug matrix enable(`D`+`X`)þ- Debug keyboard enable(`D`+`K`)þ- Debug mouse enable(`D`+`M`)þþ#### Keymapþ- Swap Control and CapsLock(`Left Control`)þ- Change CapsLock to Control(`Caps Lock`)þ- Swap LeftAlt and Gui(`Left Alt`)þ- Swap RightAlt and Gui(`Right Alt`)þ- Disable Gui(`Left Gui`)þ- Swap Grave and Escape(`Grave`)þ- Swap BackSlash and BackSpace(`Back Slash`)þ- Enable NKRO on boot(`N`)þþ#### Default Layerþ- Set Default Layer to 0(`0`)þ- Set Default Layer to 1(`1`)þ- Set Default Layer to 2(`2`)þ- Set Default Layer to 3(`3`)þ- Set Default Layer to 4(`4`)þ- Set Default Layer to 5(`5`)þ- Set Default Layer to 6(`6`)þ- Set Default Layer to 7(`7`)þþþþMechanical Locking supportþ--------------------------þThis feature makes it possible for you to use mechanical locking switch for `CapsLock`, `NumLock`þor `ScrollLock`. To enable this feature define these macros in `config.h` and use `KC_LCAP`, `KC_LNþUM` or `KC_LSCR` in keymap for locking key instead of normal `KC_CAPS`, `KC_NLCK` or `KC_SLCK`. Resþync option tries to keep switch state consistent with keyboard LED state.þþ    #define LOCKING_SUPPORT_ENABLEþ    #define LOCKING_RESYNC_ENABLEþþþþStart Your Own Projectþ-----------------------þ1. Add `tmk_core` into your repository using `git submodule` or `git subtree`.þ2. Copy files from `tmk_keybaord` or other project similar to yoursþ3. Edit those files to support your keyboard.þþSee these as examples.þ- https://github.com/tmk/infinity_ergodoxþ- https://github.com/tmk/whitefoxþþþþDebuggingþ--------þUse PJRC's `hid_listen` to see debug messages. You can use xprintf() to display debug info, see `tmk_core/common/xprintf.h`.þþ- https://www.pjrc.com/teensy/hid_listen.htmlþþþþFiles and Directoriesþ-------------------þ### Topþ* keyboard/     - keyboard projectsþ* converter/    - protocol converter projectsþ* tmk_core/     - core libraryþ* tmk_core/doc/ - documentsþþþþContributionþ------------þ- Report bugs in github **[Issues](https://github.com/tmk/tmk_keyboard/issues)**.þ- Pull requets are also welcomed.þþþþCoding Styleþ-------------þ- Doesn't use Tab to indent, use 4-spaces instead.þþþþOther Keyboard Firmware Projectsþ------------------þYou can learn a lot about keyboard firmware from these. See [Other Projects](https://github.com/tmk/tmk_keyboard/wiki/Other-Projects) other than TMK."
DaveDavenport/rofi,17272,5936,95,323,Organization,False,3182,29,29,91,False,"Rofi: A window switcher, application launcher and dmenu replacement",,7,25,3,64,842,17,60,14,222,6,19,2910,15,100,8117,6301,0,0,9,0,,davatorium/rofi,"[![Codacy Badge](https://api.codacy.com/project/badge/Grade/ca0310962a7c4b829d0c57f1ab023531)](https://app.codacy.com/app/davatorium/rofi?utm_source=github.com&utm_medium=referral&utm_content=davatorium/rofi&utm_campaign=Badge_Grade_Settings)þ[![Build Status](https://travis-ci.org/davatorium/rofi.svg?branch=master)](https://travis-ci.org/davatorium/rofi)þ[![codecov.io](https://codecov.io/github/davatorium/rofi/coverage.svg?branch=master)](https://codecov.io/github/davatorium/rofi?branch=master)þ[![Issues](https://img.shields.io/github/issues/davatorium/rofi.svg)](https://github.com/davatorium/rofi/issues)þ[![Forks](https://img.shields.io/github/forks/davatorium/rofi.svg)](https://github.com/davatorium/rofi/network)þ[![Stars](https://img.shields.io/github/stars/davatorium/rofi.svg)](https://github.com/davatorium/rofi/stargazers)þ[![Downloads](https://img.shields.io/github/downloads/davatorium/rofi/total.svg)](https://github.com/davatorium/rofi/releases)þ[![Coverity](https://scan.coverity.com/projects/3850/badge.svg)](https://scan.coverity.com/projects/davedavenport-rofi)þ[![Forum](https://img.shields.io/badge/forum-online-green.svg)](https://reddit.com/r/qtools/)þþ# A window switcher, Application launcher and dmenu replacementþþ**Rofi** started as a clone of simpleswitcher, written by [Sean Pringle](http://github.com/seanpringle/simpleswitcher) - aþpopup window switcher roughly based on [superswitcher](http://code.google.com/p/superswitcher/).þSimpleswitcher laid the foundations, and therefore Sean Pringle deserves most of the credit for this tool. **Rofi**þ(renamed, as it lost the *simple* property) has been extended with extra features, like an application launcher and þssh-launcher, and can act as a drop-in dmenu replacement, making it a very versatile tool.þþ**Rofi**, like dmenu, will provide the user with a textual list of options where one or more can be selected.þThis can either be running an application, selecting a window, or options provided by an external script.þþIts main features are:þþ * Fully configurable keyboard navigationþ * Type to filterþ    - Tokenized: type any word in any order to filterþ    - Case insensitive (togglable)þ    - Support for fuzzy-, regex-, and glob matchingþ * UTF-8 enabledþ    - UTF-8-aware string collatingþ    - International keyboard support (`e -> è)þ * RTL language supportþ * Cairo drawing and Pango font renderingþ * Built-in modes:þ    - Window switcher modeþ        - EWMH compatible WMþ    - Application launcherþ    - Desktop file application launcherþ    - SSH launcher modeþ    - Combi mode, allowing several modes to be merged into one listþ * History-based ordering — last 25 choices are ordered on top based on use (optional)þ * Levenshtein distance ordering of matches (optional)þ * Drop-in dmenu replacementþ    - Many added improvementsþ * Easily extensible using scriptsþ * Themingþþ**Rofi** has several built-in modes implementing common use cases and can be extended by scripts (either called fromþ**Rofi** or calling **Rofi**).þþBelow is a list of the different modes:þþ## Window Switcherþþ![Window List](https://davatorium.github.io/rofi/images/rofi/window-list.png)þþThe window switcher shows the following informations in columns (can be customized):þþ1. Desktop nameþ2. Window classþ3. Window titleþþWindow mode features:þþ * Closing applications with `Shift-Delete`þ * Custom command with `Shift-Return`þþþ## Application launcherþþ![run mode](https://davatorium.github.io/rofi/images/rofi/run-dialog.png)þþThe run mode allows users to quickly search for and launch a program.þþRun mode features:þþ * `Shift-Return` to run the selected program in a terminalþ * Favorites list, with frequently used programs sorted on topþ * Custom entries, like aliases, added by executing a commandþþþ## Desktop File Application launcherþþThe desktop run mode allows users to quickly search and launch an application from the *freedesktop.org* DesktopþEntries. These are used by most Desktop Environments to populate launchers and menus.þDrun mode features:þþ * Favorites list, with frequently used programs sorted on topþ * Auto starting terminal applications in a terminalþþ## SSH launcherþþ![SSH Launcher](https://davatorium.github.io/rofi/images/rofi/ssh-dialog.png)þþQuickly `ssh` into remote machines. Parses `~/.ssh/config` to find hosts.þþ## Script modeþþLoads external scripts to add modes to **Rofi**, for example a file-browser.þþ```þrofi  -show fb -modi fb:../Examples/rofi-file-browser.shþ```þþ## COMBI modeþþCombine multiple modes in one view. This is especially useful when merging the window and run mode into one view.þAllowing to quickly switch to an application, either by switching to it when it is already running or starting it.þþExample to combine Desktop run and the window switcher:þþ```þrofi -combi-modi window,drun -show combi -modi combiþ```þþ## dmenu replacementþþ![DMENU replacement (running teiler)](https://davatorium.github.io/rofi/images/rofi/dmenu-replacement.png)þþDrop in dmenu replacement. (Screenshot shows rofi used byþ[teiler](https://github.com/carnager/teiler) ).þþ**Rofi** features several improvements over dmenu to improve usability. There is the option to addþan extra message bar (`-mesg`), pre-entering of text (`-filter`), or selecting entries based on aþpattern (`-select`). Also highlighting (`-u` and `-a`) options and modi to force user to select oneþprovided option (`-only-match`). In addition to this, rofi's dmenu mode can select multiple lines andþwrite them to stdout.þþ# UsageþþIf used with `-show [mode]`, rofi will immediately open in the specified [mode].þþIf used with `-dmenu`, rofi will use data from STDIN to let the user select an option.þþFor example, to show a run dialog:þþ  `rofi -show run`þþTo show a ssh dialog:þþ  `rofi -show ssh`þþ## dmenuþþIf rofi is passed the `-dmenu` option, or run as `dmenu` (ie, /usr/bin/dmenu is symlinked to /usr/bin/rofi),þit will use the data passed from STDIN.þþ```þ~/scripts/my_script.sh | rofi -dmenuþecho -e ""Option #1\nOption #2\nOption #3"" | rofi -dmenuþ```þþIn both cases, rofi will output the user's selection to STDOUT.þþ## Switching Between ModiþþType `Shift-/Left/Right` to switch between active modi.þþþ## Key bindingsþþ| Key                                  | Action                                                             |þ|:-------------------------------------|:-------------------------------------------------------------------|þ|`Ctrl-v, Insert`                      | Paste from clipboard |þ|`Ctrl-Shift-v, Shift-Insert`          | Paste primary selection |þ|`Ctrl-w`                              | Clear the line |þ|`Ctrl-u`                              | Delete till the start of line |þ|`Ctrl-a`                              | Move to beginning of line |þ|`Ctrl-e`                              | Move to end of line |þ|`Ctrl-f, Right`                       | Move forward one character |þ|`Alt-f, Ctrl-Right`                   | Move forward one word |þ|`Ctrl-b, Left`                        | Move back one character |þ|`Alt-b, Ctrl-Left`                    | Move back one word |þ|`Ctrl-d, Delete`                      | Delete character |þ|`Ctrl-Alt-d`                          | Delete word |þ|`Ctrl-h, Backspace, Shift-Backspace`  | Backspace (delete previous character) |þ|`Ctrl-Alt-h`                          | Delete previous word |þ|`Ctrl-j,Ctrl-m,Enter`                 | Accept entry |þ|`Ctrl-n,Down`                         | Select next entry |þ|`Ctrl-p,Up`                           | Select previous entry |þ|`Page Up`                             | Go to the previous page |þ|`Page Down`                           | Go to the next page |þ|`Ctrl-Page Up`                        | Go to the previous column |þ|`Ctrl-Page Down`                      | Go to the next column |þ|`Ctrl-Enter`                          | Use entered text as a command (in `ssh/run modi`) |þ|`Shift-Enter`                         | Launch the application in a terminal (in run mode) |þ|`Shift-Enter`                         | Return the selected entry and move to the next item while keeping Rofi open. (in dmenu) |þ|`Shift-Right`                         | Switch to the next modi. The list can be customized with the -modi option. |þ|`Shift-Left`                          | Switch to the previous modi. The list can be customized with the -modi option. |þ|`Ctrl-Tab`                            | Switch to the next modi. The list can be customized with the -modi option. |þ|`Ctrl-Shift-Tab`                      | Switch to the previous modi. The list can be customized with the -modi option. |þ|`Ctrl-space`                          | Set selected item as input text. |þ|`Shift-Del`                           | Delete entry from history. |þ|`grave`                               | Toggle case sensitivity. |þ|`Alt-grave`                           | Toggle levenshtein sort. |þ|`Alt-Shift-S`                         | Take a screenshot and store it in the Pictures directory. |þþFor the full list of key bindings, see: `rofi -show keys` or `rofi -help`.þþ# ConfigurationþþThere are currently three methods of setting configuration options:þþ * Local configuration. Normally, depending on XDG, in `~/.config/rofi/config`. This uses the Xresources format.þ * Xresources: A method of storing key values in the Xserver. Seeþ   [here](https://en.wikipedia.org/wiki/X_resources) for more information.þ * Command line options: Arguments are passed to **Rofi**.þþA distribution can ship defaults in `/etc/rofi.conf`.þþThe Xresources options and the command line options are aliased. To define option X set:þþ    `rofi.X: value`þþIn the Xresources file. To set/override this from command line pass the same keyþprefixed with '-':þþ    `rofi -X value`þþTo get a list of available options formatted as Xresources entries, run:þþ    `rofi -dump-Xresources`þþor in a more readable format:þþ    `rofi -help`þþThe configuration system supports the following types:þþ * Stringþ * Integer (signed and unsigned)þ * Charþ * BooleanþþThe Boolean option has a non-default command line syntax, to enable option X you do:þþ    `rofi -X`þþto disable it:þþ    `rofi -no-X`þþ# ManpageþþFor more detailed information, please see the [manpage](doc/rofi.1.markdown), the [wiki](https://github.com/davatorium/rofi/wiki), or the [forum](https://reddit.com/r/qtools/).þþ# InstallationþþPlease see the [installation guide](https://github.com/davatorium/rofi/blob/next/INSTALL.md) for instructions on how toþinstall **Rofi**.þþ# What is rofi not?þþRofi is not:þþ * A preview application. In other words, it will not show a (small) preview of images, movies or other files.þ * A UI toolkit.þ * A library to be used in other applications.þ * An application that can support every possible use-case. It tries to be generic enough to be usable by everybody.þ   Specific functionality can be added using scripts.þ * Just a dmenu replacement. The dmenu functionality is a nice 'extra' to **rofi**, not its main purpose."
vanhoefm/krackattacks-scripts,17604,2817,225,733,User,False,13280,1,0,212,False,,,0,8,0,25,46,2,1,0,6,0,0,4492,1,1,79,27,0,0,54,,906,,
ms-iot/samples,295111,1210,298,1442,Organization,False,1657,7,4,58,False,Windows 10 IoT Core Samples,,0,7,0,23,128,0,0,5,390,0,0,1874,0,0,0,0,0,0,191,5,,,"Windows 10 IoT Core Samplesþ==============þþ##Welcome to the Windows 10 IoT Core SamplesþþPlease download, build, deploy, and contribute!!  For more information and descriptions about the samples found here, see the samples tab [here](http://ms-iot.github.io/content/en-US/win10/StartCoding.htm)þþFor more information about Windows 10 IoT Core, see our online documentation [here](http://windowsondevices.com)þþWe are working hard to improve Windows 10 IoT Core and deeply value any feedback we get."
robotmedia/RMStore,11667,2351,92,443,Organization,False,354,1,13,16,False,A lightweight iOS library for In-App Purchases,,0,8,1,68,105,1,0,24,35,0,0,2511,0,0,0,0,0,0,44,1,,,"#RMStoreþþ[![CocoaPods Version](https://cocoapod-badges.herokuapp.com/v/RMStore/badge.png)](http://cocoadocs.org/docsets/RMStore) [![Platform](https://cocoapod-badges.herokuapp.com/p/RMStore/badge.png)](http://cocoadocs.org/docsets/RMStore)þ[![Build Status](https://travis-ci.org/robotmedia/RMStore.png)](https://travis-ci.org/robotmedia/RMStore)þ[![Join the chat at https://gitter.im/robotmedia/RMStore](https://badges.gitter.im/robotmedia/RMStore.svg)](https://gitter.im/robotmedia/RMStore?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)þþA lightweight iOS library for In-App Purchases.þþRMStore adds [blocks](#storekit-with-blocks) and [notifications](#notifications) to StoreKit, plus [receipt verification](#receipt-verification), [content downloads](#downloading-content) and [transaction persistence](#transaction-persistence). All in one class without external dependencies. Purchasing a product is as simple as:þþ```objective-cþ[[RMStore defaultStore] addPayment:productID success:^(SKPaymentTransaction *transaction) {þ    NSLog(@""Purchased!"");þ} failure:^(SKPaymentTransaction *transaction, NSError *error) {þ    NSLog(@""Something went wrong"");þ}];þ```þþ##InstallationþþUsing [CocoaPods](http://cocoapods.org/):þþ```rubyþpod 'RMStore', '~> 0.7'þ```þþOr add the files from the [RMStore](https://github.com/robotmedia/RMStore/tree/master/RMStore) directory if you're doing it manually.þþCheck out the [wiki](https://github.com/robotmedia/RMStore/wiki/Installation) for more options.þþ##StoreKit with blocksþþRMStore adds blocks to all asynchronous StoreKit operations.þþ###Requesting productsþþ```objective-cþNSSet *products = [NSSet setWithArray:@[@""fabulousIdol"", @""rootBeer"", @""rubberChicken""]];þ[[RMStore defaultStore] requestProducts:products success:^(NSArray *products, NSArray *invalidProductIdentifiers) {þ    NSLog(@""Products loaded"");þ} failure:^(NSError *error) {þ    NSLog(@""Something went wrong"");þ}];þ```þþ###Add paymentþþ```objective-cþ[[RMStore defaultStore] addPayment:@""waxLips"" success:^(SKPaymentTransaction *transaction) {þ    NSLog(@""Product purchased"");þ} failure:^(SKPaymentTransaction *transaction, NSError *error) {þ    NSLog(@""Something went wrong"");þ}];þ```þþ###Restore transactionsþþ```objective-cþ[[RMStore defaultStore] restoreTransactionsOnSuccess:^(NSArray *transactions){þ    NSLog(@""Transactions restored"");þ} failure:^(NSError *error) {þ    NSLog(@""Something went wrong"");þ}];þ```þþ###Refresh receipt (iOS 7+ only)þþ```objective-cþ[[RMStore defaultStore] refreshReceiptOnSuccess:^{þ    NSLog(@""Receipt refreshed"");þ} failure:^(NSError *error) {þ    NSLog(@""Something went wrong"");þ}];þ```þþ##NotificationsþþRMStore sends notifications of StoreKit related events and extends `NSNotification` to provide relevant information. To receive them, implement the desired methods of the `RMStoreObserver` protocol and add the observer to `RMStore`.þþ###Adding and removing the observerþþ```objective-cþ[[RMStore defaultStore] addStoreObserver:self];þ...þ[[RMStore defaultStore] removeStoreObserver:self];þ```þþ###Products request notificationsþþ```objective-cþ- (void)storeProductsRequestFailed:(NSNotification*)notificationþ{þ    NSError *error = notification.rm_storeError;þ}þþ- (void)storeProductsRequestFinished:(NSNotification*)notificationþ{þ    NSArray *products = notification.rm_products;þ    NSArray *invalidProductIdentifiers = notification.rm_invalidProductIdentififers;þ}þ```þþ###Payment transaction notificationsþþPayment transaction notifications are sent after a payment has been requested or for each restored transaction.þþ```objective-cþ- (void)storePaymentTransactionFinished:(NSNotification*)notificationþ{þ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ}þþ- (void)storePaymentTransactionFailed:(NSNotification*)notificationþ{þ    NSError *error = notification.rm_storeError;þ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ}þþ// iOS 8+ onlyþþ- (void)storePaymentTransactionDeferred:(NSNotification*)notificationþ{þ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ}þ```þþ###Restore transactions notificationsþþ```objective-cþ- (void)storeRestoreTransactionsFailed:(NSNotification*)notification;þ{þ    NSError *error = notification.rm_storeError;þ}þþ- (void)storeRestoreTransactionsFinished:(NSNotification*)notificationþ{þ NSArray *transactions = notification.rm_transactions;þ}þ```þþ###Download notifications (iOS 6+ only)þþFor Apple-hosted and self-hosted downloads:þþ```objective-cþ- (void)storeDownloadFailed:(NSNotification*)notificationþ{þ    SKDownload *download = notification.rm_storeDownload; // Apple-hosted onlyþ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ    NSError *error = notification.rm_storeError;þ}þþ- (void)storeDownloadFinished:(NSNotification*)notification;þ{þ    SKDownload *download = notification.rm_storeDownload; // Apple-hosted onlyþ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ}þþ- (void)storeDownloadUpdated:(NSNotification*)notificationþ{þ    SKDownload *download = notification.rm_storeDownload; // Apple-hosted onlyþ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ    float progress = notification.rm_downloadProgress;þ}þ```þþOnly for Apple-hosted downloads:þþ```objective-cþ- (void)storeDownloadCanceled:(NSNotification*)notificationþ{þ SKDownload *download = notification.rm_storeDownload;þ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ}þþ- (void)storeDownloadPaused:(NSNotification*)notificationþ{þ SKDownload *download = notification.rm_storeDownload;þ    NSString *productIdentifier = notification.rm_productIdentifier;þ    SKPaymentTransaction *transaction = notification.rm_transaction;þ}þ```þþ###Refresh receipt notifications (iOS 7+ only)þþ```objective-cþ- (void)storeRefreshReceiptFailed:(NSNotification*)notification;þ{þ    NSError *error = notification.rm_storeError;þ}þþ- (void)storeRefreshReceiptFinished:(NSNotification*)notification { }þ```þþ##Receipt verificationþþRMStore doesn't perform receipt verification by default but provides reference implementations. You can implement your own custom verification or use the reference verifiers provided by the library.þþBoth options are outlined below. For more info, check out the [wiki](https://github.com/robotmedia/RMStore/wiki/Receipt-verification).þþ###Reference verifiersþþRMStore provides receipt verification via `RMStoreAppReceiptVerifier` (for iOS 7 or higher) and `RMStoreTransactionReceiptVerifier` (for iOS 6 or lower). To use any of them, add the corresponding files from [RMStore/Optional](https://github.com/robotmedia/RMStore/tree/master/RMStore/Optional) into your project and set the verifier delegate (`receiptVerifier`) at startup. For example:þþ```objective-cþ- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptionsþ{þ    const BOOL iOS7OrHigher = floor(NSFoundationVersionNumber) > NSFoundationVersionNumber_iOS_6_1;þ    _receiptVerifier = iOS7OrHigher ? [[RMStoreAppReceiptVerifier alloc] init] : [[RMStoreTransactionReceiptVerifier alloc] init];þ    [RMStore defaultStore].receiptVerifier = _receiptVerifier;þ    // Your codeþ    return YES;þ}þ```þþIf security is a concern you might want to avoid using an open source verification logic, and provide your own custom verifier instead.þþ###Custom verifierþþRMStore delegates receipt verification, enabling you to provide your own implementation using  the `RMStoreReceiptVerifier` protocol:þþ```objective-cþ- (void)verifyTransaction:(SKPaymentTransaction*)transactionþ                           success:(void (^)())successBlockþ                           failure:(void (^)(NSError *error))failureBlock;þ```þþCall `successBlock` if the receipt passes verification, and `failureBlock` if it doesn't. If verification could not be completed (e.g., due to connection issues), then `error` must be of code `RMStoreErrorCodeUnableToCompleteVerification` to prevent RMStore to finish the transaction.þþYou will also need to set the `receiptVerifier` delegate at startup, as indicated above.þþ##Downloading contentþþRMStore automatically downloads Apple-hosted content and provides a delegate for a self-hosted content.þþ###Apple-hosted contentþþDownloadable content hosted by Apple (`SKDownload`) will be automatically downloaded when purchasing o restoring a product. RMStore will notify observers of the download progress by calling `storeDownloadUpdate:` and finally `storeDownloadFinished:`. Additionally, RMStore notifies when downloads are paused, cancelled or have failed.þþRMStore will notify that a transaction finished or failed only after all of its downloads have been processed. If you use blocks, they will called afterwards as well. The same applies to restoring transactions.þþ###Self-hosted contentþþRMStore delegates the downloading of self-hosted content via the optional `contentDownloader` delegate. You can provide your own implementation using the `RMStoreContentDownloader` protocol:þþ```objective-cþ- (void)downloadContentForTransaction:(SKPaymentTransaction*)transactionþ                              success:(void (^)())successBlockþ                             progress:(void (^)(float progress))progressBlockþ                              failure:(void (^)(NSError *error))failureBlock;þ```þþCall `successBlock` if the download is successful, `failureBlock` if it isn't and `progressBlock` to notify the download progress. RMStore will consider that a transaction has finished or failed only after the content downloader delegate has successfully or unsuccessfully downloaded its content.þþ##Transaction persistenceþþRMStore delegates transaction persistence and provides two optional reference implementations for storing transactions in the Keychain or in `NSUserDefaults`. You can implement your transaction, use the reference implementations provided by the library or, in the case of non-consumables and auto-renewable subscriptions, get the transactions directly from the receipt.þþFor more info, check out the [wiki](https://github.com/robotmedia/RMStore/wiki/Transaction-persistence).þþþ##RequirementsþþRMStore requires iOS 5.0 or above and ARC.þþ##RoadmapþþRMStore is in initial development and its public API should not be considered stable. Future enhancements will include:þþ* [Better OS X support](https://github.com/robotmedia/RMStore/issues/4)þþ##Licenseþþ Copyright 2013-2014 [Robot Media SL](http://www.robotmedia.net)þþ Licensed under the Apache License, Version 2.0 (the ""License"");þ you may not use this file except in compliance with the License.þ You may obtain a copy of the License atþþ http://www.apache.org/licenses/LICENSE-2.0þþ Unless required by applicable law or agreed to in writing, softwareþ distributed under the License is distributed on an ""AS IS"" BASIS,þ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.þ See the License for the specific language governing permissions andþ limitations under the License."
P-H-C/phc-winner-argon2,5865,3157,123,257,Organization,False,640,4,6,61,False,"The password hash Argon2, winner of PHC",,0,7,0,30,86,1,2,8,162,2,1,1713,1,1,1,1,0,0,2,3,,,"# Argon2þþ[![Build Status](https://travis-ci.org/P-H-C/phc-winner-argon2.svg?branch=master)](https://travis-ci.org/P-H-C/phc-winner-argon2)þ[![Build status](https://ci.appveyor.com/api/projects/status/8nfwuwq55sgfkele?svg=true)](https://ci.appveyor.com/project/P-H-C/phc-winner-argon2)þ[![codecov.io](https://codecov.io/github/P-H-C/phc-winner-argon2/coverage.svg?branch=master)](https://codecov.io/github/P-H-C/phc-winner-argon2?branch=master)þþThis is the reference C implementation of Argon2, the password-hashingþfunction that won the [Password Hashing Competitionþ(PHC)](https://password-hashing.net).þþArgon2 is a password-hashing function that summarizes the state of theþart in the design of memory-hard functions and can be used to hashþpasswords for credential storage, key derivation, or other applications.þþIt has a simple design aimed at the highest memory filling rate andþeffective use of multiple computing units, while still providing defenseþagainst tradeoff attacks (by exploiting the cache and memory organizationþof the recent processors).þþArgon2 has three variants: Argon2i, Argon2d, and Argon2id. Argon2d is fasterþand uses data-depending memory access, which makes it highly resistantþagainst GPU cracking attacks and suitable for applications with no threatsþfrom side-channel timing attacks (eg. cryptocurrencies). Argon2i insteadþuses data-independent memory access, which is preferred for passwordþhashing and password-based key derivation, but it is slower as it makesþmore passes over the memory to protect from tradeoff attacks. Argon2id is aþhybrid of Argon2i and Argon2d, using a combination of data-depending andþdata-independent memory accesses, which gives some of Argon2i's resistance toþside-channel cache timing attacks and much of Argon2d's resistance to GPUþcracking attacks.þþArgon2i, Argon2d, and Argon2id are parametrized by:þþ* A **time** cost, which defines the amount of computation realized andþ  therefore the execution time, given in number of iterationsþ* A **memory** cost, which defines the memory usage, given in kibibytesþ* A **parallelism** degree, which defines the number of parallel threadsþþThe [Argon2 document](argon2-specs.pdf) gives detailed specs and designþrationale.þþPlease report bugs as issues on this repository.þþ## Usageþþ`make` builds the executable `argon2`, the static library `libargon2.a`,þand the shared library `libargon2.so` (or `libargon2.dylib` on OSX).þMake sure to run `make test` to verify that your build produces validþresults. `make install PREFIX=/usr` installs it to your system.þþ### Command-line utilityþþ`argon2` is a command-line utility to test specific Argon2 instancesþon your system. To show usage instructions, runþ`./argon2 -h` asþ```þUsage:  ./argon2 [-h] salt [-i|-d|-id] [-t iterations] [-m memory] [-p parallelism] [-l hash length] [-e|-r] [-v (10|13)]þ        Password is read from stdinþParameters:þ        salt            The salt to use, at least 8 charactersþ        -i              Use Argon2i (this is the default)þ        -d              Use Argon2d instead of Argon2iþ        -id             Use Argon2id instead of Argon2iþ        -t N            Sets the number of iterations to N (default = 3)þ        -m N            Sets the memory usage of 2^N KiB (default 12)þ        -p N            Sets parallelism to N threads (default 1)þ        -l N            Sets hash output length to N bytes (default 32)þ        -e              Output only encoded hashþ        -r              Output only the raw bytes of the hashþ        -v (10|13)      Argon2 version (defaults to the most recent version, currently 13)þ        -h              Print argon2 usageþ```þFor example, to hash ""password"" using ""somesalt"" as a salt and doing 2þiterations, consuming 64 MiB, using four parallel threads and an output hashþof 24 bytesþ```þ$ echo -n ""password"" | ./argon2 somesalt -t 2 -m 16 -p 4 -l 24þType:           Argon2iþIterations:     2þMemory:         65536 KiBþParallelism:    4þHash:           45d7ac72e76f242b20b77b9bf9bf9d5915894e669a24e6c6þEncoded:        $argon2i$v=19$m=65536,t=2,p=4$c29tZXNhbHQ$RdescudvJCsgt3ub+b+dWRWJTmaaJObGþ0.188 secondsþVerification okþ```þþ### Libraryþþ`libargon2` provides an API to both low-level and high-level functionsþfor using Argon2.þþThe example program below hashes the string ""password"" with Argon2iþusing the high-level API and then using the low-level API. While theþhigh-level API takes the three cost parameters (time, memory, andþparallelism), the password input buffer, the salt input buffer, and theþoutput buffers, the low-level API takes in these and additional parametersþ, as defined in [`include/argon2.h`](include/argon2.h).þþThere are many additional parameters, but we will highlight three of them here.þþ1. The `secret` parameter, which is used for [keyed hashing](þ   https://en.wikipedia.org/wiki/Hash-based_message_authentication_code).þ   This allows a secret key to be input at hashing time (from some externalþ   location) and be folded into the value of the hash. This means that even ifþ   your salts and hashes are compromized, an attacker cannot brute-force to findþ   the password without the key.þþ2. The `ad` parameter, which is used to fold any additional data into the hashþ   value. Functionally, this behaves almost exactly like the `secret` or `salt`þ   parameters; the `ad` parameter is folding into the value of the hash.þ   However, this parameter is used for different data. The `salt` should be aþ   random string stored alongside your password. The `secret` should be a randomþ   key only usable at hashing time. The `ad` is for any other data.þþ3. The `flags` parameter, which determines which memory should be securelyþ   erased. This is useful if you want to securly delete the `pwd` or `secret`þ   fields right after they are used. To do this set `flags` to eitherþ   `ARGON2_FLAG_CLEAR_PASSWORD` or `ARGON2_FLAG_CLEAR_SECRET`. To change howþ   internal memory is cleared, change the global flagþ   `FLAG_clear_internal_memory` (defaults to clearing internal memory).þþHere the time cost `t_cost` is set to 2 iterations, theþmemory cost `m_cost` is set to 2<sup>16</sup> kibibytes (64 mebibytes),þand parallelism is set to 1 (single-thread).þþCompile for example as `gcc test.c libargon2.a -Isrc -o test`, if the programþbelow is named `test.c` and placed in the project's root directory.þþ```cþ#include ""argon2.h""þ#include <stdio.h>þ#include <string.h>þ#include <stdlib.h>þþ#define HASHLEN 32þ#define SALTLEN 16þ#define PWD ""password""þþint main(void)þ{þ    uint8_t hash1[HASHLEN];þ    uint8_t hash2[HASHLEN];þþ    uint8_t salt[SALTLEN];þ    memset( salt, 0x00, SALTLEN );þþ    uint8_t *pwd = (uint8_t *)strdup(PWD);þ    uint32_t pwdlen = strlen((char *)pwd);þþ    uint32_t t_cost = 2;            // 1-pass computationþ    uint32_t m_cost = (1<<16);      // 64 mebibytes memory usageþ    uint32_t parallelism = 1;       // number of threads and lanesþþ    // high-level APIþ    argon2i_hash_raw(t_cost, m_cost, parallelism, pwd, pwdlen, salt, SALTLEN, hash1, HASHLEN);þþ    // low-level APIþ    argon2_context context = {þ        hash2,  /* output array, at least HASHLEN in size */þ        HASHLEN, /* digest length */þ        pwd, /* password array */þ        pwdlen, /* password length */þ        salt,  /* salt array */þ        SALTLEN, /* salt length */þ        NULL, 0, /* optional secret data */þ        NULL, 0, /* optional associated data */þ        t_cost, m_cost, parallelism, parallelism,þ        ARGON2_VERSION_13, /* algorithm version */þ        NULL, NULL, /* custom memory allocation / deallocation functions */þ        /* by default only internal memory is cleared (pwd is not wiped) */þ        ARGON2_DEFAULT_FLAGSþ    };þþ    int rc = argon2i_ctx( &context );þ    if(ARGON2_OK != rc) {þ        printf(""Error: %s\n"", argon2_error_message(rc));þ        exit(1);þ    }þ    free(pwd);þþ    for( int i=0; i<HASHLEN; ++i ) printf( ""%02x"", hash1[i] ); printf( ""\n"" );þ    if (memcmp(hash1, hash2, HASHLEN)) {þ        for( int i=0; i<HASHLEN; ++i ) {þ            printf( ""%02x"", hash2[i] );þ        }þ        printf(""\nfail\n"");þ    }þ    else printf(""ok\n"");þ    return 0;þ}þ```þþTo use Argon2d instead of Argon2i call `argon2d_hash_raw` instead ofþ`argon2i_hash_raw` using the high-level API, and `argon2d` instead ofþ`argon2i` using the low-level API. Similarly for Argon2id, call `argon2id_hash_raw`þand `argon2id`.þþTo produce the crypt-like encoding rather than the raw hash, callþ`argon2i_hash_encoded` for Argon2i, `argon2d_hash_encoded` for Argon2d, andþ`argon2id_hash_encoded` for Argon2idþþSee [`include/argon2.h`](include/argon2.h) for API details.þþ*Note: in this example the salt is set to the all-`0x00` string for theþsake of simplicity, but in your application you should use a random salt.*þþþ### Benchmarksþþ`make bench` creates the executable `bench`, which measures the executionþtime of various Argon2 instances:þþ```þ$ ./benchþArgon2d 1 iterations  1 MiB 1 threads:  5.91 cpb 5.91 McyclesþArgon2i 1 iterations  1 MiB 1 threads:  4.64 cpb 4.64 Mcyclesþ0.0041 secondsþþArgon2d 1 iterations  1 MiB 2 threads:  2.76 cpb 2.76 McyclesþArgon2i 1 iterations  1 MiB 2 threads:  2.87 cpb 2.87 Mcyclesþ0.0038 secondsþþArgon2d 1 iterations  1 MiB 4 threads:  3.25 cpb 3.25 McyclesþArgon2i 1 iterations  1 MiB 4 threads:  3.57 cpb 3.57 Mcyclesþ0.0048 secondsþþ(...)þþArgon2d 1 iterations  4096 MiB 2 threads:  2.15 cpb 8788.08 McyclesþArgon2i 1 iterations  4096 MiB 2 threads:  2.15 cpb 8821.59 Mcyclesþ13.0112 secondsþþArgon2d 1 iterations  4096 MiB 4 threads:  1.79 cpb 7343.72 McyclesþArgon2i 1 iterations  4096 MiB 4 threads:  2.72 cpb 11124.86 Mcyclesþ19.3974 secondsþþ(...)þ```þþ## BindingsþþBindings are available for the following languages (make sure to readþtheir documentation):þþ* [Android (Java/Kotlin)](https://github.com/lambdapioneer/argon2kt) by [@lambdapioneer](https://github.com/lambdapioneer)þ* [Elixir](https://github.com/riverrun/argon2_elixir) by [@riverrun](https://github.com/riverrun)þ* [Erlang](https://github.com/ergenius/eargon2) by [@ergenius](https://github.com/ergenius)þ* [Go](https://github.com/tvdburgt/go-argon2) by [@tvdburgt](https://github.com/tvdburgt)þ* [Haskell](https://hackage.haskell.org/package/argon2) by [@hvr](https://github.com/hvr)þ* [JavaScript (native)](https://github.com/ranisalt/node-argon2), by [@ranisalt](https://github.com/ranisalt)þ* [JavaScript (native)](https://github.com/jdconley/argon2themax), by [@jdconley](https://github.com/jdconley)þ* [JavaScript (ffi)](https://github.com/cjlarose/argon2-ffi), by [@cjlarose](https://github.com/cjlarose)þ* [JavaScript (browser)](https://github.com/antelle/argon2-browser), by [@antelle](https://github.com/antelle)þ* [JVM](https://github.com/phxql/argon2-jvm) by [@phXql](https://github.com/phxql)þ* [JVM (with keyed hashing)](https://github.com/kosprov/jargon2-api) by [@kosprov](https://github.com/kosprov)þ* [Lua (native)](https://github.com/thibaultCha/lua-argon2) by [@thibaultCha](https://github.com/thibaultCha)þ* [Lua (ffi)](https://github.com/thibaultCha/lua-argon2-ffi) by [@thibaultCha](https://github.com/thibaultCha)þ* [OCaml](https://github.com/Khady/ocaml-argon2) by [@Khady](https://github.com/Khady)þ* [Python (native)](https://pypi.python.org/pypi/argon2), by [@flamewow](https://github.com/flamewow)þ* [Python (ffi)](https://pypi.python.org/pypi/argon2_cffi), by [@hynek](https://github.com/hynek)þ* [Python (ffi, with keyed hashing)](https://github.com/thusoy/porridge), by [@thusoy](https://github.com/thusoy)þ* [R](https://cran.r-project.org/package=argon2) by [@wrathematics](https://github.com/wrathematics)þ* [Ruby](https://github.com/technion/ruby-argon2) by [@technion](https://github.com/technion)þ* [Rust](https://github.com/quininer/argon2-rs) by [@quininer](https://github.com/quininer)þ* [Rust](https://docs.rs/argonautica/) by [@bcmyers](https://github.com/bcmyers/)þ* [C#/.NET CoreCLR](https://github.com/kmaragon/Konscious.Security.Cryptography) by [@kmaragon](https://github.com/kmaragon)þ* [Perl](https://github.com/Leont/crypt-argon2) by [@leont](https://github.com/Leont)þ* [mruby](https://github.com/Asmod4n/mruby-argon2) by [@Asmod4n](https://github.com/Asmod4n)þ* [Swift](https://github.com/ImKcat/CatCrypto) by [@ImKcat](https://github.com/ImKcat)þþþ## Test suiteþþThere are two sets of test suites. One is a low level test for the hashþfunction, the other tests the higher level API. Both of these are built andþexecuted by running:þþ`make test`þþ## Intellectual propertyþþExcept for the components listed below, the Argon2 code in thisþrepository is copyright (c) 2015 Daniel Dinu, Dmitry Khovratovich (mainþauthors), Jean-Philippe Aumasson and Samuel Neves, and dual licensed under theþ[CC0 License](https://creativecommons.org/about/cc0) and theþ[Apache 2.0 License](http://www.apache.org/licenses/LICENSE-2.0). For more infoþsee the LICENSE file.þþThe string encoding routines in [`src/encoding.c`](src/encoding.c) areþcopyright (c) 2015 Thomas Pornin, and underþ[CC0 License](https://creativecommons.org/about/cc0).þþThe BLAKE2 code in [`src/blake2/`](src/blake2) is copyright (c) SamuelþNeves, 2013-2015, and underþ[CC0 License](https://creativecommons.org/about/cc0).þþAll licenses are therefore GPL-compatible."
msysgit/msysgit,225005,1866,226,634,Organization,False,1245,27,71,58,False,msysGit has been superseded by Git for Windows 2.x,https://git-for-windows.github.io/,0,9,0,0,209,0,0,0,123,0,0,4695,0,0,0,0,0,0,5,9,,,"# Please note!þþGit for Windows 1.x was retired on August 18th, 2015, superseded by [Git for Windows 2.x](https://git-for-windows.github.io/). The development environment of Git for Windows 2.x is no longer maintained in a monolithic Git repository but rather as the [Git SDK](https://git-for-windows.github.io/#download-sdk), a friendly fork of [MSys2](https://msys2.github.io/) pre-configured to ease the development of Git for Windows.þþ# Build environment for Git for Windows 1.xþþThis is the build environment -- also known as msysGit -- for [Git for Windows](http://msysgit.github.io/).þþThe easiest way is to install it via the [net installer](https://github.com/msysgit/msysgit/releases). This installer will clone our [two](http://github.com/msysgit/msysgit) [repositories](http://github.com/msysgit/git), including all the necessary components to build Git for Windows, and perform an initial build.þþ# The build environmentþþmsysGit brings a few components that are required to build Git:þþ- Bash, a Unix-type command-line shell. Quite a few components of Git itself are still shell scripts. Therefore, Bash is required to execute Git commands (see the output of `cd /git && git ls-files \*.sh` for a full list).þ- the GNU C Compiler. Since we try to rely only on free software (apart from the Operating System, of course), we think it makes more sense to rely on GCC than on Visual Studio express. Also, it makes the maintenance burden lighter, as [upstream Git](http://github.com/gitster/git) also targets mainly GCC.þ- GNU Make.þ- Perl. Still required for a couple of Git components (see the output of `cd /git && git ls-files \*.perl`), most notably `git svn`.þ- Tcl/Tk, a scripting language making it easy to implement cross-platform graphical user interfaces. We need this for `gitk` and `git gui`.þ- [cURL](http://curl.haxx.se), a library implementing HTTP and FTP transport.þ- many more libraries.þ- some Unix programs required by the shell scripts in Git.þþ# The relationship between _msysGit_ and _Git for Windows_þþ[Git for Windows](https://github.com/msysgit/msysgit/releases) is the software package that installs a minimal environment to run Git on Windows. It comes with a Bash (a Unix-type shell), with a Perl interpreter and with the Git executable and its dependencies.þþOn the other hand, msysGit is the software package installing the _build environment_ that can build Git for Windows.  The easiest way is to install it via the [net installer](https://github.com/msysgit/msysgit/releases).þþ# The difference between MSys and MinGWþþThe [MinGW project](http://mingw.org/)'s goal is to provide a way to compile native Windows binaries with no POSIX layer using the GNU C Compiler.þþHowever, at least the Bash needs a POSIX layer (most notably due to the absence of the `fork()` call on Windows). Therefore, MSys (the _minimal system_) is thrown in, offering the minimal system necessary to offer Bash (and Perl) functionality on Windows.þþConsequently, MSys ships with a POSIX layer (based on an old version of Cygwin) that is only used by the Bash and Perl, but not by anything compiled within that environment.þþ# Further informationþþFor more information and documentation, please have a look and enhance our [Wiki](https://github.com/msysgit/msysgit/wiki).þþFor code contributions and discussions, please see our [mailing list](http://groups.google.com/group/msysgit)."
fontforge/fontforge,96368,3163,153,487,Organization,False,19266,27,27,139,False,"Free (libre) font editor for Windows, Mac OS X and GNU+Linux",http://fontforge.github.io/,0,44,3,827,1613,80,103,10,1921,9,108,5948,13,114,122449,164423,0,0,18,19,,,"# FontForge [![Build Status](https://travis-ci.org/fontforge/fontforge.svg?branch=master)](https://travis-ci.org/fontforge/fontforge) [![Build status](https://ci.appveyor.com/api/projects/status/y5x0fd1xj23n9l2o?svg=true)](https://ci.appveyor.com/project/fontforge/fontforge) [![Coverity Scan Build Status](https://scan.coverity.com/projects/792/badge.svg?flat=1)](https://scan.coverity.com/projects/792)þþ![FontForge Logo](http://fontforge.github.io/assets/img/logo-transparent.png)þþFontForge is a free (libre) font editor for Windows, Mac OS X and GNU+Linux. þUse it to create, edit and convert fonts in OpenType, TrueType, UFO, CID-keyed, Multiple Master, and many other formats.þþ[fontforge.org](http://fontforge.org) &mdash; homepageþþ[designwithfontforge.com](http://designwithfontforge.com) &mdash; font creation manualþþ# Getting helpþþThe bug tracker is for _reporting bugs_, not for asking questions. Please direct questions to one of the following:þþ* [Mailing list](https://sourceforge.net/p/fontforge/mailman/fontforge-users/)þ* [Live chat](https://webchat.freenode.net/?channel=#fontforge) &mdash; #fontforge on [Freenode](https://freenode.net/)þþ# Installation & contributingþþ[`INSTALL.md`](INSTALL.md) &mdash; developer instructions to build from sourceþþ[`.travis.yml`](.travis.yml) &mdash; a list of installation dependenciesþþ[`CONTRIBUTING.md`](CONTRIBUTING.md) &mdash; contributing guidelines"
universal-ctags/ctags,15215,3715,103,378,Organization,False,7838,18,0,126,False,A maintained ctags implementation,https://ctags.io,8,17,6,136,663,21,45,33,1732,7,122,5017,8,385,37112,253606,0,0,11,8,,,"# Universal Ctagsþþ[![Build Status](https://travis-ci.org/universal-ctags/ctags.svg?branch=master)](https://travis-ci.org/universal-ctags/ctags)þ[![Coverity Scan Build Status](https://scan.coverity.com/projects/4355/badge.svg)](https://scan.coverity.com/projects/4355)þ[![Coverage Status](https://coveralls.io/repos/universal-ctags/ctags/badge.svg?branch=master&service=github)](https://coveralls.io/github/universal-ctags/ctags?branch=master)þ[![Build status](https://ci.appveyor.com/api/projects/status/6hk2p5lv6jsrd9o7/branch/master?svg=true)](https://ci.appveyor.com/project/universalctags/ctags/branch/master)þ[![RTD build status](https://readthedocs.org/projects/ctags/badge)](https://docs.ctags.io)þ[![CircleCI Build Status](https://circleci.com/gh/universal-ctags/ctags.svg?style=shield&circle-token=2e582261da84ebc6d21725b05381f410bc5de29d)](https://circleci.com/gh/universal-ctags)þþUniversal Ctags generates an index (or tag) file of language objects found in source files for many popular programming languages. This index makes it easy for text editors and other tools to locate the indexed items. Universal Ctags improves on traditional ctags because of its multilanguage support, its ability for the user to define new languages searched by regular expressions, and its ability to generate emacs-style TAGS files.þþuniversal-ctags has the objective of continuing the development fromþwhat existed in the Sourceforge area. Github exuberant-ctagsþrepository was started by Reza Jelveh and was later moved to theþuniversal-ctags organization.þþThe goal of the project is preparing and maintaining common/unified workingþspace where people interested in making ctags better can workþtogether.þþ## Getting PACKCC compiler-compiler ##þþPackcc is a compiler-compiler; it translates .peg grammar file to .cþfile.  packcc was originally written by Arihiro Yoshida. Its sourceþrepository is at sourceforge. It seems that packcc at sourceforge isþnot actively maintained. Some derived repositories are atþgithub. Currently, our choice isþhttps://github.com/enechaev/packcc. It is the most active one in theþderived repositories.þþThe source tree of packcc is grafted at misc/packcc directory.þBuilding packcc and ctags are integrated in the build-scripts ofþUniversal-ctags.þþ## The latest build and package ##þþIf you want to try the latest universal-ctags without building it yourself...þþ### WindowsþDaily builds are available at the [ctags-win32](https://github.com/universal-ctags/ctags-win32) project.þGo to the [releases](https://github.com/universal-ctags/ctags-win32/releases) page to download zip packages.þþ### MacþSee [Homebrew Tap for Universal Ctags](https://github.com/universal-ctags/homebrew-universal-ctags)þþ### SnapþGo to [ctags-snap](https://github.com/universal-ctags/ctags-snap) andþclone the `ctags-snap` repo. Then, follow instructions to build theþsnap package of ctags. Snapcraft will automatically fetch the sourceþcode from GitHub.þþ## How to build and install ##þþTo build with Autotools, see `docs/autotools.rst` for more information.þ(To build on GNU/Linux, Autotools is your choice.)þTo build on Windows, see `docs/windows.rst` for more information.þTo build on OSX, see `docs/osx.rst` for more information.þþ## Manual ##þMan page (ctags.1) is generated only in Autotools based building process.þIn addition rst2man command is needed.þþrst2man is part of the python-docutils package on Ubuntu.þþ## Differences ##þþYou may be interested in how universal-ctags is different fromþexuberant-ctags. The critical and attractive changes are explainedþin docs/\*.rst. The preformatted version is available on line,þhttps://docs.ctags.io/.þþThe most significant incompatible changes:þþ* Universal-ctags doesn't loadþ`~/.ctags` and `./.ctags` at starting up time. Instead, it loadsþ`~/.ctags.d/*.ctags` and `./.ctags.d/*.ctags`. See the above webþsite and man pagesþ(man/ctags.1.rst.in and man/ctags-incompatibilities.7.in in theþsource tree).þþ* Universal-ctags is more strict about characters that can beþ  used in kind letters and kind names than Exuberant-ctags.þþ  - The letter must be an alphabetical character (`[a-zA-EG-Z]`).þ    `F` is reserved for `file` kind.þþ  - The first character of the name must be alphabetic, andþ    the rest characters must be alphanumeric (`[a-zA-Z][a-zA-Z0-9]*`).þþ  See the web site and man pages. The detailed background is explainedþ  in [#1737](https://github.com/universal-ctags/ctags/pull/1737).þþ  If you want to reuse your .ctags written for Exuberant-ctags,þ  you must review kind letters and names defined with `--regex-<LANG>=...`þ  options. When updating the definitions, using `--kind-<LANG>` optionþ  is appreciated.þþPull-requests are welcome!"
alibaba/tsar,2457,2138,280,699,Organization,False,404,2,0,22,False,Taobao System Activity Reporter,,0,6,2,10,47,2,1,0,47,0,3,2707,3,3,115,92,0,0,318,151,,,"Introductionþ------------þTsar (Taobao System Activity Reporter) is a monitoring tool, which can be used to gather and summarize system information, e.g. CPU, load, IO, and application information, e.g. nginx, HAProxy, Squid, etc. The results can be stored at local disk or sent to Nagios.þþTsar can be easily extended by writing modules, which makes it a powerful and versatile reporting tool.þþModule introduction: [info](https://github.com/alibaba/tsar/blob/master/info.md)þþInstallationþ-------------þTsar is available on GitHub, you can clone and install it as follows:þþ    $ git clone https://github.com/alibaba/tsar.gitþ    $ cd tsarþ    $ makeþ    # make installþþOr you can download the zip file and install it:þþ    $ wget -O tsar.zip https://github.com/alibaba/tsar/archive/master.zip --no-check-certificateþ    $ unzip tsar.zipþ    $ cd tsarþ    $ makeþ    # make installþþAfter installation, you may see these files:þþ* `/etc/tsar/tsar.conf`, which is tsar's main configuration file;þ* `/etc/cron.d/tsar`, is used to run tsar to collect information every minute;þ* `/etc/logrotate.d/tsar` will rotate tsar's log files every month;þ* `/usr/local/tsar/modules` is the directory where all module libraries (*.so) are located;þþConfigurationþ-------------þThere is no output displayed after installation by default. Just run `tsar -l` to see if the real-time monitoring works, for instance:þþ    [kongjian@tsar]$ tsar -l -i 1þ    Time              ---cpu-- ---mem-- ---tcp-- -----traffic---- --xvda-- -xvda1-- -xvda2-- -xvda3-- -xvda4-- -xvda5--  ---load-þ    Time                util     util   retran    pktin  pktout     util     util     util     util     util     util     load1þ    11/04/13-14:09:10   0.20    11.57     0.00     9.00    2.00     0.00     0.00     0.00     0.00     0.00     0.00      0.00þ    11/04/13-14:09:11   0.20    11.57     0.00     4.00    2.00     0.00     0.00     0.00     0.00     0.00     0.00      0.00þþUsually, we configure Tsar by simply editing `/etc/tsar/tsar.conf`:þþ* To add a module, add a line like `mod_<yourmodname> on`þ* To enable or disable a module, use `mod_<yourmodname> on/off`þ* To specify parameters for a module, use `mod_<yourmodname> on parameter`þ* `output_stdio_mod` is to set modules output to standard I/Oþ* `output_file_path` is to set history data file, (you should modify the logrotate script `/etc/logrotate.d/tsar` too)þ* `output_interface` specifies tsar data output destination, which by default is a local file. See the Advanced section for more information.þþUsageþ------þ* null          :see default mods history data, `tsar`þ* --modname     :specify module to show, `tsar --cpu`þ* -L/--list     :list available module, `tsar -L`þ* -l/--live     :show real-time info, `tsar -l --cpu`þ* -i/--interval :set interval for report, `tsar -i 1 --cpu`þ* -s/--spec     :specify module detail field, `tsar --cpu -s sys,util`þ* -D/--detail   :do not conver data to K/M/G, `tsar --mem -D`þ* -m/--merge    :merge multiply item to one, `tsar --io -m`þ* -I/--item     :show spec item data, `tsar --io -I sda`þ* -d/--date     :specify data, YYYYMMDD, or n means n days agoþ* -C/--check    :show the last collect dataþ* -h/--help     :show help, `tsar -h`þþAdvancedþ--------þ* Output to NagiosþþTo turn it on, just set output type `output_interface file,nagios` in the main configuration file.þþYou should also specify Nagios' IP address, port, and sending interval, e.g.:þþ    ####The IP address or the hostname running the NSCA daemonþ    server_addr nagios.server.comþ    ####The port on which the daemon is listening - by default it is 5667þ    server_port 8086þ    ####The cycle (interval) of sending alerts to Nagiosþ    cycle_time 300þþAs tsar uses Nagios' passive mode, so you should specify the nsca binary and its configuration file, e.g.:þþ    ####nsca client programþ    send_nsca_cmd /usr/bin/send_nscaþ    send_nsca_conf /home/a/conf/amon/send_nsca.confþþThen specify the module and fields to be checked. There are 4 threshold levels.þþ    ####tsar mod alert config fileþ    ####threshold servicename.key;w-min;w-max;c-min;cmax;þ    threshold cpu.util;50;60;70;80;þþ* Output to MySQLþþTo use this feature, just add output type `output_interface file,db` in tsar's configuration file.þþThen specify which module(s) will be enabled:þþ    output_db_mod mod_cpu,mod_mem,mod_traffic,mod_load,mod_tcp,mod_udpmod_ioþþNote that you should set the IP address (or hostname) and port where tsar2db listens, e.g.:þþ    output_db_addr console2:56677þþTsar2db receives sql data and flush it to MySQL. You can find more information about tsar2db at https://github.com/alibaba/tsar2db.þþþModule developmentþ------------------þTsar is easily extended. Whenever you want information that is not collected by tsar yet, you can write a module with `C` or `Lua`.þþC Moduleþ--------þFirst, install the tsardevel tool (`make tsardevel` will do this for you):þþThen run `tsardevel <yourmodname>`, and you will get a directory named yourmodname, e.g.:þþ````bashþ[kongjian@tsar]$ tsardevel testþbuild:makeþinstall:make installþuninstall:make uninstallþþ[kongjian@tsar]$ ls testþMakefile  mod_test.c  mod_test.confþ````þþYou can modify the read_test_stats() and set_test_record() functions in mod_test.c as you need.þThen run `make;make install` to install your module and run `tsar --yourmodname` to see the output.þþLua Moduleþ----------þFirst, install the tsarluadevel tool (`make tsarluadevel` will do this for you):þþThen run `tsarluadevel <yourmodname>`, and you will get a directory named yourmodname, e.g.:þþ````bashþ[kongjian@tsar]$ tsarluadevel testþinstall:make installþuninstall:make uninstallþtest:tsar --list or tsar --lua_test --live -i 1þþ[kongjian@tsar]$ ls testþMakefile  mod_lua_test.conf  mod_lua_test.luaþ````þþYou can modify the register()、read() and set() functions in mod_lua_test.lua as you need.þThen run `make install` to install your module and run `tsar --lua_yourmodname` to see the output.þþMoreþ----þHomepage http://tsar.taobao.orgþþAny question, please feel free to contact me by kongjian@taobao.com"
ohler55/oj,3802,2574,36,181,User,False,1247,5,186,73,False,Optimized JSON,http://www.ohler.com/oj,8,0,0,13,434,7,16,1,154,1,6,3015,4,21,249,133,10841,844,30,,151,,"# [![{}j](http://www.ohler.com/dev/images/oj_comet_64.svg)](http://www.ohler.com/oj) gemþþ[![Build Status](https://img.shields.io/travis/ohler55/oj/master.svg?logo=travis)](http://travis-ci.org/ohler55/oj?branch=master) [![AppVeyor](https://img.shields.io/appveyor/ci/ohler55/oj/master.svg?logo=appveyor)](https://ci.appveyor.com/project/ohler55/oj) ![Gem](https://img.shields.io/gem/v/oj.svg) ![Gem](https://img.shields.io/gem/dt/oj.svg) [![SemVer compatibility](https://api.dependabot.com/badges/compatibility_score?dependency-name=oj&package-manager=bundler&version-scheme=semver)](https://dependabot.com/compatibility-score.html?dependency-name=oj&package-manager=bundler&version-scheme=semver) [![TideLift](https://tidelift.com/badges/github/ohler55/oj)](https://tidelift.com/subscription/pkg/rubygems-oj?utm_source=rubygems-oj&utm_medium=referral&utm_campaign=readme)þþA *fast* JSON parser and Object marshaller as a Ruby gem.þþVersion 3.0 is out! 3.0 provides better json gem and Rails compatibility. Itþalso provides additional optimization options.þþ## Usingþþ```rubyþrequire 'oj'þþh = { 'one' => 1, 'array' => [ true, false ] }þjson = Oj.dump(h)þþ# json =þ# {þ#   ""one"":1,þ#   ""array"":[þ#     true,þ#     falseþ#   ]þ# }þþh2 = Oj.load(json)þputs ""Same? #{h == h2}""þ# trueþ```þþ## Installationþ```þgem install ojþ```þþor in Bundler:þþ```þgem 'oj'þ```þþ## Supportþþ[Get supported Oj with a Tidelift Subscription.](https://tidelift.com/subscription/pkg/rubygems-oj?utm_source=rubygems-oj&utm_medium=referral&utm_campaign=readme) Security updates are [supported](https://tidelift.com/security).þþ## Further ReadingþþFor more details on options, modes, advanced features, and more follow theseþlinks.þþ - [{file:Options.md}](pages/Options.md) for parse and dump options.þ - [{file:Modes.md}](pages/Modes.md) for details on modes for strict JSON compliance, mimicing the JSON gem, and mimicing Rails and ActiveSupport behavior.þ - [{file:JsonGem.md}](pages/JsonGem.md) includes more details on json gem compatibility and use.þ - [{file:Rails.md}](pages/Rails.md) includes more details on Rails and ActiveSupport compatibility and use.þ - [{file:Custom.md}](pages/Custom.md) includes more details on Custom mode.þ - [{file:Encoding.md}](pages/Encoding.md) describes the :object encoding format.þ - [{file:Compatibility.md}](pages/Compatibility.md) lists current compatibility with Rubys and Rails.þ - [{file:Advanced.md}](pages/Advanced.md) for fast parser and marshalling features.þ - [{file:Security.md}](pages/Security.md) for security considerations.þþ## ReleasesþþSee [{file:CHANGELOG.md}](CHANGELOG.md)þþ## Linksþþ- *Documentation*: http://www.ohler.com/oj/doc, http://rubydoc.info/gems/ojþþ- *GitHub* *repo*: https://github.com/ohler55/ojþþ- *RubyGems* *repo*: https://rubygems.org/gems/ojþþFollow [@peterohler on Twitter](http://twitter.com/peterohler) for announcements and news about the Oj gem.þþ#### Performance Comparisonsþþ - [Oj Strict Mode Performance](http://www.ohler.com/dev/oj_misc/performance_strict.html) compares Oj strict mode parser performance to other JSON parsers.þþ - [Oj Compat Mode Performance](http://www.ohler.com/dev/oj_misc/performance_compat.html) compares Oj compat mode parser performance to other JSON parsers.þþ - [Oj Object Mode Performance](http://www.ohler.com/dev/oj_misc/performance_object.html) compares Oj object mode parser performance to other marshallers.þþ - [Oj Callback Performance](http://www.ohler.com/dev/oj_misc/performance_callback.html) compares Oj callback parser performance to other JSON parsers.þþ#### Links of Interestþþ - *Fast XML parser and marshaller on RubyGems*: https://rubygems.org/gems/oxþþ - *Fast XML parser and marshaller on GitHub*: https://github.com/ohler55/oxþþ - [Need for Speed](http://www.ohler.com/dev/need_for_speed/need_for_speed.html) for an overview of how Oj::Doc was designed.þþ - *OjC, a C JSON parser*: https://www.ohler.com/ojc also at https://github.com/ohler55/ojcþþ - *Agoo, a high performance Ruby web server supporting GraphQL on GitHub*: https://github.com/ohler55/agooþþ - *Agoo-C, a high performance C web server supporting GraphQL on GitHub*: https://github.com/ohler55/agoo-cþþ#### Contributingþþ+ Provide a Pull Request off the `develop` branch.þ+ Report a bugþ+ Suggest an idea"
madler/zlib,3753,2462,161,1378,User,False,419,2,72,17,False,A massively spiffy yet delicately unobtrusive compression library.,http://zlib.net/,0,0,0,96,122,21,6,147,137,16,2,3631,0,0,0,0,0,0,8,,350,,
libusb/libusb,4599,2588,262,1178,Organization,False,1522,1,340,144,False,A cross-platform library to access USB devices,https://libusb.info,3,20,2,90,406,15,33,10,234,7,23,4303,15,102,11934,23474,0,0,4,4,,,"# libusbþþ[![Build Status](https://travis-ci.org/libusb/libusb.svg?branch=master)](https://travis-ci.org/libusb/libusb)þ[![Build Status](https://ci.appveyor.com/api/projects/status/xvrfam94jii4a6lw?svg=true)](https://ci.appveyor.com/project/LudovicRousseau/libusb)þ[![Coverity Scan Build Status](https://scan.coverity.com/projects/2180/badge.svg)](https://scan.coverity.com/projects/libusb-libusb)þþlibusb is a library for USB device access from Linux, macOS,þWindows, OpenBSD/NetBSD, Haiku and Solaris userspace.þIt is written in C (Haiku backend in C++) and licensed under the GNUþLesser General Public License version 2.1 or, at your option, any laterþversion (see [COPYING](COPYING)).þþlibusb is abstracted internally in such a way that it can hopefullyþbe ported to other operating systems. Please see the [PORTING](PORTING)þfile for more information.þþlibusb homepage:þhttp://libusb.info/þþDevelopers will wish to consult the API documentation:þhttp://api.libusb.infoþþUse the mailing list for questions, comments, etc:þhttp://mailing-list.libusb.infoþþ- Hans de Goede <hdegoede@redhat.com>þ- Xiaofan Chen <xiaofanc@gmail.com>þ- Ludovic Rousseau <ludovic.rousseau@gmail.com>þ- Nathan Hjelm <hjelmn@cs.unm.edu>þ- Chris Dickens <christopher.a.dickens@gmail.com>þþ(Please use the mailing list rather than mailing developers directly)"
lloyd/yajl,2095,1945,94,390,User,False,275,9,20,20,False,A fast streaming JSON parsing library in C.,http://lloyd.github.com/yajl,0,0,0,72,43,2,0,76,37,2,0,4198,0,0,0,0,0,0,120,,568,,
IoLanguage/io,26133,2171,145,268,Organization,False,2633,1,10,121,False,"Io programming language. Inspired by Self, Smalltalk and LISP.",http://iolanguage.org,0,0,0,54,189,0,0,3,184,0,0,4835,1,1,41,0,0,0,82,1,,,"# The Io Languageþþ_Note: This document is intended to be used as a reference for setting up and configuring Io. For a guide on how to use the language itself, please visit the website at <http://iolanguage.org/guide/guide.html>._þþ# Table of Contentsþþ* [Table of Contents](#table-of-contents)þ* [What is Io?](#what-is-io)þ * [Example Code](#example-code)þ * [Quick Links](#quick-links)þ* [Installing](#installing)þ * [From a Package Manager](#from-a-package-manager)þ * [From Source](#from-source)þ  * [Linux Build Instructions](#linux-build-instructions)þ   * [Note About Building Eerie](#note-about-building-eerie)þ  * [OS X Build Instructions](#os-x-build-instructions)þ  * [Windows Build Instructions](#windows-build-instructions)þ   * [Building with MSVC](#building-with-msvc)þ   * [Building with MinGW](#building-with-mingw)þ   * [Building with MinGW-W64](#building-with-mingw-w64)þ   * [Building with Cygwin](#building-with-cygwin)þ* [Running Tests](#running-tests)þ* [Installing Addons](#installing-addons)þþWhat is Io?þ=====þþIo is a dynamic prototype-based programming language in the same realm as Smalltalk and Self. It revolves around the idea of message passing from object to object.þþFor further information, the programming guide and reference manual can be found in the docs folder.þþþExample Codeþ---þBasic Mathþþ```þIo> 1 + 1þ==> 2þþIo> 2 sqrtþ==> 1.4142135623730951þ```þþListsþþ```þIo> d := List clone append(30, 10, 5, 20)þ==> list(30, 10, 5, 20)þþIo> d := d sortþ==> list(5, 10, 20, 30)þþIo> d select (>10)þ==> list(20, 30)þ```þþObjectsþþ```þIo> Contact := Object cloneþ==>  Contact_0x7fbc3bc8a6d0:þ  type = ""Contact""þþIo> Contact name ::= nilþ==> nilþþIo> Contact address ::= nilþ==> nilþþIo> Contact city ::= nilþ==> nilþþIo> holmes := Contact clone setName(""Holmes"") setAddress(""221B Baker St"") setCity(""London"")þ==>  Contact_0x7fbc3be2b470:þ  address          = ""221B Baker St""þ  city             = ""London""þ  name             = ""Holmes""þþIo> Contact fullAddress := method(list(name, address, city) join(""\n""))þ==> method(þ    list(name, address, city) join(""\n"")þ)þþIo> holmes fullAddressþ==> Holmesþ221B Baker StþLondonþ```þþþþþQuick Linksþ---þ* The Wikipedia page for Io has a good overview and shows a few interesting examples of the language: <https://en.wikipedia.org/wiki/Io_(programming_language)>.þ* The entry on the c2 wiki has good discussion about the merits of the language: <http://wiki.c2.com/?IoLanguage>.þþþInstallingþ==========þþFrom a Package Managerþ---þþIo is currently only packaged for OS X. To install it, open a terminal and type:þþ```þbrew install ioþ```þþNote that this package may not be as updated as the version from the source repository.þþFrom Sourceþ---þþ### Linux Build InstructionsþþFirst, make sure that this repo and all of its submodules have been cloned to your computer by running `git clone` with the `--recursive` flag:þþ```þgit clone --recursive https://github.com/IoLanguage/io.gitþ```þþIo uses the [CMake build system](https://cmake.org/) and supports all of the normal flags and features provided by CMake. To prepare the project for building, run the following commands:þþ```þcd io/           # To get into the cloned folderþmkdir build      # To contain the CMake dataþcd build/þcmake ..         # This populates the build folder with a Makefile and all of the related things necessary to begin buildingþ```þþIn a production environment, pass the flag `-DCMAKE_BUILD_TYPE=release` to the `cmake` command to ensure that the C compiler does the proper optimizations. Without this flag, Io is built in debug mode without standard C optimizations.þþTo install to a different folder than `/usr/local/bin/`, pass the flag `-DCMAKE_INSTALL_PREFIX=/path/to/your/folder/` to the `cmake` command.þþTo build without Eerie, the Io package manager, pass the flag `-DWITHOUT_EERIE=1` to the `cmake` command.þþOnce CMake has finished preparing the build environment, ensure you are inside the build folder, and run:þþ```þmakeþsudo make installþ```þþThis should build and install the Io language and Eerie, the Io package manager. Io can then be run with the `io` command and Eerie can be run with the `eerie` command.þþ#### Note About Building EerieþþRunning `eerie` after installing with `sudo make install` may shoot back an error such as this one:þþ```þException: unable to open file path '/home/<user>/.eerie/config.json': Permission deniedþ---------þopenForUpdating                     Eerie.io 77þObject Eerie                         eerie 3þCLI doFile                           Z_CLI.io 140þCLI run                              IoState_runCLI() 1þ```þþIf this occurs, this is because the `~/.eerie/` folder isn't accessible due to your user permissions. To fix this, go to your home folder and run:þþ```þsudo chown -R <your username>:<your username> .eerie/þ```þþþ### OS X Build InstructionsþþSee the [Linux build instructions](#linux-build-instructions).þþ### Windows Build InstructionsþþFor all the different methods explained here, some of the addons won't compile as they depend on libraries not provided by Io.þþFor methods A and B you must download and install CMake (at least v2.8) from here: <http://www.cmake.org/cmake/resources/software.html>þþFor method C you must install the CMake Cygwin package (at least v2.8) using the Cygwin package installer.þþFor the `make install` command, if you are on Windows 7/Vista you will need to run your command prompts as Administrator: right-click on the command prompt launcher->""Run as administrator"" or something similar)þþYou will also need to add `<install_drive>:\<install_directory>\bin` and `<install_drive>:\<install_directory>\lib` to your `PATH` environment variable.þþ#### Building with MSVCþþ1. Install Microsoft Visual C++ 2008 Express (should work with other versions).þ2. Install Microsoft Windows SDK 7.0 (or newer).þ3. Install CMake (v2.8 at least)þ4. Run ""Visual Studio 2008 Command Prompt"" from the ""Microsoft Visual Studio 2008"" start menu.þ5. `cd` to `<install_drive>:\Microsoft SDKs\Windows\v7.0\Setup` then run: `WindowsSdkVer.exe -version:v7.0`þ6. Close the command prompt window and run step 4 againþ7. Ensure CMake bin path is in the `PATH` environment variable (eg: `echo %PATH%` and see that the folder is there) if not you will have to add it to your `PATH`.þ8. `cd` to your Io root folderþ9. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`þ10. a) `cmake ..`þþ orþþ b) `cmake -DCMAKE_INSTALL_PREFIX=<install_drive>:\<install_directory> ..` (eg: `cmake -DCMAKE_INSTALL_PREFIX=C:\Io ..`)þ11. `nmake`þ12. `nmake install`þþþ#### Building with MinGWþþFor automatic MinGW install: <http://sourceforge.net/projects/mingw/files/Automated%20MinGW%20Installer>þþFor non-automatic MinGW install and detailed instructions refer to: <http://www.mingw.org/wiki/InstallationHOWTOforMinGW>þþ1. `cd` to your Io root folderþ2. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`þ3. a) `cmake -G""MSYS Makefiles"" ..`þþ orþþ b) `cmake -G""MSYS Makefiles"" -DCMAKE_INSTALL_PREFIX=<install_drive>:/<install_directory> ..` (eg: `cmake -G""MSYS Makefiles"" -DCMAKE_INSTALL_PREFIX=C:/Io ..`)þ4. `make`þ5. `make install`þþ#### Building with MinGW-W64þþ1. `cd` to your Io root folderþ2. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`þ3. a) `cmake -G""MinGW Makefiles"" ..`þþ orþþ b) `cmake -G""MinGW Makefiles"" -DCMAKE_INSTALL_PREFIX=<install_drive>:/<install_directory> ..` (eg: `cmake -G""MinGW Makefiles"" -DCMAKE_INSTALL_PREFIX=C:/Io ..`)þ4. `mingw32-make install`þþþ#### Building with CygwinþþInstall Cygwin from: <http://www.cygwin.com/>þþ1. `cd` to your Io root folderþ2. We want to do an out-of-source build, so: `mkdir buildroot` and `cd buildroot`þ3. a) `cmake ..`þþ orþþ b) `cmake -DCMAKE_INSTALL_PREFIX=<install_drive>:/<install_directory> ..` (eg: `cmake -DCMAKE_INSTALL_PREFIX=C:/Io ..`)þ4. `make`þ5. `make install`þþNote: If you also have CMake 2.8 for Windows installed (apart from CMake for Cygwin) check your `PATH` environment variable so you won't be running CMake for Windows instead of Cygwin version.þþþRunning Testsþ===þþYou should be inside your out-of-source build dir. The vm tests can be run with the command:þþ io ../libs/iovm/tests/correctness/run.ioþþInstalling Addonsþ===þþMany of the common features provided by the Io language aren't prepackaged in the Io core. Instead, these features are contained in addons that get loaded when launching the Io VM. In the past, these addons were automatically installed by the build process, but now they must be installed through [Eerie](https://github.com/IoLanguage/eerie), the Io package manager.þþMost of these addons are housed under the IoLanguage group on GitHub: https://github.com/IoLanguage.þþTo install an addon, ensure both Io and Eerie are installed correctly, then run:þþ```þeerie install <link to the git repository>þ```þþFor example, to build and install the `Range` addon, run the command:þþ```þeerie install https://github.com/IoLanguage/Range.gitþ```þþTo ensure that an addon installed correctly, pull up an Io interpreter and type the name of the object provided by the addon. It should load dynamically and automatically into the interpreter session, populating a slot in `Lobby Protos Addons`."
irssi/irssi,7581,2176,108,291,Organization,False,6144,5,70,91,False,The client of the future,https://irssi.org,4,30,1,119,348,13,8,14,709,7,25,5780,4,30,864,397,0,0,4,14,,,"# [Irssi](https://irssi.org/)þþ[![Build Status](https://travis-ci.org/irssi/irssi.svg?branch=master)](https://travis-ci.org/irssi/irssi)þþIrssi is a modular chat client that is most commonly known for itsþtext mode user interface, but 80% of the code isn't text modeþspecific. Irssi comes with IRC support built in, and there areþthird party [ICB](https://github.com/jperkin/irssi-icb),þ[SILC](http://www.silcnet.org/),þ[XMPP](http://cybione.org/~irssi-xmpp/) (Jabber),þ[PSYC](http://about.psyc.eu/Irssyc) andþ[Quassel](https://github.com/phhusson/quassel-irssi) protocol modulesþavailable.þþ![irssi](https://user-images.githubusercontent.com/5665186/32180643-cf127f60-bd92-11e7-8aa2-882313ce1d8e.png)þþ## [Download information](https://irssi.org/download/)þþ#### Development source installationþþ[Ninja](https://ninja-build.org/) 1.5 and [Meson](https://mesonbuild.com/) 0.49þþ```þgit clone https://github.com/irssi/irssiþcd irssiþmeson Buildþninja -C Build && sudo ninja -C Build installþ```þþ#### Release source installationþþ* Download [release](https://github.com/irssi/irssi/releases)þ* [Verify](https://irssi.org/download/#release-sources) signatureþ```þtar xJf irssi-*.tar.xzþcd irssi-*þ./configureþmake && sudo make installþ```þþ### Requirementsþþ- [glib-2.28](https://wiki.gnome.org/Projects/GLib) or greaterþ- [openssl](https://www.openssl.org/)þ- [perl-5.6](https://www.perl.org/) or greater (for perl support)þ- terminfo or ncurses (for text frontend)þþ#### See the [INSTALL](INSTALL) file for detailsþþ## [Documentation](https://irssi.org/documentation/)þþ* [Frequently Asked Questions](https://irssi.org/documentation/faq)þ* [Startup How-To](https://irssi.org/documentation/startup)þ* Check the built-in `/HELP`, it has all the details on command syntaxþþ## [Themes](https://irssi-import.github.io/themes/)þþ## [Scripts](https://scripts.irssi.org/)þþ## [Modules](https://irssi.org/modules/)þþ## [Security information](https://irssi.org/security/)þþPlease report security issues to staff@irssi.org. Thanks!þþ## [Bugs](https://github.com/irssi/irssi/issues) / Suggestions / [Contributing](https://irssi.org/development/)þþCheck the GitHub issues if it is already listed in there; if not, openþan issue on GitHub or send a mail to [staff@irssi.org](mailto:staff@irssi.org).þþIrssi is always looking for developers. Feel free to submit patches throughþGitHub pull requests.þþYou can also contact the Irssi developers inþ[#irssi](https://irssi.org/support/irc/) on freenode."
raspberrypi/userland,35818,1650,256,942,Organization,False,799,3,0,85,False,Source code for ARM side libraries for interfacing to Raspberry Pi GPU.,,0,9,0,55,273,13,8,7,299,2,10,2791,8,36,1696,1358,0,0,25,1,,,"This repository contains the source code for the ARM side libraries used on Raspberry Pi.þThese typically are installed in /opt/vc/lib and includes source for the ARM side code to interface to:þEGL, mmal, GLESv2, vcos, openmaxil, vchiq_arm, bcm_host, WFC, OpenVG.þþUse buildme to build. It requires cmake to be installed and an ARM cross compiler. For 32-bit cross compilation it is set up to use this one:þhttps://github.com/raspberrypi/tools/tree/master/arm-bcm2708/gcc-linaro-arm-linux-gnueabihf-raspbianþþWhilst 64-bit userspace is not officially supported, some of the libraries will work for it. To cross compile, install gcc-aarch64-linux-gnu and g++-aarch64-linux-gnu first. For both native and cross compiles, add the option ```--aarch64``` to the buildme command.þþNote that this repository does not contain the source for the edidparser and vcdbg binaries due to licensing restrictions."
Qihoo360/phptrace,1704,1620,158,403,Organization,False,502,1,7,10,False,A tracing and troubleshooting tool for PHP scripts.,,0,11,0,26,63,3,0,0,17,0,0,2021,0,0,0,0,0,0,58,7,,,"# phptraceþþ> We have a new trace tool **[Molten](https://github.com/chuan-yun/Molten)**,þ> It's an OpenTracing supported tracer, for Distributed Tracing System.þþ> 我们开发了新的Trace工具 **[Molten](https://github.com/chuan-yun/Molten)**，þ> 它支持OpenTracing，用于分布式追踪系统。þþþ[![Build Status](https://travis-ci.org/Qihoo360/phptrace.svg)](https://travis-ci.org/Qihoo360/phptrace)þþ> Readme in [Chinese 中文](https://github.com/Qihoo360/phptrace/blob/master/README_ZH.md)þþphptrace is a low-overhead tracing tool for PHP.þþIt can trace all PHP executing, function calls, request information duringþrun-time. And provides features like Filter, Statistics, Current Status and soþon.þþIt is very useful to locate blocking, heavy-load problems and debug in allþenvironments, especially in production environments.þþFeatures:þ* low-overhead, when extension loaded and trace is offþ* stable, running on [Qihoo 360](http://www.360safe.com/) and tested on mainstream frameworksþ* ease of use, view PHP run-time status without extension installationþþMisc:þ- [PECL Download](https://pecl.php.net/package/trace)þþ> Newsþ> We have build another interesting project [pika](https://github.com/Qihoo360/pika).þ> It's a NoSQL compatible with Redis protocol with huge storage space.þþþ## Install from sourceþþ1. Extracting tarballþ    ```þ    tar -xf phptrace-{version}.tar.gzþ    cd phptrace-{version}/extensionþ    ```þþ2. Buildþþ    PHP Extensionþ    ```þ    {php_bin_dir}/phpizeþ    ./configure --with-php-config={php_bin_dir}/php-configþ    makeþ    ```þþ    CLI Binaryþ    ```þ    make cliþ    ```þþ3. Install & Configureþþ    Install PHP Extension, CLI Binary into PHP pathþ    ```þ    make install-allþ    ```þþ    Edit `php.ini`, add the following line. A reload is needed if PHP runningþ    on php-fpm mode.þ    ```þ    extension=trace.soþ    ```þþ4. Verifyþ    ```þ    php -r 'for ($i = 0; $i < 20; $i++) usleep(50000);' &þ    phptrace -p $!þ    ```þþ    You should see something below if it works fineþ    ```þ    process attachedþ    [pid 3600]> cli php -þ    [pid 3600]> {main}() called at [Command line code:1]þ    [pid 3600]    > usleep(50000) called at [Command line code:1]þ    [pid 3600]    < usleep(50000) = NULL called at [Command line code:1] ~ 0.051s 0.051sþ    [pid 3600]    > usleep(50000) called at [Command line code:1]þ    [pid 3600]    < usleep(50000) = NULL called at [Command line code:1] ~ 0.051s 0.051sþ    [pid 3600]    > usleep(50000) called at [Command line code:1]þ    [pid 3600]    < usleep(50000) = NULL called at [Command line code:1] ~ 0.051s 0.051sþ    [pid 3600]    > usleep(50000) called at [Command line code:1]þ    ...þ    ```þþþ## UsageþþJust try `php example.php`.þþ### Command line optionsþþ* trace     trace running php process(default)þ* status    display php process statusþ* version   show versionþ* -p        specify php process id ('all' to trace all processes)þ* -h        show helperþ* -v        same as versionþ* -f        filter data by type(url,function,class) and contentþ* -l        limit output countþ* --ptrace  in status mode fetch data using ptraceþþ### Trace executingþþ```þ$ phptrace -p 3600þþ[pid 3600]    > Me->run() called at [example.php:57]þ[pid 3600]        > Me->say(""good night"") called at [example.php:33]þ[pid 3600]        < Me->say(""good night"") = NULL called at [example.php:33] ~ 0.000s 0.000sþ[pid 3600]        > Me->sleep() called at [example.php:34]þ[pid 3600]            > Me->say(""sleeping..."") called at [example.php:27]þ[pid 3600]            < Me->say(""sleeping..."") = NULL called at [example.php:27] ~ 0.000s 0.000sþ[pid 3600]            > sleep(2) called at [example.php:28]þ[pid 3600]            < sleep(2) = 0 called at [example.php:28] ~ 2.000s 2.000sþ[pid 3600]        < Me->sleep() = NULL called at [example.php:34] ~ 2.000s 0.000sþ[pid 3600]        > Me->say(""wake up"") called at [example.php:35]þ[pid 3600]        < Me->say(""wake up"") = NULL called at [example.php:35] ~ 0.000s 0.000sþ[pid 3600]    < Me->run() = NULL called at [example.php:57] ~ 2.000s 0.000sþ```þþ### Print current statusþþ```þ$ phptrace status -p 3600þþ------------------------------- Status --------------------------------þPHP Version:       7.0.16þSAPI:              cliþscript:            example.phpþelapse:            26.958sþ------------------------------ Arguments ------------------------------þ$0þ------------------------------ Backtrace ------------------------------þ#0  fgets() called at [example.php:53]þ#1  {main}() called at [example.php:53]þ```þþ### Tracing with filter of url/class/functionþþ```þ$ phptrace -p 3600 -f type=class,content=Meþþ[pid 3600]> Me->run() called at [example.php:57]þ[pid 3600]> Me->say(""good night"") called at [example.php:33]þ[pid 3600]< Me->say(""good night"") = NULL called at [example.php:33] ~ 0.000s 0.000sþ[pid 3600]> Me->sleep() called at [example.php:34]þ[pid 3600]> Me->say(""sleeping..."") called at [example.php:27]þ[pid 3600]< Me->say(""sleeping..."") = NULL called at [example.php:27] ~ 0.000s 0.000sþ[pid 3600]< Me->sleep() = NULL called at [example.php:34] ~ 2.000s 2.000sþ[pid 3600]> Me->say(""wake up"") called at [example.php:35]þ[pid 3600]< Me->say(""wake up"") = NULL called at [example.php:35] ~ 0.000s 0.000sþ[pid 3600]< Me->run() = NULL called at [example.php:57] ~ 2.001s 0.000sþ```þþ### Limit frame/URL display timesþþ```þ$ phptrace -p 3600 -l 2þþ[pid 3600]    > Me->run() called at [example.php:57]þ[pid 3600]        > Me->say(""good night"") called at [example.php:33]þ[pid 3600]        < Me->say(""good night"") = NULL called at [example.php:33] ~ 0.000s 0.000sþ[pid 3600]        > Me->sleep() called at [example.php:34]þ[pid 3600]            > Me->say(""sleeping..."") called at [example.php:27]þ[pid 3600]            < Me->say(""sleeping..."") = NULL called at [example.php:27] ~ 0.000s 0.000sþ```þþþ## ContributingþþWelcome developers who willing to make PHP environment better.þþIf you are interested but have no idea about how to starting, please try these below:þþ- Use it on your system, [feedback](https://github.com/monque/phptrace/issues) problem, feature request.þ- Here is [roadmap](https://github.com/monque/phptrace/projects), try to develop one.þ- Any other? Contact phobosw@gmail.com.þþþ## LicenseþþThis project is released under the [Apache 2.0 License](https://raw.githubusercontent.com/Qihoo360/phptrace/master/LICENSE)."
tvheadend/tvheadend,59516,1757,256,745,Organization,False,10811,26,47,231,False,"Tvheadend is a TV streaming server for Linux supporting DVB-S, DVB-S2, DVB-C, DVB-T, ATSC, IPTV,SAT>IP and other formats through the unix pipe as input sources.",https://tvheadend.org,0,0,0,,,,,20,1325,5,18,4695,6,24,82,91,0,0,8,3,,,"Tvheadendþ========================================þ(c) 2006 - 2020 Tvheadend Foundation CICþþStatusþ------þþ[![Build Status](https://travis-ci.org/tvheadend/tvheadend.svg?branch=master)](https://travis-ci.org/tvheadend/tvheadend)þþ[![Download](https://api.bintray.com/packages/tvheadend/deb/tvheadend/images/download.svg)](https://bintray.com/tvheadend/deb/tvheadend/)þþ[![Coverity Scan](https://scan.coverity.com/projects/2114/badge.svg)](https://scan.coverity.com/projects/2114)þþWhat it isþ----------þþTvheadend is a TV streaming server and digital video recorder.þþIt supports the following inputs:þþ  * DVB-C(2)þ  * DVB-T(2)þ  * DVB-S(2)þ  * ATSCþ  * SAT>IPþ  * HDHomeRunþ  * IPTVþ    * UDPþ    * HTTPþþIt supports the following outputs:þþ  * HTTPþ  * HTSP (own protocol)þ  * SAT>IPþþHow to build for Linuxþ----------------------þþFirst you need to configure:þþ $ ./configureþþIf any dependencies are missing the configure script will complain or attemptþto disable optional features.þþBuild the binary:þþ $ makeþþAfter build, the binary resides in `build.linux` directory.þþThus, to start it, just type:þþ $ ./build.linux/tvheadendþþSettings are stored in `$HOME/.hts/tvheadend`.þþHow to build for OS Xþ---------------------þþSame build procedure applies to OS X.þAfter build, the binary resides in `build.darwin` directory.þþOnly network sources (IPTV, SAT>IP) are supported on OS X.þThere is no support for DVB USB sticks and PCI cards.þTranscoding is currently not supported.þþPackagesþ--------þþInstall instructions for various distributions can be found at the [Wiki](https://tvheadend.org/projects/tvheadend/wiki/Download).þþFurther informationþ-------------------þþFor more information about building, including generating packages, please visit:þ* https://tvheadend.org/projects/tvheadend/wiki/Buildingþ* https://tvheadend.org/projects/tvheadend/wiki/Packagingþ* https://tvheadend.org/projects/tvheadend/wiki/Gitþ* https://tvheadend.org/projects/tvheadend/wiki/Internationalization"
veracrypt/VeraCrypt,177056,2502,179,441,Organization,False,1616,2,38,43,False,Disk encryption with strong security based on TrueCrypt,https://www.veracrypt.fr,6,7,0,327,193,51,8,7,104,1,8,2553,9,58,3193,2021,24,2,8,0,,,"This archive contains the source code of VeraCrypt.þIt is based on original TrueCrypt 7.1a with security enhancements and modifications.þþþImportantþ=========þþYou may use the source code contained in this archive only if you accept andþagree to the license terms contained in the file 'License.txt', which isþincluded in this archive.þþNote that the license specifies, for example, that a derived work must not beþcalled 'TrueCrypt' or 'VeraCrypt'þþþþContentsþ========þþI. Windowsþ   Requirements for Building VeraCrypt for Windows.þ   Instructions for Building VeraCrypt for Windows.þ Instructions for Signing and Packaging VeraCrypt for Windows.þþII. Linux and Mac OS Xþ    Requirements for Building VeraCrypt for Linux and Mac OS X.þ    Instructions for Building VeraCrypt for Linux and Mac OS X.þ Mac OS X specificsþþIII. FreeBSDþþIV. Third-Party Developers (Contributors)þþV. Legal InformationþþVI. Further InformationþþþþI. Windowsþ==========þþRequirements for Building VeraCrypt for Windows:þ------------------------------------------------þþ- Microsoft Visual C++ 2010 SP1 (Professional Edition or compatible)þ- Microsoft Visual C++ 1.52 (available from MSDN Subscriber Downloads)þ- Microsoft Windows SDK for Windows 7.1 (configured for Visual C++ 2010)þ- Microsoft Windows SDK for Windows 8.1 (needed for SHA-256 code signing)þ- Microsoft Windows Driver Kit 7.1.0 (build 7600.16385.1)þ- NASM assembler 2.08 or compatibleþ- YASM 1.3.0 or newer.þ- gzip compressorþ- upx packer (available at https://upx.github.io/)þþIMPORTANT:þþThe 64-bit editions of Windows Vista and later versions of Windows, and inþsome cases (e.g. playback of HD DVD content) also the 32-bit editions, do notþallow the VeraCrypt driver to run without an appropriate digital signature.þTherefore, all .sys files in official VeraCrypt binary packages are digitallyþsigned with the digital certificate of the IDRIX, which wasþissued by Thawte certification authority. At the end of each official .exe andþ.sys file, there are embedded digital signatures and all related certificatesþ(i.e. all certificates in the relevant certification chain, such as theþcertification authority certificates, CA-MS cross-certificate, and theþIDRIX certificate).þKeep this in mind if you compile VeraCryptþand compare your binaries with the official binaries. If your binaries areþunsigned, the sizes of the official binaries will usually be approximatelyþ10 KiB greater than sizes of your binaries (there may be further differencesþif you use a different version of the compiler, or if you install a differentþor no service pack for Visual Studio, or different hotfixes for it, or if youþuse different versions of the required SDKs).þþþInstructions for Building VeraCrypt for Windows:þ------------------------------------------------þþ1) Create an environment variable 'MSVC16_ROOT' pointing to the folder 'MSVC15'þ   extracted from the Visual C++ 1.52 self-extracting package.þþ   Note: The 16-bit installer MSVC15\SETUP.EXE cannot be run on 64-bit Windows,þ   but it is actually not necessary to run it. You only need to extract theþ   folder 'MSVC15', which contains the 32-bit binaries required to build theþ   VeraCrypt Boot Loader.þþ2) If you have installed the Windows Driver Development Kit in anotherþ   directory than '%SYSTEMDRIVE%\WinDDK', create an environment variableþ   'WINDDK_ROOT' pointing to the DDK installation directory.þþ3) Open the solution file 'VeraCrypt.sln' in Microsoft Visual Studio 2010.þþ4) Select 'All' as the active solution configuration.þþ5) Build the solution.þþ6) If successful, there should be newly built VeraCrypt binaries in theþ   'Release' folder.þþInstructions for Signing and Packaging VeraCrypt for Windows:þ-------------------------------------------------------------þþFirst, create an environment variable 'WSDK81' pointing to the Windows SDKþfor Windows 8.1 installation directory.þThe folder ""Signing"" contains a batch file (sign.bat) that will sign allþVeraCrypt components using a code signing certificate present on theþcertificate store and also build the final installation setup.þThe batch file suppose that the code signing certificate is issued by Thawt.þThis is the case for IDRIX's certificate. If yours is issued by another CA,þthen you should put the Root and Intermediate certificates in the ""Signing""þfolder and then modify sign.bat accordingly.þþVeraCrypt EFI Boot Loader:þ--------------------------þþVeraCrypt source code contains pre-built EFI binaries under src\Boot\EFI.þThe source code of VeraCrypt EFI Boot Loader is licensed under LGPL and þit is available at https://github.com/veracrypt/VeraCrypt-DCS.þFor build instructions, please refer to the file src\Boot\EFI\Readme.txt.þþþII. Linux and Mac OS Xþ======================þþRequirements for Building VeraCrypt for Linux and Mac OS X:þ-----------------------------------------------------------þþ- GNU Makeþ- GNU C++ Compiler 4.0 or compatibleþ- Apple Xcode (Mac OS X only)þ- YASM 1.3.0 or newer (Linux only, x86/x64 architecture only)þ- pkg-configþ- wxWidgets 3.0 shared library and header files installed orþ  wxWidgets 3.0 library source code (available at https://www.wxwidgets.org)þ- FUSE library and header files (available at https://github.com/libfuse/libfuseþ  and https://osxfuse.github.io/)þþþInstructions for Building VeraCrypt for Linux and Mac OS X:þ-----------------------------------------------------------þþ1) Change the current directory to the root of the VeraCrypt source code.þþ2) If you have no wxWidgets shared library installed, run the followingþ   command to configure the wxWidgets static library for VeraCrypt and toþ   build it:þþ       $ make WXSTATIC=1 WX_ROOT=/usr/src/wxWidgets wxbuildþþ   The variable WX_ROOT must point to the location of the source code of theþ   wxWidgets library. Output files will be placed in the './wxrelease/'þ   directory.þþ3) To build VeraCrypt, run the following command:þþ       $ makeþþ   or if you have no wxWidgets shared library installed:þþ       $ make WXSTATIC=1þþ4) If successful, the VeraCrypt executable should be located in the directoryþ   'Main'.þþBy default, a universal executable supporting both graphical and text userþinterface (through the switch --text) is built.þOn Linux, a console-only executable, which requires no GUI library, can beþbuilt using the 'NOGUI' parameter:þþ    $ make NOGUI=1 WXSTATIC=1 WX_ROOT=/usr/src/wxWidgets wxbuildþ    $ make NOGUI=1 WXSTATIC=1þþOn MacOSX, building a console-only executable is not supported.þþMac OS X specifics:þ-----------------------------------------------------------þþUnder MacOSX, the SDK for OSX 10.7 is used by default. To use another versionþof the SDK (i.e. 10.6), you can export the environment variable VC_OSX_TARGET:þþ $ export VC_OSX_TARGET=10.6þþþBefore building under MacOSX, pkg-config must be installed if not yet available.þGet it from https://pkgconfig.freedesktop.org/releases/pkg-config-0.28.tar.gz andþcompile using the following commands :þþ $ ./configure --with-internal-glibþ $ makeþ $ sudo make installþþAfter making sure pkg-config is available, download and install OSXFuse fromþhttps://osxfuse.github.io/ (MacFUSE compatibility layer must selected)þþThe script build_veracrypt_macosx.sh available under ""src/Build"" performs theþfull build of VeraCrypt including the creation of the installer pkg. It expectsþto find the wxWidgets 3.0.3 sources at the same level as where you putþVeraCrypt sources (i.e. if ""src"" path is ""/Users/joe/Projects/VeraCrypt/src""þthen wxWidgets should be at ""/Users/joe/Projects/wxWidgets-3.0.3"")þþThe build process uses Code Signing certificates whose ID is specified inþsrc/Main/Main.make (look for lines containing ""Developer ID Application"" and þ""Developer ID Installer""). You'll have to modify these lines to put the ID ofþyour Code Signing certificates or comment them if you don't have one.þþBecause of incompatibility issues with OSXFUSE, the SDK 10.9 generates aþVeraCrypt binary that has issues communicating with the OSXFUSE kernel extension.þThus, we recommend using a different OSX SDK version for building VeraCrypt.þþþþIII. FreeBSDþ============================þþFreeBSD is supported starting from version 11.þThe build requirements and instructions are the same as Linux except that gmakeþshould be used instead of make.þþþþIV. Third-Party Developers (Contributors)þ=========================================þþIf you intend to implement a feature, please contact us first to make sure:þþ1) That the feature has not been implemented (we may have already implementedþ   it, but haven't released the code yet).þ2) That the feature is acceptable.þ3) Whether we need help of third-party developers with implementing the feature.þþInformation on how to contact us can be found at:þhttps://www.veracrypt.fr/þþþþV. Legal Informationþ====================þþCopyright Informationþ---------------------þþThis software as a whole:  þCopyright (c) 2013-2020 IDRIX. All rights reserved.þþPortions of this software:  þCopyright (c) 2013-2020 IDRIX. All rights reserved.  þCopyright (c) 2003-2012 TrueCrypt Developers Association. All rights reserved.  þCopyright (c) 1998-2000 Paul Le Roux. All rights reserved.  þCopyright (c) 1998-2008 Brian Gladman, Worcester, UK. All rights reserved.  þCopyright (c) 1995-2017 Jean-loup Gailly and Mark Adler.  þCopyright (c) 2016 Disk Cryptography Services for EFI (DCS), Alex Kolotnikov  þCopyright (c) 1999-2017 Dieter Baron and Thomas Klausner.  þCopyright (c) 2013, Alexey Degtyarev. All rights reserved.  þCopyright (c) 1999-2016 Jack Lloyd. All rights reserved.  þCopyright (c) 2013-2019 Stephan Mueller <smueller@chronox.de>þþFor more information, please see the legal notices attached to parts of theþsource code.þþTrademark Informationþ---------------------þþAny trademarks contained in the source code, binaries, and/or in theþdocumentation, are the sole property of their respective owners.þþþþVI. Further Informationþ=======================þþhttps://www.veracrypt.fr"
audacity/audacity,197499,2953,191,723,Organization,False,12094,3,18,105,False,Audio Editor,https://wiki.audacityteam.org/wiki/Fo…,5,19,3,56,58,51,36,11,443,2,77,1909,29,1066,760731,876764,0,0,5,4,,,"[![Audacity](https://forum.audacityteam.org/styles/prosilver/theme/images/Audacity-logo_75px_trans_forum.png)](https://www.audacityteam.org) þ=========================þþ[**Audacity**](https://www.audacityteam.org) is an easy-to-use, multi-track audio editor and recorder for Windows, Mac OS X, GNU/Linux and other operating systems. Developed by a group of volunteers as open source.þþ- **Recording** from any real, or virtual audio device that is available to the host system.þ- **Export / Import** a wide range of audio formats, extendible with FFmpeg.þ- **High quality** using 32-bit float audio processing.þ- **Plug-ins** Support for multiple audio plug-in formats, including VST, LV2, AU.þ- **Macros** for chaining commands and batch processing.þ- **Scripting** in Python, Perl, or any language that supports named pipes.þ- **Nyquist** Very powerful built-in scripting language that may also be used to create plug-ins.þ- **Editing** multi-track editing with sample accuracy and arbitrary sample rates.þ- **Accessibility** for VI users.þ- **Analysis and visualization** tools to analyze audio, or other signal data.þþ## Getting StartedþþFor end users, the latest Windows and macOS release version of Audacity is available from the [Audacity website](https://www.audacityteam.org/download/).þHelp with using Audacity is available from the [Audacity Forum](https://forum.audacityteam.org/).þInformation for developers is available from the [Audacity Wiki](https://wiki.audacityteam.org/wiki/For_Developers)."
zephyrproject-rtos/zephyr,335351,3332,342,2026,Organization,False,41561,20,114,699,False,"Primary Git Repository for the Zephyr Project. Zephyr is a new generation, scalable, optimized, secure RTOS for multiple hardware architectures.",https://docs.zephyrproject.org,6,177,10,1094,8006,463,845,574,16472,441,3008,2007,60,3971,507194,305777,0,0,69,13,,,
vlfeat/vlfeat,10399,1314,158,580,Organization,False,2059,2,26,18,False,An open library of computer vision algorithms,http://vlfeat.org/,0,0,2,103,52,1,1,25,30,1,0,4737,0,0,0,0,0,0,9,0,,,"# VLFeat -- Vision Lab Features Libraryþþ> Version 0.9.21þþThe VLFeat open source library implements popular computer visionþalgorithms specialising in image understanding and local featurexsþextraction and matching.  Algorithms incldue Fisher Vector, VLAD,þSIFT, MSER, k-means, hierarchical k-means, agglomerative informationþbottleneck, SLIC superpixes, quick shift superpixels, large scale SVMþtraining, and many others. It is written in C for efficiency andþcompatibility, with interfaces in MATLAB for ease of use, and detailedþdocumentation throughout. It supports Windows, Mac OS X, and Linux.þþVLFeat is distributed under the BSD license (see the `COPYING` file).þþThe documentation isþ[available online](http://www.vlfeat.org/index.html) and shipped withþthe library as `doc/index.html`. See also:þþ* [Using with MATLAB](http://www.vlfeat.org/install-matlab.html)þ* [Using the command line utilities](http://www.vlfeat.org/install-shell.html)þ* [Using the C API](http://www.vlfeat.org/install-c.html)þ* [Compiling from source](http://www.vlfeat.org/compiling.html)þþ## Quick start with MATLABþþTo start using VLFeat as a MATLAB toolbox, download the latest VLFeatþ[binary package](http://www.vlfeat.org/download/). Note that theþpre-compiled binaries require MATLAB 2009B and later. Unpack it, forþexample by using WinZIP (Windows), by double clicking on the archiveþ(Mac), or by using the command line (Linux and Mac):þþ    > tar xzf vlfeat-X.Y.Z-bin.tar.gzþþHere X.Y.Z denotes the latest version. Start MATLAB and run theþVLFeat setup command:þþ    > run <VLFEATROOT>/toolbox/vl_setupþþHere `<VLFEATROOT>` should be replaced with the path to the VLFeatþdirectory created by unpacking the archive. All VLFeat demos can nowþbe run in a row by the command:þþ    > vl_demoþþCheck out the individual demos by editing this file: `edit vl_demo`.þþ## Octave supportþþThe toolbox should be laregly compatible with GNU Octave, an openþsource MATLAB equivalent. However, the binary distribution does notþship with pre-built GNU Octave MEX files. To compile them useþþ    > cd <vlfeat directory>þ    > make MKOCTFILE=<path to the mkoctfile program>þþ# Changesþþ- **0.9.21** Maintenance release. Bugfixes.þ- **0.9.20** Maintenance release. Bugfixes.þ- **0.9.19** Maintenance release. Minor bugfixes and fixes compilationþ  with MATLAB 2014a.þ- **0.9.18** Several bugfixes. Improved documentation, particularly ofþ  the covariant detectors. Minor enhancements of the Fisher vectors.þ- **0.9.17** Rewritten SVM implementation, adding support for SGD andþ  SDCA optimisers and various loss functions (hinge, squared hinge,þ  logistic, etc.) and improving the interface. Added infrastructure toþ  support multi-core computations using OpenMP (MATLAB 2009B or laterþ  required). Added OpenMP support to KD-trees and KMeans. Added newþ  Gaussian Mixture Models, VLAD encoding, and Fisher Vector encodingsþ  (also with OpenMP support). Added LIOP feature descriptors. Addedþ  new object category recognition example code, supporting severalþ  standard benchmarks off-the-shelf.þ- **0.9.16** Added `VL_COVDET`. This function implements the followingþ  detectors: DoG, Hessian, Harris Laplace, Hessian Laplace, Multiscaleþ  Hessian, Multiscale Harris. It also implements affine adaptation,þ  estiamtion of feature orientation, computation of descriptors on theþ  affine patches (including raw patches), and sourcing of customþ  feature frame.þ- **0.9.15** Added `VL_HOG` (HOG features). Added `VL_SVMPEGASOS` andþ  a vastly improved SVM implementation. Added `VL_IHASHSUM` (hashedþ  counting). Improved INTHIST (integral histogram). Addedþ  `VL_CUMMAX`. Improved the implementation of `VL_ROC` andþ  VL_PR(). Added VL_DET() (Detection Error Trade-off (DET)þ  curves). Improved the verbosity control to AIB. Added support forþ  Xcode 4.3, improved support for past and future Xcodeþ  versions. Completed the migration of the old test code inþ  `toolbox/test`, moving the functionality to the new unit testsþ  `toolbox/xtest`.þ- **0.9.14** Added SLIC superpixels. Added VL_ALPHANUM(). Improvedþ  Windows binary package and added support for Visualþ  Studio 2010. Improved the documentation layout and added a properþ  bibliography. Bugfixes and other minor improvements. Moved from theþ  GPL to the less restrictive BSD license.þ- **0.9.13** Fixed Windows binary package.þ- **0.9.12** Fixes `vl_compile` and the architecture string on Linux 32 bit.þ- **0.9.11** Fixes a compatibility problem on older Mac OS X versions.þ  A few bugfixes are included too.þ- **0.9.10** Improves the homogeneous kernel map. Plenty of smallþ  tweaks and improvements. Make maci64 the default architecture on theþ  Mac.þ- **0.9.9** Added: sift matching example. Extended Caltech-101þ  classification example to use kd-trees.þ- **0.9.8** Added: image distance transform, PEGASOS, floating pointþ  K-means, homogeneous kernel maps, a Caltech-101 classificationþ  example. Improved documentation.þ- **0.9.7** Changed the Mac OS X binary distribution to require a lessþ  recent version of Mac OS X (10.5).þ- **0.9.6** Changed the GNU/Linux binary distribution to require aþ  less recent version of the C library.þ- **0.9.5** Added kd-tree and new SSE-accelerated vector/histogramþ  comparison code.  Improved dense SIFT (dsift) implementation.  Addedþ  Snow Leopard and MATLAB R2009b support.þ- **0.9.4** Added quick shift. Renamed dhog to dsift and improvedþ  implementation and documentation. Improved tutorials.  Added 64 bitþ  Windows binaries. Many other small changes.þ- **0.9.3** Namespace change (everything begins with a vl_ prefixþ  now). Many other changes to provide compilation support on Windowsþ  with MATLAB 7.þ- **beta-3** Completes to the ikmeans code.þ- **beta-2** Many additions.þ- **beta-1** Initial public release."
dlundquist/sniproxy,1094,1761,113,323,User,False,741,13,10,25,False,Proxies incoming HTTP and TLS connections based on the hostname contained in the initial request of the TCP session.,,0,11,0,79,179,8,1,11,88,2,2,3463,1,1,2,2,0,0,40,,69,,"SNI Proxyþ=========þþProxies incoming HTTP and TLS connections based on the hostname contained inþthe initial request of the TCP session. This enables HTTPS name-based virtualþhosting to separate backend servers without installing the private key on theþproxy machine.þþFeaturesþ--------þ+ Name-based proxying of HTTPS without decrypting traffic. No keys orþ  certificates required.þ+ Supports both TLS and HTTP protocols.þ+ Supports IPv4, IPv6 and Unix domain sockets for both back end servers andþ  listeners.þ+ Supports multiple listening sockets per instance.þ+ Supports HAProxy proxy protocol to propagate original source address toþ  backend servers.þþUsageþ-----þþ    Usage: sniproxy [-c <config>] [-f] [-n <max file descriptor limit>] [-V]þ        -c  configuration file, defaults to /etc/sniproxy.confþ        -f  run in foreground, do not drop privilegesþ        -n  specify file descriptor limitþ        -V  print the version of SNIProxy and exitþþþInstallationþ------------þþFor Debian or Fedora based Linux distributions see building packages below.þþ**Prerequisites**þþ+ Autotools (autoconf, automake, gettext and libtool)þ+ libev4, libpcre and libudns development headersþ+ Perl and cURL for test suiteþþ**Install**þþ    ./autogen.sh && ./configure && make check && sudo make installþþ**Building Debian/Ubuntu package**þþThis is the preferred installation method on recent Debian based distributions:þþ1. Install required packagesþþ        sudo apt-get install autotools-dev cdbs debhelper dh-autoreconf dpkg-dev gettext libev-dev libpcre3-dev libudns-dev pkg-config fakeroot devscriptsþþ2. Build a Debian packageþþ        ./autogen.sh && dpkg-buildpackageþþ3. Install the resulting packageþþ        sudo dpkg -i ../sniproxy_<version>_<arch>.debþþ**Building Fedora/RedHat package**þþThis is the preferred installation method for modern Fedora based distributions.þþ1. Install required packagesþþ        sudo yum install autoconf automake curl gettext-devel libev-devel pcre-devel perl pkgconfig rpm-build udns-develþþ2. Build a distribution tarball:þþ        ./autogen.sh && ./configure && make distþþ3. Build a RPM packageþþ        rpmbuild --define ""_sourcedir `pwd`"" -ba redhat/sniproxy.specþþ4. Install resulting RPMþþ        sudo yum install ../sniproxy-<version>.<arch>.rpmþþI've used Scientific Linux 6 a fair amount, but I prefer Debian basedþdistributions. RPM builds are tested in Travis-CI on Ubuntu, but not natively.þThis build process may not follow the current Fedora packaging standards, andþmay not even work.þþ***Building on OS X with Homebrew***þþ1. install dependencies.þþ        brew install libev pcre udns autoconf automake gettext libtoolþþ2. Read the warning about gettext and force link it so autogen.sh works. We need the GNU gettext for the macro `AC_LIB_HAVE_LINKFLAGS` which isn't present in the default OS X package.þþ        brew link --force gettextþþ3. Make it soþþ        ./autogen.sh && ./configure && makeþþOS X support is a best effort, and isn't a primary target platform.þþþConfiguration Syntaxþ--------------------þþ    user daemonþþ    pidfile /tmp/sniproxy.pidþþ    error_log {þ        syslog daemonþ        priority noticeþ    }þþ    listener 127.0.0.1:443 {þ        protocol tlsþ        table TableNameþþ        # Specify a server to use if the initial client request doesn't containþ        # a hostnameþ        fallback 192.0.2.5:443þ    }þþ    table TableName {þ        # Match exact request hostnamesþ        example.com 192.0.2.10:4343þ        # If port is not specified the listener port will be usedþ        example.net [2001:DB8::1:10]þ        # Or use regular expression to matchþ        .*\\.com    [2001:DB8::1:11]:443þ        # Combining regular expression and wildcard will resolve the hostnameþ        # client requested and proxy to itþ        .*\\.edu    *:443þ    }þþDNS Resolutionþ--------------þþUsing hostnames or wildcard entries in the configuration requires sniproxy toþbe built with [UDNS](http://www.corpit.ru/mjt/udns.html). SNIProxy will stillþbuild without UDNS, but these features will be unavailable.þþUDNS uses a single UDP socket for all queries, so it is recommended you use aþlocal caching DNS resolver (with a single socket each DNS query is protected byþspoofing by a single 16 bit query ID, which makes it relatively easy to spoof)."
paparazzi/paparazzi,77100,1093,170,933,Organization,False,15595,31,58,107,False,Paparazzi is a free and open-source hardware and software project for unmanned (air) vehicles. This is the main software repository.,http://paparazziuav.org,7,42,1,74,632,4,3,11,1821,8,46,5619,15,103,21662,3443,0,0,29,10,,,"# MAIN READMEþþPaparazzi UASþ=============þþ[![Build Status](https://travis-ci.org/paparazzi/paparazzi.png?branch=master)](https://travis-ci.org/paparazzi/paparazzi) [![Gitter chat](https://badges.gitter.im/paparazzi/discuss.svg)](https://gitter.im/paparazzi/discuss)þ[![Codacy Badge](https://api.codacy.com/project/badge/Grade/811c4398588f435fa8bc926f53d40e9f)](https://app.codacy.com/app/gautierhattenberger/paparazzi?utm_source=github.com&utm_medium=referral&utm_content=paparazzi/paparazzi&utm_campaign=Badge_Grade_Dashboard)þ<a href=""https://scan.coverity.com/projects/paparazzi-paparazzi"">þ  <img alt=""Coverity Scan Build Status""þ       src=""https://scan.coverity.com/projects/4928/badge.svg""/>þ</a>þþPaparazzi is a free open source software package for Unmanned (Air) Vehicle Systems.þFor many years, the system has been used successfuly by hobbyists, universities and companies all over the world, on vehicles of various sizes (11.9g to 25kg).þPaparazzi supports fixed wing, rotorcraft, hybrids, flapping vehicles and it is even possible to use it for boats and surface vehicles.þþUp to date information is available on the wiki http://wiki.paparazziuav.orgþþTo get in touch, subscribe to the mailing list [paparazzi-devel@nongnu.org] (http://savannah.nongnu.org/mail/?group=paparazzi), the IRC channel (freenode, #paparazzi) and Gitter (https://gitter.im/paparazzi/discuss).þþRequired softwareþ-----------------þþInstructions for installation can be found on the wiki (http://wiki.paparazziuav.org/wiki/Installation).þþFor Ubuntu users, required packages are available in the [paparazzi-uav PPA] (https://launchpad.net/~paparazzi-uav/+archive/ppa),þDebian users can use the [OpenSUSE Build Service repository] (http://download.opensuse.org/repositories/home:/flixr:/paparazzi-uav/Debian_7.0/)þþDebian/Ubuntu packages:þ- **paparazzi-dev** is the meta-package on which the Paparazzi software depends to compile and run the ground segment and simulator.þ- **paparazzi-jsbsim** is needed for using JSBSim as flight dynamics model for the simulator.þþRecommended cross compiling toolchain: https://launchpad.net/gcc-arm-embeddedþþþDirectories quick and dirty description:þ----------------------------------------þþ_conf_: the configuration directory (airframe, radio, ... descriptions).þþ_data_: where to put read-only data (e.g. maps, terrain elevation files, icons)þþ_doc_: documentation (diagrams, manual source files, ...)þþ_sw_: software (onboard, ground station, simulation, ...)þþ_var_: products of compilation, cache for the map tiles, ...þþþCompilation and demo simulationþ-------------------------------þþ1. type ""make"" in the top directory to compile all the libraries and tools.þþ2. ""./paparazzi"" to run the Paparazzi Centerþþ3. Select the ""Microjet"" aircraft in the upper-left A/C combo box.þ  Select ""sim"" from upper-middle ""target"" combo box. Click ""Build"".þ  When the compilation is finished, select ""Simulation"" fromþ  the upper-right session combo box and click ""Execute"".þþ4. In the GCS, wait about 10s for the aircraft to be in the ""Holding point"" navigation block.þ  Switch to the ""Takeoff"" block (lower-left blue airway button in the strip).þ  Takeoff with the green launch button.þþUploading the embedded softwareþ----------------------------------þþ1. Power the flight controller board while it is connected to the PC with the USB cable.þþ2. From the Paparazzi center, select the ""ap"" target, and click ""Upload"".þþþFlightþ------þþ1.  From the Paparazzi Center, select the flight session and ... do the same as in simulation !"
netblue30/firejail,13151,2550,102,344,User,False,6238,8,48,192,False,Linux namespaces and seccomp-bpf sandbox,https://firejail.wordpress.com,0,15,0,204,1976,61,182,6,1263,5,131,1776,29,445,47378,43074,0,0,6,,93,,"# Firejailþ[![Test Status](https://travis-ci.org/netblue30/firejail.svg?branch=master)](https://travis-ci.org/netblue30/firejail)þ[![Build Status](https://gitlab.com/Firejail/firejail_ci/badges/master/pipeline.svg)](https://gitlab.com/Firejail/firejail_ci/pipelines/)þ[![Packaging status](https://repology.org/badge/tiny-repos/firejail.svg)](https://repology.org/project/firejail/versions)þþFirejail is a SUID sandbox program that reduces the risk of security breaches by restrictingþthe running environment of untrusted applications using Linux namespaces, seccomp-bpfþand Linux capabilities. It allows a process and all its descendants to have their own privateþview of the globally shared kernel resources, such as the network stack, process table, mount table.þFirejail can work in a SELinux or AppArmor environment, and it is integrated with Linux Control Groups.þþWritten in C with virtually no dependencies, the software runs on any Linux computer with a 3.x kernelþversion or newer. It can sandbox any type of processes: servers, graphical applications, and evenþuser login sessions. The software includes sandbox profiles for a number of more common Linux programs,þsuch as Mozilla Firefox, Chromium, VLC, Transmission etc.þþThe sandbox is lightweight, the overhead is low. There are no complicated configuration files to edit,þno socket connections open, no daemons running in the background. All security features areþimplemented directly in Linux kernel and available on any Linux computer.þþ<table><tr>þþ<td>þ<a href=""http://www.youtube.com/watch?feature=player_embedded&v=7RMz7tePA98þ"" target=""_blank""><img src=""http://img.youtube.com/vi/7RMz7tePA98/0.jpg""þalt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Firejail Intro</a>þ</td>þþ<td>þ<a href=""http://www.youtube.com/watch?feature=player_embedded&v=J1ZsXrpAgBUþ"" target=""_blank""><img src=""http://img.youtube.com/vi/J1ZsXrpAgBU/0.jpg""þalt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Firejail Demo</a>þ</td>þþ<td>þ<a href=""http://www.youtube.com/watch?feature=player_embedded&v=EyEz65RYfw4þ"" target=""_blank""><img src=""http://img.youtube.com/vi/EyEz65RYfw4/0.jpg""þalt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Debian Install</a>þ</td>þþþ</tr><tr>þ<td>þ<a href=""http://www.youtube.com/watch?feature=player_embedded&v=Uy2ZTHc4s0wþ"" target=""_blank""><img src=""http://img.youtube.com/vi/Uy2ZTHc4s0w/0.jpg""þalt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Arch Linux Install</a>þþ</td>þ<td>þ<a href=""http://www.youtube.com/watch?feature=player_embedded&v=xuMxRx0zSfQþ"" target=""_blank""><img src=""http://img.youtube.com/vi/xuMxRx0zSfQ/0.jpg""þalt=""Firejail Intro video"" width=""240"" height=""180"" border=""10"" /><br/>Disable Network Access</a>þþ</td>þ</tr></table>þþProject webpage: https://firejail.wordpress.com/þþDownload and Installation: https://firejail.wordpress.com/download-2/þþFeatures: https://firejail.wordpress.com/features-3/þþDocumentation: https://firejail.wordpress.com/documentation-2/þþFAQ: https://github.com/netblue30/firejail/wiki/Frequently-Asked-QuestionsþþWiki: https://github.com/netblue30/firejail/wikiþþTravis-CI status: https://travis-ci.org/netblue30/firejailþþGitLab-CI status: https://gitlab.com/Firejail/firejail_ci/pipelines/þþþ## Security vulnerabilitiesþþWe take security bugs very seriously. If you believe you have found one, please report it by emailing us at netblue30@yahoo.comþþ## InstallingþþTry installing Firejail from your system packages first. Firejail is included in Alpine, ALT Linux, Arch, Chakra, Debian, Deepin, Devuan, Fedora, Gentoo, Manjaro, Mint, NixOS, Parabola, Parrot, PCLinuxOS, ROSA, Solus, Slackware/SlackBuilds, Trisquel, Ubuntu, Void and possibly others.þþThe firejail 0.9.52-LTS version is deprecated. On Ubuntu 18.04 LTS users are advised to use the [PPA](https://launchpad.net/~deki/+archive/ubuntu/firejail). On Debian buster we recommend to use the [backports](https://packages.debian.org/buster-backports/firejail) package.þþYou can also install one of the [released packages](http://sourceforge.net/projects/firejail/files/firejail), or clone Firejail’s source code from our Git repository and compile manually:þþ`````þ$ git clone https://github.com/netblue30/firejail.gitþ$ cd firejailþ$ ./configure && make && sudo make install-stripþ`````þOn Debian/Ubuntu you will need to install git and gcc compiler. AppArmorþdevelopment libraries and pkg-config are required when using --apparmorþ./configure option:þ`````þ$ sudo apt-get install git build-essential libapparmor-dev pkg-configþ`````þFor --selinux option, add libselinux1-dev (libselinux-devel for Fedora).þþDetailed information on using firejail from git is available on the [wiki](https://github.com/netblue30/firejail/wiki/Using-firejail-from-git).þþ## Running the sandboxþþTo start the sandbox, prefix your command with “firejail”:þþ`````þ$ firejail firefox            # starting Mozilla Firefoxþ$ firejail transmission-gtk   # starting Transmission BitTorrentþ$ firejail vlc                # starting VideoLAN Clientþ$ sudo firejail /etc/init.d/nginx startþ`````þRun ""firejail --list"" in a terminal to list all active sandboxes. Example:þ`````þ$ firejail --listþ1617:netblue:/usr/bin/firejail /usr/bin/firefox-esrþ7719:netblue:/usr/bin/firejail /usr/bin/transmission-qtþ7779:netblue:/usr/bin/firejail /usr/bin/galculatorþ7874:netblue:/usr/bin/firejail /usr/bin/vlc --started-from-file file:///home/netblue/firejail-whitelist.mp4þ7916:netblue:firejail --listþ`````þþ## Desktop integrationþþIntegrate your sandbox into your desktop by running the following two commands:þ`````þ$ firecfg --fix-soundþ$ sudo firecfgþ`````þþThe first command solves some shared memory/PID namespace bugs in PulseAudio software prior to version 9.þThe second command integrates Firejail into your desktop. You would need to logout and login back to applyþPulseAudio changes.þþStart your programs the way you are used to: desktop manager menus, file manager, desktop launchers.þThe integration applies to any program supported by default by Firejail. There are about 250 default applicationsþin current Firejail version, and the number goes up with every new release.þWe keep the application list in [/usr/lib/firejail/firecfg.config](https://github.com/netblue30/firejail/blob/master/src/firecfg/firecfg.config) file.þþ## Security profilesþþMost Firejail command line options can be passed to the sandbox using profile files.þYou can find the profiles for all supported applications in [/etc/firejail](https://github.com/netblue30/firejail/tree/master/etc) directory.þþIf you keep additional Firejail security profiles in a public repository, please give us a link:þþ* https://github.com/chiraag-nataraj/firejail-profilesþþ* https://github.com/triceratops1/feþþUse this issue to request new profiles: [#1139](https://github.com/netblue30/firejail/issues/1139)þþYou can also use this tool to get a list of syscalls needed by a program: [contrib/syscalls.sh](contrib/syscalls.sh).þþWe also keep a list of profile fixes for previous released versions in [etc-fixes](https://github.com/netblue30/firejail/tree/master/etc-fixes) directory.þ`````þþ`````þ## Latest released version: 0.9.62þþ## Current development version: 0.9.63þþ### Profile StatisticsþþA small tool to print profile statistics. Compile as usual and run:þ`````þ$ makeþ$ cd etcþ$ ./profstats *.profileþ    profiles   966þ    include local profile 966   (include profile-name.local)þ    include globals  966   (include globals.local)þ    blacklist ~/.ssh  951   (include disable-common.inc)þ    seccomp   908þ    capabilities  965þ    noexec   830   (include disable-exec.inc)þ    memory-deny-write-execute 214þ    apparmor   488þ    private-bin   483þ    private-dev   829þ    private-etc   366þ    private-tmp   726þ    whitelist var  638   (include whitelist-var-common.inc)þ    whitelist run/user  282   (include whitelist-runuser-common.incþ     or blacklist ${RUNUSER})þ    whitelist usr/share  275   (include whitelist-usr-share-common.incþ    net none   313þ`````þþRun ./profstats -h for help.þþ### New profiles:þþgfeeds, firefox-x11, tvbrowser, rtv, clipgrab, gnome-passwordsafe, bibtex, gummi, latex, pdflatex, tex, wpp, wpspdf, wps, et,þmultimc, gnome-hexgl, com.github.johnfactotum.Foliate, desktopeditors, impressive, mupdf-gl, mupdf-x11, mupdf-x11-curl,þmuraster, mutool, planmaker18, planmaker18free, presentations18, presentations18free, textmaker18, textmaker18free, teams, xournal,þgnome-screenshot, ripperX, sound-juicer, iagno, com.github.dahenson.agenda, gnome-pomodoro, gnome-todo, kmplayer,þpenguin-command, x2goclient, frogatto, gnome-mines, gnome-nibbles, lightsoff, ts3client_runscript.sh, warmux, ferdi, abiword,þfour-in-a-row, gnome-mahjongg, gnome-robots, gnome-sudoku, gnome-taquin, gnome-tetravex, blobwars, gravity-beams-and-evaporating-stars,þhyperrogue, jumpnbump-menu, jumpnbump, magicor, mindless, mirrormagic, mrrescue, scorched3d-wrapper, scorchwentbonkers,þseahorse-adventures, wordwarvi, xbill, gnome-klotski, five-or-more, swell-foop, fdns, jitsi-meet-desktop, nicontine, steam-runtime, apostrophe, quadrapassel, dino-im"
panda-re/panda,169826,1626,141,386,Organization,False,54809,34,16,770,False,Platform for Architecture-Neutral Dynamic Analysis,,3,7,1,62,211,28,19,7,398,4,111,4149,6,218,33149,28712,0,0,6,4,,,"# PANDAþþ[![Build Status](https://travis-ci.org/panda-re/panda.svg?branch=master)](https://travis-ci.org/panda-re/panda)þ![Autobuild Docker Container](https://github.com/panda-re/panda/workflows/Build%20and%20Publish%20Docker%20Container/badge.svg)þþPANDA is an open-source Platform for Architecture-Neutral Dynamic Analysis. Itþis built upon the QEMU whole system emulator, and so analyses have access to allþcode executing in the guest and all data. PANDA adds the ability to record andþreplay executions, enabling iterative, deep, whole system analyses. Further, theþreplay log files are compact and shareable, allowing for repeatable experiments.þA nine billion instruction boot of FreeBSD, e.g., is represented by only a fewþhundred MB. PANDA leverages QEMU's support of thirteen different CPUþarchitectures to make analyses of those diverse instruction sets possible withinþthe LLVM IR. In this way, PANDA can have a single dynamic taint analysis, forþexample, that precisely supports many CPUs. PANDA analyses are written in aþsimple plugin architecture which includes a mechanism to share functionalityþbetween plugins, increasing analysis code re-use and simplifying complexþanalysis development.þþIt is currently being developed in collaboration with MIT LincolnþLaboratory, NYU, and Northeastern University. PANDA is released underþthe [GPLv2 license](LICENSE).þþ---------------------------------------------------------------------þþ## Buildingþ### Quickstart: DockerþThe latest version of PANDA's master branch is automatically built as a docker imageþfrom both Ubuntu Bionic (18.04) and Xenial (16.04). These images are available [here](https://hub.docker.com/r/pandare/panda).þþTo pull the latest docker container and run PANDAþ```þ$ docker pull pandare/pandaþ$ docker run --rm pandare/panda -- /bin/panda-system-i386 --helpþ```þþ###  Debian, UbuntuþBecause PANDA has a few dependencies, we've encoded the build instructions intoþthe [install\_ubuntu.sh](panda/scripts/install\_ubuntu.sh). The script shouldþwork on the latest Debian stable/Ubuntu LTS versions.þIf you wish to build PANDA manually, you can also check theþ[step-by-step instructions](panda/docs/build\_ubuntu.md) in the documentationþdirectory.þþWe currently only vouch for buildability on the latest Debian stable/Ubuntu LTS,þbut we welcome pull requests to fix issues with other distros.þFor other distributions, it should be straightforward to translate the `apt-get`þcommands into whatever package manager your distribution uses.þþNote that if you want to use our LLVM features (mainly the dynamic taintþsystem), you will need to install LLVM 3.3 from OS packages or compiled fromþsource. On Ubuntu this should happen automatically via `install_ubuntu.sh`.þAdditionally, it is **strongly** recommended that you only build PANDA as 64bitþbinary. Creating a 32bit build should be possible, but best avoided.þSee the limitations section for details.þþ### ArchþThe [install\_arch.sh](panda/scripts/install\_arch.sh) has been contributedþfor building PANDA on Arch Linux.þCurrently, the script has only been tested on Arch Linux 4.17.5-1-MANJARO.þYou can also findþ[step-by-step instructions for building on Arch](panda/docs/build\_arch.md)þin the documentation directory.þþ### MacOSþBuilding on Mac is less well-tested, but has been known to work. There is a script,þ[install\_osx.sh](panda/scripts/install\_osx.sh) to build under OS X.þThe script uses [homebrew](https://brew.sh) to install the PANDA dependencies.þAs homebrew is known to be very fast in deprecating support for older versionsþof OS X and supported packages, expect this to be broken.þþ### InstallationþAfter successfully building PANDA, you can copy the build to a system-wideþlocation by running `make install`. The default installation path is `/usr/local`.þYou can specify an alternate installation path through the `prefix` configurationþoption. E.g. `--prefix=/opt/panda`.  Note that your system must have `chrpath`þinstalled in order for `make install` to succeed.þþIf the `bin` directory containing the PANDA binaries is in your `PATH` environmentþvariable, then you can run PANDA similarly to QEMU:þþ    panda-system-i386 -m 2G -hda guest.img -monitor stdioþþ---------------------------------------------------------------------þþ## Limitationsþþ### LLVM SupportþPANDA uses the LLVM architecture from the [S2E project](https://github.com/dslab-epfl/s2e).þThis allows translating the TCG intermediate code representation used by QEMU,þto LLVM IR. The latter has the advantages of being easier to work with, as wellþas platform independent. This enables the implementation of complex analysesþlike the `taint2` plugin.þHowever, S2E is not actively updated to work with the latest LLVM toolchain.þAs a consequence, PANDA still requires specifically LLVM 3.3 in order to beþbuilt with taint analysis support.þof the plugins.þþ### Cross-architecture record/replayþGreat effort is put to maintain the PANDA trace format stable so that existingþtraces remain replayable in the future. Changes that will break existing tracesþare avoided.þHowever, currently, record/replay is only guaranteed between PANDA builds of theþsame address length. E.g. you can't replay a trace captured on a 32bit build ofþPANDA on a 64bit of PANDA. The reason for this is that some raw pointers managedþto creep into the trace format (see headers in `panda/rr`).þþGiven the memory limitations of 32bit builds, almost all PANDA users use 64bit.þAs a result, this issue should affect only a tiny minority of users.þThis is also supported by the fact that the issue remained unreported for aþlong time (>3 years). Therefore, when a fix is to be implemented, it may beþassessed that migrating existing recordings captured by 32bit builds is notþworth the effort.þþFor this, it is **strongly** recommended that you only create and use 64bitþbuilds of PANDA. If you happen to already have a dataset of traces capturedþby a 32bit build of PANDA, you should contact the community ASAP to discussþpossible options.þþ---------------------------------------------------------------------þþ## Documentation and Supportþþ### PANDA manualþPANDA currently supports whole-system record/replay execution, as well asþtime-travel debugging, of x86, x86\_64, and ARM guests.þDetails about the implementation and use of PANDA can be found in theþ[PANDA manual](panda/docs/manual.md). Some of the topics covered are:þþ  * [details about record/replay](panda/docs/manual.md#recordreplay-details)þ  * the [architecture-neutral plugin interface](panda/docs/manual.md#plugin-architecture)þ  * the [callbacks provided by PANDA](panda/docs/manual.md#appendix-a-callback-list)þ  * [plugin zoo](panda/docs/manual.md#plugin-zoo)þþDocumentation for individual plugins is provided by the `README.md` fileþin the plugin directory. See [panda/plugins](panda/plugins) directory.þþ### SupportþIf you need help with PANDA, or want to discuss the project, you can join ourþIRC channel at #panda-re on Freenode, or join the [PANDA mailingþlist](http://mailman.mit.edu/mailman/listinfo/panda-users).þþ---------------------------------------------------------------------þþ## Publicationsþþ* [1] B. Dolan-Gavitt, T. Leek, J. Hodosh, W. Lee.  Tappan Zee (North) Bridge:þMining Memory Accesses for Introspection. 20th ACM Conference on Computer andþCommunications Security (CCS), Berlin, Germany, November 2013.þþ* [2] R. Whelan, T. Leek, D. Kaeli.  Architecture-Independent DynamicþInformation Flow Tracking. 22nd International Conference on CompilerþConstruction (CC), Rome, Italy, March 2013.þþ* [3] B. Dolan-Gavitt, J. Hodosh, P. Hulin, T. Leek, R. Whelan.þRepeatable Reverse Engineering with PANDA. 5th Program Protection and ReverseþEngineering Workshop, Los Angeles, California, December 2015.þþ* [4] M. Stamatogiannakis, P. Groth, H. Bos. Decoupling ProvenanceþCapture and Analysis from Execution. 7th USENIX Workshop on the Theoryþand Practice of Provenance, Edinburgh, Scotland, July 2015.þþ* [5] B. Dolan-Gavitt, P. Hulin, T. Leek, E. Kirda, A. Mambretti,þW. Robertson, F. Ulrich, R. Whelan. LAVA: Large-scale Automated VulnerabilityþAddition. 37th IEEE Symposium on Security and Privacy, San Jose,þCalifornia, May 2016.þþ---------------------------------------------------------------------þþ## AcknowledgementsþþThis material is based upon work supported under Air Force Contract No.þFA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions, findings,þconclusions or recommendations expressed in this material are those ofþthe author(s) and do not necessarily reflect the views of the U.S. AirþForce."
megous/megatools,1,1466,90,182,User,False,2,1,0,1,False,Open-source command line tools for accessing Mega.co.nz cloud storage.,https://megatools.megous.com,2,13,0,33,377,2,2,0,44,0,0,432,0,0,0,0,0,0,3,,75,,"## Repostiory has been movedþþIf you want to reach me as the maintainer, send an e-mail to megatools@megous.com (What you send there will be pubic, so be careful.) Questions and patches are welcome. I no longer monitor or update my github account.þþOfficial web page is:þþhttps://megatools.megous.comþþThe main git repository is hosted at:þþhttps://megous.com/git/megatools"
google/honggfuzz,191908,1903,131,395,Organization,False,3780,2,18,45,False,"Security oriented software fuzzer. Supports evolutionary, feedback-driven fuzzing based on code coverage (SW and HW based)",https://honggfuzz.dev,3,12,0,11,169,9,28,1,163,1,13,1839,11,331,7293,4831,0,0,1,1,,,"# Honggfuzzþþ## DescriptionþþA security oriented, feedback-driven, evolutionary, easy-to-use fuzzer with interesting analysis options. See the [Usage document](https://github.com/google/honggfuzz/blob/master/docs/USAGE.md) for a primer on Honggfuzz use.þþ## Codeþþ  * Latest stable version: [2.2](https://github.com/google/honggfuzz/releases)þ  * [Changelog](https://github.com/google/honggfuzz/blob/master/CHANGELOG)þþ## Featuresþþ  * It's __multi-process__ and __multi-threaded__: there's no need to run multiple copies of your fuzzer, as honggfuzz can unlock potential of all your available CPU cores with a single running instance. The file corpus is automatically shared and improved between all fuzzed processes.þ  * It's blazingly fast when the [persistent fuzzing mode](https://github.com/google/honggfuzz/blob/master/docs/PersistentFuzzing.md)) is used. A simple/empty _LLVMFuzzerTestOneInput_ function can be tested with __up to 1mo iterations per second__ on a relatively modern CPU (e.g. i7-6700K).þ  * Has a [solid track record](#trophies) of uncovered security bugs: the __only__ (to the date) __vulnerability in OpenSSL with the [critical](https://www.openssl.org/news/secadv/20160926.txt) score mark__ was discovered by honggfuzz. See the [Trophies](#trophies) paragraph for the summary of findings to the date.þ  * Uses low-level interfaces to monitor processes (e.g. _ptrace_ under Linux and NetBSD). As opposed to other fuzzers, it __will discover and report hijacked/ignored signals from crashes__ (intercepted and potentially hidden by a fuzzed program).þ  * Easy-to-use, feed it a simple corpus directory (can even be empty for the [feedback-driven fuzzing](https://github.com/google/honggfuzz/blob/master/docs/FeedbackDrivenFuzzing.md)), and it will work its way up, expanding it by utilizing feedback-based coverage metrics.þ  * Supports several (more than any other coverage-based feedback-driven fuzzer) hardware-based (CPU: branch/instruction counting, __Intel BTS__, __Intel PT__) and software-based [feedback-driven fuzzing](https://github.com/google/honggfuzz/blob/master/docs/FeedbackDrivenFuzzing.md) modes. Also, see the new __[qemu mode](https://github.com/google/honggfuzz/tree/master/qemu_mode)__ for blackbox binary fuzzing.þ  * Works (at least) under GNU/Linux, FreeBSD, NetBSD, Mac OS X, Windows/CygWin and [Android](https://github.com/google/honggfuzz/blob/master/docs/Android.md).þ  * Supports the __persistent fuzzing mode__ (long-lived process calling a fuzzed API repeatedly). More on that can be found [here](https://github.com/google/honggfuzz/blob/master/docs/PersistentFuzzing.md).þ  * It comes with the __[examples](https://github.com/google/honggfuzz/tree/master/examples) directory__, consisting of real world fuzz setups for widely-used software (e.g. Apache HTTPS, OpenSSL, libjpeg etc.).þ  * Provides a __[corpus minimization](https://github.com/google/honggfuzz/blob/master/docs/USAGE.md#corpus-minimization--m)__ mode.þþ---þþ<p align=""center"">þ <img src=""https://raw.githubusercontent.com/google/honggfuzz/master/screenshot-honggfuzz-1.png"" width=""75%"" height=""75%"">þ</p>þþ---þþ## Requirementsþþ  * **Linux** - The BFD library (libbfd-dev) and libunwind (libunwind-dev/libunwind8-dev), clang-5.0 or higher for software-based coverage modesþ  * **FreeBSD** - gmake, clang-5.0 or newerþ  * **NetBSD** - gmake, clang, capstone, libBlocksRuntimeþ  * **Android** - Android SDK/NDK. Also see [this detailed doc](https://github.com/google/honggfuzz/blob/master/docs/Android.md) on how to build and run itþ  * **Windows** - CygWinþ  * **Darwin/OS X** - Xcode 10.8+þ  * if **Clang/LLVM** is used to compile honggfuzz - link it with the BlocksRuntime Library (libblocksruntime-dev)þþ## TrophiesþþHonggfuzz has been used to find a few interesting security problems in major software packages; An incomplete list:þþþ  * [Pre-auth remote crash in __OpenSSH__](https://anongit.mindrot.org/openssh.git/commit/?id=28652bca29046f62c7045e933e6b931de1d16737)þ  * __Apache HTTPD__þ    * [Remote crash in __mod\_http2__ • CVE-2017-7659](http://seclists.org/oss-sec/2017/q2/504)þ    * [Use-after-free in __mod\_http2__ • CVE-2017-9789](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9789)þ    * [Memory leak in __mod\_auth\_digest__ • CVE-2017-9788](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-9788)þ    * [Out of bound access • CVE-2018-1301](http://seclists.org/oss-sec/2018/q1/265)þ    * [Write after free in HTTP/2 • CVE-2018-1302](http://seclists.org/oss-sec/2018/q1/268)þ    * [Out of bound read • CVE-2018-1303](http://seclists.org/oss-sec/2018/q1/266)þ  * Various __SSL__ libsþ    * [Remote OOB read in __OpenSSL__ • CVE-2015-1789]( https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-1789)þ    * [Remote Use-after-Free (potential RCE, rated as __critical__) in __OpenSSL__ • CVE-2016-6309](https://www.openssl.org/news/secadv/20160926.txt)þ    * [Remote OOB write in __OpenSSL__ • CVE-2016-7054](https://www.openssl.org/news/secadv/20161110.txt)þ    * [Remote OOB read in __OpenSSL__ • CVE-2017-3731](https://www.openssl.org/news/secadv/20170126.txt)þ    * [Uninitialized mem use in __OpenSSL__](https://github.com/openssl/openssl/commit/bd5d27c1c6d3f83464ddf5124f18a2cac2cbb37f)þ    * [Crash in __LibreSSL__](https://github.com/openbsd/src/commit/c80d04452814d5b0e397817ce4ed34edb4eb520d)þ    * [Invalid free in __LibreSSL__](https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-2.6.2-relnotes.txt)þ    * [Uninitialized mem use in __BoringSSL__](https://github.com/boringssl/boringssl/commit/7dccc71e08105b100c3acd56fa5f6fc1ba9b71d3)þ  * [Adobe __Flash__ memory corruption • CVE-2015-0316](http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-0316)þ  * [Multiple bugs in the __libtiff__ library](http://bugzilla.maptools.org/buglist.cgi?query_format=advanced;emailreporter1=1;email1=robert@swiecki.net;product=libtiff;emailtype1=substring)þ  * [Multiple bugs in the __librsvg__ library](https://bugzilla.gnome.org/buglist.cgi?query_format=advanced;emailreporter1=1;email1=robert%40swiecki.net;product=librsvg;emailtype1=substring)þ  * [Multiple bugs in the __poppler__ library](http://lists.freedesktop.org/archives/poppler/2010-November/006726.html)þ  * [Multiple exploitable bugs in __IDA-Pro__](https://www.hex-rays.com/bugbounty.shtml)þ  * [Remote DoS in __Crypto++__ • CVE-2016-9939](http://www.openwall.com/lists/oss-security/2016/12/12/7)þ  * Programming language interpretersþ    * [__PHP/Python/Ruby__](https://github.com/dyjakan/interpreter-bugs)þ    * [PHP WDDX](https://bugs.php.net/bug.php?id=74145)þ    * [PHP](https://bugs.php.net/bug.php?id=74194)þ    * Perl: [#1](https://www.nntp.perl.org/group/perl.perl5.porters/2018/03/msg250072.html), [#2](https://github.com/Perl/perl5/issues/16468), [#3](https://github.com/Perl/perl5/issues/16015)þ  * [Double-free in __LibXMP__](https://github.com/cmatsuoka/libxmp/commit/bd1eb5cfcd802820073504c234c3f735e96c3355)þ  * [Heap buffer overflow in SAPCAR • CVE-2017-8852](https://www.coresecurity.com/blog/sapcar-heap-buffer-overflow-crash-exploit)þ  * [Crashes in __libbass__](http://seclists.org/oss-sec/2017/q4/185)þ  * __FreeType 2__:þ    * [CVE-2010-2497](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2497)þ    * [CVE-2010-2498](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2498)þ    * [CVE-2010-2499](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2499)þ    * [CVE-2010-2500](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2500)þ    * [CVE-2010-2519](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2519)þ    * [CVE-2010-2520](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2520)þ    * [CVE-2010-2527](https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2010-2527)þ  * Stack corruption issues in the Windows OpenType parser: [#1](https://github.com/xinali/AfdkoFuzz/blob/4eadcb19eacb2fb73e4b0f0b34f382a9331bb3b4/CrashesAnalysis/CrashesAnalysis_3/README.md), [#2](https://github.com/xinali/AfdkoFuzz/blob/master/CVE-2019-1117/README.md), [#3](https://github.com/xinali/AfdkoFuzz/tree/f6d6562dd19403cc5a1f8cef603ee69425b68b20/CVE-2019-1118)þ  * [Infinite loop in __NGINX Unit__](https://github.com/nginx/unit/commit/477e8177b70acb694759e62d830b8a311a736324)þ  * A couple of problems in the [__MATLAB MAT File I/O Library__](https://sourceforge.net/projects/matio): [#1](https://github.com/tbeu/matio/commit/406438f497931f45fb3edf6de17d3a59a922c257), [#2](https://github.com/tbeu/matio/commit/406438f497931f45fb3edf6de17d3a59a922c257), [#3](https://github.com/tbeu/matio/commit/a55b9c2c01582b712d5a643699a13b5c41687db1), [#4](https://github.com/tbeu/matio/commit/3e6283f37652e29e457ab9467f7738a562594b6b), [#5](https://github.com/tbeu/matio/commit/783ee496a6914df68e77e6019054ad91e8ed6420)þ  * __Samba__ [tdbdump + tdbtool](http://seclists.org/oss-sec/2018/q2/206), [#2](https://github.com/samba-team/samba/commit/183da1f9fda6f58cdff5cefad133a86462d5942a), [#3](https://github.com/samba-team/samba/commit/33e9021cbee4c17ee2f11d02b99902a742d77293), [#4](https://github.com/samba-team/samba/commit/ac1be895d2501dc79dcff2c1e03549fe5b5a930c), [#5](https://github.com/samba-team/samba/commit/b1eda993b658590ebb0a8225e448ce399946ed83), [#6](https://github.com/samba-team/samba/commit/f7f92803f600f8d302cdbb668c42ca8b186a797f) [CVE-2019-14907](https://www.samba.org/samba/security/CVE-2019-14907.html)þ  * [Crash in __djvulibre__](https://github.com/barak/djvulibre/commit/89d71b01d606e57ecec2c2930c145bb20ba5bbe3)þ  * [Multiple crashes in __VLC__](https://www.pentestpartners.com/security-blog/double-free-rce-in-vlc-a-honggfuzz-how-to/)þ  * [Buffer overflow in __ClassiCube__](https://github.com/UnknownShadow200/ClassiCube/issues/591)þ  * [Heap buffer-overflow (or UAF) in __MPV__](https://github.com/mpv-player/mpv/issues/6808)þ  * [Heap buffer-overflow in __picoc__](https://gitlab.com/zsaleeba/picoc/issues/44)þ  * Crashes in __OpenCOBOL__: [#1](https://sourceforge.net/p/open-cobol/bugs/586/), [#2](https://sourceforge.net/p/open-cobol/bugs/587/)þ  * DoS in __ProFTPD__: [#1](https://twitter.com/SecReLabs/status/1186548245553483783), [#2](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-18217)þ  * [Multiple security problems in ImageIO (iOS/MacOS)](https://googleprojectzero.blogspot.com/2020/04/fuzzing-imageio.html)þ  * [Memory corruption in __htmldoc__](https://github.com/michaelrsweet/htmldoc/issues/370)þ  * [Memory corruption in __OpenDetex__](https://github.com/pkubowicz/opendetex/issues/60)þ  * [Memory corruption in __Yabasic__](https://github.com/marcIhm/yabasic/issues/36)þ  * [Memory corruption in __Xfig__](https://sourceforge.net/p/mcj/tickets/67/)þ  * [Memory corruption in __LibreOffice__](https://github.com/LibreOffice/core/commit/0754e581b0d8569dd08cf26f88678754f249face)þ  * [Memory corruption in __ATasm__](https://sourceforge.net/p/atasm/bugs/8/)þ  * __Rust__:þ    * panic() in regex [#1](https://github.com/rust-lang/regex/issues/464), [#2](https://github.com/rust-lang/regex/issues/465), [#3](https://github.com/rust-lang/regex/issues/465#issuecomment-381412816)þ    * panic() in h2 [#1](https://github.com/carllerche/h2/pull/260), [#2](https://github.com/carllerche/h2/pull/261), [#3](https://github.com/carllerche/h2/pull/262)þ    * panic() in sleep-parser [#1](https://github.com/datrs/sleep-parser/issues/3)þ    * panic() in lewton [#1](https://github.com/RustAudio/lewton/issues/27)þ    * panic()/DoS in Ethereum-Parity [#1](https://srlabs.de/bites/ethereum_dos/)þ    * crash() in Parts - a GPT partition manager [#1](https://github.com/DianaNites/parts/commit/d8ab05d48d87814f362e94f01c93d9eeb4f4abf4)þ    * crashes in rust-bitcoin/rust-lightning [#1](https://github.com/rust-bitcoin/rust-lightning/commit/a9aa3c37fe182dd266e0faebc788e0c9ee724783)þ  * ... and moreþþ## Projects utilizing or inspired-by Honggfuzzþþ  * [__QuickFuzz__ by CIFASIS](http://quickfuzz.org)þ  * [__OSS-Fuzz__](https://github.com/google/oss-fuzz)þ  * [__Frog And Fuzz__](https://github.com/warsang/FrogAndFuzz/tree/develop)þ  * [__interpreters fuzzing__: by dyjakan](https://github.com/dyjakan/interpreter-bugs)þ  * [__riufuzz__: honggfuzz with AFL-like UI](https://github.com/riusksk/riufuzz)þ  * [__h2fuzz__: fuzzing Apache's HTTP/2 implementation](https://github.com/icing/h2fuzz)þ  * [__honggfuzz-dharma__: honggfuzz with dharma grammar fuzzer](https://github.com/Sbouber/honggfuzz-dharma)þ  * [__Owl__: a system for finding concurrency attacks](https://github.com/hku-systems/owl)þ  * [__honggfuzz-docker-apps__](https://github.com/skysider/honggfuzz_docker_apps)þ  * [__FFW__: Fuzzing For Worms](https://github.com/dobin/ffw)þ  * [__honggfuzz-rs__: fuzzing Rust with Honggfuzz](https://docs.rs/honggfuzz/)þ  * [__roughenough-fuzz__](https://github.com/int08h/roughenough-fuzz)þ  * [__Monkey__: a HTTP server](https://github.com/monkey/monkey/blob/master/FUZZ.md)þ  * [__Killerbeez API__: a modular fuzzing framework](https://github.com/grimm-co/killerbeez)þ  * [__FuzzM__: a gray box model-based fuzzing framework](https://github.com/collins-research/FuzzM)þ  * [__FuzzOS__: by Mozilla Security](https://github.com/MozillaSecurity/fuzzos)þ  * [__Android__: by OHA](https://android.googlesource.com/platform/external/honggfuzz)þ  * [__QDBI__: by Quarkslab](https://project.inria.fr/FranceJapanICST/files/2019/04/19-Kyoto-Fuzzing_Binaries_using_Dynamic_Instrumentation.pdf)þ  * [__fuzzer-test-suite__: by Google](https://github.com/google/fuzzer-test-suite)þ  * [__DeepState__: by Trail-of-Bits](https://github.com/trailofbits/deepstate)þ  * [__Quiche-HTTP/3__: by Cloudflare](https://github.com/cloudflare/quiche/pull/179)þ  * [__Bolero__: fuzz and property testing framework](https://github.com/camshaft/bolero)þ  * [__pwnmachine__: a vagrantfile for exploit development on Linux](https://github.com/kapaw/pwnmachine/commit/9cbfc6f1f9547ed2d2a5d296f6d6cd8fac0bb7e1)þ  * [__Quick700__: analyzing effectiveness of fuzzers on web browsers and web servers](https://github.com/Quick700/Quick700)þ  * [__python-fuzz__: gluing honggfuzz and python3](https://github.com/thebabush/python-hfuzz)þ  * [__Magma__: a ground-truth fuzzing benchmark](https://github.com/HexHive/magma)þ  * [__arbitrary-model-tests__: a procedural macro for testing stateful models](https://github.com/jakubadamw/arbitrary-model-tests)þ  * [__Clusterfuzz__: the fuzzing engine behind OSS-fuzz/Chrome-fuzzing](https://github.com/google/clusterfuzz/issues/1128)þ  * [__Apache HTTP Server__](https://github.com/apache/httpd/commit/d7328a07d7d293deb5ce62a60c2ce6029104ebad)þ  * [__centos-fuzz__](https://github.com/truelq/centos-fuzz)þ  * [__FLUFFI__: Fully Localized Utility For Fuzzing Instantaneously by Siemens](https://github.com/siemens/fluffi)þ  * [__Fluent Bit__: a fast log processor and forwarder for Linux](https://github.com/fluent/fluent-bit/search?q=honggfuzz&unscoped_q=honggfuzz)þ  * [__Samba__: a SMB server](https://github.com/samba-team/samba/blob/2a90202052558c945e02675d1331e65aeb15f9fa/lib/fuzzing/README.md)þ  * [__universal-fuzzing-docker__: by nnamon](https://github.com/nnamon/universal-fuzzing-docker)þ  * [__Canokey Core__: core implementations of an open-source secure key](https://github.com/canokeys/canokey-core/search?q=honggfuzz&unscoped_q=honggfuzz)þ  * [__uberfuzz2__: a cooperative fuzzing framework](https://github.com/acidghost/uberfuzz2)þ  * [__TiKV__: a distributed transactional key-value database](https://github.com/tikv/tikv/tree/99a922564face31bdb59b5b38962339f79e0015c/fuzz)þ  * [__fuzz-monitor__](https://github.com/acidghost/fuzz-monitor/search?q=honggfuzz&unscoped_q=honggfuzz)þ  * [__libmutator__: a C library intended to generate random test cases by mutating legitimate test cases](https://github.com/denandz/libmutator)þ  * [__StatZone__: a DNS zone file analyzer](https://github.com/fcambus/statzone)þ  * [__shub-fuzz/honggfuzz__: singularity image for honggfuzz](https://github.com/shub-fuzz/honggfuzz)þ  * [__Code Intelligence__: fuzzing-as-a-service](https://www.code-intelligence.com/technology.html)þ  * [__SpecFuzz__: fuzzing for Spectre vulnerabilities](https://github.com/OleksiiOleksenko/SpecFuzz)þ  * [__rcc__: a Rust C compiler](https://github.com/jyn514/rcc#testing)þ  * [__EIP1962Fuzzing__: Fuzzy testing of various EIP1962 implementations](https://github.com/matter-labs/eip1962_fuzzing)þ  * [__wasm-fuzz__: Fuzzing of wasmer](https://github.com/wasmerio/wasm-fuzz/blob/master/honggfuzz.md), [blog post](https://medium.com/wasmer/fuzz-testing-in-webassembly-vms-3a301f982e5a)þ  * [__P0__: Fuzzing ImageIO](https://googleprojectzero.blogspot.com/2020/04/fuzzing-imageio.html)þ    * [__TrapFuzz__: by P0](https://github.com/googleprojectzero/p0tools/tree/master/TrapFuzz)þ  * [__Rust's fuzztest__](https://docs.rs/crate/fuzztest)þ    * [_and multiple Rust projecs_](https://github.com/search?q=%22extern+crate+honggfuzz%22&type=Code)þþ## Contactþþ  * User mailing list: [honggfuzz@googlegroups.com](mailto:honggfuzz@googlegroups.com), sign up with [this link](https://groups.google.com/forum/#!forum/honggfuzz).þþ__This is NOT an official Google product__"
a0rtega/pafish,1147,1399,159,303,User,False,166,2,16,9,False,Pafish is a demonstration tool that employs several techniques to detect sandboxes and analysis environments in the same way as malware families do.,,6,7,0,13,23,0,0,0,30,0,0,2399,0,0,0,0,0,0,15,,222,,"# Pafishþ## (Paranoid Fish)þþPafish is a demonstration tool that employs several techniques to detect sandboxes and analysis environments in the same way as malware families do.þþThe project is open source, you can read the code of all anti-analysis checks. You can also **[download](https://github.com/a0rtega/pafish/raw/master/pafish.exe)** the executable of the latest stable version.þþIt is licensed under GNU/GPL version 3.þþ![Pafish screenshot](https://raw.githubusercontent.com/a0rtega/pafish/dev-chaos/screenshots/v057/pafish_vbox_win8.png)þþ# ScopeþþThe objective of this project is to collect usual tricks seen in malware samples. This allows us to study them, and test if our analysis environments are properly implemented.þþ# BuildþþPafish is written in C and can be built with MinGW (gcc + make).þþCheck out ""[How to build](https://github.com/a0rtega/pafish/wiki/How-to-build)"" for detailed instructions.þþ# AuthorþþAlberto Ortega (@[a0rtega](https://twitter.com/#!/a0rtega) - [profile](http://aortega.badtrace.com))"
cesanta/v7,19733,1248,93,160,Organization,False,1911,7,3,17,False,Embedded JavaScript engine for C/C++,,0,11,4,48,103,0,0,2,436,0,0,2378,0,0,0,0,0,0,58,1,,,"V7: Embedded JavaScript engineþ==============================þþ**NOTE: this project is deprecated in favor of https://github.com/cesanta/mjs**þþ[![License](https://img.shields.io/badge/license-GPL_2-green.svg)](https://github.com/cesanta/v7/blob/master/LICENSE)þþV7 is the smallest JavaScript engine written in C. V7 features are:þþ- Cross-platform: works on anything, starting from Arduino to MS Windowsþ- Small size. Compiled static size is in 40k - 120k range, RAMþ  footprint on initialization is about 800 bytes with freeze feature,þ  15k without freeze featureþ- Simple and intuitive C/C++ API. It is easy to export existing C/C++þ  functions into JavaScript environmentþ- Standard: V7 implements JavaScript 5.1þ- Usable out-of-the-box: V7 provides an auxiliary library withþ  Hardware (SPI, UART, etc), File, Crypto, Network APIþ- Source code is both ISO C and ISO C++ compliantþ- Very easy to integrate: simply copy two files: [v7.h](v7.h)þ   and [v7.c](v7.c) into your projectþþV7 makes it possible to program Internet of Things (IoT) embedded devicesþin JavaScript. V7 is a part of the full stackþ[Mongoose OS Platform](https://github.com/cesanta/mongoose-os).þþ## Examples & Documentationþþ- [Developer Centre](https://docs.cesanta.com/v7/dev) - User Guide and API referenceþ- [Examples](https://github.com/cesanta/v7/tree/master/examples) - Collection of well-commented examplesþ- [Support Forum](http://forum.cesanta.com/index.php?p=/categories/v7) - Ask questions on our support forumþþ# ContributionsþþTo submit contributions, signþ[Cesanta CLA](https://docs.cesanta.com/contributors_la.shtml)þand send GitHub pull request. You retain the copyright on your contributions.þþ# LicensingþþV7 is released under commercial and [GNU GPL v.2](http://www.gnu.org/licenses/old-licenses/gpl-2.0.html) open source licenses.þþCommercial Projects:þOnce your project becomes commercialised GPLv2 licensing dictates that you need to either open your source fully or purchase a commercial license. Cesanta offer full, royalty-free commercial licenses without any GPL restrictions. If your needs require a custom license, we’d be happy to work on a solution with you. [Contact us for pricing.] (https://www.cesanta.com/contact)þþPrototyping:þWhile your project is still in prototyping stage and not for sale, you can use V7’s open source code without license restrictions."
sgminer-dev/sgminer,16123,565,129,854,Organization,False,6498,6,13,84,False,Scrypt GPU miner,,0,16,4,137,244,0,1,17,107,0,0,3400,0,0,0,0,0,0,1,0,,,"# sgminerþþþ## IntroductionþþThis is a multi-threaded multi-pool GPU miner with ATI GPU monitoring,þ(over)clocking and fanspeed support for scrypt-based cryptocurrency. It isþbased on cgminer by Con Kolivas (ckolivas), which is in turn based onþcpuminer by Jeff Garzik (jgarzik).þþ**releases**: https://github.com/sgminer-dev/sgminer/releasesþþ**git tree**: https://github.com/sgminer-dev/sgminerþþ**bugtracker**: https://github.com/sgminer-dev/sgminer/issuesþþ**irc**: `#sgminer` and `#sgminer-dev` on freenodeþþ**mailing lists**: https://sourceforge.net/p/sgminer/mailman/þþLicense: GPLv3.  See `COPYING` for details.þþþ## DocumentationþþDocumentation is available in directory `doc`. It is organised by topics:þþ* `API` for the RPC API specification;þ* `configuration.md` for (largely incomplete) detailed information on allþ  configuration options;þ* `FAQ.md` for frequently asked questions;þ* `GPU` for semi-obsolete information on GPU configuration options and miningþ  SHA256d-based coins;þ* `kernel.md` for OpenCL kernel-related information, including developmentþ  procedure;þ* `MINING.md` for how to find the right balance in GPU configuration to mineþ  Scrypt-based coins efficiently;þ* `windows-build.txt` for information on how to build on Windows.þþNote that **most of the documentation is outdated or incomplete**. Ifþyou want to contribute, fork this repository, update as needed, andþsubmit a pull request.þþþ## Buildingþþ### DependenciesþþMandatory:þþ* [curl dev library](http://curl.haxx.se/libcurl/) - `libcurl4-openssl-dev` on Debianþ* [pkg-config](http://www.freedesktop.org/wiki/Software/pkg-config)þ* [libtool](http://www.gnu.org/software/libtool/)þ* [AMD APP SDK](http://developer.amd.com/tools-and-sdks/heterogeneous-computing/amd-accelerated-parallel-processing-app-sdk/downloads/) - available under various names as a package on different GNU/Linux distributionsþþOptional:þþ* curses dev library - `libncurses5-dev` on Debian or `libpdcurses` on WIN32, for text user interfaceþ* [AMD ADL SDK](http://developer.amd.com/tools-and-sdks/graphics-development/display-library-adl-sdk/) - version 6, required for ATI GPU monitoring & clockingþþIf building from git:þþ* autoconfþ* automakeþþsgminer-specific configuration options:þþ    --disable-adl           Override detection and disable building with adlþ --disable-adl-checksþ    --without-curses        Do not compile support for curses TUIþþ#### Debian Exampleþþ    apt-get install libcurl4-openssl-dev pkg-config libtool libncurses5-devþAMD APP SDK and AMD ADL SDK must be downloaded from the amd websites.þþ### *nix build instructionsþþIf needed, place include headers (`*.h` files) from `ADL_SDK_*<VERSION>*.zip` in `sgminer/ADL_SDK`.þþThen:þþ    git submodule initþ    git submodule updateþ    autoreconf -iþ    CFLAGS=""-O2 -Wall -march=native -std=gnu99"" ./configure <options>þ    makeþþTo compile a version that can be used accross machines, removeþ`-march=native`.þþTo compile a debug version, replace `-O2` with `-ggdb`.þþDepending on your environment, replace `-std=gnu99` with `-std=c99`.þþSystemwide installation is optional. You may run `sgminer` from the buildþdirectory directly, or `make install` if you wish to installþ`sgminer` to a system location or a location you specified with `--prefix`.þþ### Windows build instructionsþþSee `doc/windows-build.txt` for MinGW compilation and cross-compiation,þ`doc/cygwin-build.txt` for building using Cygwin, or use the providedþ`winbuild` Microsoft Visual Studio project (tested on MSVS2010), withþinstructions in `winbuild/README.txt`.þþþ## Basic Usageþþ**WARNING**: documentation below this point has not been updated since theþfork.þþAfter saving configuration from the menu, you do not need to give sgminerþany arguments and it will load your configuration.þþAny configuration file may also contain a singleþþ    ""include"" : ""filename""þþto recursively include another configuration file.þþWriting the configuration will save all settings from all files in theþoutput.þþSingle pool:þþsgminer -o http://pool:port -u username -p passwordþþMultiple pools:þþsgminer -o http://pool1:port -u pool1username -p pool1password -o http://pool2:port -u pool2usernmae -p pool2passwordþþSingle pool with a standard http proxy, regular desktop:þþsgminer -o ""http:proxy:port|http://pool:port"" -u username -p passwordþþSingle pool with a socks5 proxy, regular desktop:þþsgminer -o ""socks5:proxy:port|http://pool:port"" -u username -p passwordþþSingle pool with stratum protocol support:þþsgminer -o stratum+tcp://pool:port -u username -p passwordþþThe list of proxy types are:þ http:    standard http 1.1 proxyþ http0:   http 1.0 proxyþ socks4:  socks4 proxyþ socks5:  socks5 proxyþ socks4a: socks4a proxyþ socks5h: socks5 proxy using a hostnameþþIf you compile sgminer with a version of CURL before 7.19.4 then some ofþthe above will not be available. All are available since CURL versionþ7.19.4.þþIf you specify the --socks-proxy option to sgminer, it will only beþapplied to all pools that don't specify their own proxy setting likeþabove.þþFor more advanced usage , run `sgminer --help`.þþSee `doc/GPU` for more information regarding GPU mining andþ`doc/SCRYPT` for more information regarding Scrypt mining.þþþ## Runtime usageþþThe following options are available while running with a single keypress:þþ[P]ool management [G]PU management [S]ettings [D]isplay options [Q]uitþþP gives you:þþCurrent pool management strategy: Failoverþ[F]ailover only disabledþ[A]dd pool [R]emove pool [D]isable pool [E]nable poolþ[C]hange management strategy [S]witch pool [I]nformationþþþS gives you:þþ[Q]ueue: 1þ[S]cantime: 60þ[E]xpiry: 120þ[W]rite config fileþ[C]gminer restartþþþD gives you:þþ[N]ormal [C]lear [S]ilent mode (disable all output)þ[D]ebug:offþ[P]er-device:offþ[Q]uiet:offþ[V]erbose:offþ[R]PC debug:offþ[W]orkTime details:offþco[M]pact: offþ[L]og interval:5þþþQ quits the application.þþþG gives you something like:þþGPU 0: [124.2 / 191.3 Mh/s] [A:77  R:33  HW:0  U:1.73/m  WU 1.73/m]þTemp: 67.0 CþFan Speed: 35% (2500 RPM)þEngine Clock: 960 MHzþMemory Clock: 480 MhzþVddc: 1.200 VþActivity: 93%þPowertune: 0%þLast initialised: [2011-09-06 12:03:56]þThread 0: 62.4 Mh/s Enabled ALIVEþThread 1: 60.2 Mh/s Enabled ALIVEþþ[E]nable [D]isable [R]estart GPU [C]hange settingsþOr press any other key to continueþþþThe running log shows output like this:þþ [2012-10-12 18:02:20] Accepted f0c05469 Diff 1/1 GPU 0 pool 1þ [2012-10-12 18:02:22] Accepted 218ac982 Diff 7/1 GPU 1 pool 1þ [2012-10-12 18:02:23] Accepted d8300795 Diff 1/1 GPU 3 pool 1þ [2012-10-12 18:02:24] Accepted 122c1ff1 Diff 14/1 GPU 1 pool 1þþThe 8 byte hex value are the 2nd 8 bytes of the share being submitted to theþpool. The 2 diff values are the actual difficulty target that share reachedþfollowed by the difficulty target the pool is currently asking for.þþThe output line shows the following:þ(5s):1713.6 (avg):1707.8 Mh/s | A:729  R:8  HW:0  WU:22.53/mþþEach column is as follows:þ5s:  A 5 second exponentially decaying average hash rateþavg: An all time average hash rateþA:  The total difficulty of Accepted sharesþR:  The total difficulty of Rejected sharesþHW:  The number of HardWare errorsþWU:  The Work Utility defined as the number of diff1 shares work / minuteþ     (accepted or rejected).þþ GPU 1: 73.5C 2551RPM | 427.3/443.0Mh/s | A:8 R:0 HW:0 WU:4.39/mþþEach column is as follows:þTemperature (if supported)þFanspeed (if supported)þA 5 second exponentially decaying average hash rateþAn all time average hash rateþThe total difficulty of accepted sharesþThe total difficulty of rejected sharesþThe number of hardware erorrsþThe work utility defined as the number of diff1 shares work / minuteþþThe sgminer status line shows:þ ST: 1  SS: 0  NB: 1  LW: 8  GF: 1  RF: 1þþST is STaged work items (ready to use).þSS is Stale Shares discarded (detected and not submitted so don't count as rejects)þNB is New Blocks detected on the networkþLW is Locally generated Work itemsþGF is Getwork Fail Occasions (server slow to provide work)þRF is Remote Fail occasions (server slow to accept work)þþThe block display shows:þBlock: 0074c5e482e34a506d2a051a...  Started: [17:17:22]  Best share: 2.71KþþThis shows a short stretch of the current block, when the new block started,þand the all time best difficulty share you've found since starting sgminerþthis time.þþþ## Multipoolþþ### Failover strategiesþþA number of different strategies for dealing with multipool setups areþavailable. Each has their advantages and disadvantages so multiple strategiesþare available by user choice, as per the following list:þþ#### FailoverþþThe default strategy is failover. This means that if you input a number ofþpools, it will try to use them as a priority list, moving away from the 1stþto the 2nd, 2nd to 3rd and so on. If any of the earlier pools recover, it willþmove back to the higher priority ones.þþ#### Round robinþþThis strategy only moves from one pool to the next when the current one fallsþidle and makes no attempt to move otherwise.þþ#### RotateþþThis strategy moves at user-defined intervals from one active pool to the next,þskipping pools that are idle.þþ#### Load balanceþþThis strategy sends work to all the pools on a quota basis. By default, allþpools are allocated equal quotas unless specified with --quota. Thisþapportioning of work is based on work handed out, not shares returned so isþindependent of difficulty targets or rejected shares. While a pool is disabledþor dead, its quota is dropped until it is re-enabled. Quotas are forwardþlooking, so if the quota is changed on the fly, it only affects future work.þIf all pools are set to zero quota or all pools with quota are dead, it willþfall back to a failover mode. See quota below for more information.þþThe failover-only flag has special meaning in combination with load-balanceþmode and it will distribute quota back to priority pool 0 from any pools thatþare unable to provide work for any reason so as to maintain quota ratiosþbetween the rest of the pools.þþ#### BalanceþþThis strategy monitors the amount of difficulty 1 shares solved for each poolþand uses it to try to end up doing the same amount of work for all pools.þþþ### QuotasþþThe load-balance multipool strategy works off a quota based scheduler. Theþquotas handed out by default are equal, but the user is allowed to specify anyþarbitrary ratio of quotas. For example, if all the quota values add up to 100,þeach quota value will be a percentage, but if 2 pools are specified and pool0þis given a quota of 1 and pool1 is given a quota of 9, pool0 will get 10% ofþthe work and pool1 will get 90%. Quotas can be changed on the fly by the API,þand do not act retrospectively. Setting a quota to zero will effectivelyþdisable that pool unless all other pools are disabled or dead. In thatþscenario, load-balance falls back to regular failover priority-based strategy.þWhile a pool is dead, it loses its quota and no attempt is made to catch upþwhen it comes back to life.þþTo specify quotas on the command line, pools should be specified with aþsemicolon separated --quota(or -U) entry instead of --url. Pools specified withþ--url are given a nominal quota value of 1 and entries can be mixed.þþFor example:þ--url poola:porta -u usernamea -p passa --quota ""2;poolb:portb"" -u usernameb -p passbþWill give poola 1/3 of the work and poolb 2/3 of the work.þþWriting configuration files with quotas is likewise supported. To useþthe above quotas in a configuration file they would be specified thus:þþ    ""pools"" : [þ        {þ                ""url"" : ""poola:porta"",þ                ""user"" : ""usernamea"",þ                ""pass"" : ""passa""þ        },þ        {þ                ""quota"" : ""2;poolb:portb"",þ                ""user"" : ""usernameb"",þ                ""pass"" : ""passb""þ        }þ    ]þþþ### Extra File ConfigurationþþIf you want to store a number of pools in your configuration file, butþdon't always want them automatically enabled at start up (or restart),þthen the ""state"" option with a value of ""disabled"" can be used:þþ    ""pools"" : [þ        {þ                ""url"" : ""poola:porta"",þ                ""user"" : ""usernamea"",þ                ""pass"" : ""passa""þ        },þ        {þ                ""quota"" : ""2;poolb:portb"",þ                ""user"" : ""usernameb"",þ                ""pass"" : ""passb"",þ                ""state"" : ""disabled""þ        }þ    ]þþIt is then trivial to change the ""state"" setting to ""enabled"" in theþconfiguration file at anytime and then restart the miner (see below).þYou can enable the pool whilst the miner is still running ('p' followedþby 'e' followed by pool number) - but the pool will still be disabled onþrestart if the config file is not changed.þþ""state"" can also be set to ""hidden"". This allows the json file toþcontain a large number of pools, of which some could be automaticallyþculled at start up. This makes it easy to swap pools in and out of theþruntime selection, without having a large list of pools cluttering upþthe display.þþ    ""pools"" : [þ        {þ                ""poolname"" : ""Main Pool"",þ                ""url"" : ""poola:porta"",þ                ""user"" : ""usernamea"",þ                ""pass"" : ""passa"",þ                ""state"" : ""disabled""þ        },þ        {þ                ""poolname"" : ""Joe's Weekend Pool"",þ                ""quota"" : ""2;poolb:portb"",þ                ""user"" : ""usernameb"",þ                ""pass"" : ""passb"",þ                ""state"" : ""hidden""þ        }þ    ]þþThese options are considered experimental and therefore will NOT beþcreated when the 'Write config file' option is used ('s' followed byþ'w').þþA restart of the miner ('s' followed by 'c') will reload the configþfile and any changes that may have been made.þþþ## Loggingþþsgminer will log to stderr if it detects stderr is being redirected to aþfile. To enable logging simply append `2>logfile.txt` to your command lineþand `logfile.txt` will contain all debug output unless you set `debug-log`þto `false`, in which case it will only contain output at the log level youþspecified (notice by default).þþThere is also the -m option on Linux which will spawn a command of your choiceþand pipe the output directly to that command.þþThe WorkTime details 'debug' option adds details on the end of each lineþdisplayed for Accepted or Rejected work done. An example would be:þþ <-00000059.ed4834a3 M:X D:1.0 G:17:02:38:0.405 C:1.855 (2.995) W:3.440 (0.000) S:0.461 R:17:02:47þþThe first 2 hex codes are the previous block hash, the rest are reported inþseconds unless stated otherwise:þThe previous hash is followed by the getwork mode used M:X where X is one ofþP:Pool, T:Test Pool, L:LP or B:Benchmark,þthen D:d.ddd is the difficulty required to get a share from the work,þthen G:hh:mm:ss:n.nnn, which is when the getwork or LP was sent to the pool andþthe n.nnn is how long it took to reply,þfollowed by 'O' on it's own if it is an original getwork, or 'C:n.nnn' if it wasþa clone with n.nnn stating how long after the work was recieved that it was cloned,þ(m.mmm) is how long from when the original work was received until work started,þW:n.nnn is how long the work took to process until it was ready to submit,þ(m.mmm) is how long from ready to submit to actually doing the submit, this isþusually 0.000 unless there was a problem with submitting the work,þS:n.nnn is how long it took to submit the completed work and await the reply,þR:hh:mm:ss is the actual time the work submit reply was receivedþþIf you start sgminer with the --sharelog option, you can get detailedþinformation for each share found. The argument to the option may be ""-"" forþstandard output (not advisable with the ncurses UI), any valid positive numberþfor that file descriptor, or a filename.þþTo log share data to a file named ""share.log"", you can use either:þ./sgminer --sharelog 50 -o xxx -u yyy -p zzz 50>share.logþ./sgminer --sharelog share.log -o xxx -u yyy -p zzzþþFor every share found, data will be logged in a CSV (Comma Separated Value)þformat:þ    timestamp,disposition,target,pool,dev,thr,sharehash,sharedataþFor example (this is wrapped, but it's all on one line for real):þ    1335313090,reject,þ    ffffffffffffffffffffffffffffffffffffffffffffffffffffffff00000000,þ    http://localhost:8337,GPU0,0,þ    6f983c918f3299b58febf95ec4d0c7094ed634bc13754553ec34fc3800000000,þ    00000001a0980aff4ce4a96d53f4b89a2d5f0e765c978640fe24372a000001c5þ    000000004a4366808f81d44f26df3d69d7dc4b3473385930462d9ab707b50498þ    f681634a4f1f63d01a0cd43fb338000000000080000000000000000000000000þ    0000000000000000000000000000000000000000000000000000000080020000"
haproxy-unofficial-obsolete-mirrors/haproxy,19218,919,98,317,Organization,False,6096,1,196,120,False,UNOFFICIAL fork of haproxy development repository - ISSUE REPORTS ARE IGNORED!,http://haproxy.1wt.eu/,0,6,0,,,,,0,20,0,0,4478,0,0,0,0,0,0,9,0,,,
eunyoung14/mtcp,123163,1484,151,365,Organization,False,384,8,2,18,False,mTCP: A Highly Scalable User-level TCP Stack for Multicore Systems,,0,8,0,62,189,14,5,3,45,3,2,2266,0,0,0,0,0,0,2,2,,mtcp-stack/mtcp,"[![Build Status](https://travis-ci.org/eunyoung14/mtcp.svg?branch=master)](https://travis-ci.org/eunyoung14/mtcp)þ[![Build Status](https://scan.coverity.com/projects/11896/badge.svg)](https://scan.coverity.com/projects/eunyoung14-mtcp)þþ# READMEþþmTCP is a highly scalable user-level TCP stack for multicore systems. þmTCP source code is distributed under the Modified BSD License. For þmore detail, please refer to the LICENSE. The license term of io_engine þdriver and ported applications may differ from the mTCP’s.þþ## PrerequisitesþþWe require the following libraries to run mTCP.þ- `libdpdk` (Intel's DPDK package*) or `libps` (PacketShader I/O engine library) or `netmap` driver þ- `libnuma`þ- `libpthread`þ- `librt`þ- `libgmp` (for DPDK/ONVM driver)þþCompling PSIO/DPDK/NETMAP/ONVM driver requires kernel headers.þ- For Debian/Ubuntu, try ``apt-get install linux-headers-$(uname -r)``þþWe have modified the dpdk package to export net_device stat data þ(for Intel-based Ethernet adapters only) to the OS. To achieve this, we haveþcreated a new LKM dpdk-iface-kmow. We also modified þ``mk/rte.app.mk`` file to ease the compilationþprocess of mTCP applications. We recommend using our package for DPDKþinstallation.þþ### CCP supportþþYou can optionally use [CCP](https://ccp-project.github.io/)'s congestion þcontrol implementation rather than mTCP's. You'll have wider selection of þcongestion control algorithms with CCP.þ(Currently this feature is experimental and under revision.)þþUsing [CCP](https://ccp-project.github.io/) for congestion control (disabled byþdefault), requires the CCP library. If you would like to enable CCP, simply runþconfigure script with `--enable-ccp` option.þþ1. Install Rust. Any installation method should be fine. We recommend usingþ   rustup:þþ    ```bashþ    curl https://sh.rustup.rs -sSf | sh -- -y -v --default-toolchain nightlyþ    ````þþ2. Install the CCP command line utility:þþ    ```bashþ    cargo install portus --bin ccpþ    ```þþ3. Build the library (comes with Reno and Cubic by default, use `ccp get` to add others):þþ    ```þ    ccp makelibþ    ```þþ4. You will also need to link your application against `-lccp` and `-lstartccp` as demonstrated in apps/example/Makefie.inþþ## Included directoriesþþmtcp: mtcp source code directoryþ- mtcp/src: source codeþ- mtcp/src/include: mTCP’s internal header filesþ- mtcp/lib: library fileþ- mtcp/include: header files that applications will useþþio_engine: event-driven packet I/O engine (io_engine)þ- io_engine/driver - driver source codeþ- io_engine/lib - io_engine libraryþ- io_engine/include - io_engine header filesþ- io_engine/samples - sample io_engine applications (not mTCP’s)þþdpdk - Intel's Data Plane Development Kitþ- dpdk/...þþapps: mTCP applicationsþ- apps/example - example applications (see README)þ- apps/lighttpd-1.4.32 - mTCP-ported lighttpd (see INSTALL)þ- apps/apache_benchmark - mTCP-ported apache benchmark (ab) (see README-mtcp)þþutil: useful source code for applicationsþþconfig: sample mTCP configuration files (may not be necessary)þþþ## Install guidesþþmTCP can be prepared in four ways.þþ### ***DPDK VERSION***þþ1. Download DPDK submodule.þþ    ```bashþ    git submodule initþ    git submodule updateþ    ```þþ2. Setup DPDK.þþ    ```bashþ ./setup_mtcp_dpdk_env.sh [<path to $RTE_SDK>]þ    ```þþ    - Press [15] to compile x86_64-native-linuxapp-gcc versionþ    - Press [18] to install igb_uio driver for Intel NICsþ    - Press [22] to setup 2048 2MB hugepagesþ    - Press [24] to register the Ethernet portsþ    - Press [35] to quit the toolþþ    - Only those devices will work with DPDK drivers that are listedþ      on this page: http://dpdk.org/doc/nics. Please make sure that yourþ      NIC is compatible before moving on to the next step.þþ    - We use `dpdk/` submodule as our DPDK driver. FYI, you can pass a differentþ      dpdk source directory as command line argument.þþ3. Bring the dpdk compatible interfaces up, andþ   then set RTE_SDK and RTE_TARGET environment variables. If you are using Intelþ   NICs, the interfaces will have dpdk prefix.þþ     ```bashþ    sudo ifconfig dpdk0 x.x.x.x netmask 255.255.255.0 upþ    export RTE_SDK=`echo $PWD`/dpdkþ    export RTE_TARGET=x86_64-native-linuxapp-gccþ     ```þþ4. Setup mtcp library:þþ    ```bashþ    ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGETþ    makeþ    ```þþ    - By default, mTCP assumes that there are 16 CPUs in your system.þ      You can set the CPU limit, e.g. on a 32-core system, by using the following command:þþ        ```bashþ        ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET CFLAGS=""-DMAX_CPUS=32""þ        ```þ    Please note that your NIC should support RSS queues equal to the MAX_CPUS valueþ    (since mTCP expects a one-to-one RSS queue to CPU binding).þ    þ    - In case `./configure` script prints an error, run theþ      following command; and then re-do step-4 (configure again):þ        ```bashþ        autoreconf -ivfþ        ```þ   þ    - checksum offloading in the NIC is now ENABLED (by default)!!!þ        - this only works for dpdk at the momentþ        - use ```./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --disable-hwcsum``` to disable checksum offloading.þ    - check `libmtcp.a` in `mtcp/lib`þ    - check header files in `mtcp/include`þ    - check example binary files in `apps/example`þþ5. Check the configurations in `apps/example`þ   - `epserver.conf` for server-side configurationþ   - `epwget.conf` for client-side configurationþ   - you may write your own configuration file for your applicationþþ6. Run the applications!þþ7. You can revert back all your changes by running the following script.þþ    ```bashþ    ./setup_linux_env.sh [<path to $RTE_SDK>]þ    ```þ   þ    - Press [29] to unbind the Ethernet portsþ    - Press [30] to remove igb_uio.ko driverþ    - Press [33] to remove hugepage mappingsþ    - Press [34] to quit the toolþþþ### ***PSIO VERSION***þþ1. make in io_engine/driver:þþ    ```bashþ    makeþ    ```þþ    - check ps_ixgbe.koþ    - please note that psio only runs on linux-2.6.x kernelsþ      (linux-2.6.32 ~ linux-2.6.38)þþ2. install the driver:þ   þ   ```bashþ   ./install.py <# cores> <# cores>þ   ```þþ    - refer to http://shader.kaist.edu/packetshader/io_engine/þ    - you may need to change the ip address in install.py:46þþ3. Setup mtcp library:þ   þ    ```bashþ    ./configure --with-psio-lib=<$path_to_ioengine>þ    # e.g. ./configure --with-psio-lib=`echo $PWD`/io_engineþ    makeþ    ```þþ    - By default, mTCP assumes that there are 16 CPUs in your system.þ      You can set the CPU limit, e.g. on a 8-core system, by using the following command:þþ        ```bashþ        ./configure --with-psio-lib=`echo $PWD`/io_engine CFLAGS=""-DMAX_CPUS=8""þ        ```þ    þ    Please note that your NIC should support RSS queues equal to the MAX_CPUS valueþ    (since mTCP expects a one-to-one RSS queue to CPU binding).þþ    - In case `./configure` script prints an error, run theþ      following command; and then re-do step-3 (configure again):þþ        ```bashþ        autoreconf -ivfþ        ```þþ    - check `libmtcp.a` in `mtcp/lib`þ    - check header files in `mtcp/include`þ    - check example binary files in `apps/example`þþ4. Check the configurations in `apps/example`þ   - `epserver.conf` for server-side configurationþ   - `epwget.conf` for client-side configurationþ   - you may write your own configuration file for your applicationþþ5. Run the applications!þþþ### ***ONVM VERSION***þþ***NEW***: Now you can run mTCP applications (server + client) locally.þA local setup is useful when only 1 machine is available for the experiment. þONVM configurations are placed as `.conf` files in apps/example directory.þONVM basics are explained in https://github.com/sdnfv/openNetVM.þþ**Before running the applications make sure that onvm_mgr is running.**  þ*Also, no core overlap between applications and onvm_mgr is allowed.*þþ1. [Install openNetVM following these instructions](https://github.com/sdnfv/openNetVM/blob/master/docs/Install.md)þþ2. Set up the dpdk interfaces:þþ    ```bashþ ./setup_mtcp_onvm_env.shþ    ```þþ3. Next bring the dpdk-registered interfaces up. This can be setup using:  þþ    ```bashþ    sudo ifconfig dpdk0 x.x.x.x netmask 255.255.255.0 upþ    ```þþ4. Setup mtcp libraryþ    ```bashþ    ./configure --with-dpdk-lib=$<path_to_dpdk> --with-onvm-lib=$<path_to_onvm_lib>þ    # e.g. ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --with-onvm-lib=`echo $ONVM_HOME`/onvmþ    makeþ    ```þþ    - By default, mTCP assumes that there are 16 CPUs in your system.þ    You can set the CPU limit, e.g. on a 32-core system, by using the following command:þ    þ        ```bashþ        ./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --with-onvm-lib=$<path_to_onvm_lib> CFLAGS=""-DMAX_CPUS=32""þ        ```þþ    Please note that your NIC should support RSS queues equal to the MAX_CPUS valueþ    (since mTCP expects a one-to-one RSS queue to CPU binding).þ    þ    - In case `./configure` script prints an error, run theþ    following command; and then re-do step-4 (configure again):þ    þ        ```bashþ        autoreconf -ivfþ        ```þþ    - checksum offloading in the NIC is now ENABLED (by default)!!!þ    - this only works for dpdk at the momentþ    - use ```./configure --with-dpdk-lib=$RTE_SDK/$RTE_TARGET --with-onvm-lib=$<path_to_onvm_lib> --disable-hwcsum``` to disable checksum offloading.þ    - check `libmtcp.a` in `mtcp/lib`þ    - check header files in `mtcp/include`þ    - check example binary files in `apps/example`þþ5. Check the configurations in `apps/example`þ   - `epserver.conf` for server-side configurationþ   - `epwget.conf` for client-side configurationþ   - you may write your own configuration file for your applicationþþ6. Run the applications!þþ7. You can revert back all your changes by running the following script.þþ    ```bashþ    ./setup_linux_env.shþ    ```þ   þ    - Press [29] to unbind the Ethernet portsþ    - Press [30] to remove igb_uio.ko driverþ    - Press [33] to remove hugepage mappingsþ    - Press [34] to quit the toolþþ**Notes**þþOnce you have started onvm_mgr, sometimes an mTCP application may fail to get launched dueþto an error resembling the one mentioned below:þþ- ```EAL: FATAL: Cannot init memory```þ- ``` Cannot mmap memory for rte_config at [0x7ffff7fb6000], got [0x7ffff7e74000] - please use '--base-virtaddr' option```þ- ```EAL: Cannot mmap device resource file /sys/bus/pci/devices/0000:06:00.0/resource3 to address: 0x7ffff7ff1000```þþTo prevent this, use the base virtual address parameter to run the ONVM manager (core list arg `0xf8` isn't actually used by mtcp NFs but is required), e.g.:þþ```bashþcd openNetVM/onvm  þ./go.sh 1,2,3 1 0xf8 -s stdout -a 0x7f000000000 þ```þþ### ***NETMAP VERSION***þþSee README.netmap for details.þþþ## Tested environmentsþþmTCP runs on Linux-based operating systems (2.6.x for PSIO) with generic þx86_64 CPUs, but to help evaluation, we provide our tested environments þas follows.þþ    Intel Xeon E5-2690 octacore CPU @ 2.90 GHz 32 GB of RAM (4 memory channels)þ    10 GbE NIC with Intel 82599 chipset (specifically Intel X520-DA2)þ    Debian 6.0.7 (Linux 2.6.32-5-amd64)þþ    Intel Core i7-3770 quadcore CPU @ 3.40 GHz 16 GB of RAM (2 memory channels)þ    10 GbE NIC with Intel 82599 chipset (specifically Intel X520-DA2)þ    Ubuntu 10.04 (Linux 2.6.32-47)þþEvent-driven PacketShader I/O engine (extended io_engine-0.2)þþ- PSIO is currently only compatible with Linux-2.6.þþWe tested the DPDK version (polling driver) with Linux-3.13.0 kernel.þþ## Notesþþ1. mTCP currently runs with fixed memory pools. That means, the size ofþ   TCP receive and send buffers are fixed at the startup and does not þ   increase dynamically. This could be performance limit to the large þ   long-lived connections. Be sure to configure the buffer size þ   appropriately to your size of workload.þþ2. The client side of mTCP supports mtcp_init_rss() to create an þ   address pool that can be used to fetch available address space in þ   O(1). To easily congest the server side, this function should be þ   called at the application startup.þþ3. The supported socket options are limited for right now. Please refer þ   to the mtcp/src/api.c for more detail.þþ4. The counterpart of mTCP should enable TCP timestamp.þþ5. mTCP has been tested with the following Ethernet adapters:þþ    1. Intel-82598       ixgbe          (Max-queue-limit: 16)þ    2. Intel-82599       ixgbe          (Max-queue-limit: 16)þ    3. Intel-I350        igb            (Max-queue-limit: 08)þ    4. Intel-X710        i40e           (Max-queue-limit: ~)þ    5. Intel-X722        i40e           (Max-queue-limit: ~)þ þ## Frequently asked questionsþþ1. How can I quit the application?þ    - Use ^C to gracefully shutdown the application. Two consecutive þ    ^C (separated by 1 sec) will force quit.þþ2. My application doesn't use the address specified from ifconfig.þ    - For some Linux distros(e.g. Ubuntu), NetworkManager may re-assignþ    a different IP address, or delete the assigned IP address.þþ    - Disable NetworkManager temporarily if that's the case.þ    NetworkManager will be re-enabled upon reboot.þþ     ```bashþ    sudo service network-manager stopþ     ```þþ3. Can I statically set the routing or arp table?þ    - Yes, mTCP allows static route and arp configuration. Go to the þ    config directory and see sample_route.conf or sample_arp.conf. þ    Copy and adapt it to your condition and link (ln -s) the config þ    directory to the application directory. mTCP will find þ    config/route.conf and config/arp.conf for static configuration.þþ## Cautionþþ1. Do not remove I/O driver (```ps_ixgbe/igb_uio```) while running mTCP þ   applications. The application will panic!þþ2. Use the ps_ixgbe/dpdk driver contained in this package, not the one þ   from some other place (e.g., from io_engine github).þþ## ContactsþþGitHub issue board is the preferred way to report bugs and ask questions about mTCP.þþ***CONTACTS FOR THE AUTHORS***þþ    User mailing list <mtcp-user at list.ndsl.kaist.edu>þ    EunYoung Jeong <notav at ndsl.kaist.edu>þ    M. Asim Jamshed <ajamshed at ndsl.kaist.edu>"
jgarff/rpi_ws281x,362,1225,107,465,User,False,211,3,0,49,False,Userspace Raspberry Pi PWM library for WS281X LEDs,,0,8,0,67,216,8,4,2,117,1,5,2112,4,4,20,13,0,0,2,,63,,"rpi_ws281xþ==========þþUserspace Raspberry Pi library for controlling WS281X LEDs.þThis includes WS2812 and SK6812RGB RGB LEDsþPreliminary support is now included for SK6812RGBW LEDs (yes, RGB + W)þThe LEDs can be controlled by either the PWM (2 independent channels)þor PCM controller (1 channel) or the SPI interface (1 channel).þþ### Bindings:þþLanguage-specific bindings for rpi_ws281x are available in:þþ* Python - https://github.com/rpi-ws281x/rpi-ws281x-pythonþ* Rust - https://github.com/rpi-ws281x/rpi-ws281x-rustþ* Powershell - https://github.com/rpi-ws281x/rpi-ws281x-powershellþ* Java - https://github.com/rpi-ws281x/rpi-ws281x-javaþ* CSharp - https://github.com/rpi-ws281x/rpi-ws281x-csharpþ* Go - https://github.com/rpi-ws281x/rpi-ws281x-goþþ### Background:þþThe BCM2835 in the Raspberry Pi has both a PWM and a PCM module thatþare well suited to driving individually controllable WS281X LEDs.þUsing the DMA, PWM or PCM FIFO, and serial mode in the PWM, it'sþpossible to control almost any number of WS281X LEDs in a chain connectedþto the appropriate output pin.þFor SPI the Raspbian spidev driver is used (`/dev/spidev0.0`).þThis library and test program set the clock rate to 3X the desired outputþfrequency and creates a bit pattern in RAM from an array of colors whereþeach bit is represented by 3 bits as follows.þþ    Bit 1 - 1 1 0þ    Bit 0 - 1 0 0þþþ### GPIO Usage:þþThe GPIOs that can be used are limited by the hardware of the Pi and willþvary based on the method used to drive them (PWM, PCM or SPI).þBeware that the GPIO numbers are not the same as the physical pin numbersþon the header.þþPWM:þ```þ        PWM0, which can be set to use GPIOs 12, 18, 40, and 52.þ        Only 12 (pin 32) and 18 (pin 12) are available on the B+/2B/3Bþþ        PWM1 which can be set to use GPIOs 13, 19, 41, 45 and 53.þ        Only 13 is available on the B+/2B/PiZero/3B, on pin 33þ```þþPCM:þ```þ        PCM_DOUT, which can be set to use GPIOs 21 and 31.þ        Only 21 is available on the B+/2B/PiZero/3B, on pin 40.þ```þþSPI:þ```þ        SPI0-MOSI is available on GPIOs 10 and 38.þ        Only GPIO 10 is available on all models.þ        See also note for RPi 3 below.þ```þþþ### Power and voltage requirementsþþWS281X LEDs are generally driven at 5V. Depending on your actualþLED model and data line length you might be able to successfully driveþthe data input with 3.3V. However in the general case you probablyþwant to use a level shifter to convert from the Raspberry Pi GPIO/PWM to 5V.þþIt is also possible to run the LEDs from a 3.3V - 3.6V power source, andþconnect the GPIO directly at a cost of brightness, but this isn'tþrecommended.þþThe test program is designed to drive a 8x8 grid of LEDs e.g.fromþAdafruit (http://www.adafruit.com/products/1487) or Pimoroniþ(https://shop.pimoroni.com/products/unicorn-hat).þPlease see the Adafruit and Pimoroni websites for more information.þþKnow what you're doing with the hardware and electricity.  I take noþreponsibility for damage, harm, or mistakes.þþ### Build:þþ- Install Scons (on raspbian, `apt-get install scons`).þ- Make sure to adjust the parameters in main.c to suit your hardware.þ  - Signal rate (400kHz to 800kHz).  Default 800kHz.þ  - ledstring.invert=1 if using a inverting level shifter.þ  - Width and height of LED matrix (height=1 for LED string).þ- Type `scons` from inside the source directory.þþ### Running:þþ- Type `sudo ./test` (default uses PWM channel 0).þ- That's it.  You should see a moving rainbow scroll across theþ  display.þ- More options are available, `./test -h` should show them:þ```þ./test version 1.1.0þUsage: ./testþ-h (--help)    - this informationþ-s (--strip)   - strip type - rgb, grb, gbr, rgbwþ-x (--width)   - matrix width (default 8)þ-y (--height)  - matrix height (default 8)þ-d (--dma)     - dma channel to use (default 10)þ-g (--gpio)    - GPIO to useþ                 If omitted, default is 18 (PWM0)þ-i (--invert)  - invert pin output (pulse LOW)þ-c (--clear)   - clear matrix on exit.þ-v (--version) - version informationþ```þþ### Important warning about DMA channelsþþYou must make sure that the DMA channel you choose to use for the LEDs is not [already in use](https://www.raspberrypi.org/forums/viewtopic.php?p=609380#p609380) by the operating system.þþFor example, **using DMA channel 5 [will cause](https://github.com/jgarff/rpi_ws281x/issues/224) filesystem corruption** on the Raspberry Pi 3 Model B.þþThe default DMA channel (10) should be safe for the Raspberry Pi 3 Model B, but this may change in future software releases.þþ### Limitations:þþ#### PWMþþSince this library and the onboard Raspberry Pi audioþboth use the PWM, they cannot be used together.  You will need toþblacklist the Broadcom audio kernel module by creating a fileþ`/etc/modprobe.d/snd-blacklist.conf` withþþ    blacklist snd_bcm2835þþIf the audio device is still loading after blacklisting, you may alsoþneed to comment it out in the /etc/modules file.þþOn headless systems you may also need to force audio through hdmiþEdit config.txt and add:þþ    hdmi_force_hotplug=1þ    hdmi_force_edid_audio=1þþA reboot is required for this change to take effectþþSome distributions use audio by default, even if nothing is being played.þIf audio is needed, you can use a USB audio device instead.þþ#### PCMþþWhen using PCM you cannot use digital audio devices which use I2S since I2Sþuses the PCM hardware, but you can use analog audio.þþ#### SPIþþWhen using SPI the ledstring is the only device which can be connected toþthe SPI bus. Both digital (I2S/PCM) and analog (PWM) audio can be used.þþMany distributions have a maximum SPI transfer of 4096 bytes. This can beþchanged in `/boot/cmdline.txt` by appendingþ```þ    spidev.bufsiz=32768þ```þOn a RPi 3 you have to change the GPU core frequency to 250 MHz, otherwiseþthe SPI clock has the wrong frequency.þDo this by adding the following line to /boot/config.txt and reboot.þ```þ    core_freq=250þ```þþSPI requires you to be in the `gpio` group if you wish to control your LEDsþwithout root.þþ### Comparison PWM/PCM/SPIþþBoth PWM and PCM use DMA transfer to output the control signal for the LEDs.þThe max size of a DMA transfer is 65536 bytes. Since each LED needs 12 bytesþ(4 colors, 8 symbols per color, 3 bits per symbol) this means you canþcontrol approximately 5400 LEDs for a single strand in PCM and 2700 LEDs per stringþfor PWM (Only PWM can control 2 independent strings simultaneously)þSPI uses the SPI device driver in the kernel. For transfers larger thanþ96 bytes the kernel driver also uses DMA.þOf course there are practical limits on power and signal quality. These willþbe more constraining in practice than the theoretical limits above.þþWhen controlling a LED string of 240 LEDs the CPU load on the original Pi 2 (BCM2836) are:þ  PWM  5%þ  PCM  5%þ  SPI  1%þþ### Usage:þþThe API is very simple.  Make sure to create and initialize the `ws2811_t`þstructure as seen in [`main.c`](main.c).  From there it can be initializedþby calling `ws2811_init()`.  LEDs are changed by modifying the color inþthe `.led[index]` array and calling `ws2811_render()`.þThe rest is handled by the library, which either creates the DMA memory andþstarts the DMA for PWM and PCM or prepares the SPI transfer buffer and sendsþit out on the MISO pin.þþMake sure to hook a signal handler for SIGKILL to do cleanup.  From theþhandler make sure to call `ws2811_fini()`.  It'll make sure that the DMAþis finished before program execution stops and cleans up after itself."
the-tcpdump-group/tcpdump,25285,1326,122,574,Organization,False,6157,20,42,119,False,the TCPdump network dissector,https://www.tcpdump.org/,8,60,2,60,491,12,10,33,273,2,19,7278,7,179,3814,2340,0,0,8,4,,,"# tcpdumpþþ[![Build Status](https://travis-ci.org/the-tcpdump-group/tcpdump.svg?branch=master)](https://travis-ci.org/the-tcpdump-group/tcpdump)þþ[![Build Status](https://ci.appveyor.com/api/projects/status/github/the-tcpdump-group/tcpdump?branch=master&svg=true)](https://ci.appveyor.com/project/guyharris/tcpdump)þþTo report a security issue please send an e-mail to security@tcpdump.org.þþTo report bugs and other problems, contribute patches, request aþfeature, provide generic feedback etc please see the fileþCONTRIBUTING in the tcpdump source tree root.þþTCPDUMP 4.x.yþNow maintained by ""The Tcpdump Group""þSee   https://www.tcpdump.orgþþAnonymous Git is available via:þþ git clone git://bpf.tcpdump.org/tcpdumpþþformerly from  Lawrence Berkeley National Laboratoryþ  Network Research Group <tcpdump@ee.lbl.gov>þ  ftp://ftp.ee.lbl.gov/old/tcpdump.tar.Z (3.4)þþThis directory contains source code for tcpdump, a tool for networkþmonitoring and data acquisition.  This software was originallyþdeveloped by the Network Research Group at the Lawrence BerkeleyþNational Laboratory.  The original distribution is available viaþanonymous ftp to `ftp.ee.lbl.gov`, in `tcpdump.tar.Z`.  More recentþdevelopment is performed at tcpdump.org, https://www.tcpdump.org/.þþTcpdump uses libpcap, a system-independent interface for user-levelþpacket capture.  Before building tcpdump, you must first retrieve andþbuild libpcap, also originally from LBL and now being maintained byþtcpdump.org; see https://www.tcpdump.org/.þþOnce libpcap is built (either install it or make sure it's inþ`../libpcap`), you can build tcpdump using the procedure in the `INSTALL.txt`þfile.þþThe program is loosely based on SMI's ""etherfind"" although none of theþetherfind code remains.  It was originally written by Van Jacobson asþpart of an ongoing research project to investigate and improve tcp andþinternet gateway performance.  The parts of the program originallyþtaken from Sun's etherfind were later re-written by Steven McCanne ofþLBL.  To insure that there would be no vestige of proprietary code inþtcpdump, Steve wrote these pieces from the specification given by theþmanual entry, with no access to the source of tcpdump or etherfind.þþOver the past few years, tcpdump has been steadily improved by theþexcellent contributions from the Internet community (just browseþthrough the `CHANGES` file).  We are grateful for all the input.þþRichard Stevens gives an excellent treatment of the Internet protocolsþin his book *""TCP/IP Illustrated, Volume 1""*. If you want to learn moreþabout tcpdump and how to interpret its output, pick up this book.þþSome tools for viewing and analyzing tcpdump trace files are availableþfrom the Internet Traffic Archive:þþ* http://ita.ee.lbl.gov/þþAnother tool that tcpdump users might find useful is tcpslice:þþ* https://github.com/the-tcpdump-group/tcpsliceþþIt is a program that can be used to extract portions of tcpdump binaryþtrace files. See the above distribution for further details andþdocumentation.þþCurrent versions can be found at https://www.tcpdump.org.þþ - The TCPdump groupþþoriginal text by: Steve McCanne, Craig Leres, Van Jacobsonþþ-------------------------------------þ```þThis directory also contains some short awk programs intended asþexamples of ways to reduce tcpdump data when you're trackingþparticular network problems:þþsend-ack.awkþ Simplifies the tcpdump trace for an ftp (or other unidirectionalþ tcp transfer).  Since we assume that one host only sends andþ the other only acks, all address information is left off andþ we just note if the packet is a ""send"" or an ""ack"".þþ There is one output line per line of the original trace.þ Field 1 is the packet time in decimal seconds, relativeþ to the start of the conversation.  Field 2 is delta-timeþ from last packet.  Field 3 is packet type/direction.þ ""Send"" means data going from sender to receiver, ""ack""þ means an ack going from the receiver to the sender.  Aþ preceding ""*"" indicates that the data is a retransmission.þ A preceding ""-"" indicates a hole in the sequence spaceþ (i.e., missing packet(s)), a ""#"" means an odd-size (not maxþ seg size) packet.  Field 4 has the packet flagsþ (same format as raw trace).  Field 5 is the sequenceþ number (start seq. num for sender, next expected seq numberþ for acks).  The number in parens following an ack isþ the delta-time from the first send of the packet to theþ ack.  A number in parens following a send is theþ delta-time from the first send of the packet to theþ current send (on duplicate packets only).  Duplicateþ sends or acks have a number in square brackets showingþ the number of duplicates so far.þþ Here is a short sample from near the start of an ftp:þ  3.00    0.20   send . 512þ  3.20    0.20    ack . 1024  (0.20)þ  3.20    0.00   send P 1024þ  3.40    0.20    ack . 1536  (0.20)þ  3.80    0.40 * send . 0  (3.80) [2]þ  3.82    0.02 *  ack . 1536  (0.62) [2]þ Three seconds into the conversation, bytes 512 through 1023þ were sent.  200ms later they were acked.  Shortly thereafterþ bytes 1024-1535 were sent and again acked after 200ms.þ Then, for no apparent reason, 0-511 is retransmitted, 3.8þ seconds after its initial send (the round trip time for thisþ ftp was 1sec, +-500ms).  Since the receiver is expectingþ 1536, 1536 is re-acked when 0 arrives.þþpacketdat.awkþ Computes chunk summary data for an ftp (or similarþ unidirectional tcp transfer). [A ""chunk"" refers toþ a chunk of the sequence space -- essentially the packetþ sequence number divided by the max segment size.]þþ A summary line is printed showing the number of chunks,þ the number of packets it took to send that many chunksþ (if there are no lost or duplicated packets, the numberþ of packets should equal the number of chunks) and theþ number of acks.þþ Following the summary line is one line of informationþ per chunk.  The line contains eight fields:þ    1 - the chunk numberþ    2 - the start sequence number for this chunkþ    3 - time of first sendþ    4 - time of last sendþ    5 - time of first ackþ    6 - time of last ackþ    7 - number of times chunk was sentþ    8 - number of times chunk was ackedþ (all times are in decimal seconds, relative to the startþ of the conversation.)þþ As an example, here is the first part of the output forþ an ftp trace:þþ # 134 chunks.  536 packets sent.  508 acks.þ 1       1       0.00    5.80    0.20    0.20    4       1þ 2       513     0.28    6.20    0.40    0.40    4       1þ 3       1025    1.16    6.32    1.20    1.20    4       1þ 4       1561    1.86    15.00   2.00    2.00    6       1þ 5       2049    2.16    15.44   2.20    2.20    5       1þ 6       2585    2.64    16.44   2.80    2.80    5       1þ 7       3073    3.00    16.66   3.20    3.20    4       1þ 8       3609    3.20    17.24   3.40    5.82    4       11þ 9       4097    6.02    6.58    6.20    6.80    2       5þþ This says that 134 chunks were transferred (about 70Kþ since the average packet size was 512 bytes).  It tookþ 536 packets to transfer the data (i.e., on the averageþ each chunk was transmitted four times).  Looking at,þ say, chunk 4, we see it represents the 512 bytes ofþ sequence space from 1561 to 2048.  It was first sentþ 1.86 seconds into the conversation.  It was lastþ sent 15 seconds into the conversation and was sentþ a total of 6 times (i.e., it was retransmitted everyþ 2 seconds on the average).  It was acked once, 140msþ after it first arrived.þþstime.awkþatime.awkþ Output one line per send or ack, respectively, in the formþ  <time> <seq. number>þ where <time> is the time in seconds since the start of theþ transfer and <seq. number> is the sequence number being sentþ or acked.  I typically plot this data looking for suspiciousþ patterns.þþþThe problem I was looking at was the bulk-data-transferþthroughput of medium delay network paths (1-6 sec.  round tripþtime) under typical DARPA Internet conditions.  The trace of theþftp transfer of a large file was used as the raw data source.þThe method was:þþ  - On a local host (but not the Sun running tcpdump), connect toþ    the remote ftp.þþ  - On the monitor Sun, start the trace going.  E.g.,þ      tcpdump host local-host and remote-host and port ftp-data >tracefileþþ  - On local, do either a get or put of a large file (~500KB),þ    preferably to the null device (to minimize effects likeþ    closing the receive window while waiting for a disk write).þþ  - When transfer is finished, stop tcpdump.  Use awk to make upþ    two files of summary data (maxsize is the maximum packet size,þ    tracedata is the file of tcpdump tracedata):þ      awk -f send-ack.awk packetsize=avgsize tracedata >saþ      awk -f packetdat.awk packetsize=avgsize tracedata >pdþþ  - While the summary data files are printing, take a look atþ    how the transfer behaved:þ      awk -f stime.awk tracedata | xgraphþ    (90% of what you learn seems to happen in this step).þþ  - Do all of the above steps several times, both directions,þ    at different times of day, with different protocolþ    implementations on the other end.þþ  - Using one of the Unix data analysis packages (in my case,þ    S and Gary Perlman's Unix|Stat), spend a few months staringþ    at the data.þþ  - Change something in the local protocol implementation andþ    redo the steps above.þþ  - Once a week, tell your funding agent that you're discoveringþ    wonderful things and you'll write up that research reportþ    ""real soon now"".þ```"
Celtoys/Remotery,1231,1466,77,162,User,False,666,2,0,30,False,"Single C file, Realtime CPU/GPU Profiler with Remote Web Viewer",,8,7,0,28,74,0,0,2,63,0,3,2287,4,11,296,193,0,0,17,,94,,
traviscross/mtr,1304,1447,87,245,User,False,656,4,73,53,False,"Official repository for mtr, a network diagnostic tool",http://www.bitwizard.nl/mtr/,0,6,0,97,92,12,7,7,155,1,5,7908,4,8,34,14,0,0,103,,65,,"WHAT IS MTR?þ===þþmtr combines the functionality of the 'traceroute' and 'ping' programsþin a single network diagnostic tool.þþAs mtr starts, it investigates the network connection between the hostþmtr runs on and a user-specified destination host.  After itþdetermines the address of each network hop between the machines,þit sends a sequence of ICMP ECHO requests to each one to determine theþquality of the link to each machine.  As it does this, it printsþrunning statistics about each machine.þþmtr is distributed under the GNU General Public License version 2.þSee the COPYING file for details.þþINSTALLINGþ===þþIf you're building this from a tarball, compiling mtr is asþsimple as:þþ ./configure && makeþþ(in the past, there was a Makefile in the distribution that didþthe `./configure` for you and then ran make again with the generatedþMakefile, but this has suffered some bitrot. It didn't work wellþwith git.)þþIf you're building from the git repository, you'll need to run:þþ ./bootstrap.sh && ./configure && makeþþWhen it looks as if the compilation was succesful, you canþtest mtr withþþ sudo ./mtr <host>þþ(fill in a hostname or IP address where it says <host>) orþimmediately continue on to installing:þþ make installþþNote that mtr-packet must be suid-root because it requires access toþraw IP sockets.  See SECURITY for security information.þþOlder versions used to require a non-existent path to GTK for aþcorrect build of a non-gtk version while GTK was installed. This isþno longer necessary. `./configure --without-gtk` should now work.þIf it doesn't, try `make WITHOUT_X11=YES` as the make step.þþOn Solaris, you'll need to use GNU make to build.þ(Use `gmake` rather than `make`.)þþOn Solaris (and possibly other systems) the ""gtk"" library may beþinstalled in a directory where the dynamic linker refuses to look whenþa binary is setuid. Roman Shterenzon reports that addingþ        -Wl,-rpath=/usr/libþto the commandline will work if you are using gnu LD. He tells me thatþyou're out of luck when you use the sun LD. That's not quite true, asþyou can move the gtk libraries to `/usr/lib` instead of leaving them inþ`/usr/local/lib`.  (when the ld tells you that `/usr/local/lib` is untrustedþand `/usr/lib` is trusted, and you trust the gtk libs enough to want themþin a setuid program, then there is something to say for moving themþto the ""trusted"" directory.)þþBuilding on MacOS should not require any special steps.þþBUILDING FOR WINDOWSþ===þþBuilding for Windows requires Cygwin.  To obtain Cygwin, seeþhttps://cygwin.com/install.html.þNext, re-run cygwin's `setup-x86.exe` (or `setup-x86_64.exe` if you're using 64bit cygwin) with the following arguments,  þwhich will install the packages required for building:þþ        setup-x86.exe --package-manager --wait --packages automake,pkg-config,make,gcc-core,libncurses-develþþBuild as under Unix:þþ        ./bootstrap.sh && ./configure && makeþþFinally, install the built binaries:þþ        make installþþþWHERE CAN I GET THE LATEST VERSION OR MORE INFORMATION?þ===þþmtr is now hosted on github.þhttps://github.com/traviscross/mtrþþSee the mtr web page at http://www.BitWizard.nl/mtr/þþBug reports and feature requests should be submitted to the Github bug tracking system.þþPatches can be submitted by cloning the Github repository and issuingþa pull request, or by email to me. Please use unified diffs. Usuallyþthe diff is sort of messy, so please check that the diff is clean andþdoesn't contain too much of your local stuff (for example, I don'tþwant/need the ""configure"" script that /your/ automake made for you).þþ(There used to be a mailinglist, but all it got was spam. Soþwhen the server was upgraded, the mailing list died.)þþþREW"
EtchedPixels/FUZIX,13728,1172,128,143,User,False,7573,1,0,30,False,FuzixOS: Because Small Is Beautiful,,0,14,1,40,274,8,12,1,475,0,9,2056,5,14,85,67,0,0,24,,198,,"[![Build Status][travis-image]][travis-url]þþ**FuzixOS**: Because Small Is BeautifulþþThis is the initial public tree for the FuzixOS project. It is not yet useful although you can build and boot it and runþtest application code. A lot of work is needed on the utilities and libraries.þþ# FUZIXþþFUZIX is a fusion of various elements from the assorted UZI forks andþbranches beaten together into some kind of semi-coherent platform and thenþextended from V7 to somewhere in the SYS3 to SYS5.x world with bits of POSIXþthrown in for good measure. Various learnings and tricks from ELKS and fromþOMU also got blended inþþ# Pre-built imagesþþSome pre-built filesystems are now available on www.fuzix.org, and otherþimages should follow in time.þþ## What does FUZIX have over UZIþþþ* Support for multiple processes in banked memory (as per UZI180) butþ with Minix style chmem and efficient use of bank allocations.þ* Support for multiple processes via hard disk or non mappable RAMþ    drive switching (as per UZI, UZIX).þ* Support for ""real"" swapping combined with banked memory.þ* Proper sane off_t and lseekþ* Normal dev_tþ* 30 character filenamesþ* Proper sane time_tþ* System 5 signalsþ* Posix termios (does all the original UZI tty did but much can be added)þ* Blocking on carrier for terminalsþ* Optimisations to avoid bogus uarea copying compared to UZI180þ* More modern system call API: 3 argument open, mkdir, rmdir, rename,þ chroot (with correct .. semantics), fchdir, fchmod, fchown, fstat,þ fcntl, setpgrp, sighold and friends, waitpid, setpgrp, niceþ O_NDELAY, O_CLOEXEC, F_SETFL, F_DUPFD etcþ* Address validation checks on all syscall copiesþ* Builds with a modern ANSI C compiler (SDCC)þ* Kernel boots to userspace on 6303, 6502, 65C816, 68000, 6803, 6809, 8080, 8085, MSP430 (bitrotted) and Z80/Z180þ* Core code can be built for 6303, 6502, 65C816, 68000, 6803, 6809, 8080, 8085, 8086, MSP430, pdp11, rabbit r2k/r3k and Z80/Z180 so should be far more portableþ* Core architecture designed to support building and maintainingþ multiple target machines without forking each oneþ* Helpers to make many bits of implementation wrappers to core codeþ* Lots more bugs right nowþþ## What does UZI have over FUZIXþþ* Can run in 64K of RAM (32K kernel/32K user). FUZIX would needþ banked ROM or similar to pull this off. If you have bankedþ ROM then our kernel footprint in RAM is about 8K plus userspaceþ plus any framebuffers and similar overhead. On a 6809 it's justþ about possible to run in a straight 64Kþþ## What do the UZI branches have that FUZIX has not yet integratedþþ* Symbolic links (UZIX)þ* Various clever fusions of syscalls that may save a few bytesþ (UZIX)þ* setprio (UZIX)þ* Rather crude loadable drivers (UZIX)þ* Use of __naked and __asm for Z80 specific bits to avoid moreþ .S files than are needed (UMZIX)þþPlus OMU has a really clever function passing trick for open/creat andþfriends, while UMZIX has a neat unified ""make anything"" function.þþ## What Key Features Are Missing Stillþþ* ptrace, most of ulimitþ* root reserved disk blocksþ* banked executablesþ* TCP/IP (in progress)þ* select/poll() (in progress)þ* Support for > 32MB filesystems (but first figure out how to fsckþ a giant fs on a slow 8bit micro!)þ* Smarter schedulerþ* Optimisations for disk block/inode allocator (2.11BSD)þþ## Tool Issuesþþ* 6809 gcc and cc65 don't have long long 64bit (for sane time_t)þ* SDCC can generate ROMmable binaries but not banked ones (hack fixes done)þ* None of the above have an O88 style common sequence compressorþ* CC65 can't handle larger objects on stack, and lacks float supportþ* We need a 'proper' 65C816 C compilerþ* ACK 8080 lacks floating point supportþþ[travis-image]: https://travis-ci.org/EtchedPixels/FUZIX.png?branch=masterþ[travis-url]: https://travis-ci.org/EtchedPixels/FUZIX"
raysan5/raylib,343468,4087,187,475,User,False,3975,3,16,144,False,A simple and easy-to-use library to enjoy videogames programming,http://www.raylib.com,10,14,0,15,695,12,136,1,558,1,85,2427,46,549,62606,88761,0,0,12,,663,,"<img align=""left"" src=""https://github.com/raysan5/raylib/blob/master/logo/raylib_256x256.png"" width=256>þþ**raylib is a simple and easy-to-use library to enjoy videogames programming.**þþraylib is highly inspired by Borland BGI graphics lib and by XNA framework and it's specially well suited for prototyping, tooling, graphical applications, embedded systems and education.þþ*NOTE for ADVENTURERS: raylib is a programming library to enjoy videogames programming; no fancy interface, no visual helpers, no auto-debugging... just coding in the most pure spartan-programmers way.*þþReady to learn? Jump to [code examples!](http://www.raylib.com/examples.html)þþ<br>þþ[![GitHub contributors](https://img.shields.io/github/contributors/raysan5/raylib)](https://github.com/raysan5/raylib/graphs/contributors)þ[![GitHub All Releases](https://img.shields.io/github/downloads/raysan5/raylib/total)](https://github.com/raysan5/raylib/releases)þ[![GitHub commits since tagged version](https://img.shields.io/github/commits-since/raysan5/raylib/3.0.0)](https://github.com/raysan5/raylib/commits/master)þ[![License](https://img.shields.io/badge/license-zlib%2Flibpng-blue.svg)](LICENSE)þþ[![Chat on Discord](https://img.shields.io/discord/426912293134270465.svg?logo=discord)](https://discord.gg/VkzNHUE)þ[![GitHub stars](https://img.shields.io/github/stars/raysan5/raylib?style=social)](https://github.com/raysan5/raylib/stargazers)þ[![Twitter Follow](https://img.shields.io/twitter/follow/raysan5?style=social)](https://twitter.com/raysan5)þ[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/raylib?style=social)](https://www.reddit.com/r/raylib/)þþ[![Travis (.org)](https://img.shields.io/travis/raysan5/raylib?label=Travis%20CI%20Build%20Status%20-%20Linux,%20OSX,%20Android,%20Windows)](https://travis-ci.org/raysan5/raylib)þ[![AppVeyor](https://img.shields.io/appveyor/build/raysan5/raylib?label=AppVeyor%20CI%20Build%20Status%20-%20Windows%20(mingw,%20msvc15))](https://ci.appveyor.com/project/raysan5/raylib)þþ[![Actions Status](https://github.com/raysan5/raylib/workflows/CI%20-%20Source%20&%20Examples%20-%20Windows/badge.svg)](https://github.com/raysan5/raylib/actions)þ[![Actions Status](https://github.com/raysan5/raylib/workflows/CI%20-%20Source%20&%20Examples%20-%20Linux/badge.svg)](https://github.com/raysan5/raylib/actions)þ[![Actions Status](https://github.com/raysan5/raylib/workflows/CI%20-%20Source%20&%20Examples%20-%20macOS/badge.svg)](https://github.com/raysan5/raylib/actions)þþfeaturesþ--------þ  - **NO external dependencies**, all required libraries are bundled into raylibþ  - Multiple platforms supported: **Windows, Linux, MacOS, Android, HTML5... and more!**þ  - Written in plain C code (C99) in PascalCase/camelCase notationþ  - Hardware accelerated with OpenGL (**1.1, 2.1, 3.3 or ES 2.0**)þ  - **Unique OpenGL abstraction layer** (usable as standalone module): [rlgl](https://github.com/raysan5/raylib/blob/master/src/rlgl.h)þ  - Multiple **Fonts** formats supported (TTF, XNA fonts, AngelCode fonts)þ  - Outstanding texture formats support, including compressed formats (DXT, ETC, ASTC)þ  - **Full 3D support**, including 3D Shapes, Models, Billboards, Heightmaps and more! þ  - Flexible Materials system, supporting classic maps and **PBR maps**þ  - **Animated 3D models** supported (skeletal bones animation)þ  - Shaders support, including model and **postprocessing** shaders.þ  - **Powerful math module** for Vector, Matrix and Quaternion operations: [raymath](https://github.com/raysan5/raylib/blob/master/src/raymath.h)þ  - Audio loading and playing with streaming support (WAV, OGG, MP3, FLAC, XM, MOD)þ  - **VR stereo rendering** support with configurable HMD device parametersþ  - Huge examples collection with [+115 code examples](https://github.com/raysan5/raylib/tree/master/examples)!þ  - Bindings to [+40 programming languages](https://github.com/raysan5/raylib/blob/master/BINDINGS.md)!þ  - Free and open source.þþraylib uses on its [core](https://github.com/raysan5/raylib/blob/master/src/core.c) module the outstanding [GLFW3](http://www.glfw.org/) library, embedded in the form of [rglfw](https://github.com/raysan5/raylib/blob/master/src/rglfw.c) module, to avoid external dependencies.þþraylib uses on its [raudio](https://github.com/raysan5/raylib/blob/master/src/raudio.c) module, the amazing [miniaudio](https://github.com/dr-soft/miniaudio) library to support multiple platforms and multiple audio backends.þþraylib uses internally several single-file header-only libraries to support different fileformats loading and saving, all those libraries are embedded with raylib and available in [src/external](https://github.com/raysan5/raylib/tree/master/src/external) directory. Check [raylib Wiki](https://github.com/raysan5/raylib/wiki/raylib-dependencies) for a detailed list.þþ*On Android platform, `native_app_glue` module (provided by Android NDK) and native Android libraries are used to manage window/context, inputs and activity life cycle.*þþ*On Raspberry Pi platform (native mode), `Videocore API` and `EGL` libraries are used for window/context management. Inputs are processed using `evdev` Linux libraries*þþ*On Web platform, raylib uses `emscripten` provided libraries for several input events management, specially noticeable the touch events support.*þþbuild and installationþ----------------------þþraylib binary releases for Windows, Linux and macOS are available at the [Github Releases page](https://github.com/raysan5/raylib/releases).þþraylib is also available via multiple [package managers](https://github.com/raysan5/raylib/issues/613) on multiple OS distributions.þþ#### Installing and building raylib via vcpkgþþYou can download and install raylib using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:þþ      git clone https://github.com/Microsoft/vcpkg.gitþ      cd vcpkgþ      ./bootstrap-vcpkg.shþ      ./vcpkg integrate installþ      vcpkg install raylibþþ*The raylib port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.*þþ#### Building raylib on multiple platformsþþ[raylib Wiki](https://github.com/raysan5/raylib/wiki#development-platforms) contains detailed instructions on building and usage on multiple platforms.þþ - [Working on Windows](https://github.com/raysan5/raylib/wiki/Working-on-Windows)þ - [Working on macOS](https://github.com/raysan5/raylib/wiki/Working-on-macOS)þ - [Working on GNU Linux](https://github.com/raysan5/raylib/wiki/Working-on-GNU-Linux)þ - [Working on FreeBSD](https://github.com/raysan5/raylib/wiki/Working-on-FreeBSD)þ - [Working on Raspberry Pi](https://github.com/raysan5/raylib/wiki/Working-on-Raspberry-Pi)þ - [Working for Android](https://github.com/raysan5/raylib/wiki/Working-for-Android)þ - [Working for Web (HTML5)](https://github.com/raysan5/raylib/wiki/Working-for-Web-(HTML5))þ - [Working for UWP (Universal Window Platform)](https://github.com/raysan5/raylib/wiki/Working-for-UWP)þ - [Working anywhere with CMake](https://github.com/raysan5/raylib/wiki/Working-with-CMake)þþ*Note that Wiki is open for edit, if you find some issue while building raylib for your target platform, feel free to edit the Wiki or open and issue related to it.*þþ#### Using raylib with multiple IDEsþþraylib has been developed on Windows platform using [Notepad++](https://notepad-plus-plus.org/) and [MinGW GCC](http://mingw-w64.org/doku.php) compiler but it can be used with other IDEs on multiple platforms.þþ[Projects directory](https://github.com/raysan5/raylib/tree/master/projects) contains several ready-to-use **project templates** to build raylib and code examples with multiple IDEs.þþ*Note that there are lots of IDEs supported, some of the provided templates could require some review, please, if you find some issue with some template or you think they could be improved, feel free to send a PR or open a related issue.*þþcontactþ-------þþ   * Webpage: [http://www.raylib.com](http://www.raylib.com)þ   * Discord: [https://discord.gg/raylib](https://discord.gg/VkzNHUE)þ   * Twitter: [http://www.twitter.com/raysan5](http://www.twitter.com/raysan5)þ   * Twitch: [http://www.twitch.tv/raysan5](http://www.twitch.tv/raysan5)þ   * Reddit: [https://www.reddit.com/r/raylib](https://www.reddit.com/r/raylib)þ   * Patreon: [https://www.patreon.com/raylib](https://www.patreon.com/raylib)þ   * YouTube: [https://www.youtube.com/channel/raylib](https://www.youtube.com/channel/UC8WIBkhYb5sBNqXO1mZ7WSQ)þþIf you are using raylib and enjoying it, please, join our [Discord server](https://discord.gg/VkzNHUE) and let us know! :)þþlicenseþ-------þþraylib is licensed under an unmodified zlib/libpng license, which is an OSI-certified, BSD-like license that allows static linking with closed source software. Check [LICENSE](LICENSE) for further details."
koush/android_system_core,2833,27,3,969,User,False,858,2,0,29,False,Android System Core (CM),,0,0,0,,,,,0,0,0,0,4247,0,0,0,0,0,0,297,,165,,
vmware/open-vm-tools,22368,1260,147,278,Organization,False,4654,30,88,9,False,Official repository of VMware open-vm-tools project,http://sourceforge.net/projects/open-…,0,8,0,139,239,16,19,33,24,3,4,2175,1,120,6681,3188,0,0,193,150,,,"# Generalþ## What is the open-vm-tools project?þopen-vm-tools is a set of services and modules that enable several features in VMware products for better management of, and seamless user interactions with, guests. It includes kernel modules for enhancing the performance of virtual machines running Linux or other VMware supported Unix like guest operating systems. þ þopen-vm-tools enables the following features in VMware products:þþ- The ability to perform virtual machine power operations gracefully.þ- Execution of VMware provided or user configured scripts in guests during various power operations.þ- The ability to run programs, commands and file system operation in guests to enhance guest automation.þ- Authentication for guest operations. þ- Periodic collection of network, disk, and memory usage information from the guest.þ- Generation of heartbeat from guests to hosts so VMware's HA solution can determine guests' availability.þ- Clock synchronization between guests and hosts or client desktops.þ- Quiescing guest file systems to allow hosts to capture file-system-consistent guest snapshots.þ- Execution of pre-freeze and post-thaw scripts while quiescing guest file systems.þ- The ability to customize guest operating systems immediately after powering on virtual machines.þ- Enabling shared folders between host and guest file systems on VMware Workstation and VMware Fusion.þ- Copying and pasting text, graphics, and files between guests and hosts or client desktops.þþ## Can you provide more details on the actual code being released?þThe following components have been released as open source software:þ- Linux, Solaris and FreeBSD drivers for various devices and file system access.þ- The memory balloon driver for reclaiming memory from guests.þ- The PowerOps plugin to perform graceful power operation and run power scripts.þ- The VIX plugin to run programs and commands, and perform file system operations in guests.þ- The GuestInfo plugin to periodically collect various statistics from guests.þ- The TimeSync plugin to perform time synchronization.þ- The dndcp plugin to support drag and drop, and text and file copy/paste operations.þ- The ResolutionSet plugin to adjust guest screen resolutions automatically based on window sizes.þ- The guest authentication service.þ- The toolbox command to perform disk wiping and shrinking, manage power scripts, and time synchronization.þ- The guest SDK libraries to provide information about virtual machines to guests.þ- Clients and servers for shared folders support.þ- Multiple monitor support.þ- The GTK Toolbox UI.þ þ## Is open-vm-tools available with Linux distributions?þYes. open-vm-tools packages for user space components are available with new versions of major Linux distributions, and are installed as part of the OS installation in several cases. Please refer to VMware KB article http://kb.vmware.com/kb/2073803 for details. All leading Linux vendors support open-vm-tools and bundle it with their products. For information about OS compatibility for open-vm-tools, see the þVMware Compatibility Guide at http://www.vmware.com/resources/compatibilityþAutomatic installation of open-vm-tools along with the OS installation eliminates the need to separately install open-vm-tools in guests. If open-vm-tools is not installed automatically, you may be able to manually install it from the guest OS vendor's public repository. Installing open-vm-tools from the Linux vendor's repository reduces virtual machine downtime because future updates to open-vm-tools are included with the OS maintenance patches and updates.þ**NOTE**: Most of the Linux distributions ship two or more open-vm-tools packages. ""open-vm-tools"" is the core package without any dependencies on X libraries and ""open-vm-tools-desktop"" is an additional package with dependencies on ""open-vm-tools"" core package and X libraries. The ""open-vm-tools-sdmp"" package contains a plugin for Service Discovery. There may be additional packages, please refer to the documentation of the OS vendor. Note that the open-vm-tools packages available with Linux distributions do not include Linux drivers because Linux drivers are available as part of Linux kernel itself. Linux kernel versions 3.10 and later include all of the Linux drivers present in open-vm-tools except the vmhgfs driver. The vmhgfs driver was required for enabling shared folders feature, but is superseded by vmhgfs-fuse which does not require a kernel driver.þþ## Will there be continued support for VMware Tools and OSP? þVMware Tools will continue to be available under a commercial license. It is recommended that open-vm-tools be used for the Linux distributions where open-vm-tools is available. VMware will not provide OSPs for operating systems where open-vm-tools is available.þþ## How does this benefit other open source projects?þUnder the terms of the GPL, open source community members are able to use the open-vm-tools code to develop their own applications, extend it, and contribute to the community. They can also incorporate some or all of the code into their projects, provided they comply with the terms of the GPL.þþ# License Relatedþ## What license is the code being released under?þThe code is being released under GPL v2 and GPL v2 compatible licenses. To be more specific, the Linux kernel modules are being released under the GPL v2, while almost all of the user level components are being released under the LGPL v2.1. The SVGA and mouse drivers have been available under the X11 license for quite some time. There are certain third party components released under BSD style licenses, to which VMware has in some cases contributed, and will continue to distribute with open-vm-tools.þ þ## Why did you choose these licenses?þWe chose the GPL v2 for the kernel components to be consistent with the Linux kernel's license. We chose the LGPL v2.1 for the user level components because some of the code is implemented as shared libraries and we do not wish to restrict proprietary code from linking against those libraries. For consistency, we decided to license the rest of the userlevel code under the LGPL v2.1 as well.þþ## What are the obligations that the license(s) impose?þEach of these licenses have different obligations.þFor questions about the GPL, LGPL licenses, the Free Software Foundation's GPL FAQ page provides lots of useful information. þFor questions about the other licenses like the X11, BSD licenses, the Open Source Initiative has numerous useful resources including mailing lists. þThe Software Freedom Law Center provides legal expertise and consulting for free and open source software (FOSS) developers.þþ## Can I use all or part of this code in my proprietary software? Do I have to release the source code if I do?þDifferent open source licenses have different requirements regarding the release of source code. Since the code is being released under various open source licenses, you will need to comply with the terms of the corresponding licenses.þþ## Am I required to contribute back any changes I make to the code?þNo, you aren't required to contribute any changes that you make back to the open-vm-tools project. However, we encourage you to do so.þþ## Can I use all or part of this code in another open source package?þYes, as long as you comply with the appropriate license(s).þ þ## Can I package this for my favorite operating system?þYes! Please do. þþ## Will the commercial version (VMware Tools) differ from the open source version (open-vm-tools)? If so, how?þOur goal is to work towards making the open source version as close to the commercial version as possible. However, we do currently make use of certain components licensed from third parties as well as components from other VMware products which are only available in binary form.þþ## If I use the code from the open-vm-tools project in my project/product, can I call my project/product VMware Tools?þNo, since your project/product is not a VMware project/product.þþ# Building open-vm-toolsþ## How do I build open-vm-tools?þopen-vm-tools uses the GNU Automake tool for generating Makefiles to build all sources. More information about Automake can be found here: http://www.gnu.org/software/automake/þ## Project build information:þThe following steps will work on most recent Linux distributions:þ```þautoreconf -iþ./configureþmakeþsudo make installþsudo ldconfigþ```þþTo build the optional sdmp (Service Discovery) plugin use the `--enable-servicediscovery` option to invoke the configure script:þ```þ./configure --enable-servicediscoveryþ```þþ## Getting configure options and helpþIf you are looking for help or additional settings for the building of this project, the following configure command will display a list of help options:þ```þ./configure --helpþ```þWhen using configure in the steps above it is only necessary to call ./configure once unless there was a problem after the first invocation.þþ# Getting Involvedþ## How can I get involved today?þYou can get involved today in several different ways:þ- Start using open-vm-tools today and give us feedback.þ- Suggest feature enhancements.þ- Identify and submit bugs under issues section: https://github.com/vmware/open-vm-tools/issuesþ- Start porting the code to other operating systems.   Here is the list of operating systems with open-vm-tools:þþ  * Red Hat Enterprise Linux 7.0 and later releasesþ  * SUSE Linux Enterprise 12 and later releasesþ  * Ubuntu 14.04 and later releasesþ  * CentOS 7 and later releasesþ  * Debian 7.x and later releasesþ  * Oracle Linux 7 and later þ  * Fedora 19 and later releasesþ  * openSUSE 11.x and later releasesþ þ## Will external developers be allowed to become committers to the project?þYes. Initially, VMware engineers will be the only committers. As we roll out our development infrastructure, we will be looking to add external committers to the project as well.þþ## How can I submit code changes like bug fixes, patches, new features to the project?þInitially, you can submit bug fixes, patches and new features to the project development mailing list as attachments to emails or bug reports. To contribute source code, you will need to fill out a contribution agreement form as part of the submission process. We will have more details on this process shortly.þþ## What is the governance model for managing this as an open source project?þThe feature roadmap and schedules for the open-vm-tools project will continue to be defined by VMware. Initially, VMware engineers will be the only approved committers. We will review incoming submissions for suitability for merging into the project. We will be looking to add community committers to the project based on their demonstrated contributions to the project. Finally, we also plan to set up a process for enhancement proposals, establishing sub-projects and so on.þþ## Will you ship code that I contribute with VMware products? If so, will I get credit for my contributions?þContributions that are accepted into the open-vm-tools project's main source tree will likely be a part of VMware Tools. We also recognize the value of attribution and value your contributions. Consequently, we will acknowledge contributions from the community that are distributed with VMware's products.þþ## Do I need to sign something before making a contribution?þYes. We have a standard contribution agreement that covers all contributions made to the project. It gives VMware and you joint copyright interests in the code you are contributing. The agreement also gives VMware flexibility with licensing and also helps avoid any copyright/licensing related issues that may arise in the future. In order for us to include your contribution in our source tree, we ask that you send us a signed copy of the agreement. You can do this in one of two ways:þFax to +1.650.427.5003, Attn: Product & Technology Law GroupþScan and email it to oss-queries_at_vmware.comþAgreement: http://open-vm-tools.sourceforge.net/files/vca.pdfþþ# Compatibiltyþþ## What Operating Systems are supported for customization?þThe [Guest OS Customization Support Matrix](http://partnerweb.vmware.com/programs/guestOS/guest-os-customization-matrix.pdf) provides details about the guest operating systems supported for customization.þþ## Which versions of open-vm-tools are compatible with other VMware products?þþThe [VMware Product Interoperability Matrix](http://partnerweb.vmware.com/comp_guide2/sim/interop_matrix.php) provides details about the compatibility of different versions of VMware Tools (includes open-vm-tools) and other VMware Products.þþ#Internationalizationþ## Which languages are supported?þþopen-vm-tools supports the following languages:þ- Englishþ- Frenchþ- Germanþ- Spanishþ- Italianþ- Japaneseþ- Koreanþ- Simplified Chineseþ- Traditional Chineseþþ# Otherþ## Mailing ListsþPlease send an email to one of these mailing lists based on the nature of your question.þ- Development related questions : open-vm-tools-devel@lists.sourceforge.netþ- Miscellaneous questions: open-vm-tools-discuss@lists.sourceforge.netþ- General project announcements: open-vm-tools-announce@lists.sourceforge.net"
nesbox/TIC-80,121703,1892,88,189,User,False,1227,3,31,41,False,"TIC-80 is a fantasy computer for making, playing and sharing tiny games.",https://tic.computer,9,27,1,269,652,40,43,4,185,3,52,1440,12,136,94713,64208,0,0,22,,323,,"[![Build Status](https://travis-ci.org/nesbox/TIC-80.svg?branch=master)](https://travis-ci.org/nesbox/TIC-80)þ[![Build status](https://ci.appveyor.com/api/projects/status/1pflw77cjd8mqggb/branch/master?svg=true)](https://ci.appveyor.com/project/nesbox/tic-80)þþ![TIC-80](https://tic.computer/img/logo64.png)þ**TIC-80 TINY COMPUTER** - [https://tic.computer/](https://tic.computer/)þþ# AboutþTIC-80 is a **FREE** and **OPEN SOURCE** fantasy computer for making, playing and sharing tiny games.þþWith TIC-80 you get built-in tools for development: code, sprites, maps, sound editors and the command line, which is enough to create a mini retro game.þþGames are packaged into a cartridge file, which can be easily distributed. TIC-80 works on all popular platforms. This means your cartridge can be played in any device.þþTo make a retro styled game, the whole process of creation and execution takes place under some technical limitations: 240x136 pixel display, 16 color palette, 256 8x8 color sprites, 4 channel sound, etc.þþ![TIC-80](https://user-images.githubusercontent.com/1101448/29687467-3ddc432e-8925-11e7-8156-5cec3700cc04.gif)þþ### Featuresþ- Multiple programming languages: [Lua](https://www.lua.org),þ  [Moonscript](https://moonscript.org),þ  [Javascript](https://developer.mozilla.org/en-US/docs/Web/JavaScript),þ  [Wren](http://wren.io/), and [Fennel](https://fennel-lang.org).þ- Games can have mouse and keyboard as inputþ- Games can have up to 4 controllers as input (with up to 8 buttons, each)þ- Built-in editors: for code, sprites, world maps, sound effects and musicþ- An aditional memory bank: load different assets from your cartridge while your game is executingþþ# Binary DownloadsþYou can download compiled versions for the major operating systems directly from our [releases page](https://github.com/nesbox/TIC-80/releases).þþ# Pro VersionþTo help support TIC-80 development, we have a [PRO Version](https://nesbox.itch.io/tic).þThis version has a few additional features and binaries can only be downloaded on [our Itch.io page](https://nesbox.itch.io/tic).þþFor users who can't spend the money, we made it easy to build the pro version from the source code.þþ### Pro featuresþþ- Save/load cartridges in text format, and create your game in any editor you want, also useful for version control systems.þ- Even more memory banks: instead of having only 1 memory bank you have 8.þ- Export your game without editors, and then publish it to app stores (WIP).þþ# CommunityþYou can play and share games, tools and music at [tic.computer](https://tic.computer/play).þþThe community also hangs out and discusses on [Discord chat](https://discord.gg/DkD73dP).þþ# ContributingþYou are can contribute by issuing a bug or requesting a new feature on our [issues page](https://github.com/nesbox/tic.computer/issues).þKeep in mind when engaging on a discussion to follow our [Code of Conduct](https://github.com/nesbox/TIC-80/blob/master/CODE_OF_CONDUCT.md).þþYou can also contribute by reviewing or improving our [wiki](https://github.com/nesbox/tic.computer/wiki).þThe [wiki](https://github.com/nesbox/tic.computer/wiki) holds TIC-80 documentation, code snippets and game development tutorials.þþ# Build instructionsþþ## Windowsþ### with Visual Studio 2017þ- install `Visual Studio 2017`þ- install `git`þ- run following commands in `cmd`þ```þgit clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/buildþcmake -G ""Visual Studio 15 2017 Win64"" ..þ```þ- open `TIC-80.sln` and buildþ- enjoy :)þþ### with MinGWþ- install `mingw-w64` (http://mingw-w64.org) and add `.../mingw/bin` path to the *System Variables Path*þ- install `git`þ- install `cmake` (https://cmake.org)þ- run following commands in `terminal`þ```þgit clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/buildþcmake -G ""MinGW Makefiles"" ..þmingw32-make -j4þ```þþ## Linux þ### Ubuntu 14.04þrun the following commands in the Terminalþ```þsudo apt-get install git cmake libgtk-3-dev libgles1-mesa-dev libglu-dev -yþgit clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/buildþcmake ..þmake -j4þ```þþto install the latest CMake:þ```þwget ""https://cmake.org/files/v3.12/cmake-3.12.0-Linux-x86_64.sh""þsudo sh cmake-3.12.0-Linux-x86_64.sh --skip-license --prefix=/usrþ```þþ### Ubuntu 18.04þþrun the following commands in the Terminalþ```þsudo apt-get install git cmake libgtk-3-dev libglvnd-dev libglu1-mesa-dev freeglut3-dev -yþgit clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/buildþcmake ..þmake -j4þ```þþ## Macþinstall `Command Line Tools for Xcode` and `brew` package managerþþrun the following commands in the Terminalþ```þbrew install git cmakeþgit clone --recursive https://github.com/nesbox/TIC-80 && cd TIC-80/buildþcmake ..þmake -j4þ```þþ## iOS / tvOSþYou can find iOS/tvOS version here þ- 0.60.3: https://github.com/brunophilipe/TIC-80þ- 0.45.0: https://github.com/CliffsDover/TIC-80"
confluentinc/confluent-kafka-python,1816,1822,233,486,Organization,False,591,69,175,54,False,Confluent's Kafka Python Client,http://docs.confluent.io/current/clie…,5,22,1,184,427,74,30,21,264,8,39,1524,13,65,14983,1073,1300,107,168,170,,,"Confluent's Python Client for Apache Kafka<sup>TM</sup>þ=======================================================þþ**confluent-kafka-python** provides a high-level Producer, Consumer and AdminClient compatible with allþ[Apache Kafka<sup>TM<sup>](http://kafka.apache.org/) brokers >= v0.8, [Confluent Cloud](https://www.confluent.io/confluent-cloud/)þand the [Confluent Platform](https://www.confluent.io/product/compare/). The client is:þþ- **Reliable** - It's a wrapper around [librdkafka](https://github.com/edenhill/librdkafka) (provided automatically via binary wheels) which is widely deployed in a diverse set of production scenarios. It's tested using [the same set of system tests](https://github.com/confluentinc/confluent-kafka-python/tree/master/confluent_kafka/kafkatest) as the Java client [and more](https://github.com/confluentinc/confluent-kafka-python/tree/master/tests). It's supported by [Confluent](https://confluent.io).þþ- **Performant** - Performance is a key design consideration. Maximum throughput is on par with the Java client for larger message sizes (where the overhead of the Python interpreter has less impact). Latency is on par with the Java client.þþ- **Future proof** - Confluent, founded by theþcreators of Kafka, is building a [streaming platform](https://www.confluent.io/product/compare/)þwith Apache Kafka at its core. It's high priority for us that client features keepþpace with core Apache Kafka and components of the [Confluent Platform](https://www.confluent.io/product/compare/).þþþSee the [API documentation](http://docs.confluent.io/current/clients/confluent-kafka-python/index.html) for more info.þþ**License**: [Apache License v2.0](http://www.apache.org/licenses/LICENSE-2.0)þþþUsageþ=====þþBelow are some examples of typical usage. For more examples, see the [examples](examples) directory or the [confluentinc/examples](https://github.com/confluentinc/examples/tree/master/clients/cloud/python) github repo for a [Confluent Cloud](https://www.confluent.io/confluent-cloud/) example.þþþ**Producer**þþ```pythonþfrom confluent_kafka import Producerþþþp = Producer({'bootstrap.servers': 'mybroker1,mybroker2'})þþdef delivery_report(err, msg):þ    """""" Called once for each message produced to indicate delivery result.þ        Triggered by poll() or flush(). """"""þ    if err is not None:þ        print('Message delivery failed: {}'.format(err))þ    else:þ        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))þþfor data in some_data_source:þ    # Trigger any available delivery report callbacks from previous produce() callsþ    p.poll(0)þþ    # Asynchronously produce a message, the delivery report callbackþ    # will be triggered from poll() above, or flush() below, when the message hasþ    # been successfully delivered or failed permanently.þ    p.produce('mytopic', data.encode('utf-8'), callback=delivery_report)þþ# Wait for any outstanding messages to be delivered and delivery reportþ# callbacks to be triggered.þp.flush()þ```þþþ**High-level Consumer**þþ```pythonþfrom confluent_kafka import Consumerþþþc = Consumer({þ    'bootstrap.servers': 'mybroker',þ    'group.id': 'mygroup',þ    'auto.offset.reset': 'earliest'þ})þþc.subscribe(['mytopic'])þþwhile True:þ    msg = c.poll(1.0)þþ    if msg is None:þ        continueþ    if msg.error():þ        print(""Consumer error: {}"".format(msg.error()))þ        continueþþ    print('Received message: {}'.format(msg.value().decode('utf-8')))þþc.close()þ```þþ**AvroProducer**þþ```pythonþfrom confluent_kafka import avroþfrom confluent_kafka.avro import AvroProducerþþþvalue_schema_str = """"""þ{þ   ""namespace"": ""my.test"",þ   ""name"": ""value"",þ   ""type"": ""record"",þ   ""fields"" : [þ     {þ       ""name"" : ""name"",þ       ""type"" : ""string""þ     }þ   ]þ}þ""""""þþkey_schema_str = """"""þ{þ   ""namespace"": ""my.test"",þ   ""name"": ""key"",þ   ""type"": ""record"",þ   ""fields"" : [þ     {þ       ""name"" : ""name"",þ       ""type"" : ""string""þ     }þ   ]þ}þ""""""þþvalue_schema = avro.loads(value_schema_str)þkey_schema = avro.loads(key_schema_str)þvalue = {""name"": ""Value""}þkey = {""name"": ""Key""}þþþdef delivery_report(err, msg):þ    """""" Called once for each message produced to indicate delivery result.þ        Triggered by poll() or flush(). """"""þ    if err is not None:þ        print('Message delivery failed: {}'.format(err))þ    else:þ        print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))þþþavroProducer = AvroProducer({þ    'bootstrap.servers': 'mybroker,mybroker2',þ    'on_delivery': delivery_report,þ    'schema.registry.url': 'http://schema_registry_host:port'þ    }, default_key_schema=key_schema, default_value_schema=value_schema)þþavroProducer.produce(topic='my_topic', value=value, key=key)þavroProducer.flush()þ```þþ**AvroConsumer**þþ```pythonþfrom confluent_kafka.avro import AvroConsumerþfrom confluent_kafka.avro.serializer import SerializerErrorþþþc = AvroConsumer({þ    'bootstrap.servers': 'mybroker,mybroker2',þ    'group.id': 'groupid',þ    'schema.registry.url': 'http://127.0.0.1:8081'})þþc.subscribe(['my_topic'])þþwhile True:þ    try:þ        msg = c.poll(10)þþ    except SerializerError as e:þ        print(""Message deserialization failed for {}: {}"".format(msg, e))þ        breakþþ    if msg is None:þ        continueþþ    if msg.error():þ        print(""AvroConsumer error: {}"".format(msg.error()))þ        continueþþ    print(msg.value())þþc.close()þ```þþ**AdminClient**þþCreate topics:þþ```pythonþfrom confluent_kafka.admin import AdminClient, NewTopicþþa = AdminClient({'bootstrap.servers': 'mybroker'})þþnew_topics = [NewTopic(topic, num_partitions=3, replication_factor=1) for topic in [""topic1"", ""topic2""]]þ# Note: In a multi-cluster production scenario, it is more typical to use a replication_factor of 3 for durability.þþ# Call create_topics to asynchronously create topics. A dictþ# of <topic,future> is returned.þfs = a.create_topics(new_topics)þþ# Wait for each operation to finish.þfor topic, f in fs.items():þ    try:þ        f.result()  # The result itself is Noneþ        print(""Topic {} created"".format(topic))þ    except Exception as e:þ        print(""Failed to create topic {}: {}"".format(topic, e))þ```þþþþThread Safetyþ-------------þþThe `Producer`, `Consumer` and `AdminClient` are all thread safe.þþþInstallþ=======þþ**Install self-contained binary wheels**þþ    $ pip install confluent-kafkaþþ**NOTE:** The pre-built Linux wheels do NOT contain SASL Kerberos/GSSAPI support.þ          If you need SASL Kerberos/GSSAPI support you must install librdkafka andþ          its dependencies using the repositories below and then buildþ          confluent-kafka  using the command in the ""Install fromþ          source from PyPi"" section below.þþ**Install AvroProducer and AvroConsumer**þþ    $ pip install ""confluent-kafka[avro]""þþ**Install from source from PyPi**þ*(requires librdkafka + dependencies to be installed separately)*:þþ    $ pip install --no-binary :all: confluent-kafkaþþþFor source install, see *Prerequisites* below.þþþBroker Compatibilityþ====================þThe Python client (as well as the underlying C library librdkafka) supportsþall broker versions &gt;= 0.8.þBut due to the nature of the Kafka protocol in broker versions 0.8 and 0.9 itþis not safe for a client to assume what protocol version is actually supportedþby the broker, thus you will need to hint the Python client what protocolþversion it may use. This is done through two configuration settings:þþ * `broker.version.fallback=YOUR_BROKER_VERSION` (default 0.9.0.1)þ * `api.version.request=true|false` (default true)þþWhen using a Kafka 0.10 broker or later you don't need to do anythingþ(`api.version.request=true` is the default).þIf you use Kafka broker 0.9 or 0.8 you must setþ`api.version.request=false` and setþ`broker.version.fallback` to your broker version,þe.g `broker.version.fallback=0.9.0.1`.þþMore info here:þhttps://github.com/edenhill/librdkafka/wiki/Broker-version-compatibilityþþþSSL certificatesþ================þIf you're connecting to a Kafka cluster through SSL you will need to configureþthe client with `'security.protocol': 'SSL'` (or `'SASL_SSL'` if SASLþauthentication is used).þþThe client will use CA certificates to verify the broker's certificate.þThe embedded OpenSSL library will look for CA certificates in `/usr/lib/ssl/certs/`þor `/usr/lib/ssl/cacert.pem`. CA certificates are typically provided by theþLinux distribution's `ca-certificates` package which needs to be installedþthrough `apt`, `yum`, et.al.þþIf your system stores CA certificates in another location you will need toþconfigure the client with `'ssl.ca.location': '/path/to/cacert.pem'`. þþAlternatively, the CA certificates can be provided by the [certifi](https://pypi.org/project/certifi/)þPython package. To use certifi, add an `import certifi` line and configure theþclient's CA location with `'ssl.ca.location': certifi.where()`.þþþPrerequisitesþ=============þþ * Python >= 2.7 or Python 3.xþ * [librdkafka](https://github.com/edenhill/librdkafka) >= 1.4.0 (latest release is embedded in wheels)þþlibrdkafka is embedded in the macosx manylinux wheels, for other platforms, SASL Kerberos/GSSAPI support orþwhen a specific version of librdkafka is desired, following these guidelines:þþ  * For **Debian/Ubuntu** based systems, add this APT repo and then do `sudo apt-get install librdkafka-dev python-dev`:þhttp://docs.confluent.io/current/installation.html#installation-aptþþ * For **RedHat** and **RPM**-based distros, add this YUM repo and then do `sudo yum install librdkafka-devel python-devel`:þhttp://docs.confluent.io/current/installation.html#rpm-packages-via-yumþþ * On **OSX**, use **homebrew** and do `brew install librdkafka`þþþDeveloper Notesþ===============þþInstructions on building and testing confluent-kafka-python can be found [here](DEVELOPER.md)."
karlstav/cava,7041,1623,44,121,User,False,615,4,16,40,False,Console-based Audio Visualizer for Alsa,,0,6,0,12,203,3,26,0,146,0,43,2280,10,96,6610,4504,0,0,3,,29,,"C.A.V.A. [![Build Status](https://github.com/karlstav/cava/workflows/build/badge.svg)](https://github.com/karlstav/cava/actions)þ====================þþ**C**onsole-based **A**udio **V**isualizer for **A**LSAþþalso supports audio input from Pulseaudio, fifo (mpd), sndio, squeezelite and portaudio.þþNow also works on macOS!þþby [Karl Stavestrand](mailto:karl@stavestrand.no)þþ![spectrum](https://raw.githubusercontent.com/karlstav/cava/gh-pages/cava_gradient.gif ""spectrum"")þþthanks to [anko](https://github.com/anko) for the gif, here is the [recipe]( http://unix.stackexchange.com/questions/113695/gif-screencastng-the-unix-way).þþ[Demo video](https://youtu.be/9PSp8VA6yjU)þþ<!-- START doctoc generated TOC please keep comment here to allow auto update -->þ<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->þ**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*þþ- [What it is](#what-it-is)þ- [Installing](#installing)þ  - [From Source](#from-source)þ    - [Installing Build Requirements](#installing-build-requirements)þ    - [Building](#building)þ    - [Installing](#installing-1)þ    - [Uninstalling](#uninstalling)þ  - [Some distro specific pre-made binaries/recipes](#some-distro-specific-pre-made-binariesrecipes)þ    - [openSUSE](#opensuse)þ    - [Fedora](#fedora)þ    - [Arch](#arch)þ    - [Ubuntu](#ubuntu)þ- [Capturing audio](#capturing-audio)þ  - [From Pulseaudio monitor source (Easy, default if supported)](#from-pulseaudio-monitor-source-easy-default-if-supported)þ  - [From ALSA-loopback device (Tricky)](#from-alsa-loopback-device-tricky)þ  - [From mpd's fifo output](#from-mpds-fifo-output)þ  - [sndio](#sndio)þ  - [squeezelite](#squeezelite)þ  - [macOS](#macos)þ- [Running via ssh](#running-via-ssh)þ  - [Raw Output](#raw-output)þ- [Font notes](#font-notes)þ  - [In ttys](#in-ttys)þ  - [In terminal emulators](#in-terminal-emulators)þ- [Latency notes](#latency-notes)þ- [Usage](#usage)þ  - [Controls](#controls)þ- [Configuration](#configuration)þ- [Contribution](#contribution)þþ<!-- END doctoc generated TOC please keep comment here to allow auto update -->þþþWhat it isþ----------þþC.A.V.A. is a bar spectrum audio visualizer for the Linux terminal using ALSA, pulseaudio or fifo buffer for input.þþThis program is not intended for scientific use. It's written to look responsive and aesthetic when used to visualize music. þþþInstallingþ------------------þþ### From Sourceþþ#### Installing Build RequirementsþþRequired components:þ* [FFTW](http://www.fftw.org/)þ* libtoolþ* automakeþ* build-essentialsþ* [iniparser](https://github.com/ndevilla/iniparser)þþþRecomended components:þ* [ncursesw dev files](http://www.gnu.org/software/ncurses/) (bundled in ncurses in arch)þ* [ALSA dev files](http://alsa-project.org/), orþ* [Pulseaudio dev files](http://freedesktop.org/software/pulseaudio/doxygen/), orþ* Portaudio, orþ* SndioþþOnly FFTW and the other build tools are actually required for CAVA to compile, but this will only give you the ability to read from fifo files. To more easly grab audio from your system pulseaudio, alsa, sndio or portaudio dev files are recommended (depending on what audio system you are using). Not sure how to get the pulseaudio dev files for other distros than debian/ubuntu or if they are bundled in pulseaudio. þþþFor better a better visual experience ncurses is also recomended.þþIniparser is also required, but if it is not already installed, a bundled version will be used.þþAll the requirements can be installed easily in all major distros:þþDebian Buster or higher/Ubuntu 18.04 or higher :þþ    apt-get install libfftw3-dev libasound2-dev libncursesw5-dev libpulse-dev libtool automake libiniparser-devþ    export CPPFLAGS=-I/usr/include/iniparserþþþolder Debian/Ubuntu:þþ    apt-get install libfftw3-dev libasound2-dev libncursesw5-dev libpulse-dev libtool automakeþþþArchLinux:þþ    pacman -S base-devel fftw ncurses alsa-lib iniparser pulseaudioþþþopenSUSE:þþ    zypper install alsa-devel ncurses-devel fftw3-devel libpulse-devel libtoolþþþFedora:þþ    dnf install alsa-lib-devel ncurses-devel fftw3-devel pulseaudio-libs-devel libtoolþþ    þmacOS:þþFirst install homebrew if you have't already:þþ    /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""þþThen install prerequisites:þþ    brew install fftw ncurses libtool automake portaudioþ    þThen fix macOS not finding libtool and ncursesw:þþ    export LIBTOOL=`which glibtool`þ    export LIBTOOLIZE=`which glibtoolize`þ    ln -s `which glibtoolize` libtoolizeþ    ln -s /usr/lib/libncurses.dylib /usr/local/lib/libncursesw.dylibþþTested on macOS High Sierra.þþþþ#### Buildingþ First of all clone this repo and cd in to it, then run:þ þ    ./autogen.shþ    ./configureþ    makeþþIf you have a recommended component installed, but do not wish to use it (perhaps if building a binary on one machine to be used on another), then the corresponding feature can be disabled during configuration (see configure --help for details).þþþ    þ#### Installing þþInstall `cava` to default `/usr/local`:þþ    make installþþOr you can change `PREFIX`, for example:þþ   ./configure --prefix=PREFIXþþ#### Uninstallingþþ    make uninstallþþþ### Some distro specific pre-made binaries/recipes    þ#### openSUSEþþTumbleweed users have cava in their repo. They can just use:þþ    zypper in cavaþþLeap users need to add the multimedia:apps repository first:þþ    zypper ar -f obs://multimedia:apps/openSUSE_Leap_42.2 multimediaþþIf you use another version just replace *openSUSE_Leap_42.2* with *openSUSE_13.2*, adjust it to your version.þþ#### FedoraþþCava is available in Fedora 26 and later.  You can install Cava byþrunning:þþ    dnf install cavaþþ#### ArchþþCava is in [AUR](https://aur.archlinux.org/packages/cava/).þþ    pacaur -S cavaþþ#### UbuntuþþMichael Nguyen has added CAVA to his PPA, it can be installed with:þþ    sudo add-apt-repository ppa:tehtotalpwnage/ppaþ    sudo apt-get updateþ    sudo apt-get install cavaþ    þFor Ubuntu 18 or newer, you can use Harshal Sheth's PPA:þþ    sudo add-apt-repository ppa:hsheth2/ppaþ    sudo apt-get updateþ    sudo apt-get install cavaþþAll distro specific instalation sources might be out of date.þþþCapturing audioþ---------------þþ### From Pulseaudio monitor source (Easy, default if supported)þþFirst make sure you have installed pulseaudio dev files and that cava has been built with pulseaudio support (it should be automatically if the dev files are found).þþIf you're lucky all you have to do is to uncomment this line in the config file under input:þþ    method = pulseþ þIf nothing happens you might have to use a different source than the default. The default might also be your microphone. Look at the config file for help. þþþ### From ALSA-loopback device (Tricky)þþSetþþ    method = alsaþþin the config file.þþALSA can be difficult because there is no native way to grab audio from an output. If you want to capture audio straight fom the output (not just mic or line-in), you must create an ALSA loopback interface, then output the audio simultaneously to both the loopback and your normal interface.þþTo create a loopback interface simply run:þþ`sudo modprobe snd_aloop`þþHopefully your `aplay -l` should now contain a loopback interface.þþTo make it persistent across boot add the line `snd-aloop` to ""/etc/modules"". To keep it from being loaded as the first soundcard add the line `options snd-aloop index=1` to ""/etc/modprobe.d/alsa-base.conf"", this will load it at '1'. You can replace '1' with whatever makes most sense in your audio setup.þþPlaying the audio through your Loopback interface makes it possible for cava to capture it, but there will be no sound in your speakers. In order to play audio on the loopback interface and your actual interface you must make use of the ALSA multi channel.þþLook at the included example file `example_files/etc/asound.conf` on how to use the multi channel. I was able to make this work on my laptop (an Asus UX31 running Ubuntu), but I had no luck with the ALSA method on my Raspberry Pi (Rasbian) with an USB DAC. The PulseAudio method however works perfectly on my Pi. þþRead more about the ALSA method [here](http://stackoverflow.com/questions/12984089/capture-playback-on-play-only-sound-card-with-alsa).þþIf you are having problems with the alsa method on Rasberry PI, try enabling `mmap` by adding the following line to `/boot/config.txt` and reboot:þþ```þdtoverlay=i2s-mmapþ```þþ### From mpd's fifo outputþþAdd these lines in mpd:þþ    audio_output {þ        type                    ""fifo""þ        name                    ""my_fifo""þ        path                    ""/tmp/mpd.fifo""þ        format                  ""44100:16:2""þ    }þþUncomment and change input method to `fifo` in the config file.þþThe path of the fifo can be specified with the `source` parameter.þþI had some trouble with sync (the visualizer was ahead of the sound). Reducing the ALSA buffer in mpd fixed it:þþ    audio_output {þ            type            ""alsa""þ            name            ""My ALSA""þ            buffer_time     ""50000""   # (50ms); default is 500000 microseconds (0.5s)þ    }þþ### sndioþþsndio is the audio framework used on OpenBSD, but it's also available onþFreeBSD and Linux. So far this is only tested on FreeBSD.þþTo test itþ```bashþ# Start sndiod with a monitor sub-deviceþ$ sndiod -dd -s default -m mon -s monitorþþ# Set the AUDIODEVICE environment variable to override the defaultþ# sndio device and run cavaþ$ AUDIODEVICE=snd/0.monitor cavaþ```þþ### squeezeliteþ[squeezelite](https://en.wikipedia.org/wiki/Squeezelite) is one of several software clients available for the Logitech Media Server. Squeezelite can export its audio data as shared memory, which is what this input module uses.þJust adapt your config:þ```þmethod = shmemþsource = /squeezelite-AA:BB:CC:DD:EE:FFþ```þwhere `AA:BB:CC:DD:EE:FF` is squeezelite's MAC address (check the LMS Web GUI (Settings>Information) if unsure).þNote: squeezelite must be started with the `-v` flag to enable visualizer support.þþ### macOSþInstall [Soundflower](https://github.com/mattingalls/Soundflower) to create a loopback interface. Use Audio MIDI Setup to configure a virtual interface that outputs audio to both your speakers and the loopbacl interface, following [this](https://github.com/RogueAmoeba/Soundflower-Original/issues/44#issuecomment-151586106) recipe.þþThen edit your config to use this interface with portaudio:þþ```þmethod = portaudioþsource = ""Soundflower (2ch)""þ```þþNote: cava looks no good in the default macOS terminal. For a better look install [kitty](https://sw.kovidgoyal.net/kitty/index.html). Be where that you might run into #109, that can be fixed like [this](https://stackoverflow.com/questions/7165108/in-os-x-lion-lang-is-not-set-to-utf-8-how-to-fix-it).þþþRunning via sshþ---------------þþTo run via ssh to an external monitor, redirect output to `/dev/console`:þþ     ~# ./cava  <> /dev/console >&0 2>&1þþexit with ctrl+z then run 'bg' to keep it running after you log out.þþ(You must be root to redirect to console. Simple sudo is not enough: Run `sudo su` first.)þþ### Raw OutputþþYou can also use Cava's output for other programs by using raw output mode, which will write bar data to `STDOUT` that can be piped into other processes. More information on this option is documented in [the example config file](/example_files/config).þþFont notesþ----------þþSince the graphics are simply based on characters, performance is dependent on the terminal font.þþ### In ttysþþIf you run this in a TTY the program will change the font to the included `cava.psf` (actually a slightly modified ""unifont"").þþIn console fonts it seems that only 256 Unicode characters are supported, probably because they are bitmap fonts. I could not find a font with Unicode characters 2581-2587 (the 1/8 - 7/8 blocks used on the top of each bar to increase resolution).þþSo in `cava.psf`, the characters 1-7 are actually replaced by Unicode characters 2581-2587. When cava exits, it changes the font back. If cava exits abnormally and you notice that 1-7 are replaced by partial blocks, just change the font with `setfont`.þþActually, `setfont` is supposed to return the default font, but this usually isn't set. I haven't found another way to get the current font. So cava sets the font to ""Lat2-Fixed16"" when interrupted. All major distros should have it. It will revert to your default font at reboot.þþ### In terminal emulatorsþþIn terminal emulators like `xterm`, the font settings is chosen in the software and cannot be changed by an application. So find your terminal settings and try out different fonts and settings. Also character spacing affects the look of the bar spectrum.þþPerformance is also different, urxvt is the best I found so far, while Gnome-terminal is quite slow.þþCava also disables the terminal cursor, and turns it back on on exit, but in case it terminates unexpectedly, run `setterm -cursor on` to get it back.þþTip: Cava will look much nicer in small font sizes. Use a second terminal emulator for cava and set the font size to 1. Warning, can cause high CPU usage and latency if the terminal window is too large!þþþLatency notesþ-------------þþIf you see latency issues (sound before image) in a terminal emulator, try increasing the font size. This will reduce the number of characters that have to be shown.þþIf your audio device has a huge buffer, you might experience that cava is actually faster than the audio you hear. This reduces the experience of the visualization. To fix this, try decreasing the buffer settings in your audio playing software.þþUsageþ-----þþ    Usage : cava [options]þ    Visualize audio input in terminal. þþ    Options:þ         -p          path to config fileþ         -v          print versionþþþþExit with ctrl+c or q.þþIf cava quits unexpectedly or is force killed, echo must be turned on manually with `stty -echo`.þþ### ControlsþþNOTE: only works in ncurses output mode.þþ| Key | Description |þ| --- | ----------- |þ| <kbd>up</kbd> / <kbd>down</kbd>| increase/decrease sensitivity |þ| <kbd>left</kbd> / <kbd>right</kbd>| increase/decrease bar width |þ| <kbd>f</kbd> / <kbd>b</kbd>| change foreground/background color |þ| <kbd>r</kbd> | Reload configuration |þ| <kbd>c</kbd> | Reload colors only |þ| <kbd>q</kbd> or <kbd>CTRL-C</kbd>| Quit C.A.V.A. |þþConfigurationþ-------------þþAs of version 0.4.0 all options are done in the config file, no more command-line arguments!þþBy default a configuration file is located in `$XDG_CONFIG_HOME/cava/config` or `$HOME/.config/cava/config`, but cava can also be made to use a different file with the `-p` option.þþIf for some reason the config file is not in the config dir, copy the [bundled configuration file](/example_files/config) to the config dir manually.þþSending cava a SIGUSR1 signal, will force cava to reload its configuration file. Thus, it behaves as if the user pressed <kbd>r</kbd> in the terminal. One might send a SIGUSR1 signal using `pkill` or `killall`.þFor example:þ```þ$ pkill -USR1 cavaþ```þþSimilarly, sending cava a SIGUSR2 signal will only reload the colors from the configuration file, which is the same as pressing <kbd>c</kbd> in the terminal. This is slightly faster than reloading the entire config as the audio processing does not need to reinitialize.  þ```þ$ pkill -USR2 cavaþ```þþ**Examples on how the equalizer works:**þþ    [eq]þ    1=0þ    2=1þ    3=0þ    4=1þ    5=0þþ![3_138](https://cloud.githubusercontent.com/assets/6376571/8670183/a54a851e-29e8-11e5-9eff-346bf6ed91e0.png)þþ    [eq]þ    1=2þ    2=2þ    3=1þ    4=1þ    5=0.5þþ![3_139](https://cloud.githubusercontent.com/assets/6376571/8670181/9db0ef50-29e8-11e5-81bc-3e2bb9892da0.png)þþContributionþ------þþPlease read CONTRIBUTING.md before opening a pull request.þþThanks to:þ* [CelestialWalrus](https://github.com/CelestialWalrus)þ* [anko](https://github.com/anko)þ* [livibetter](https://github.com/livibetter)þþfor major contributions in the early development of this project.þþAlso thanks to [dpayne](https://github.com/dpayne/) for figuring out how to find the pulseaudio default sink name."
iNavFlight/inav,249989,1291,149,742,Organization,False,9689,60,66,261,False,INAV: Navigation-enabled flight control software,https://inavflight.github.io,6,22,2,301,2766,104,221,69,2697,31,215,2630,20,291,10825,7978,0,0,14,3,,,"# INAV - navigation capable flight controllerþþ## F3 based flight controllersþþ> STM32 F3 flight controllers like Omnibus F3 or SP Racing F3 are deprecated and soon they will reach the end of support in INAV. If you are still using F3 boards, please migrate to F4 or F7.þþ![INAV](http://static.rcgroups.net/forums/attachments/6/1/0/3/7/6/a9088858-102-inav.png)þ![Travis CI status](https://travis-ci.org/iNavFlight/inav.svg?branch=master)þþ## Featuresþþ* Runs on the most popular F4 and F7 flight controllersþ* Outstanding performance out of the boxþ* Position Hold, Altitude Hold, Return To Home and Missionsþ* Excellent support for fixed wing UAVs: airplanes, flying wings þ* Fully configurable mixer that allows to run any hardware you want: multirotor, fixed wing, rovers, boats and other experimental devicesþ* Multiple sensor support: GPS, Pitot tube, sonar, lidar, temperature, ESC with BlHeli_32 telemetryþ* SmartAudio and IRC Tramp VTX supportþ* DSHOT and Multishot ESCsþ* Blackbox flight recorder loggingþ* On Screen Display (OSD) - both character and pixel styleþ* Telemetry: SmartPort, FPort, MAVlink, LTMþ* Multi-color RGB LED Strip supportþ* Advanced gyro filtering: Matrix Filter and RPM filterþ* Logic Conditions, Global Functions and Global Variables: you can program INAV with a GUIþ* And many more!þþFor a list of features, changes and some discussion please review consult the releases [page](https://github.com/iNavFlight/inav/releases) and the documentation.þþ## Toolsþþ### INAV ConfiguratorþþOfficial tool for INAV can be downloaded [here](https://github.com/iNavFlight/inav-configurator/releases). It can be run on Windows, MacOS and Linux machines and standalone application.  þþ### INAV Blackbox ExplorerþþTool for Blackbox logs analysis is available [here](https://github.com/iNavFlight/blackbox-log-viewer/releases)þþ### Telemetry screen for OpenTXþþUsers of FrSky Taranis X9 and Q X7 can use INAV Lua Telemetry screen created by @teckel12 . Software and installation instruction are available here: [https://github.com/iNavFlight/LuaTelemetry](https://github.com/iNavFlight/LuaTelemetry)þþ## InstallationþþSee: https://github.com/iNavFlight/inav/blob/master/docs/Installation.mdþþ## Documentation, support and learning resourcesþ* [Fixed Wing Guide](docs/INAV_Fixed_Wing_Setup_Guide.pdf)þ* [Autolaunch Guide](docs/INAV_Autolaunch.pdf)þ* [Modes Guide](docs/INAV_Modes.pdf)þ* [Wing Tuning Masterclass](docs/INAV_Wing_Tuning_Masterclass.pdf)þ* [Official documentation](https://github.com/iNavFlight/inav/tree/master/docs)þ* [Official Wiki](https://github.com/iNavFlight/inav/wiki)þ* [INAV Official on Telegram](https://t.me/INAVFlight)þ* [INAV Official on Facebook](https://www.facebook.com/groups/INAVOfficial)þ* [RC Groups Support](https://www.rcgroups.com/forums/showthread.php?2495732-Cleanflight-iNav-(navigation-rewrite)-project)þ* [Video series by Painless360](https://www.youtube.com/playlist?list=PLYsWjANuAm4qdXEGFSeUhOZ10-H8YTSnH)þ* [Video series by Paweł Spychalski](https://www.youtube.com/playlist?list=PLOUQ8o2_nCLloACrA6f1_daCjhqY2x0fB)þþ## ContributingþþContributions are welcome and encouraged.  You can contribute in many ways:þþ* Documentation updates and corrections.þ* How-To guides - received help?  help others!þ* Bug fixes.þ* New features.þ* Telling us your ideas and suggestions.þ* Buying your hardware from this [link](https://inavflight.com/shop/u/bg/)þþA good place to start is Telegram channel or Facebook group. Drop in, say hi.þþGithub issue tracker is a good place to search for existing issues or report a new bug/feature request:þþhttps://github.com/iNavFlight/inav/issuesþþhttps://github.com/iNavFlight/inav-configurator/issuesþþBefore creating new issues please check to see if there is an existing one, search first otherwise you waste peoples time when they could be coding instead!þþ## DevelopersþþPlease refer to the development section in the [docs/development](https://github.com/iNavFlight/inav/tree/master/docs/development) folder.þþþ## INAV Releasesþhttps://github.com/iNavFlight/inav/releases"
openssh/openssh-portable,21422,1104,108,850,Organization,False,10542,53,140,51,False,Portable OpenSSH,,6,6,0,,,,,37,147,14,17,7537,10,353,10414,6886,0,0,3,0,,,"# Portable OpenSSHþþ[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/openssh.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:openssh)þþOpenSSH is a complete implementation of the SSH protocol (version 2) for secure remote login, command execution and file transfer. It includes a client ``ssh`` and server ``sshd``, file transfer utilities ``scp`` and ``sftp`` as well as tools for key generation (``ssh-keygen``), run-time key storage (``ssh-agent``) and a number of supporting programs.þþThis is a port of OpenBSD's [OpenSSH](https://openssh.com) to most Unix-like operating systems, including Linux, OS X and Cygwin. Portable OpenSSH polyfills OpenBSD APIs that are not available elsewhere, adds sshd sandboxing for more operating systems and includes support for OS-native authentication and auditing (e.g. using PAM).þþ## DocumentationþþThe official documentation for OpenSSH are the man pages for each tool:þþ* [ssh(1)](https://man.openbsd.org/ssh.1)þ* [sshd(8)](https://man.openbsd.org/sshd.8)þ* [ssh-keygen(1)](https://man.openbsd.org/ssh-keygen.1)þ* [ssh-agent(1)](https://man.openbsd.org/ssh-agent.1)þ* [scp(1)](https://man.openbsd.org/scp.1)þ* [sftp(1)](https://man.openbsd.org/sftp.1)þ* [ssh-keyscan(8)](https://man.openbsd.org/ssh-keyscan.8)þ* [sftp-server(8)](https://man.openbsd.org/sftp-server.8)þþ## Stable ReleasesþþStable release tarballs are available from a number of [download mirrors](https://www.openssh.com/portable.html#downloads). We recommend the use of a stable release for most users. Please read the [release notes](https://www.openssh.com/releasenotes.html) for details of recent changes and potential incompatibilities.þþ## Building Portable OpenSSHþþ### DependenciesþþPortable OpenSSH is built using autoconf and make. It requires a working C compiler, standard library and headers, and [zlib](https://www.zlib.net/). ``libcrypto`` from either [LibreSSL](https://www.libressl.org/) or [OpenSSL](https://www.openssl.org) may also be used, but OpenSSH may be built without it supporting a subset of crypto algorithms.þþFIDO security token support need [libfido2](https://github.com/Yubico/libfido2) and its dependencies. Also, certain platforms and build-time options may require additional dependencies, see README.platform for details.þþ### Building a releaseþþReleases include a pre-built copy of the ``configure`` script and may be built using:þþ```þtar zxvf openssh-X.YpZ.tar.gzþcd opensshþ./configure # [options]þmake && make testsþ```þþSee the [Build-time Customisation](#build-time-customisation) section below for configure options. If you plan on installing OpenSSH to your system, then you will usually want to specify destination paths.þ þ### Building from gitþþIf building from git, you'll need [autoconf](https://www.gnu.org/software/autoconf/) installed to build the ``configure`` script. The following commands will check out and build portable OpenSSH from git:þþ```þgit clone https://github.com/openssh/openssh-portable # or https://anongit.mindrot.org/openssh.gitþcd openssh-portableþautoreconfþ./configureþmake && make testsþ```þþ### Build-time CustomisationþþThere are many build-time customisation options available. All Autoconf destination path flags (e.g. ``--prefix``) are supported (and are usually required if you want to install OpenSSH).þþFor a full list of available flags, run ``configure --help`` but a few of the more frequently-used ones are described below. Some of these flags will require additional libraries and/or headers be installed.þþFlag | Meaningþ--- | ---þ``--with-pam`` | Enable [PAM](https://en.wikipedia.org/wiki/Pluggable_authentication_module) support. [OpenPAM](https://www.openpam.org/), [Linux PAM](http://www.linux-pam.org/) and Solaris PAM are supported.þ``--with-libedit`` | Enable [libedit](https://www.thrysoee.dk/editline/) support for sftp.þ``--with-kerberos5`` | Enable Kerberos/GSSAPI support. Both [Heimdal](https://www.h5l.org/) and [MIT](https://web.mit.edu/kerberos/) Kerberos implementations are supported.þ``--with-selinux`` | Enable [SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux) support.þ``--with-security-key-builtin`` | Include built-in support for U2F/FIDO2 security keys. This requires [libfido2](https://github.com/Yubico/libfido2) be installed.þþ## DevelopmentþþPortable OpenSSH development is discussed on the [openssh-unix-dev mailing list](https://lists.mindrot.org/mailman/listinfo/openssh-unix-dev) ([archive mirror](https://marc.info/?l=openssh-unix-dev)). Bugs and feature requests are tracked on our [Bugzilla](https://bugzilla.mindrot.org/).þþ## Reporting bugsþþ_Non-security_ bugs may be reported to the developers via [Bugzilla](https://bugzilla.mindrot.org/) or via the mailing list above. Security bugs should be reported to [openssh@openssh.com](mailto:openssh.openssh.com)."
alibaba/ApsaraCache,69744,896,119,158,Organization,False,6408,3,0,229,False,ApsaraCache is a Redis branch originated from Alibaba Group.,,0,8,0,12,2,1,0,2,2,0,0,4100,0,0,0,0,0,0,318,151,,,"What is ApsaraCache ?þ--------------þþApsaraCache is based on the Redis official release 4.0 and has many features and performance enhancements. ApsaraCache has proven to be very stable and efficient in production environment.þþThere are many features in ApsaraCache, the following two are included in this release and the other features will be gradually released in the subsequent, so stay tuned.þþ* ApsaraCache supports two main protocols of Memcached: the classic ASCII, and the newer binary. You can use ApsaraCache just as Memcached, and no client code need to be modified. You can persist your data by using ApsaraCache in Memcached mode just like Redis.þ* In short connection scenario, ApsaraCache makes 30% performance increase compared with the open source version.þþþBuilding ApsaraCacheþ--------------þþIt is as simple as:þþ    % makeþ    þRunning ApsaraCacheþ-------------þIn default, ApsaraCache run in Redis mode. If you want ApsaraCache to run in Memcached mode, just add option þþ       protocol  memcacheþ       þto redis.conf.þþTo run ApsaraCache with the default configuration just type:þþ    % cd srcþ    % ./redis-serverþþIf you want to provide your redis.conf, you have to run it using an additionalþparameter (the path of the configuration file):þþ    % cd srcþ    % ./redis-server /path/to/redis.confþþIt is possible to alter the ApsaraCache configuration by passing parameters directlyþas options using the command line. Examples:þþ    % ./redis-server --port 9999 --slaveof 127.0.0.1 6379þ    % ./redis-server /etc/redis/6379.conf --loglevel debugþþAll the options in redis.conf are also supported as options using the commandþline, with exactly the same name.þþPlaying with ApsaraCache in Redis modeþ------------------þþYou can use redis-cli to play with ApsaraCache. Start a redis-server instance,þthen in another terminal try the following:þþ    % cd srcþ    % ./redis-cliþ    redis> pingþ    PONGþ    redis> set foo barþ    OKþ    redis> get fooþ    ""bar""þ    redis> incr mycounterþ    (integer) 1þ    redis> incr mycounterþ    (integer) 2þ    redis>þþYou can find the list of all the available commands at http://redis.io/commands.þþPlaying with ApsaraCache in Memcached modeþ------------------þþYou can use telnet to visit ApsaraCache(telnet use the classic ASCII protocol).þþ     % telnet your-host your-port(usually 11211)þ     þ       set key 10 3600 2þ       okþ       STOREDþ       get keyþ       VALUE key 10 2þ       okþ       END  þþþþEnjoy!þþþDocumentationþ------------------þ* [Documentation Home](https://github.com/alibaba/ApsaraCache/wiki/ApsaraCache-document)þ* [Frequently Asked Questions](https://github.com/alibaba/ApsaraCache/wiki/frequently-ask-questions)þþContributingþ------------------þSee [ApsaraCache Contributing Guide](https://github.com/alibaba/ApsaraCache/wiki/CONTRIBUTING) for more information."
orangeduck/Corange,240515,1165,95,145,User,False,355,1,0,9,False,Pure C Game Engine,http://www.youtube.com/watch?v=482Gxq…,0,0,0,10,24,7,0,1,24,1,4,3330,1,2,4,4,0,0,16,,1,,"Corange game engineþ===================þ þVersion 0.8.0þþWritten in Pure C, SDL and OpenGL.þþRunningþ-------þ þCorange is a library, but to take a quick look at some of the things it does you can [Look at some of the Demos](http://www.youtube.com/watch?v=482GxqTWXtA). Warning: Some things shown are from a previous version and may not remain the same in this version.þ þþCompilingþ---------þ þTo compile on Windows you need MinGW and then you should be able to run ""make"" as usual. You will need to have installed SDL, SDL_Mixer and SDL_Net.þþ    makeþþTo compile on Linux you need to install SDL2. Then you should run ""make""þþ    sudo apt-get install libsdl2-devþ    sudo apt-get install libsdl2-mixer-devþ    sudo apt-get install libsdl2-net-devþ    makeþþþOverviewþ--------þþ* Small, Simple, Powerful, Cross platformþ* Clean and easy Asset, UI, Entity managementþ* Modern Deferred rendererþþþDemosþ-----þþI'm a graphics programmer by trade so apologies that most of the demos are graphical apps; they're just what I love!þþ* __renderers__ Shows off the various renderers with shaders, shadows, animation etc.þ* __metaballs__ Uses OpenCL/OpenGL interop to do Metaball rendering.þ* __noise__ Feedback based noise pattern on screen using shader. Can generate tileable perlin noise in software.þ* __platformer__ Basic platforming game. Fairly well commented.þ* __sea__ Renders a sea-like surface, a ship, and some collision detection.þ* __scotland__ Demonstrates terrain system.þ* __tessellation__ Demo showing tessellation shaders in OpenGL 4.þþ þFAQþ---þþ* __How is that pronounced?__þþ Rhymes with Purple.þþ* __Why not C++?__þ þ There are plenty of C++ engines which do what I've done here and better. Pure C game engines on the other hand are much rarer. Corange provided me an outlet to practice my C skills. Of course if you are just linking to it you can still program your game/executable using C++.þ þ* __What stuff does it do?__þþ I've used it as a platform for trying out all sorts of techniques and effects. These features are not out-of-the-box or plug-in-and-play, but if you are a developer who has knowledge of what they are, you should be able to utilize what I have written. Some are WIP or rough around the edges.þ þ Deferred Rendering / UI Rendering / Text Rendering. Multiple Lights. Post effects. SSAO. Shadow Mapping. Color Correction. Skeletal Animation. Inverse Kinematics. Collision Detection. OpenCL support. Asset / Entity / UI Management. Terrain. File loaders including .dds, .wav, .bmp, .obj, .smd. Maths and Geometry. And More...þ þ* __Can I use this for 2D stuff?__þ þ Certainly. Though Corange doesn't provide a 2D renderer for you. That you can write yourself. Believe it or not, making a generalized 2D renderer can be exceedingly complicated when you have to optimise for different sprites, tile sets, dynamic objects and all sorts of other effects. You're better off writing the rendering code application specific.þ þ* __Can I contact you about something?__þþ Yes - `contact@theorangeduck.com`þþ  þUsing / Contributingþ--------------------þ þThis is still mainly a personal project and so there are going to be lots of bugs, unfinished features and messy bits of code. The engine is heavily WIP and subject to sweeping changes. It isn't really viable to use without also being part of the project development and in communication with me. Rather than a full game engine like Unity, Corange is more of a framework and gives you access to features at about the same level as XNA.þþI have a big backlog of Work in Progress changes I need to push up to the repository once they get to a reasonable point so if you are interested in those please contact me.þ  þSaying that, it is a great excuse to practise your C and I very much welcome help. If the project appeals to you here are a couple of quick things that might help get you started.þ  þ* First take a look at the demos. These give a brief overview of how Corange can be used. The platformer demo is probably the most commented.þþ* There is no real documentation so your first port of call is the header files and your second is the c files. The code has very minimal comments but should be pretty clear most of the time.þþ* Corange doesn't hide anything from you. OpenGL and SDL calls are in the namespace so you've got access to the basics. The corange_init and corange_finish functions are fairly short so it is even possible to not call them and only use the components you want.þþ* Structs are typedefed without their pointer. The reason for this is a personal choice but there are also quite a few data types which are passed by value on the stack (vectors, matrices, spheres, boxes). I didn't want the notion of these to get confused.þþ* Some important parts of the engine are the asset, UI and entity managers. These basically let you access and store assets (models, textures - objects in the file system) and entities (lights, cameras, engine objects) and UI elements. They clean up memory on destruction and let you get pointers from all parts of the code.þþ* Corange mangles the namespace pretty badly, taking names such as ""error"", ""warning"", ""vec2"" and ""image"". It isn't a general purpose library. But I've still tried to decouple stuff so it should be possible to extract certain code if you need it."
bitcraze/crazyflie-firmware,13423,629,100,628,Organization,False,1560,17,26,56,False,"The main firmware for the Crazyflie Nano Quadcopter, Crazyflie Bolt Quadcopter and Roadrunner Positioning Tag.",,0,9,1,70,323,22,23,2,196,2,17,2644,9,111,32062,8732,0,0,46,8,,,"# Crazyflie Firmware  [![Build Status](https://api.travis-ci.org/bitcraze/crazyflie-firmware.svg)](https://travis-ci.org/bitcraze/crazyflie-firmware)þþThis project contains the source code for the firmware used in the Crazyflie range of platforms, includingþthe Crazyflie 2.X and the Roadrunner.þþ### Crazyflie 1.0 supportþþThe 2017.06 release was the last release with Crazyflie 1.0 support. If you wantþto play with the Crazyflie 1.0 and modify the code, please clone this repo andþbranch off from the 2017.06 tag.þþ## DependenciesþþYou'll need to use either the [Crazyflie VM](https://wiki.bitcraze.io/projects:virtualmachine:index),þ[the toolbelt](https://wiki.bitcraze.io/projects:dockerbuilderimage:index) orþinstall some ARM toolchain.þþ### Install a toolchainþþ#### OS Xþ```bashþbrew tap PX4/homebrew-px4þbrew install gcc-arm-none-eabiþ```þþ#### Debian/UbuntuþþTested on Ubuntu 14.04 64b, Ubuntu 16.04 64b, and Ubuntu 18.04 64b:þþFor Ubuntu 14.04 :þþ```bashþsudo add-apt-repository ppa:terry.guo/gcc-arm-embeddedþsudo apt-get updateþsudo apt-get install libnewlib-arm-none-eabiþ```þþFor Ubuntu 16.04 and Ubuntu 18.04:þþ```bashþsudo add-apt-repository ppa:team-gcc-arm-embedded/ppaþsudo apt-get updateþsudo apt install gcc-arm-embeddedþ```þþNote: Do not use the `gcc-arm-none-eabi` package that is part of the Ubuntu repository as this is outdated.þþ#### Arch Linuxþþ```bashþsudo pacman -S community/arm-none-eabi-gcc community/arm-none-eabi-gdb community/arm-none-eabi-newlibþ```þþ#### WindowsþþThe GCC ARM Embedded toolchain for Windows is available at [launchpad.net](https://launchpad.net/gcc-arm-embedded/+download). Download the zip archive rather than the executable installer. There are a few different systems for running UNIX-style shells and build systems on Windows; the instructions below are for [Cygwin](https://www.cygwin.com/).þþInstall Cygwin with [setup-x86_64.exe](https://www.cygwin.com/setup-x86_64.exe). Use the standard `C:\cygwin64` installation directory and install at least the `make` and `git` packages.þþDownload the latest `gcc-arm-none-eabi-*-win32.zip` archive from [launchpad.net](https://launchpad.net/gcc-arm-embedded/+download). Create the directory `C:\cygwin64\opt\gcc-arm-none-eabi` and extract the contents of the zip file to it.þþLaunch a Cygwin terminal and run the following to append to your `~/.bashrc` file:þ```bashþecho '[[ $PATH == */opt/gcc-arm-none-eabi/bin* ]] || export PATH=/opt/gcc-arm-none-eabi/bin:$PATH' >>~/.bashrcþsource ~/.bashrcþ```þþVerify the toolchain installation with `arm-none-eabi-gcc --version`þþ### CloningþþThis repository uses git submodules. Clone with the `--recursive` flagþþ```bashþgit clone --recursive https://github.com/bitcraze/crazyflie-firmware.gitþ```þþIf you already have cloned the repo without the `--recursive` option, you need toþget the submodules manuallyþþ```bashþcd crazyflie-firmwareþgit submodule initþgit submodule updateþ```þþþ## Compilingþþ### Crazyflie 2.XþþThis is the default build so just running ```make``` is enough or:þ```bashþmake PLATFORM=cf2þ```þþor with the toolbeltþþ```bashþtb make PLATFORM=cf2þ```þþ### RoadrunnerþþUse the ```tag``` platformþþ```bashþmake PLATFORM=tagþ```þþor with the toolbeltþþ```bashþtb make PLATFORM=tagþ```þþþ### config.mkþTo create custom build options create a file called `config.mk` in the `tools/make/`þfolder and fill it with options. E.g.þ```þPLATFORM=CF2þDEBUG=1þ```þMore information can be found on theþ[Bitcraze documentation](https://www.bitcraze.io/documentation/repository/crazyflie-firmware/master/)þþþ```þ# Make targets:þ```þall        : Shortcut for buildþcompile    : Compile cflie.hex. WARNING: Do NOT update version.cþbuild      : Update version.c and compile cflie.elf/hexþclean_o    : Clean only the Objects files, keep the executables (ie .elf, .hex)þclean      : Clean every compiled filesþmrproper   : Clean every compiled files and the classical editors backup filesþþcload      : If the crazyflie-clients-python is placed on the same directory level andþ             the Crazyradio/Crazyradio PA is inserted it will try to flash the firmwareþ             using the wireless bootloader.þflash      : Flash .elf using OpenOCDþhalt       : Halt the target using OpenOCDþreset      : Reset the target using OpenOCDþopenocd    : Launch OpenOCDþ```þþ# Unit testingþþ## Running all unit testsþþWith the environment set up locallyþþ        make unitþþwith the docker builder image and the toolbeltþþ        tb make unitþþ## Running one unit testþþWhen working with one specific file it is often convenient to run only one unit testþþ       make unit FILES=test/utils/src/test_num.cþþor with the toolbeltþþ       tb make unit FILES=test/utils/src/test_num.cþþ## Running unit tests with specific build settingsþþDefines are managed by make and are passed on to the unit test code. Use theþnormal ways of configuring make when running tests. For instance to run testþfor Crazyflie 1þþ      make unit LPS_TDOA_ENABLE=1þþ## DependenciesþþFrameworks for unit testing and mocking are pulled in as git submodules.þþThe testing framework uses ruby and rake to generate and run code.þþTo minimize the need for installations and configuration, use the docker builderþimage (bitcraze/builder) that contains all tools needed. All scripts in theþtools/build directory are intended to be run in the image. Theþ[toolbelt](https://wiki.bitcraze.io/projects:dockerbuilderimage:index) makes itþeasy to run the tool scripts."
danielgtaylor/jpeg-archive,2052,1021,47,106,User,False,159,2,8,18,False,Utilities for archiving JPEGs for long term storage.,,0,6,0,36,52,3,1,4,27,1,2,2350,2,2,6,6,0,0,86,,272,,"JPEG Archive [![Build Status](http://img.shields.io/travis/danielgtaylor/jpeg-archive.svg?style=flat)](https://travis-ci.org/danielgtaylor/jpeg-archive) [![Build status](https://ci.appveyor.com/api/projects/status/1p7hrrq380xuqlyh?svg=true)](https://ci.appveyor.com/project/danielgtaylor/jpeg-archive) [![Version](http://img.shields.io/badge/version-2.2.0-blue.svg?style=flat)](https://github.com/danielgtaylor/jpeg-archive/releases) [![License](http://img.shields.io/badge/license-MIT-red.svg?style=flat)](http://dgt.mit-license.org/)þ============þUtilities for archiving photos for saving to long term storage or serving over the web. The goals are:þþ * Use a common, well supported format (JPEG)þ * Minimize storage space and costþ * Identify duplicates / similar photosþþApproach:þþ * Command line utilities and scriptsþ * Simple options and useful helpþ * Good quality output via sane defaultsþþContributions to this project are very welcome.þþDownloadþ--------þYou can download the latest source and binary releases from the [JPEG Archive releases page](https://github.com/danielgtaylor/jpeg-archive/releases). Windows binaries for the latest commit are available from the [Windows CI build server](https://ci.appveyor.com/project/danielgtaylor/jpeg-archive/build/artifacts).þþIf you are looking for an easy way to run these utilities in parallel over many files to utilize all CPU cores, please also download [Ladon](https://github.com/danielgtaylor/ladon) or [GNU Parallel](https://www.gnu.org/software/parallel/). You can then use the `jpeg-archive` command below or use `ladon` directly. Example:þþ```bashþ# Re-compress JPEGs and replace the originalsþladon ""Photos/**/*.jpg"" -- jpeg-recompress FULLPATH FULLPATHþþ# Re-compress JPEGs into the new directory 'Comp'þladon -m Comp/RELDIR ""Photos/**/*.jpg"" -- jpeg-recompress FULLPATH Comp/RELPATHþ```þþUtilitiesþ---------þThe following utilities are part of this project. All of them accept a `--help` parameter to see the available options.þþ### jpeg-archiveþCompress RAW and JPEG files in a folder utilizing all CPU cores. This is a simple shell script that uses the utilities below. It requires:þþ* a POSIX-compatible shell such as Bashþ* [Ladon](https://github.com/danielgtaylor/ladon) or [GNU Parallel](https://www.gnu.org/software/parallel/)þ* [dcraw](http://www.cybercom.net/~dcoffin/dcraw/)þ* [exiftool](http://www.sno.phy.queensu.ca/~phil/exiftool/)þ* jpeg-recompress (part of this project)þþ```bashþ# Compress a folder of imagesþcd path/to/photosþjpeg-archiveþþ# Custom quality and metricþjpeg-archive --quality medium --method smallfryþ```þþ### jpeg-recompressþCompress JPEGs by re-encoding to the smallest JPEG quality while keeping _perceived_ visual quality the same and by making sure huffman tables are optimized. This is a __lossy__ operation, but the images are visually identical and it usually saves 30-70% of the size for JPEGs coming from a digital camera, particularly DSLRs. By default all EXIF/IPTC/XMP and color profile metadata is copied over, but this can be disabled to save more space if desired.þþThere is no need for the input file to be a JPEG. In fact, you can use `jpeg-recompress` as a replacement for `cjpeg` by using PPM input and the `--ppm` option.þþThe better the quality of the input image is, the better the output will be.þþSome basic photo-related editing options are available, such as removing fisheye lens distortion.þþ#### DemoþBelow are two 100% crops of [Nikon's D3x Sample Image 2](http://static.nikonusa.com/D3X_gallery/index.html). The left shows the original image from the camera, while the others show the output of `jpeg-recompress` with the `medium` quality setting and various comparison methods. By default SSIM is used, which lowers the file size by **88%**. The recompression algorithm chooses a JPEG quality of 80. By comparison the `veryhigh` quality setting chooses a JPEG quality of 93 and saves 70% of the file size.þþ![JPEG recompression comparison](https://cloud.githubusercontent.com/assets/106826/3633843/5fde26b6-0eff-11e4-8c98-f18dbbf7b510.png)þþWhy are they different sizes? The default quality settings are set to average out to similar visual quality over large data sets. They may differ on individual photos (like above) because each metric considers different parts of the image to be more or less important for compression.þþ#### Image Comparison MetricsþThe following metrics are available when using `jpeg-recompress`. SSIM is the default.þþName     | Option        | Descriptionþ-------- | ------------- | -----------þMPE      | `-m mpe`      | Mean pixel error (as used by [imgmin](https://github.com/rflynn/imgmin))þSSIM     | `-m ssim`     | [Structural similarity](http://en.wikipedia.org/wiki/Structural_similarity) **DEFAULT**þMS-SSIM* | `-m ms-ssim`  | Multi-scale structural similarity (slow!) ([2008 paper](https://doi.org/10.1117/12.768060))þSmallFry | `-m smallfry` | Linear-weighted BBCQ-like ([original project](https://github.com/dwbuiten/smallfry), [2011 BBCQ paper](http://spie.org/Publications/Proceedings/Paper/10.1117/12.872231))þþ**Note**: The SmallFry algorithm may be [patented](http://www.jpegmini.com/main/technology) so use with caution.þþ#### SubsamplingþThe JPEG format allows for subsampling of the color channels to save space. For each 2x2 block of pixels per color channel (four pixels total) it can store four pixels (all of them), two pixels or a single pixel. By default, the JPEG encoder subsamples the non-luma channels to two pixels (often referred to as 4:2:0 subsampling). Most digital cameras do the same because of limitations in the human eye. This may lead to unintended behavior for specific use cases (see #12 for an example), so you can use `--subsample disable` to disable this subsampling.þþ#### Example Commandsþþ```bashþ# Default settingsþjpeg-recompress image.jpg compressed.jpgþþ# High quality example settingsþjpeg-recompress --quality high --min 60 image.jpg compressed.jpgþþ# Slow high quality settings (3-4x slower than above, slightly more accurate)þjpeg-recompress --accurate --quality high --min 60 image.jpg compressed.jpgþþ# Use SmallFry instead of SSIMþjpeg-recompress --method smallfry image.jpg compressed.jpgþþ# Use 4:4:4 sampling (disables subsampling).þjpeg-recompress --subsample disable image.jpg compressed.jpgþþ# Remove fisheye distortion (Tokina 10-17mm on APS-C @ 10mm)þjpeg-recompress --defish 2.6 --zoom 1.2 image.jpg defished.jpgþþ# Read from stdin and write to stdout with '-' as the filenameþjpeg-recompress - - <image.jpg >compressed.jpgþþ# Convert RAW to JPEG via PPM from stdinþdcraw -w -q 3 -c IMG_1234.CR2 | jpeg-recompress --ppm - compressed.jpgþþ# Disable progressive mode (not recommended)þjpeg-recompress --no-progressive image.jpg compressed.jpgþþ# Disable all output except for errorsþjpeg-recompress --quiet image.jpg compressed.jpgþ```þþ### jpeg-compareþCompare two JPEG photos to judge how similar they are. The `fast` comparison method returns an integer from 0 to 99, where 0 is identical. PSNR, SSIM, and MS-SSIM return floats but require images to be the same dimensions.þþ```bashþ# Do a fast compare of two imagesþjpeg-compare image1.jpg image2.jpgþþ# Calculate PSNRþjpeg-compare --method psnr image1.jpg image2.jpgþþ# Calculate SSIMþjpeg-compare --method ssim image1.jpg image2.jpgþ```þþ### jpeg-hashþCreate a hash of an image that can be used to compare it to other images quickly.þþ```bashþjpeg-hash image.jpgþ```þþBuildingþ--------þ### Dependenciesþ * [mozjpeg](https://github.com/mozilla/mozjpeg)þþ#### UbuntuþUbuntu users can install via `apt-get`:þþ```bashþsudo apt-get install build-essential autoconf pkg-config nasm libtoolþgit clone https://github.com/mozilla/mozjpeg.gitþcd mozjpegþautoreconf -fivþ./configure --with-jpeg8þmakeþsudo make installþ```þþ#### Mac OS XþMac users can install it via [Homebrew](http://brew.sh/):þþ```bashþbrew install mozjpegþ```þþ#### FreeBSDþþ```bashþpkg install mozjpegþgit clone https://github.com/danielgtaylor/jpeg-archive.gitþcd jpeg-archive/þgmakeþsudo gmake installþ```þþ#### WindowsþThe `Makefile` should work with MinGW/Cygwin/etc and standard GCC. Patches welcome.þþTo get everything you need to build, install these:þþ* [CMake](https://cmake.org/download/)þ* [NASM](https://www.nasm.us/)þ* [MinGW](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/installer/mingw-w64-install.exe/download) (installed to e.g. `C:\mingw`)þ* [Github for Windows](https://windows.github.com/)þþRun Github for windows. In the settings, set **Git Bash** as the shell. Open Git Shell from the start menu.þþ```bashþ# Update PATH to include MinGW/NASM bin folder, location on your system may varyþexport PATH=/c/mingw/mingw32/bin:/c/Program\ Files \(x68\)/nasm:$PATHþþ# Build mozjpeg or download https://www.dropbox.com/s/98jppfgds2xjblu/libjpeg.aþgit clone https://github.com/mozilla/mozjpeg.gitþcd mozjpegþcmake -G ""MSYS Makefiles"" -D CMAKE_C_COMPILER=gcc.exe -D CMAKE_MAKE_PROGRAM=mingw32-make.exe  -D WITH_JPEG8=1þmingw32-makeþcd ..þþ# Build jpeg-archiveþgit clone https://github.com/danielgtaylor/jpeg-archiveþcd jpeg-archiveþCC=gcc mingw32-makeþ```þþJPEG-Archive should now be built.þþ### Compiling (Linux and Mac OS X)þThe `Makefile` should work as-is on Ubuntu and Mac OS X. Other platforms may need to set the location of `libjpeg.a` or make other tweaks.þþ```bashþmakeþ```þþ### InstallationþInstall the binaries into `/usr/local/bin`:þþ```bashþsudo make installþ```þþLinks / Alternativesþ--------------------þ* https://github.com/rflynn/imgminþ* https://news.ycombinator.com/item?id=803839þþLicenseþ-------þ* JPEG-Archive is copyright &copy; 2015 Daniel G. Taylorþ* Image Quality Assessment (IQA) is copyright 2011, Tom Distler (http://tdistler.com)þ* SmallFry is copyright 2014, Derek Buitenhuis (https://github.com/dwbuiten)þþAll are released under an MIT license.þþhttp://dgt.mit-license.org/"
seemoo-lab/nexmon,370760,1436,126,312,Organization,False,820,4,8,19,False,"The C-based Firmware Patching Framework for Broadcom/Cypress WiFi Chips that enables Monitor Mode, Frame Injection and much more",,7,13,0,204,132,24,20,18,54,4,2,1328,3,15,20280,49,0,0,47,6,,,"![NexMon logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/nexmon.png)þþ# What is nexmon?þNexmon is our C-based firmware patching framework for Broadcom/Cypress WiFi chips þthat enables you to write your own firmware patches, for example, to enable monitorþmode with radiotap headers and frame injection.þþBefore we started to work on this repository, we developed patches for the Nexus 5 (with bcm4339 WiFi chip) in the [bcm-public](https://github.com/seemoo-lab/bcm-public)  repository and those for the Raspberry Pi 3 (with bcm43430a1 WiFi chip) in the [bcm-rpi3](https://github.com/seemoo-lab/bcm-rpi3) repository. To remove the development overhead of maintaining multiple separate repositories, we decided to merge them in this repository and add support for some additional devices. In contrast to the former repositories, here, you can only build the firmware patch without drivers and kernels. The Raspberry Pi 3 makes an exception, as here it is always required to also build the driver.þþ# Give FeedbackþWe setup a survey to learn about who uses Nexmon to which purpose and how we could improve Nexmon. We would be happy if every Nexmon user filled out this survey: https://nexmon.org/surveyþþ# WARNINGþOur software may damage your hardware and may void your hardware’s warranty! You use our tools at your own risk and responsibility! If you don't like these terms, don't use nexmon!þþ# Important changesþ* We started to collect usage statistics. In the file [STATISTICS.md](STATISTICS.md), you can find information on which data we collect and how you can opt-out of the statistics collectionþ* Starting with commit 4f8697743dc46ffc37d87d960825367531baeef9 the brcmfmac driver for the RPi3 can now be used as a regular interface. You need to use nexutil to activate monitor mode (`nexutil -m2` for monitor mode with radiotap headers), which will automtically adjust the interface type.þ* Starting with commit 184480edd6696392aae5f818f305f244606f2d17 you can choose different monitor mode options using nexutil. Use `nexutil -m1` to activate monitor mode without radiotap headers, `nexutil -m2` to activate it with radiotap headers. The numbers were chosen as non-Nexmon firmwares also support native monitor mode without radiotap headers by activating monitor mode with `nexutil -m1`.þ* Starting with commit 1bcfdc95b4395c2e8bdd962791ae20c4ba602f5b we changed the nexutil interface. Instead of calling `nexutil -m true` to activate monitor mode, you should now write `nexutil -m1`. To get the current monitor mode state execute `nexutil -m` instead of `nexutil -n`.þþ# Supported DevicesþThe following devices are currently supported by our nexmon firmware patch.þþWiFi Chip                 | Firmware Version     | Used in                   | Operating System          |  M  | RT  |  I  | FP  | UC  | CT þ------------------------- | -------------------- | ------------------------- | ------------------------- | --- | --- | --- | --- | --- | ---þbcm4330                   | 5_90_100_41_sta      | Samsung Galaxy S2         | Cyanogenmod 13.0          |  X  |  X  |     |  X  |  X  |  O þbcm4335b0                 | 6.30.171.1_sta       | Samsung Galaxy S4         | LineageOS 14.1            |  X  |  X  |  X  |     |  X  |  O þbcm4339                   | 6_37_34_43           | Nexus 5                   | Android 6 Stock           |  X  |  X  |  X  |  X  |  X  |  O þbcm43430a1<sup>1</sup>    | 7_45_41_26           | Raspberry Pi 3 and Zero W | Raspbian 8                |  X  |  X  |  X  |  X  |  X  |  O þbcm43430a1<sup>1</sup>    | 7_45_41_46           | Raspberry Pi 3 and Zero W | Raspbian Stretch          |  X  |  X  |  X  |  X  |  X  |  O þbcm43451b1                | 7_63_43_0            | iPhone 6                  | iOS 10.1.1 (14B100)       |     |     |     |  X  |  X  |    þbcm43455                  | 7_45_77_0_hw         | Huawei P9                 | Android 7 Stock           |  X  |  X  |  X  |  X  |  X  |    þbcm43455                  | 7_120_5_1_sta_C0     | Galaxy J7 2017            | ?                         |     |     |     |  X  |  X  |    þbcm43455                  | 7_45_77_0_hw(8-2017) | Huawei P9                 | Android 7 Stock           |  X  |  X  |  X  |  X  |  X  |    þbcm43455<sup>5</sup>      | 7_46_77_11_hw        | Huawei P9                 | Android 8 China Stock     |  X  |  X  |  X  |  X  |  X  |    þbcm43455                  | 7_45_59_16           | Sony Xperia Z5 Compact    | LineageOS 14.1            |  X  |  X  |  X  |  X  |  X  |    þbcm43455c0                | 7_45_154             | Raspberry Pi B3+/B4       | Raspbian Kernel 4.9/14/19 |  X  |  X  |     |  X  |  X  |    þbcm43455c0                | 7_45_189             | Raspberry Pi B3+/B4       | Raspbian Kernel 4.14/19   |  X  |  X  |     |  X  |  X  |    þbcm4356                   | 7_35_101_5_sta       | Nexus 6                   | Android 7.1.2             |  X  |  X  |     |  X  |  X  |  O þbcm4358                   | 7_112_200_17_sta     | Nexus 6P                  | Android 7 Stock           |  X  |  X  |     |  X  |  X  |  O þbcm4358                   | 7_112_201_3_sta      | Nexus 6P                  | Android 7.1.2 Stock       |  X  |  X  |     |  X  |  X  |  O þbcm4358<sup>2</sup>       | 7_112_300_14_sta     | Nexus 6P                  | Android 8.0.0 Stock       |  X  |  X  |  X  |  X  |  X  |  O þbcm43596a0<sup>3</sup>    | 9_75_155_45_sta_c0   | Samsung Galaxy S7         | Android 7 Stock           |  X  |     |     |  O  |  X  |    þbcm43596a0<sup>3,2</sup>  | 9_96_4_sta_c0        | Samsung Galaxy S7         | LineageOS 14.1            |  X  |  X  |  X  |  O  |  X  |    þbcm4375b1<sup>3,5,6</sup> | 18_38_18_sta         | Samsung Galaxy S10        | LineageOS 16              |     |     |     |  O  |  X  |    þqca9500<sup>4</sup>       | 4-1-0_55             | TP-Link Talon AD7200      | Custom LEDE Image         |     |     |     |     |     |    þþ<sup>1</sup> bcm43430a1 was wrongly labeled bcm43438 in the past.þþ<sup>2</sup> use LD_PRELOAD=libnexmon.so instead of LD_PRELOAD=libfakeioctl.so to inject frames through ioctlsþþ<sup>3</sup> flash patches need to be 8 bytes long and aligned on an 8 byte boundaryþþ<sup>4</sup> 802.11ad Wi-Fi chip from first 60 GHz Wi-Fi router Talon AD7200. Patch your firmware using [nexmon-arc](https://github.com/seemoo-lab/nexmon-arc) and run it with our custom LEDE image [lede-ad7200](https://github.com/seemoo-lab/lede-ad7200)þþ<sup>5</sup> Disabled the execution protection (called Execute Never) on region 1, because it interferes with the nexmon code (Permission fault on Section)þþ<sup>6</sup> To use nexutil, you need to deactivate SELinux or set it to permissiveþþ## Legendþ- M = Monitor Modeþ- RT = Monitor Mode with RadioTap headersþ- I = Frame Injectionþ- FP = Flash Patchingþ- UC = Ucode Compressionþ- CT = c't Article Support (for consistent support, use our ct-artikel branch)þþ# Steps to create your own firmware patchesþþ## Build patches for bcm4330, bcm4339 and bcm4358 using a x86 computer running Linux (e.g. Ubuntu 16.04)þ* Install some dependencies: `sudo apt-get install git gawk qpdf adb flex bison`þ* **Only necessary for x86_64 systems**, install i386 libs: þþ  ```þ  sudo dpkg --add-architecture i386þ  sudo apt-get updateþ  sudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386þ  ```þ* Clone our repository: `git clone https://github.com/seemoo-lab/nexmon.git`þ* In the root directory of the repository: `cd nexmon`þ  * Setup the build environment: `source setup_env.sh`þ  * Compile some build tools and extract the ucode and flashpatches from the original firmware files: `make`þ* Go to the *patches* folder of your target device (e.g. bcm4339 for the Nexus 5): `cd patches/bcm4339/6_37_34_43/nexmon/`þ  * Compile a patched firmware: `make`þ  * Generate a backup of your original firmware file: `make backup-firmware`þ  * Install the patched firmware on your smartphone: `make install-firmware` (make sure your smartphone is connected to your machine beforehand)þþ### Using the Monitor Mode patchþ* Install at least *nexutil* and *libfakeioctl* from our utilities. The easiest way to do this is by using this app: https://nexmon.org/app. But you can also build it from the source by executing `make` in the *utilties* folder (Note: you will need the Android NDK properly installed for this).þ* Connect to your Android phone using the ADB tools: `adb shell`þ* Make sure you are **not** connected to an access pointþ* Use *nexutil* to enable monitor mode: `nexutil -m2`þ* At this point the monitor mode is active. There is no need to call *airmon-ng*. þ* **Important:** Most tools need a Radiotap interface to work properly. *libfakeioctl* emulates this type of interface for you, therefore, use LD_PRELOAD to load this library when you call the favourite tool (e.g. tcpdump or airodump-ng): `LD_PRELOAD=libfakeioctl.so tcpdump -i wlan0`þ* *untested hint:* Thanks to XDA member ruleh, there is a bcmdhd driver patch to activate native monitor mode, see: https://github.com/ruleh/misc/tree/master/monitorþþ### Using nexutil over UDP on Nexus 5þTo be able to communicate with the firmware without root priviledges, we created a UDP interface accessible through the `libnexio`, which is also used by `nexutil`. You first have to prove to the firmware that you generally have root priviledges by setting a security cookie. Then you can use it for UDP based connections. Your wlan0 interface also needs an IP address in the 192.168.222.0/24 range or you have to change the default nexutil `broadcast-ip`:þ* Set the IP address of the wlan0 interface: `ifconfig wlan0 192.168.222.1 netmask 255.255.255.0`þ* Set the security cookie as root: `nexutil -x<cookie (uint)>`þ* Start a UDP connection for example to activate monitor mode: `nexutil -X<cookie> -m1`þþ## Build patches for bcm43430a1 on the RPI3/Zero W or bcm434355c0 on the RPI3+/RPI4 using Raspbian (recommended)þ**Note:** We currently support Kernel Version 4.4 (depricated), 4.9, 4.14 and 4.19. Raspbian contains firmware version 7.45.154 for the bcm43455c0. We also support the newer firmware release 7.45.189 from Cypress. Please, try which works best for you.þ* Make sure the following commands are executed as root: `sudo su`þ* Upgrade your Raspbian installation: `apt-get update && apt-get upgrade`þ* Install the kernel headers to build the driver and some dependencies: `sudo apt install raspberrypi-kernel-headers git libgmp3-dev gawk qpdf bison flex make`þ* Clone our repository: `git clone https://github.com/seemoo-lab/nexmon.git`þ* Go into the root directory of our repository: `cd nexmon`þ* Check if `/usr/lib/arm-linux-gnueabihf/libisl.so.10` exists, if not, compile it from source:þ  * `cd buildtools/isl-0.10`, `./configure`, `make`, `make install`, `ln -s /usr/local/lib/libisl.so /usr/lib/arm-linux-gnueabihf/libisl.so.10`þ* Check if `/usr/lib/arm-linux-gnueabihf/libmpfr.so.4` exists, if not, compile it from source:þ  * `cd buildtools/mpfr-3.1.4`, `autoreconf -f -i`, `./configure`, `make`, `make install`, `ln -s /usr/local/lib/libmpfr.so /usr/lib/arm-linux-gnueabihf/libmpfr.so.4`þ* Then you can setup the build environment for compiling firmware patchesþ  * Setup the build environment: `source setup_env.sh`þ  * Compile some build tools and extract the ucode and flashpatches from the original firmware files: `make`þ* Go to the *patches* folder for the bcm43430a1/bcm43455c0 chipset: `cd patches/bcm43430a1/7_45_41_46/nexmon/` / `patches/bcm43455c0/<7_45_154 or 7_45_189>/nexmon/`þ  * Compile a patched firmware: `make`þ  * Generate a backup of your original firmware file: `make backup-firmware`þ  * Install the patched firmware on your RPI3: `make install-firmware`þ* Install nexutil: from the root directory of our repository switch to the nexutil folder: `cd utilities/nexutil/`. Compile and install nexutil: `make && make install`.þ* *Optional*: remove wpa_supplicant for better control over the WiFi interface: `apt-get remove wpasupplicant`þ* **Note:** To connect to regular access points you have to execute `nexutil -m0` firstþþ### Using the Monitor Mode patchþ* Thanks to the prior work of Mame82, you can setup a new monitor mode interface by executing:þ```iw phy `iw dev wlan0 info | gawk '/wiphy/ {printf ""phy"" $2}'` interface add mon0 type monitor```þ* To activate monitor mode in the firmware, simply set the interface up: `ifconfig mon0 up`.þ* At this point, monitor mode is active. There is no need to call *airmon-ng*. þ* The interface already set the Radiotap header, therefore, tools like *tcpdump* or *airodump-ng* can be used out of the box: `tcpdump -i mon0`þ* *Optional*: To make the RPI3 load the modified driver after reboot:þ  * Find the path of the default driver at reboot: `modinfo brcmfmac` #the first line should be the full pathþ  * Backup the original driver: `mv ""<PATH TO THE DRIVER>/brcmfmac.ko"" ""<PATH TO THE DRIVER>/brcmfmac.ko.orig""`þ  * Copy the modified driver (Kernel 4.9): `cp /home/pi/nexmon/patches/bcm43430a1/7_45_41_46/nexmon/brcmfmac_kernel49/brcmfmac.ko ""<PATH TO THE DRIVER>/""`þ  * Copy the modified driver (Kernel 4.14): `cp /home/pi/nexmon/patches/bcm43430a1/7_45_41_46/nexmon/brcmfmac_4.14.y-nexmon/brcmfmac.ko ""<PATH TO THE DRIVER>/""`þ  * Probe all modules and generate new dependency: `depmod -a`þ  * The new driver should be loaded by default after reboot: `reboot`þ  * **Note:** It is possible to connect to an access point or run your own access point in parallel to the monitor mode interface on the `wlan0` interface.þþ# How to build the utilitiesþTo build the utilities such as nexmon or dhdutil for Android, you need to download the **old** NDK version 11c,þextract it and export the environment variable `NDK_ROOT` pointing to the directory where you extracted the NDK þfiles.þþ# How to extract the ROMþThe Wi-Fi firmware consists of a read-only part stored in the ROM of every Wi-Fi chip and another part that is þloaded by the driver into the RAM. To analyze the whole firmware, one needs to extract the ROM. There are two þoptions to do this. Either you write a firmware patch that simply copies the contents of the ROM to RAM and then þyou dump the RAM, or you directly dump the ROM after loading the regular firmware into the RAM. Even though, þthe second option is easier, it only works, if the ROM can be directly accessed by the driver, which is not always þthe case. Additionally, the firmware loaded into RAM can contain ROM patches that overlay the data stored in ROM. þBy dumping the ROM after loading the original RAM firmware, it contains flash patches. Hence, the ROM needs to be þdumped again for every RAM firmware update to be consistent. As a conclusion, we prefer to dump the clean ROM after þcopying it to RAM.þþ## Dumping the ROM directlyþTo dump the ROM directly, you need to know, where to find it and how large it is. On chips with Cortex-M3 it is þusually at upper addresses such as 0x800000, while on chips with Cortex-R4 it is likely at 0x0. Run dhdutil to þperform the dump:þ```þdhdutil membytes -r 0x0 0xA0000 > rom.binþ```þþ## Dumping a clean ROM after copying to RAMþFor the BCM4339 and BCM4358, we created `rom_extraction` projects that load a firmware patch that copies ROM to þRAM and them dumps it using dhdutil. To dump the ROM simply execute the following in the project directory:þ```þmake dump-romþ```þþAfter ROM extraction, the `rom.bin` file will be copies to the corresponding firmwares subdirectory. To apply the þflash patches of a specific RAM firmware version, enter its directory and execute:þ```þmake rom.binþ```þþþþ# Structure of this repositoryþ* `buildtools`: Contains compilers and other tools to build the firmwareþ* `firmwares`þ  * `<chip version>`þ    * `<firmware version>`þ      * `<firmware file>`: The original firmware that will be loaded into the RAM of the WiFi Chipþ      * `definitions.mk`: Contains mainly firmware specific addressesþ      * `structs.h`: Structures only valid for this firmware versionþ      * `Makefile`: Used to extract flashpatches and ucodeþ      * `flashpatches.c` (generated by Makefile): Contains flashpatchesþ      * `ucode.bin` (extracted by Makefile): Contains uncompressed Ucodeþ    * `structs.common.h`: Structures that are common between firmware versionsþ* `patches`þ  * `<chip version>`þ    * `<firmware version>`þ      * `nexmon`þ        * `Makefile`: Used to build the firmwareþ        * `patch.ld`: Linker fileþ        * `src`þ          * `patch.c`: General patches to the firmwareþ          * `injection.c`: Code related to frame injectionþ          * `monitormode.c`: Code related to monitor mode with radiotap headersþ          * `ioctl.c`: Handling of custom IOCTLsþ          * ...þ        * `obj` (generated by Makefile): Object files created from C filesþ        * `log` (generated by Makefile): Logs written during compilationþ        * `gen` (generated by Makefile): Files generated during the build processþ          * `nexmon.pre` (generated by gcc plugin): Extracted at-attributes and targetregion-pragmasþ          * `nexmon.ld` (generated from nexmon.pre): Linker file use to place patch code at defined addresses in the firmwareþ          * `nexmon.mk` (generated from nexmon.pre): Make file used take code from patch.elf and place it into firmwareþ          * `flashpatches.ld` (generated from nexmon.pre): Linker file that places flashpatches at target locations in firmware ROMþ          * `flashpatches.mk` (generated from nexmon.pre): Make file used to insert flashpatch config and data structures into firmwareþ          * `patch.elf` (generated from object files and linker scripts): contains the newly compiled code placed at predefined addressesþ    * `common`þ      * `wrapper.c`: Wrappers for functions that already exist in the firmwareþ      * `ucode_compression.c`: [tinflate](http://achurch.org/tinflate.c) based ucode decompressionþ      * `radiotap.c`: RadioTap header parserþ      * `helper.c`: Helpful utility functionsþ    * `include`: Common include filesþ      * `firmware_version.h`: Definitions of chip and firmware versionsþ      * `patcher.h`: Macros use to perform patching for existing firmware code (e.g., BPatch patches a branch instruction)þ      * `capabilities.h`: Allows to indicate capabilities (such as, monitor mode and frame injection)þ      * `nexioctl.h`: Defines custom IOCTL numbersþþ# Related projectsþ* [bcmon](https://bcmon.blogspot.de/): Monitor Mode and Frame Injection for the bcm4329 and bcm4330þ* [monmob](https://github.com/tuter/monmob): Monitor Mode and Frame Injection for the bcm4325, bcm4329 and bcm4330þ* [P4wnP1](https://github.com/mame82/P4wnP1): Highly customizable attack platform, based on Raspberry Pi Zero W and Nexmonþ* [kali Nethunter OS](https://github.com/nethunteros): ROM that brings Kali Linux to smartphones with Nexmon supportþ* [dustcloud-nexmon](https://github.com/dgiese/dustcloud-nexmon): Nexmon for Xiaomi IoT devices (ARM based)þ* [InternalBlue](https://github.com/seemoo-lab/internalblue): Bluetooth experimentation framework based on Reverse Engineering of Broadcom Bluetooth Controllersþþ# Interesting articles on firmware hacksþIf you know more projects that use nexmon or perform similar firmware hacks, let us know and we will add a link.þþ* [Project Zero](https://googleprojectzero.blogspot.de/2017/09/over-air-vol-2-pt-1-exploiting-wi-fi.html): Over The Air - Vol. 2, Pt. 1: Exploiting The Wi-Fi Stack on Apple Devicesþ* [broadpwn](https://blog.exodusintel.com/2017/07/26/broadpwn/): Remotely Compromising Android and IOS via a Bug in Broadcom's Wi-Fi Chipsetsþ* [Project Zero](https://googleprojectzero.blogspot.de/2017/04/over-air-exploiting-broadcoms-wi-fi_4.html): Over The Air: Exploiting Broadcom's Wi-Fi Stack (Part 1)þ* [Project Zero](https://googleprojectzero.blogspot.de/2017/04/over-air-exploiting-broadcoms-wi-fi_11.html): Over The Air: Exploiting Broadcom's Wi-Fi Stack (Part 2) þþ# Read my PhD thesisþ* Matthias Schulz. [**Teaching Your Wireless Card New Tricks: Smartphone Performance and Security Enhancements through Wi-Fi Firmware Modifications**](http://tuprints.ulb.tu-darmstadt.de/7243/). Dr.-Ing. thesis, Technische Universität Darmstadt, Germany, February 2018. [pdf](http://tuprints.ulb.tu-darmstadt.de/7243/7/dissertation_2018_matthias_thomas_schulz.pdf)þþ# Read our papersþ* F. Gringoli, M. Schulz, J. Link, and M. Hollick. [**Free Your CSI: A Channel State Information Extraction Platform For Modern Wi-Fi Chipsets**](https://doi.org/10.1145/3349623.3355477). Accepted to appear in *Proceedings of the 13th Workshop on Wireless Network Testbeds, Experimental evaluation & CHaracterization (WiNTECH 2019)*, October 2019. [code](https://nexmon.org/csi)þ* D. Mantz, J. Classen, M. Schulz, and M. Hollick. [**InternalBlue - Bluetooth Binary Patching and Experimentation Framework**](https://dl.acm.org/citation.cfm?id=3326089). *In Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys '19)*. June 2019.þ* M. Schuß, C. A. Boano, M. Weber, M. Schulz, M. Hollick, K. Römer. [**JamLab-NG: Benchmarking Low-Power Wireless Protocols under Controlable and Repeatable Wi-Fi Interference**](https://dl.acm.org/citation.cfm?id=3324331). *Proceedings of the 2019 International Conference on Embedded Wireless Systems and Networks (EWSN 2019)*, February 2019.þ* M. Schulz, D. Wegemer, and M. Hollick. [**The Nexmon Firmware Analysis and Modification Framework: Empowering Researchers to Enhance Wi-Fi Devices**](https://doi.org/10.1016/j.comcom.2018.05.015). *Elsevier Computer Communications (COMCOM) Journal*. 2018.þ* M. Schulz, J. Link, F. Gringoli, and M. Hollick. [**Shadow Wi-Fi: Teaching Smart- phones to Transmit Raw Signals and to Extract Channel State Information to Implement Practical Covert Channels over Wi-Fi**](https://dl.acm.org/citation.cfm?id=3210333). Accepted to appear in *Proceedings of the 16th ACM International Conference on Mobile Systems, Applications, and Services*, MobiSys 2018, June 2018.þ* D. Steinmetzer, D. Wegemer, M. Schulz, J. Widmer, M. Hollick. [**Compressive Millimeter-Wave Sector Selection in Off-the-Shelf IEEE 802.11ad Devices**](https://dl.acm.org/citation.cfm?id=3143384). *Proceedings of the 13th International Conference on emerging Networking EXperiments and Technologies*, CoNEXT 2017, December 2017.þ* M. Schulz, D. Wegemer, M. Hollick. [**Nexmon: Build Your Own Wi-Fi Testbeds With Low-Level MAC and PHY-Access Using Firmware Patches on Off-the-Shelf Mobile Devices**](https://dl.acm.org/citation.cfm?id=3131476). *Proceedings of the 11th ACM International Workshop on Wireless Network Testbeds, Experimental Evaluation & Characterization (WiNTECH 2017)*, October 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wintech2017) [video](https://youtu.be/m5Zrk4n4hoE)þ* M. Schulz, F. Knapp, E. Deligeorgopoulos, D. Wegemer, F. Gringoli, M. Hollick. [**DEMO: Nexmon in Action: Advanced Applications Powered by the Nexmon Firmware Patching Framework**](https://dl.acm.org/citation.cfm?id=3133333), Accepted for publication in *Proceedings of the 11th ACM International Workshop on Wireless Network Testbeds, Experimental Evaluation & Characterization (WiNTECH 2017)*, October 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wintech2017demo)þ* M. Schulz, F. Gringoli, D. Steinmetzer, M. Koch and M. Hollick. [**Massive Reactive Smartphone-Based Jamming using Arbitrary Waveforms and Adaptive Power Control**](https://dl.acm.org/citation.cfm?id=3098253). Proceedings of the *10th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec 2017)*, July 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wisec2017) [video](https://youtu.be/S2XPBK0KdiQ)þ* M. Schulz, E. Deligeorgopoulos, M. Hollick and F. Gringoli. [**DEMO: Demonstrating Reactive Smartphone-Based Jamming**](https://dl.acm.org/citation.cfm?id=3106022). Proceedings of the *10th ACM Conference on Security and Privacy in Wireless and Mobile Networks (WiSec 2017)*, July 2017. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wisec2017demo)þ* M. Schulz. [**Nexmon - Wie man die eigene WLAN-Firmware hackt**](http://heise.de/-3538660), þc't 26/2016, S. 168, Heise Verlag, 2016.þ* M. Schulz, D. Wegemer, M. Hollick. [**DEMO: Using NexMon, the C-based WiFi þfirmware modification framework**](https://dl.acm.org/citation.cfm?id=2942419), þProceedings of the *9th ACM Conference on Security and Privacy in Wireless and þMobile Networks (WiSec 2016)*, July 2016. [pdf](https://www.seemoo.tu-darmstadt.de/mschulz/wisec2016demo1)þ* M. Schulz, D. Wegemer and M. Hollick. [**NexMon: A Cookbook for Firmware þModifications on Smartphones to Enable Monitor Mode**](http://arxiv.org/abs/1601.07077), þCoRR, vol. abs/1601.07077, December 2015. þ[bibtex](http://dblp.uni-trier.de/rec/bibtex/journals/corr/SchulzWH16)þþ[Get references as bibtex file](https://nexmon.org/bib)þþ# Reference our projectþAny use of this project which results in an academic publication or other publication which includes a bibliography should include a citation to the Nexmon project and probably one of our papers depending on the code you use. Find all references in our [bibtex file](nexmon.bib). Here is the reference for the project only:þ```þ@electronic{nexmon:project,þ author = {Schulz, Matthias and Wegemer, Daniel and Hollick, Matthias},þ title = {Nexmon: The C-based Firmware Patching Framework},þ url = {https://nexmon.org},þ year = {2017}þ}þ```þþ# Contactþ* [Matthias Schulz](https://seemoo.tu-darmstadt.de/mschulz) <mschulz@seemoo.tu-darmstadt.de>þ* Daniel Wegemer <dwegemer@seemoo.tu-darmstadt.de>þþ# Powered Byþ## Secure Mobile Networking Lab (SEEMOO)þ<a href=""https://www.seemoo.tu-darmstadt.de"">![SEEMOO logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/seemoo.png)</a>þ## Networked Infrastructureless Cooperation for Emergency Response (NICER)þ<a href=""https://www.nicer.tu-darmstadt.de"">![NICER logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/nicer.png)</a>þ## Multi-Mechanisms Adaptation for the Future Internet (MAKI)þ<a href=""http://www.maki.tu-darmstadt.de/"">![MAKI logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/maki.png)</a>þ## Technische Universität Darmstadtþ<a href=""https://www.tu-darmstadt.de/index.en.jsp"">![TU Darmstadt logo](https://github.com/seemoo-lab/nexmon/raw/master/gfx/tudarmstadt.png)</a>"
libav/libav,88763,791,79,350,Organization,False,45232,9,116,495,False,"Libav github mirror, clone of git://git.libav.org/libav",http://www.libav.org,3,0,0,,,,,7,27,1,0,7103,0,0,0,0,0,0,3,4,,,"Libavþ=====þþ[![Build Status](https://travis-ci.org/libav/libav.svg)](https://travis-ci.org/libav/libav)þþLibav is a collection of libraries and tools to process multimedia contentþsuch as audio, video, subtitles and related metadata.þþ## Librariesþþ* `libavcodec` provides implementation of a wider range of codecs.þ* `libavformat` implements streaming protocols, container formats and basic I/O access.þ* `libavutil` includes hashers, decompressors and miscellaneous utility functions.þ* `libavfilter` provides a mean to alter decoded Audio and Video through chain of filters.þ* `libavdevice` provides an abstraction to access capture and playback devices.þ* `libavresample` implements audio mixing and resampling routines.þ* `libswscale` implements color conversion and scaling routines.þþ## Toolsþþ* [avconv](http://libav.org/avconv.html) is a command line toolbox toþ  manipulate, convert and stream multimedia content.þ* [avplay](http://libav.org/avplay.html) is a minimalistic multimedia player.þ* [avprobe](http://libav.org/avprobe.html) is a simple analisys tool to inspectþ  multimedia content.þ* Additional small tools such as `aviocat`, `ismindex` and `qt-faststart`.þþ## DocumentationþþThe offline documentation is available in the **doc/** directory.þþThe online documentation is available in the main [website](http://libav.org)þand in the [wiki](http://wiki.libav.org).þþ### ExamplesþþConding examples are available in the **doc/example** directory.þþ## LicenseþþLibav codebase is mainly LGPL-licensed with optional components licensed underþGPL. Please refer to the LICENSE file for detailed information."
checkpoint-restore/criu,17483,1112,73,264,Organization,False,10244,9,60,94,False,Checkpoint/Restore tool,http://criu.org,17,33,3,283,489,41,48,32,298,24,109,3190,17,153,4881,1403,0,0,4,12,,,"[![master](https://travis-ci.org/checkpoint-restore/criu.svg?branch=master)](https://travis-ci.org/checkpoint-restore/criu)þ[![development](https://travis-ci.org/checkpoint-restore/criu.svg?branch=criu-dev)](https://travis-ci.org/checkpoint-restore/criu)þ[![Codacy Badge](https://api.codacy.com/project/badge/Grade/55251ec7db28421da4481fc7c1cb0cee)](https://www.codacy.com/app/xemul/criu?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=xemul/criu&amp;utm_campaign=Badge_Grade)þ<p align=""center""><img src=""https://criu.org/w/images/1/1c/CRIU.svg"" width=""256px""/></p>þþ## CRIU -- A project to implement checkpoint/restore functionality for LinuxþþCRIU (stands for Checkpoint and Restore in Userspace) is a utility to checkpoint/restore Linux tasks.þþUsing this tool, you can freeze a running application (or part of it) and checkpoint þit to a hard drive as a collection of files. You can then use the files to restore and run theþapplication from the point it was frozen at. The distinctive feature of the CRIUþproject is that it is mainly implemented in user space. There are some more projectsþdoing C/R for Linux, and so far CRIU [appears to be](https://criu.org/Comparison_to_other_CR_projects) þthe most feature-rich and up-to-date with the kernel.þþThe project [started](https://criu.org/History) as the way to do live migration for OpenVZþLinux containers, but later grew to more sophisticated and flexible tool. It is currently þused by (integrated into) OpenVZ, LXC/LXD, Docker, and other software, project gets tremendous þhelp from the community, and its packages are included into many Linux distributions.þþThe project home is at http://criu.org. This wiki contains all the knowledge base for CRIU we have.þPages worth starting with are:þ- [Installation instructions](http://criu.org/Installation)þ- [A simple example of usage](http://criu.org/Simple_loop)þ- [Examples of more advanced usage](https://criu.org/Category:HOWTO)þ- Troubleshooting can be hard, some help can be found [here](https://criu.org/When_C/R_fails), [here](https://criu.org/What_cannot_be_checkpointed) and [here](https://criu.org/FAQ)þþ### Checkpoint and restore of simple loop process þ[<p align=""center""><img src=""https://asciinema.org/a/232445.png"" width=""572px"" height=""412px""/></p>](https://asciinema.org/a/232445)þþ## Advanced featuresþþAs main usage for CRIU is live migration, there's a library for it called P.Haul. Also theþproject exposes two cool core features as standalone libraries. These are libcompel for parasite code þinjection and libsoccr for TCP connections checkpoint-restore.þþ### Live migrationþþTrue [live migration](https://criu.org/Live_migration) using CRIU is possible, but doingþall the steps by hands might be complicated. The [phaul sub-project](https://criu.org/P.Haul)þprovides a Go library that encapsulates most of the complexity. This library and the Go bindingsþfor CRIU are stored in the [go-criu](https://github.com/checkpoint-restore/go-criu) repository.þþþ### Parasite code injectionþþIn order to get state of the running process CRIU needs to make this process executeþsome code, that would fetch the required information. To make this happen withoutþkilling the application itself, CRIU uses the [parasite code injection](https://criu.org/Parasite_code)þtechnique, which is also available as a standalone library called [libcompel](https://criu.org/Compel).þþ### TCP sockets checkpoint-restoreþþOne of the CRIU features is the ability to save and restore state of a TCP socketþwithout breaking the connection. This functionality is considered to be useful byþitself, and we have it available as the [libsoccr library](https://criu.org/Libsoccr).þþ## How to contributeþþCRIU project is (almost) the never-ending story, because we have to always keep up with theþLinux kernel supporting checkpoint and restore for all the features it provides. Thus we'reþlooking for contributors of all kinds -- feedback, bug reports, testing, coding, writing, etc.þHere are some useful hints to get involved.þþ* We have both -- [very simple](https://github.com/checkpoint-restore/criu/issues?q=is%3Aissue+is%3Aopen+label%3Aenhancement) and [more sophisticated](https://github.com/checkpoint-restore/criu/issues?q=is%3Aissue+is%3Aopen+label%3A%22new+feature%22) coding tasks;þ* CRIU does need [extensive testing](https://github.com/checkpoint-restore/criu/issues?q=is%3Aissue+is%3Aopen+label%3Atesting);þ* Documentation is always hard, we have [some information](https://criu.org/Category:Empty_articles) that is to be extracted from people's heads into wiki pages as well as [some texts](https://criu.org/Category:Editor_help_needed) that all need to be converted into useful articles;þ* Feedback is expected on the github issues page and on the [mailing list](https://lists.openvz.org/mailman/listinfo/criu);þ* We accept github pull requests and this is the preferred way to contribute to CRIU. If you prefer to send patches by email, you are welcome to send them to [the devel list](http://criu.org/How_to_submit_patches);þ* Spread the word about CRIU in [social networks](http://criu.org/Contacts);þ* If you're giving a talk about CRIU -- let us know, we'll mention it on the [wiki main page](https://criu.org/News/events);þþ## LicenceþþThe project is licensed under GPLv2 (though files sitting in the lib/ directory are LGPLv2.1)."
commonmark/cmark,4681,986,46,332,Organization,False,2444,6,34,74,False,CommonMark parsing and rendering library and program in C,,0,7,0,24,155,6,9,6,153,0,12,2154,5,80,1276,752,0,0,5,1,,,"cmarkþ=====þþ[![CIþtests](https://github.com/commonmark/cmark/workflows/CI%20tests/badge.svg)](https://github.com/commonmark/cmark/actions)þþ`cmark` is the C reference implementation of [CommonMark], aþrationalized version of Markdown syntax with a [spec][the spec].þ(For the JavaScript reference implementation, seeþ[commonmark.js].)þþIt provides a shared library (`libcmark`) with functions for parsingþCommonMark documents to an abstract syntax tree (AST), manipulatingþthe AST, and rendering the document to HTML, groff man, LaTeX,þCommonMark, or an XML representation of the AST.  It also provides aþcommand-line program (`cmark`) for parsing and rendering CommonMarkþdocuments.þþAdvantages of this library:þþ- **Portable.**  The library and program are written in standardþ  C99 and have no external dependencies.  They have been tested withþ  MSVC, gcc, tcc, and clang.þþ- **Fast.** cmark can render a Markdown version of *War and Peace* inþ  the blink of an eye (127 milliseconds on a ten year old laptop,þ  vs. 100-400 milliseconds for an eye blink).  In our [benchmarks],þ  cmark is 10,000 times faster than the original `Markdown.pl`, andþ  on par with the very fastest available Markdown processors.þþ- **Accurate.** The library passes all CommonMark conformance tests.þþ- **Standardized.** The library can be expected to parse CommonMarkþ  the same way as any other conforming parser.  So, for example,þ  you can use `commonmark.js` on the client to preview content thatþ  will be rendered on the server using `cmark`.þþ- **Robust.** The library has been extensively fuzz-tested usingþ  [american fuzzy lop].  The test suite includes pathological casesþ  that bring many other Markdown parsers to a crawl (for example,þ  thousands-deep nested bracketed text or block quotes).þþ- **Flexible.** CommonMark input is parsed to an AST which can beþ  manipulated programmatically prior to rendering.þþ- **Multiple renderers.**  Output in HTML, groff man, LaTeX, CommonMark,þ  and a custom XML format is supported. And it is easy to write newþ  renderers to support other formats.þþ- **Free.** BSD2-licensed.þþIt is easy to use `libcmark` in python, lua, ruby, and other dynamicþlanguages: see the `wrappers/` subdirectory for some simple examples.þþThere are also libraries that wrap `libcmark` forþ[Go](https://github.com/rhinoman/go-commonmark),þ[Haskell](https://hackage.haskell.org/package/cmark),þ[Ruby](https://github.com/gjtorikian/commonmarker),þ[Lua](https://github.com/jgm/cmark-lua),þ[Perl](https://metacpan.org/release/CommonMark),þ[Python](https://pypi.python.org/pypi/paka.cmark),þ[R](https://cran.r-project.org/package=commonmark) andþ[Scala](https://github.com/sparsetech/cmark-scala).þþInstallingþ----------þþBuilding the C program (`cmark`) and shared library (`libcmark`)þrequires [cmake].  If you modify `scanners.re`, then you will alsoþneed [re2c] \(>= 0.14.2\), which is used to generate `scanners.c` fromþ`scanners.re`.  We have included a pre-generated `scanners.c` inþthe repository to reduce build dependencies.þþIf you have GNU make, you can simply `make`, `make test`, and `makeþinstall`.  This calls [cmake] to create a `Makefile` in the `build`þdirectory, then uses that `Makefile` to create the executable andþlibrary.  The binaries can be found in `build/src`.  The defaultþinstallation prefix is `/usr/local`.  To change the installationþprefix, pass the `INSTALL_PREFIX` variable if you run `make` for theþfirst time: `make INSTALL_PREFIX=path`.þþFor a more portable method, you can use [cmake] manually. [cmake] knowsþhow to create build environments for many build systems.  For example,þon FreeBSD:þþ    mkdir buildþ    cd buildþ    cmake ..  # optionally: -DCMAKE_INSTALL_PREFIX=pathþ    make      # executable will be created as build/src/cmarkþ    make testþ    make installþþOr, to create Xcode project files on OSX:þþ    mkdir buildþ    cd buildþ    cmake -G Xcode ..þ    open cmark.xcodeprojþþThe GNU Makefile also provides a few other targets for developers.þTo run a benchmark:þþ    make benchþþFor more detailed benchmarks:þþ    make newbenchþþTo run a test for memory leaks using `valgrind`:þþ    make leakcheckþþTo reformat source code using `clang-format`:þþ    make formatþþTo run a ""fuzz test"" against ten long randomly generated inputs:þþ    make fuzztestþþTo do a more systematic fuzz test with [american fuzzy lop]:þþ    AFL_PATH=/path/to/afl_directory make aflþþFuzzing with [libFuzzer] is also supported but, because libFuzzer is stillþunder active development, may not work with your system-installed version ofþclang. Assuming LLVM has been built in `$HOME/src/llvm/build` the fuzzer can beþrun with:þþ    CC=""$HOME/src/llvm/build/bin/clang"" LIB_FUZZER_PATH=""$HOME/src/llvm/lib/Fuzzer/libFuzzer.a"" make libFuzzerþþTo make a release tarball and zip archive:þþ    make archiveþþInstalling (Windows)þ--------------------þþTo compile with MSVC and NMAKE:þþ    nmakeþþYou can cross-compile a Windows binary and dll on linux if you have theþ`mingw32` compiler:þþ    make mingwþþThe binaries will be in `build-mingw/windows/bin`.þþUsageþ-----þþInstructions for the use of the command line program and library canþbe found in the man pages in the `man` subdirectory.þþSecurityþ--------þþBy default, the library will scrub raw HTML and potentiallyþdangerous links (`javascript:`, `vbscript:`, `data:`, `file:`).þþTo allow these, use the option `CMARK_OPT_UNSAFE` (orþ`--unsafe`) with the command line program. If doing so, weþrecommend you use a HTML sanitizer specific to your needs toþprotect against [XSSþattacks](http://en.wikipedia.org/wiki/Cross-site_scripting).þþContributingþ------------þþThere is a [forum for discussingþCommonMark](http://talk.commonmark.org); you should use it instead ofþgithub issues for questions and possibly open-ended discussions.þUse the [github issue tracker](http://github.com/commonmark/CommonMark/issues)þonly for simple, clear, actionable issues.þþAuthorsþ-------þþJohn MacFarlane wrote the original library and program.þThe block parsing algorithm was worked out together with DavidþGreenspan. Vicent Marti optimized the C implementation forþperformance, increasing its speed tenfold.  Kārlis Gaņģis helpedþwork out a better parsing algorithm for links and emphasis,þeliminating several worst-case performance issues.þNick Wellnhofer contributed many improvements, includingþmost of the C library's API and its test harness.þþ[benchmarks]: benchmarks.mdþ[the spec]: http://spec.commonmark.orgþ[CommonMark]: http://commonmark.orgþ[cmake]: http://www.cmake.org/download/þ[re2c]: http://re2c.orgþ[commonmark.js]: https://github.com/commonmark/commonmark.jsþ[Build Status]: https://img.shields.io/travis/commonmark/cmark/master.svg?style=flatþ[Windows Build Status]: https://ci.appveyor.com/api/projects/status/h3fd91vtd1xfmp69?svg=trueþ[american fuzzy lop]: http://lcamtuf.coredump.cx/afl/þ[libFuzzer]: http://llvm.org/docs/LibFuzzer.html"
gpac/gpac,121966,1003,87,310,Organization,False,9813,7,7,44,False,GPAC main code repository,http://www.gpac.io,4,17,1,78,1103,29,96,5,320,1,15,5437,6,101,2305,894,0,0,22,0,,,"[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu64)](https://buildbot.gpac.io/#/grid?branch=master)þ[![Tests](https://tests.gpac.io/testres/badge/tests/linux64)](https://tests.gpac.io/)þþ[![Build Status](https://tests.gpac.io/testres/badge/build/ubuntu32)](https://buildbot.gpac.io/#/grid?branch=master)þ[![Tests](https://tests.gpac.io/testres/badge/tests/linux32)](https://tests.gpac.io/)þþ[![Build Status](https://tests.gpac.io/testres/badge/build/windows64)](https://buildbot.gpac.io/#/grid?branch=master)þ[![Tests](https://tests.gpac.io/testres/badge/tests/win64)](https://tests.gpac.io/)þþ[![Build Status](https://tests.gpac.io/testres/badge/build/windows32)](https://buildbot.gpac.io/#/grid?branch=master)þ[![Tests](https://tests.gpac.io/testres/badge/tests/win32)](https://tests.gpac.io/)þþ[![Build Status](https://tests.gpac.io/testres/badge/build/macos)](https://buildbot.gpac.io/#/grid?branch=master)þ[![Tests](https://tests.gpac.io/testres/badge/tests/macos)](https://tests.gpac.io/)þþ[![Build Status](https://tests.gpac.io/testres/badge/build/ios)](https://buildbot.gpac.io/#/grid?branch=master)þþ[![Build Status](https://tests.gpac.io/testres/badge/build/android)](https://buildbot.gpac.io/#/grid?branch=master)þþ[![Coverage](https://tests.gpac.io/testres/badge/cov/linux64)](https://tests.gpac.io/)þþ![License](https://img.shields.io/badge/license-LGPL-blue.svg)þþREADME for GPAC version 0.8.0þþGPAC is a multimedia framework oriented towards rich media and distributed under the LGPL license (see COPYING).þþGPAC supports many multimedia formats, from simple audiovisual containers (avi, mov, mpg) to complexþpresentation formats (MPEG-4 Systems, SVG Tiny 1.2, VRML/X3D) and 360 videos. GPAC supports presentation scripting for MPEG4/VRML/X3D through mozilla SpiderMonkey javascript engine.þþGPAC currently supports local playback, http progressive download, Adaptive HTTP Streaming (MPEG-DASH, HLS), RTP/RTSP streaming over UDP (unicast or multicast) or TCP and TS demuxing (from file, IP or DVB4Linux).þþGPAC also features MP4Box, a multimedia swiss-army knife for the prompt, and MP42TS, a fast TS multiplexer from MP4 and RTP sources.þþFor compilation and installation instruction, check INSTALLME fileþþFor GPAC configuration instruction, check gpac/doc/configuration.html or gpac/doc/man/gpac.1 (man gpac when installed)þþFor more information, visit the GPAC website:þ http://gpac.io"
kaltura/nginx-vod-module,3985,1148,125,286,Organization,False,1283,9,31,18,False,NGINX-based MP4 Repackager,,13,7,0,100,543,23,33,8,498,2,12,2161,3,19,1012,307,0,0,325,4,,,"# NGINX-based VOD Packagerþ## nginx-vod-module [![Build Status](https://travis-ci.org/kaltura/nginx-vod-module.svg?branch=master)](https://travis-ci.org/kaltura/nginx-vod-module)þþ[Join the list of organizations using this video packager project](https://github.com/kaltura/nginx-vod-module/issues/730/).þþ### Featuresþþ* On-the-fly repackaging of MP4 files to DASH, HDS, HLS, MSSþþ* Working modes:þ  1. Local - serve locally accessible files (local disk/NFS mounted)þ  2. Remote - serve files accessible via HTTP using range requestsþ  3. Mapped - serve files according to a specification encoded in JSON format. The JSON can pulled from a remote server, or read from a local fileþþ* Adaptive bitrate supportþþ* Playlist support (playing several different media files one after the other) - mapped mode onlyþþ* Simulated live support (generating a live stream from MP4 files) - mapped mode onlyþþ* Fallback support for file not found in local/mapped modes (useful in multi-datacenter environments)þ  þ* Video codecs: H264, H265 (DASH/HLS), VP9 (DASH)þþ* Audio codecs: AAC, MP3 (HLS/HDS/MSS), AC-3 (DASH/HLS), E-AC-3 (DASH/HLS), OPUS (DASH)þþ* Captions support - þ  þ  Input:þ  1. WebVTTþ  2. SRTþ  3. DFXP/TTMLþ  4. CAP (Cheetah)þ  þ  Output:þ  1. DASH - served as a single WebVTTþ  2. HLS - segmented WebVTT (m3u8)þ  3. MSS - converted to TTML and packaged in fragmented MP4 (no support for styling)þþ* Audio only/video only filesþþ* Alternative audio renditions - supporting both:þ  1. Generation of manifest with different audio renditions, allowing selection on the client sideþ  2. Muxing together audio and video streams from separate files / tracks - provides the abilityþ to serve different audio renditions of a single video, without the need for any special supportþ on the client side.þþ* Track selection for multi audio/video MP4 filesþþ* Playback rate change - 0.5x up to 2x (requires libavcodec and libavfilter)þþ* Source file clipping (only from I-Frame to P-frame)þþ* Support for variable segment lengths - enabling the player to select the optimal bitrate fast,þwithout the overhead of short segments for the whole duration of the videoþþ* Clipping of MP4 files for progressive download playbackþþ* Thumbnail capture (requires libavcodec) and resize (requires libswscale)þþ* Volume map (requires libavcodec) - returns a CSV containing the volume level in each intervalþþ* Decryption of CENC-encrypted MP4 files (it is possible to create such files with MP4Box)þþ* DASH: common encryption (CENC) supportþþ* MSS: PlayReady encryption supportþþ* HLS: Generation of I-frames playlist (EXT-X-I-FRAMES-ONLY)þþ* HLS: support for AES-128 / SAMPLE-AES encryptionþþ### Limitationsþþ* Track selection and playback rate change are not supported in progressive downloadþþ* I-frames playlist generation is not supported when encryption is enabledþþ* Tested on Linux onlyþþ### Compilationþþ#### DependenciesþþIn general, if you have the dependencies that are required to build nginx, you should be able to build nginx-vod-module.þHowever, some optional features of this module depend on additional packages. The module detects these packages þduring `configure` - if a package is missing, the respective feature will be disabled.þþThe optional features are:þ1. Thumbnail capture & volume map - depend on ffmpeg (3.0 or newer)þ2. Audio filtering (for changing playback rate / gain) - depends on ffmpeg (3.0 or newer) and also on libfdk_aac.þ Due to licensing issues, libfdk_aac is not built into kaltura ffmpeg packagesþ3. Encryption / decryption (DRM / HLS AES) - depends on opensslþ4. DFXP captions - depends on libxml2þ5. UTF-16 encoded SRT files - depends on iconvþþ#### BuildþþTo link statically against nginx, cd to nginx source directory and execute:þþ    ./configure --add-module=/path/to/nginx-vod-moduleþ    makeþ    make installþþTo compile as a dynamic module (nginx 1.9.11+), use:þ  þ ./configure --add-dynamic-module=/path/to/nginx-vod-moduleþþIn this case, the `load_module` directive should be used in nginx.conf in order to load the module.þþOptional recommended settings:þ1. `--with-file-aio` - enable asynchronous I/O support, highly recommended, relevant only to local and mapped modesþ2. `--with-threads` (nginx 1.7.11+) - enable asynchronous file open using thread pool (also requires `vod_open_file_thread_pool` in nginx.conf), relevant only to local and mapped modesþ3. `--with-cc-opt=""-O3""` - enable additional compiler optimizations (we saw about 8% reduction in the mp4 parse timeþ and frame processing time compared to the nginx default `-O`)þþDebug settings:þ1. `--with-debug` - enable debug messages (also requires passing `debug` in the `error_log` directive in nginx.conf).þ2. `--with-cc-opt=""-O0""` - disable compiler optimizations (for debugging with gdb)þþ### Installationþþ#### RHEL/CentOS 6/7 RPMþ```þ# rpm -ihv http://installrepo.kaltura.org/releases/kaltura-release.noarch.rpmþ# yum install kaltura-nginxþ```þþ#### Debian/Ubuntu deb packageþ*Ubuntu NOTE: before trying to install kaltura-nginx, you must also make sure the multiverse repo is enabled*þþFor Debian Wheezy [7], Debian Jessie [8], Ubuntu 14.04 and 14.10, add this repo:þ```þ# wget -O - http://installrepo.kaltura.org/repo/apt/debian/kaltura-deb-curr.gpg.key|apt-key add -þ# echo ""deb [arch=amd64] http://installrepo.kaltura.org/repo/apt/debian propus main"" > /etc/apt/sources.list.d/kaltura.listþ```þþFor Ubuntu 16.04, 16.10 add this repo:þ```þ# wget -O - http://installrepo.kaltura.org/repo/apt/xenial/kaltura-deb-curr-256.gpg.key|apt-key add -þ# echo ""deb [arch=amd64] http://installrepo.kaltura.org/repo/apt/xenial propus main"" > /etc/apt/sources.list.d/kaltura.listþ```þþThen install the kaltura-nginx package:þ```þ# apt-get updateþ# apt-get install kaltura-nginxþ```þþþIf you wish to make use of the following features:þ- Thumbnail captureþ- Playback rate change - 0.5x up to 2xþþYou will also need to install the kaltura-ffmpeg (>= 3.1) package.þþ### URL structureþþ#### Basic URL structureþþThe basic structure of an nginx-vod-module URL is:þ`http://<domain>/<location>/<fileuri>/<filename>`þþWhere:þ* domain - the domain of the nginx-vod-module serverþ* location - the location specified in the nginx confþ* fileuri - a URI to the mp4 file:þ  * local mode - the full file path is determined according to the root / alias nginx.conf directivesþ  * mapped mode - the full file path is determined according to the JSON received from the upstream / local fileþ  * remote mode - the mp4 file is read from upstream in chunksþ  * Note: in mapped & remote modes, the URL of the upstream request is `http://<upstream>/<location>/<fileuri>?<extraargs>`þ  (extraargs is determined by the `vod_upstream_extra_args` parameter)þ* filename - detailed belowþþ#### Multi URL structureþþMulti URLs are used to encode several URLs on a single URL. A multi URL can be used to specifyþthe URLs of several different MP4 files that should be included together in a DASH MPD for example.þþThe structure of a multi URL is:þ`http://<domain>/<location>/<prefix>,<middle1>,<middle2>,<middle3>,<postfix>.urlset/<filename>`þþThe sample URL above represents 3 URLs:þ* `http://<domain>/<location>/<prefix><middle1><postfix>/<filename>`þ* `http://<domain>/<location>/<prefix><middle2><postfix>/<filename>`þ* `http://<domain>/<location>/<prefix><middle3><postfix>/<filename>`þþThe suffix `.urlset` (can be changed using `vod_multi_uri_suffix`) indicates that the URL should be treated as a multi URL.þFor example - the URL `http://example.com/hls/videos/big_buck_bunny_,6,9,15,00k.mp4.urlset/master.m3u8` will return a manifest containing:þ* http://example.com/hls/videos/big_buck_bunny_600k.mp4/index.m3u8þ* http://example.com/hls/videos/big_buck_bunny_900k.mp4/index.m3u8þ* http://example.com/hls/videos/big_buck_bunny_1500k.mp4/index.m3u8þþ#### URL path parametersþþThe following parameters are supported on the URL path:þ* clipFrom - an offset in milliseconds since the beginning of the video, where the generated stream should start. þ For example, `.../clipFrom/10000/...` will generate a stream that starts 10 seconds into the video.þ* clipTo - an offset in milliseconds since the beginning of the video, where the generated stream should end.þ For example, `.../clipTo/60000/...` will generate a stream truncated to 60 seconds.þ* tracks - can be used to select specific audio/video tracks. The structure of the parameter is: `v<id1>-v<id2>-a<id1>-a<id2>...`þ For example, `.../tracks/v1-a1/...` will select the first video track and first audio track.þ The default is to include all tracks.þ* shift - can be used to apply a timing shift to one or more streams. The structure of the parameter is: `v<vshift>-a<ashift>-s<sshift>`þ For example, `.../shift/v100/...` will apply a forward shift of 100ms to the video timestamps.þþ#### Filename structureþþThe structure of filename is:þ`<basename>[<seqparams>][<fileparams>][<trackparams>][<langparams>].<extension>`þþWhere:þ* basename + extension - the set of options is packager specific (the list below applies to the default settings):þ  * dash - manifest.mpdþ  * hds - manifest.f4mþ  * hls master playlist - master.m3u8þ  * hls media playlist - index.m3u8þ  * mss - manifestþ  * thumb - `thumb-<offset>[<resizeparams>].jpg` (offset is the thumbnail video offset in milliseconds)þ  * volume_map - `volume_map.csv`þ* seqparams - can be used to select specific sequences by id (provided in the mapping JSON), e.g. master-sseq1.m3u8.þ* fileparams - can be used to select specific sequences by index when using multi URLs.þ For example, manifest-f1.mpd will return an MPD only from the first URL.þ* trackparams - can be used to select specific audio/video tracks.þ For example, manifest-a1.f4m will return an F4M containing only the first audio stream of each sequence.þ The default is to include the first audio and first video tracks of each file.þ The tracks selected on the file name are AND-ed with the tracks selected with the /tracks/ path parameter.þ v0/a0 select all video/audio tracks respectively.þ The a/v parameters can be combined with f/s, e.g. f1-v1-f2-a1 = video1 of file1 + audio1 of file2, f1-f2-v1 = video1 of file1 + video1 of file2.þ* langparams - can be used to filter audio tracks/subtitles according to their language (ISO639-3 code).þ For example, master-leng.m3u8 will return only english audio tracks.þ* resizeparams - can be used to resize the returned thumbnail image. For example, thumb-1000-w150-h100.jpg captures a thumbnailþ 1 second into the video, and resizes it to 150x100. If one of the dimensions is omitted, its value is set so that the þ resulting image will retain the aspect ratio of the video frame.þþ### Mapping response formatþþWhen configured to run in mapped mode, nginx-vod-module issues an HTTP request to a configured upstream server þin order to receive the layout of media streams it should generate.þThe response has to be in JSON format. þþThis section contains a few simple examples followed by a reference of the supported objects and fields. þBut first, a couple of definitions:þþ1. `Source Clip` - a set of audio and/or video frames (tracks) extracted from a single media fileþ2. `Generator` - a component that can generate audio/video frames. Currently, the only supported generator is the silence generator.þ3. `Filter` - a manipulation that can be applied on audio/video frames. The following filters are supported: þ  * rate (speed) change - applies to both audio and videoþ  * audio volume changeþ  * mix - can be used to merge several audio tracks together, or to merge the audio of source A with the video of source Bþ4. `Clip` - the result of applying zero or more filters on a set of source clipsþ5. `Dynamic Clip` - a clip whose contents is not known in advance, e.g. targeted ad contentþ6. `Sequence` - a set of clips that should be played one after the other. þ7. `Set` - several sequences that play together as an adaptive set, each sequence must have the same number of clips.þþ#### Simple mappingþþThe JSON below maps the request URI to a single MP4 file:þ```þ{þ ""sequences"": [þ  {þ   ""clips"": [þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video.mp4""þ    }þ   ]þ  }þ ]þ}þ```þþWhen using multi URLs, this is the only allowed JSON pattern. In other words, it is notþpossible to combine more complex JSONs using multi URL.þþ#### Adaptive setþþAs an alternative to using multi URL, an adaptive set can be defined via JSON:þ```þ{þ ""sequences"": [þ  {þ   ""clips"": [þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/bitrate1.mp4""þ    }þ   ]þ  },þ  {þ   ""clips"": [þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/bitrate2.mp4""þ    }þ   ]þ  }þ ]þ}þ```þþ#### PlaylistþþThe JSON below will play 35 seconds of video1 followed by 22 seconds of video2:þ```þ{þ ""durations"": [ 35000, 22000 ],þ ""sequences"": [þ  {þ   ""clips"": [þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video1.mp4""þ    },þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video2.mp4""þ    }þ   ]þ  }þ ]þ}þ```þþ#### FiltersþþThe JSON below takes video1, plays it at x1.5 and mixes the audio of the result with the audio of video2,þafter reducing it to 50% volume:þ```þ{þ ""sequences"": [þ  {þ   ""clips"": [þ    {þ     ""type"": ""mixFilter"",þ     ""sources"": [þ      {þ       ""type"": ""rateFilter"",þ       ""rate"": 1.5,þ       ""source"": {þ        ""type"": ""source"",þ        ""path"": ""/path/to/video1.mp4""þ       }þ      },þ      {þ       ""type"": ""gainFilter"",þ       ""gain"": 0.5,þ       ""source"": {þ        ""type"": ""source"",þ        ""path"": ""/path/to/video2.mp4"",þ        ""tracks"": ""a1""þ       }þ      }þ     ]þ    }þ   ]þ  }þ ]þ}þ```þþ#### Continuous liveþþThe JSON below is a sample of a continuous live stream (=a live stream in which all videos have exactly the same encoding parameters).þIn practice, this JSON will have to be generated by some script, since it is time dependent.þ(see test/playlist.php for a sample implementation)þ```þ{þ ""playlistType"": ""live"",þ ""discontinuity"": false,þ ""segmentBaseTime"": 1451904060000,þ ""firstClipTime"": 1451917506000,þ ""durations"": [83000, 83000],þ ""sequences"": [þ  {þ   ""clips"": [þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video1.mp4""þ    },þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video2.mp4""þ    }þ   ]þ  }þ ]þ}þ```þþ#### Non-continuous liveþþThe JSON below is a sample of a non-continuous live stream (=a live stream in which the videos have different encoding parameters).þIn practice, this JSON will have to be generated by some script, since it is time dependent þ(see test/playlist.php for a sample implementation)þ```þ{þ ""playlistType"": ""live"",þ ""discontinuity"": true,þ ""initialClipIndex"": 171,þ ""initialSegmentIndex"": 153,þ ""firstClipTime"": 1451918170000,þ ""durations"": [83000, 83000],þ ""sequences"": [þ  {þ   ""clips"": [þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video1.mp4""þ    },þ    {þ     ""type"": ""source"",þ     ""path"": ""/path/to/video2.mp4""þ    }þ   ]þ  }þ ]þ}þ```þþ### Mapping referenceþþ#### Set (top level object in the mapping JSON)þþMandatory fields:þ* `sequences` - array of Sequence objects. þ The mapping has to contain at least one sequence and up to 32 sequences.þ þOptional fields:þ* `id` - a string that identifies the set. The id can be retrieved by `$vod_set_id`.þ* `playlistType` - string, can be set to `live` or `vod`, default is `vod`.þ* `durations` - an array of integers representing clip durations in milliseconds.þ This field is mandatory if the mapping contains more than a single clip per sequence.þ If specified, this array must contain at least one element and up to 128 elements.þ* `discontinuity` - boolean, indicates whether the different clips in each sequence haveþ different media parameters. This field has different manifestations according to the þ delivery protocol - a value of true will generate `#EXT-X-DISCONTINUITY` in HLS, þ and a multi period MPD in DASH. The default value is true, set to false only if the mediaþ files were transcoded with exactly the same parameters (in AVC for example, þ the clips should have exactly the same SPS/PPS).þ* `segmentDuration` - integer, sets the segment duration in milliseconds. This field, þ if specified, takes priority over the value set in `vod_segment_duration`.þ* `consistentSequenceMediaInfo` - boolean, currently affects only DASH. When set to true (default)þ the MPD will report the same media parameters in each period element. Setting to falseþ can have severe performance implications for long sequences (nginx-vod-module has þ to read the media info of all clips included in the mapping in order to generate the MPD)þ* `referenceClipIndex` - integer, sets the (1-based) index of the clip that should be used þ to retrieve the video metadata for manifest requests (codec, width, height etc.)þ If `consistentSequenceMediaInfo` is set to false, this parameter has no effect -þ all clips are parsed. If this parameter is not specified, nginx-vod-module uses the last clip þ by default.þ* `notifications` - array of notification objects (see below), when a segment is requested,þ all the notifications that fall between the start/end times of the segment are fired.þ the notifications must be ordered in an increasing offset order.þ* `clipFrom` - integer, contains a timestamp indicating where the returned stream should start.þ Setting this parameter is equivalent to passing /clipFrom/ on the URL.þ* `clipTo` - integer, contains a timestamp indicating where the returned stream should end.þ Setting this parameter is equivalent to passing /clipTo/ on the URL.þ þLive fields:þ* `firstClipTime` - integer, mandatory for all live playlists unless `clipTimes` is specified.þ Contains the absolute time of the first clip in the playlist, in milliseconds since the epoch (unixtime x 1000)þ* `clipTimes` - array of integers, sets the absolute time of all the clips in the playlist, þ in milliseconds since the epoch (unixtime x 1000). This field can be used only when þ `discontinuity` is set to true. The timestamps may contain gaps, but they are not allowed to overlapþ (`clipTimes[n + 1] >= clipTimes[n] + durations[n]`)þ* `segmentBaseTime` - integer, mandatory for continuous live streams, contains the absoluteþ time of the first segment of the stream, in milliseconds since the epoch (unixtime x 1000).þ This value must not change during playback.þ For discontinuous live streams, this field is optional:þ * if not set, sequential segment indexes will be used throughout the playlist.þ  In this case, the upstream server generating the mapping json has to maintain state,þ  and update initialSegmentIndex every time a clip is removed from the playlist.þ * if set, the timing gaps between clips must not be lower than `vod_segment_duration`.þ* `firstClipStartOffset` - integer, optional, measured in milliseconds. This field contains theþ difference between first clip time, and the original start time of the first clip -þ the time it had when it was initially added (before the live window shifted)þ* `initialClipIndex` - integer, mandatory for non-continuous live streams that mix videos havingþ different encoding parameters (SPS/PPS), contains the index of the first clip in the playlist. þ Whenever a clip is pushed out of the head of the playlist, this value must be incremented by one.þ* `initialSegmentIndex` - integer, mandatory for live streams that do not set `segmentBaseTime`, þ contains the index of the first segment in the playlist. Whenever a clip is pushed out of the head ofþ the playlist, this value must be incremented by the number of segments in the clip.þ* `presentationEndTime` - integer, optional, measured in milliseconds since the epoch.þ when supplied, the module will compare the current time to the supplied value, þ and signal the end of the live presentation if `presentationEndTime` has passed. þ In HLS, for example, this parameter controls whether an `#EXT-X-ENDLIST` tag should be þ included in the media playlist.þ When the parameter is not supplied, the module will not signal live presentation end.þ* `expirationTime` - integer, optional, measured in milliseconds since the epoch.þ when supplied, the module will compare the current time to the supplied value, þ and if `expirationTime` has passed, the module will return a 404 error for manifest requests þ (segment requests will continue to be served).þ when both presentationEndTime and expirationTime have passed, presentationEndTime takesþ priority, i.e. manifest requests will be served and signal presentation end.þ* `liveWindowDuration` - integer, optional, provides a way to override `vod_live_window_duration`þ specified in the configuration. If the value exceeds the absolute value specified in þ `vod_live_window_duration`, it is ignored.þ þ#### SequenceþþMandatory fields:þ* `clips` - array of Clip objects (mandatory). The number of elements must match the numberþ the durations array specified on the set. If the durations array is not specified,þ the clips array must contain a single element.þ þOptional fields:þ* `id` - a string that identifies the sequence. The id can be retrieved by `$vod_sequence_id`.þ* `language` - a 3-letter (ISO-639-2) language code, this field takes priority over any languageþ specified on the media file (MP4 mdhd atom)þ* `label` - a friendly string that identifies the sequence. If a language is specified,þ a default label will be automatically derived by it - e.g. if language is `ita`, þ by default `italiano` will be used as the label.þ* `bitrate` - an object that can be used to set the bitrate for the different media types,þ in bits per second. For example, `{""v"": 900000, ""a"": 64000}`. If the bitrate is not supplied,þ nginx-vod-module will estimate it based on the last clip in the sequence.þ* `avg_bitrate` - an object that can be used to set the average bitrate for the different media types,þ in bits per second. See `bitrate` above for a sample object. If specified, the module will useþ the value to populate the AVERAGE-BANDWIDTH attribute of `#EXT-X-STREAM-INF` in HLS.þþ#### Clip (abstract)þþMandatory fields:þ* `type` - a string that defines the type of the clip. Allowed values are:þ * sourceþ * rateFilterþ * mixFilterþ * gainFilterþ * silenceþ * concatþ * dynamicþþOptional fields:þ* `keyFrameDurations` - array of integers, containing the durations in milliseconds of the video key framesþ in the clip. This property can only be supplied on the top level clips of each sequence,þ supplying this property on nested clips has no effect.þ Supplying the key frame durations enables the module to both:þ 1. align the segments to key frames þ 2. report the correct segment durations in the manifest - providing an alternative to settingþ  `vod_manifest_segment_durations_mode` to `accurate`, which is not supported for multi clipþ  media sets (for performance reasons).þ* `firstKeyFrameOffset` - integer, offset of the first video key frame in the clip, þ measured in milliseconds relative to `firstClipTime`. Defaults to 0 if not supplied.þþ#### Source clipþþMandatory fields:þ* `type` - a string with the value `source`þ* `path` - a string containing the path of the MP4 file. The string `""empty""` can be used to representþ an empty captions file (useful in case only some videos in a playlist have captions)þþOptional fields:þ* `tracks` - a string that specifies the tracks that should be used, the default is ""v1-a1"",þ which means the first video track and the first audio trackþ* `clipFrom` - an integer that specifies an offset in milliseconds, from the beginning of the þ media file, from which to start loading framesþ* `encryptionKey` - a base64 encoded string containing the key (128/192/256 bit) that should be usedþ to decrypt the file.þ* `encryptionIv` - a base64 encoded string containing the iv (128 bit) that should be usedþ to decrypt the file.þ* `encryptionScheme` - the encryption scheme that was used to encrypt the file. Currently,þ only two schemes are supported - `cenc` for MP4 files, `aes-cbc` for caption files.þþ#### Rate filter clipþþMandatory fields:þ* `type` - a string with the value `rateFilter`þ* `rate` - a float that specified the acceleration factor, e.g. a value of 2 means double speed.þ Allowed values are in the range 0.5 - 2 with up to two decimal pointsþ* `source` - a clip object on which to perform the rate filteringþþ#### Gain filter clipþþMandatory fields:þ* `type` - a string with the value `gainFilter`þ* `gain` - a float that specified the amplification factor, e.g. a value of 2 means twice as loud.þ The gain must be positive with up to two decimal pointsþ* `source` - a clip object on which to perform the gain filteringþþ#### Mix filter clipþþMandatory fields:þ* `type` - a string with the value `mixFilter`þ* `sources` - an array of Clip objects to mix. This array must contain at least one clip andþ up to 32 clips.þþ#### Concat clipþþMandatory fields:þ* `type` - a string with the value `concat`þ* `durations` - an array of integers representing MP4 durations in milliseconds,þ this array must match the `paths` array in count and order.þþOptional fields:þ* `paths` - an array of strings, containing the paths of the MP4 files. Either `paths` or `clipIds` must be specified.þ* `clipIds` - an array of strings, containing the ids of source clips. þ The ids are translated to paths by issuing a request to the uri specified in `vod_source_clip_map_uri`.þ Either `paths` or `clipIds` must be specified.þ* `tracks` - a string that specifies the tracks that should be used, the default is ""v1-a1"",þ which means the first video track and the first audio trackþ* `offset` - an integer in milliseconds that indicates the timestamp offset of the þ first frame in the concatenated stream relative to the clip start timeþ* `basePath` - a string that should be added as a prefix to all the pathsþ* `notifications` - array of notification objects (see below), when a segment is requested,þ all the notifications that fall between the start/end times of the segment are fired.þ the notifications must be ordered in an increasing offset order.þþ#### Dynamic clipþþMandatory fields:þ* `type` - a string with the value `dynamic`þ* `id` - a string that uniquely identifies the dynamic clip, used for mapping the clip to its contentþþ#### NotificationþþMandatory fields:þ* `offset` - an integer in milliseconds that indicates the time in which the notification should be fired.þ when the notification object is contained in the media set, `offset` is relative to `firstClipTime`þ (0 for vod). when the notification object is contained in a concat clip, `offset` is relative toþ the beginning of the concat clip.þ* `id` - a string that identifies the notification, this id can be referenced by `vod_notification_uri`þ using the variable `$vod_notification_id`þþ### Securityþþ#### Authorizationþþ##### CDN-based deliveryþþMedia packaged by nginx-vod-module can be protected using CDN tokens, this works as follows:þ* Some application authenticates the user and decides whether the user should be allowed þ to watch a specific video. If the user is allowed, the application generates a tokenizedþ URL for the manifest of the video.þ* The CDN validates the token, and if found to be valid, forwards the request to nginx-vod-module þ on the origin. þ* The nginx server builds the manifest response and generates tokens for the segment URLsþ contained inside it. The module https://github.com/kaltura/nginx-secure-token-module canþ be used to accomplish this task, it currently support Akamai tokens and CloudFront tokens.þ See the readme of this module for more details.þ* The CDN validates the token on each segment that is requested.þþIn this setup it also highly recommended to block direct access to the origin server byþauthenticating the CDN requests. Without this protection, a user who somehow gets the addressþof the origin will be able to bypass the CDN token enforcement. If using Akamai, this canþbe accomplished using https://github.com/refractalize/nginx_mod_akamai_g2o.þFor other CDNs, it may be possible to configure the CDN to send a secret header to the originþand then simply enforce the header using an nginx if statement:þ```þ  if ($http_x_secret_origin_header != ""secret value"") {þ   return 403;þ  }þ```þþIn addition to the above, most CDNs support other access control settings, such as geo-location.þThese restrictions are completely transparent to the origin and should work well. þþ##### Direct deliveryþþDeployments in which the media is pulled directly from nginx-vod-module can protect the mediaþusing nginx access control directives, such `allow`, `deny`, or `access_by_lua` (for more complexþscenarios).þþIn addition, it is possible to build a token based solution (as detailed in the previous section) þwithout a CDN, by having the nginx server validate the token. þThe module https://github.com/kaltura/nginx-akamai-token-validate-module can be usedþto validate Akamai tokens. Locations on which the module is enabled will return 403 unless the þrequest contains a valid Akamai token. See the readme of this module for more details.þþ#### URL encryptionþþAs an alternative to tokenization, URL encryption can be used to prevent an attacker from beingþable to craft a playable URL. URL encryption can be implemented with þhttps://github.com/kaltura/nginx-secure-token-module, and is supported for HLS and DASH (with þmanifest format set to segmentlist). þþIn terms of security, the main advantage of CDN tokens over URL encryption is that CDN tokensþusually expire, while encrypted URLs do not (someone who obtains a playable URL will be able toþuse it indefinitely)þþ#### Media encryptionþþNginx-vod-module supports AES-128 and SAMPLE-AES HLS encryption schemes. The main difference betweenþmedia encryption and DRM (detailed below) is the mechanism used to transfer the encryption key to þthe client. With media encryption the key is fetched by the client by performing a simple GET requestþto nginx-vod-module, while with DRM the key is returned inside a vendor specific license response.þþMedia encryption reduces the problem of securing the media to the need to secure the encryption key. þThe media segment URLs (which compose the vast majority of the traffic) can be completely unprotected, þand easily cacheable by any proxies between the client and servers (unlike tokenization). þThe encryption key request can then be protected using one of the methods mentioned above (CDN tokens,þnginx access rules etc.). þþIn addition, it is possible to configure nginx-vod-module to return the encryption key over HTTPSþwhile having the segments delivered over HTTP. The way to configure this is to set `vod_segments_base_url`þto `http://nginx-vod-host` and set `vod_base_url` to `https://nginx-vod-host`.þþ#### DRMþþNginx-vod-module has the ability to perform on-the-fly encryption for MPEG DASH (CENC), MSS Play Ready and FairPlay HLS.þAs in the case of media encryption, the encryption is performed while serving a video/audio segment to the client, þtherefore, when working with DRM it is recommended not to serve the content directly from nginx-vod-module to end-users.þA more scalable architecture would be to use proxy servers or a CDN in order to cache the encrypted segments.þþIn order to perform the encryption, nginx-vod-module needs several parameters, including key & key_id, these parametersþare fetched from an external server via HTTP GET requests.þThe `vod_drm_upstream_location` parameter specifies an nginx location that is used to access the DRM server,þand the request uri is configured using `vod_drm_request_uri` (this parameter can include nginx variables). þThe response of the DRM server is a JSON, with the following format:þþ```þ[{þ ""pssh"": [{þ   ""data"": ""CAESEGMyZjg2MTczN2NjNGYzODIaB2thbHR1cmEiCjBfbmptaWlwbXAqBVNEX0hE"", þ   ""uuid"": ""edef8ba9-79d6-4ace-a3c8-27dcd51d21ed""þ  }], þ ""key"": ""GzoNU9Dfwc//Iq3/zbzMUw=="", þ ""key_id"": ""YzJmODYxNzM3Y2M0ZjM4Mg==""þ}]þ```þþ* `pssh.data` - base64 encoded binary data, the format of this data is drm vendor specificþ* `pssh.uuid` - the drm system UUID, in this case, edef8ba9-79d6-4ace-a3c8-27dcd51d21ed stands for Widevineþ* `key` - base64 encoded encryption key (128 bit)þ* `key_id` - base64 encoded key identifier (128 bit)þ* `iv` - optional base64 encoded initialization vector (128 bit). The IV is currently used only in HLS (FairPlay), þ in the other protocols an IV is generated automatically by nginx-vod-module.þþ##### Sample configurationsþþApple FairPlay HLS:þ```þlocation ~ ^/fpshls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {þ vod hls;þ vod_hls_encryption_method sample-aes;þ vod_hls_encryption_key_uri ""skd://entry-$2"";þ vod_hls_encryption_key_format ""com.apple.streamingkeydelivery"";þ vod_hls_encryption_key_format_versions ""1"";þþ vod_drm_enabled on;þ vod_drm_request_uri ""/udrm/system/ovp/$vod_suburi"";þþ vod_last_modified_types *;þ add_header Access-Control-Allow-Headers '*';þ add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';þ add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';þ add_header Access-Control-Allow-Origin '*';þ expires 100d;þ}þ```þþCommon Encryption HLS:þ```þlocation ~ ^/cenchls/p/\d+/(sp/\d+/)?serveFlavor/entryId/([^/]+)/(.*) {þ vod hls;þ vod_hls_encryption_method sample-aes-cenc;þ vod_hls_encryption_key_format ""urn:uuid:edef8ba9-79d6-4ace-a3c8-27dcd51d21ed"";þ vod_hls_encryption_key_format_versions ""1"";þþ vod_drm_enabled on;þ vod_drm_request_uri ""/udrm/system/ovp/$vod_suburi"";þþ vod_last_modified_types *;þ add_header Access-Control-Allow-Headers '*';þ add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';þ add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';þ add_header Access-Control-Allow-Origin '*';þ expires 100d;þ}þ```þþ##### Verified configurationsþþFollowing is a list of configurations that were tested and found working:þ* DASH/CENC with PlayReady & Widevine PSSH togetherþ* MSS PlayReadyþ* HLS FairPlayþþ### Performance recommendationsþþ1. For medium/large scale deployments, don't have users play the videos directly from nginx-vod-module.þ Since all the different streaming protocols supported by nginx vod are HTTP based, they can be cached by standard HTTP proxies / CDNs. þ For medium scale add a layer of caching proxies between the vod module and the end users þ (can use standard nginx servers with proxy_pass & proxy_cache). þ For large scale deployments, it is recommended to use a CDN (such as Akamai, Level3 etc.). þ þ In general, it's best to have nginx vod as close as possible to where the mp4 files are stored, þ and have the caching proxies as close as possible to the end users.þ2. Enable nginx-vod-module caches:þ * `vod_metadata_cache` - saves the need to re-read the video metadata for each segment. This cache should be rather large, in the order of GBs.þ * `vod_response_cache` - saves the responses of manifest requests. This cache may not be required when using a second layer of caching servers before nginx vod. þ  No need to allocate a large buffer for this cache, 128M is probably more than enough for most deployments.þ * `vod_mapping_cache` - for mapped mode only, few MBs is usually enough.þ * nginx's open_file_cache - caches open file handles.þþ The hit/miss ratios of these caches can be tracked by enabling performance counters (`vod_performance_counters`)þ and setting up a status page for nginx vod (`vod_status`)þ3. In local & mapped modes, enable aio. - nginx has to be compiled with aio support, and it has to be enabled in nginx conf (aio on). þ You can verify it works by looking at the performance counters on the vod status page - read_file (aio off) vs. async_read_file (aio on)þ4. In local & mapped modes, enable asynchronous file open - nginx has to be compiled with threads support, and `vod_open_file_thread_pool`þ has to be specified in nginx.conf. You can verify it works by looking at the performance counters on the vod status page - þ open_file vs. async_open_file. Note that open_file may be nonzero with vod_open_file_thread_pool enabled, due to the open file cache - þ open requests that are served from cache will be counted as synchronous open_file.þ5. When using DRM enabled DASH/MSS, if the video files have a single nalu per frame, set `vod_min_single_nalu_per_frame_segment` to non-zero.þ6. The muxing overhead of the streams generated by this module can be reduced by changing the following parameters:þ * HDS - set `vod_hds_generate_moof_atom` to offþ * HLS - set `vod_hls_mpegts_align_frames` to off and `vod_hls_mpegts_interleave_frames` to onþ7. Enable gzip compression on manifest responses - þþ `gzip_types application/vnd.apple.mpegurl video/f4m application/dash+xml text/xml`þ8. Apply common nginx performance best practices, such as tcp_nodelay=on, client_header_timeout etc.þþ### Configuration directives - baseþþ#### vodþ* **syntax**: `vod segmenter`þ* **default**: `n/a`þ* **context**: `location`þþEnables the nginx-vod module on the enclosing location. þThe allowed values for `segmenter` are:þþ1. `none` - serves the MP4 files as is / clippedþ2. `dash` - Dynamic Adaptive Streaming over HTTP packagerþ3. `hds` - Adobe HTTP Dynamic Streaming packagerþ4. `hls` - Apple HTTP Live Streaming packagerþ5. `mss` - Microsoft Smooth Streaming packagerþ6. `thumb` - thumbnail captureþ7. `volume_map` - audio volume mapþþ#### vod_modeþ* **syntax**: `vod_mode mode`þ* **default**: `local`þ* **context**: `http`, `server`, `location`þþSets the file access mode - local, remote or mapped (see the features section above for more details)þþ#### vod_statusþ* **syntax**: `vod_status`þ* **default**: `n/a`þ* **context**: `location`þþEnables the nginx-vod status page on the enclosing location. þþ### Configuration directives - segmentationþþ#### vod_segment_durationþ* **syntax**: `vod_segment_duration duration`þ* **default**: `10s`þ* **context**: `http`, `server`, `location`þþSets the segment duration in milliseconds. It is highly recommended to use a segment duration that is a multiple of the GOP duration.þIf the segment duration is not a multiple of GOP duration, and `vod_align_segments_to_key_frames` is enabled, there could be significantþdifferences between the segment duration that is reported in the manifest and the actual segment duration. This could also lead toþthe appearance of empty segments within the stream.þþ#### vod_live_window_durationþ* **syntax**: `vod_live_window_duration duration`þ* **default**: `30000`þ* **context**: `http`, `server`, `location`þþSets the total duration in milliseconds of the segments that should be returned in a live manifest.þIf the value is positive, nginx vod returns a range of maximum `vod_live_window_duration` milliseconds, ending at the current server time.þIf the value is negative, nginx vod returns a range of maximum `-vod_live_window_duration` milliseconds from the end of the mapping json.þIf the value is set to zero, the live manifest will contain all the segments that are fully contained in the mapping json time frame.þþ#### vod_force_playlist_type_vodþ* **syntax**: `vod_force_playlist_type_vod on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþGenerate a vod stream even when the media set has `playlistType=live`. þEnabling this setting has the following effects:þ1. Frame timestamps will be continuous and start from zeroþ2. Segment indexes will start from oneþ3. In case of HLS, the returned manifest will have both `#EXT-X-PLAYLIST-TYPE:VOD` and `#EXT-X-ENDLIST`þþThis can be useful for clipping vod sections out of a live stream.þþ#### vod_force_continuous_timestampsþ* **syntax**: `vod_force_continuous_timestamps on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþGenerate continuous timestamps even when the media set has gaps (gaps can created by the use of `clipTimes`)þIf ID3 timestamps are enabled (`vod_hls_mpegts_output_id3_timestamps`), they contain the original timestamps that were set in `clipTimes`.þþ#### vod_bootstrap_segment_durationsþ* **syntax**: `vod_bootstrap_segment_durations duration`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþAdds a bootstrap segment duration in milliseconds. This setting can be used to make the first few segmentsþshorter than the default segment duration, thus making the adaptive bitrate selection kick-in earlier without þthe overhead of short segments throughout the video.þþ#### vod_align_segments_to_key_framesþ* **syntax**: `vod_align_segments_to_key_frames on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, the module forces all segments to start with a key frame. Enabling this setting can lead to differencesþbetween the actual segment durations and the durations reported in the manifest (unless `vod_manifest_segment_durations_mode` is set to accurate).þþ#### vod_segment_count_policyþ* **syntax**: `vod_segment_count_policy last_short/last_long/last_rounded`þ* **default**: `last_short`þ* **context**: `http`, `server`, `location`þþConfigures the policy for calculating the segment count, for segment_duration = 10 seconds:þ* last_short - a file of 33 sec is partitioned as - 10, 10, 10, 3þ* last_long - a file of 33 sec is partitioned as - 10, 10, 13þ* last_rounded - a file of 33 sec is partitioned as - 10, 10, 13, a file of 38 sec is partitioned as 10, 10, 10, 8þþ#### vod_manifest_duration_policyþ* **syntax**: `vod_manifest_duration_policy min/max`þ* **default**: `max`þ* **context**: `http`, `server`, `location`þþConfigures the policy for calculating the duration of a manifest containing multiple streams:þ* max - uses the maximum stream duration (default)þ* min - uses the minimum non-zero stream durationþþ#### vod_manifest_segment_durations_modeþ* **syntax**: `vod_manifest_segment_durations_mode estimate/accurate`þ* **default**: `estimate`þ* **context**: `http`, `server`, `location`þþConfigures the calculation mode of segment durations within manifest requests:þ* estimate - reports the duration as configured in nginx.conf, e.g. if `vod_segment_duration` has the value 10000,þan HLS manifest will contain #EXTINF:10þ* accurate - reports the exact duration of the segment, taking into account the frame durations, e.g. for a þframe rate of 29.97 and 10 second segments it will report the first segment as 10.01. accurate mode alsoþtakes into account the key frame alignment, in case `vod_align_segments_to_key_frames` is onþþ#### vod_media_set_override_jsonþ* **syntax**: `vod_media_set_override_json json`þ* **default**: `{}`þ* **context**: `http`, `server`, `location`þþThis parameter provides a way to override portions of the media set JSON (mapped mode only).þFor example, `vod_media_set_override_json '{""clipTo"":20000}'` clips the media set to 20 sec.þThe parameter value can contain variables.þþ### Configuration directives - upstreamþþ#### vod_upstream_locationþ* **syntax**: `vod_upstream_location location`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets an nginx location that is used to read the MP4 file (remote mode) or mapping the request URI (mapped mode).þþ#### vod_remote_upstream_locationþ* **syntax**: `vod_remote_upstream_location location`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets an nginx location that is used to read the MP4 file on remote or mapped mode. If this directive is set on mapped mode, the module reads þthe MP4 files over HTTP, treating the paths in the mapping JSON as URIs (the default behavior is to read from local files)þþ#### vod_max_upstream_headers_sizeþ* **syntax**: `vod_max_upstream_headers_size size`þ* **default**: `4k`þ* **context**: `http`, `server`, `location`þþSets the size that is allocated for holding the response headers when issuing upstream requests (to vod_xxx_upstream_location).þþ#### vod_upstream_extra_argsþ* **syntax**: `vod_upstream_extra_args ""arg1=value1&arg2=value2&...""`þ* **default**: `empty`þ* **context**: `http`, `server`, `location`þþExtra query string arguments that should be added to the upstream request (remote/mapped modes only).þThe parameter value can contain variables.þþ#### vod_media_set_map_uriþ* **syntax**: `vod_media_set_map_uri uri`þ* **default**: `$vod_suburi`þ* **context**: `http`, `server`, `location`þþSets the uri of media set mapping requests, the parameter value can contain variables.þIn case of multi url, `$vod_suburi` will be the current sub uri (a separate request is issued per sub URL)þþ#### vod_path_response_prefixþ* **syntax**: `vod_path_response_prefix prefix`þ* **default**: `{""sequences"":[{""clips"":[{""type"":""source"",""path"":""`þ* **context**: `http`, `server`, `location`þþSets the prefix that is expected in URI mapping responses (mapped mode only).þþ#### vod_path_response_postfixþ* **syntax**: `vod_path_response_postfix postfix`þ* **default**: `""}]}]}`þ* **context**: `http`, `server`, `location`þþSets the postfix that is expected in URI mapping responses (mapped mode only).þþ#### vod_max_mapping_response_sizeþ* **syntax**: `vod_max_mapping_response_size length`þ* **default**: `1K`þ* **context**: `http`, `server`, `location`þþSets the maximum length of a path returned from upstream (mapped mode only).þþ### Configuration directives - fallbackþþ#### vod_fallback_upstream_locationþ* **syntax**: `vod_fallback_upstream_location location`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets an nginx location to which the request is forwarded after encountering a file not found error (local/mapped modes only).þþ#### vod_proxy_header_nameþ* **syntax**: `vod_proxy_header_name name`þ* **default**: `X-Kaltura-Proxy`þ* **context**: `http`, `server`, `location`þþSets the name of an HTTP header that is used to prevent fallback proxy loops (local/mapped modes only).þþ#### vod_proxy_header_valueþ* **syntax**: `vod_proxy_header_value name`þ* **default**: `dumpApiRequest`þ* **context**: `http`, `server`, `location`þþSets the value of an HTTP header that is used to prevent fallback proxy loops (local/mapped modes only).þþ### Configuration directives - performanceþþ#### vod_metadata_cacheþ* **syntax**: `vod_metadata_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the video metadata cache. For MP4 files, this cache holds the moov atom.þþ#### vod_mapping_cacheþ* **syntax**: `vod_mapping_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the mapping cache for vod (mapped mode only).þþ#### vod_live_mapping_cacheþ* **syntax**: `vod_live_mapping_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the mapping cache for live (mapped mode only).þþ#### vod_response_cacheþ* **syntax**: `vod_response_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the response cache. The response cache holds manifestsþand other non-video content (like DASH init segment, HLS encryption key etc.). Video segments are not cached.þþ#### vod_live_response_cacheþ* **syntax**: `vod_live_response_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the response cache for time changing live responses. þThis cache holds the following types of responses for live: DASH MPD, HLS index M3U8, HDS bootstrap, MSS manifest.þþ#### vod_initial_read_sizeþ* **syntax**: `vod_initial_read_size size`þ* **default**: `4K`þ* **context**: `http`, `server`, `location`þþSets the size of the initial read operation of the MP4 file.þþ#### vod_max_metadata_sizeþ* **syntax**: `vod_max_metadata_size size`þ* **default**: `128MB`þ* **context**: `http`, `server`, `location`þþSets the maximum supported video metadata size (for MP4 - moov atom size)þþ#### vod_max_frames_sizeþ* **syntax**: `vod_max_frames_size size`þ* **default**: `16MB`þ* **context**: `http`, `server`, `location`þþSets the limit on the total size of the frames of a single segmentþþ#### vod_cache_buffer_sizeþ* **syntax**: `vod_cache_buffer_size size`þ* **default**: `256K`þ* **context**: `http`, `server`, `location`þþSets the size of the cache buffers used when reading MP4 frames.þþ#### vod_open_file_thread_poolþ* **syntax**: `vod_open_file_thread_pool pool_name`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþEnables the use of asynchronous file open via thread pool.þThe thread pool must be defined with a thread_pool directive, if no pool name is specified the default pool is used.þThis directive is supported only on nginx 1.7.11 or newer when compiling with --add-threads.þNote: this directive currently disables the use of nginx's open_file_cache by nginx-vod-moduleþþ#### vod_output_buffer_poolþ* **syntax**: `vod_output_buffer_pool size count`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþPre-allocates buffers for generating response data, saving the need allocate/free the buffers on every request.þþ#### vod_performance_countersþ* **syntax**: `vod_performance_counters zone_name`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the shared memory object name of the performance countersþþ### Configuration directives - url structureþþ#### vod_base_urlþ* **syntax**: `vod_base_url url`þ* **default**: `see below`þ* **context**: `http`, `server`, `location`þþSets the base URL (scheme + domain) that should be returned in manifest responses.þThe parameter value can contain variables, if the parameter evaluates to an empty string, relative URLs will be used.þIf the parameter evaluates to a string ending with /, it is assumed to be a full URL - the module only appends theþfile name to it, instead of a full URI.þIf not set, the base URL is determined as follows:þ1. If the request did not contain a host header (HTTP/1.0) relative URLs will be returnedþ2. Otherwise, the base URL will be `$scheme://$http_host`þThe setting currently affects only HLS and DASH. In MSS and HDS, relative URLs are always returned.þþ#### vod_segments_base_urlþ* **syntax**: `vod_segments_base_url url`þ* **default**: `see below`þ* **context**: `http`, `server`, `location`þþSets the base URL (scheme + domain) that should be used for delivering video segments.þThe parameter value can contain variables, if the parameter evaluates to an empty string, relative URLs will be used.þIf not set, vod_base_url will be used.þThe setting currently affects only HLS.þþ#### vod_multi_uri_suffixþ* **syntax**: `vod_multi_uri_suffix suffix`þ* **default**: `.urlset`þ* **context**: `http`, `server`, `location`þþA URL suffix that is used to identify multi URLs. A multi URL is a way to encode several different URLsþthat should be played together as an adaptive streaming set, under a single URL. When the default suffix isþused, an HLS set URL may look like: þhttp://host/hls/common-prefix,bitrate1,bitrate2,common-suffix.urlset/master.m3u8þþ#### vod_clip_to_param_nameþ* **syntax**: `vod_clip_to_param_name name`þ* **default**: `clipTo`þ* **context**: `http`, `server`, `location`þþThe name of the clip to request parameter.þþ#### vod_clip_from_param_nameþ* **syntax**: `vod_clip_from_param_name name`þ* **default**: `clipFrom`þ* **context**: `http`, `server`, `location`þþThe name of the clip from request parameter.þþ#### vod_tracks_param_nameþ* **syntax**: `vod_tracks_param_name name`þ* **default**: `tracks`þ* **context**: `http`, `server`, `location`þþThe name of the tracks request parameter.þþ#### vod_time_shift_param_nameþ* **syntax**: `vod_time_shift_param_name name`þ* **default**: `shift`þ* **context**: `http`, `server`, `location`þþThe name of the shift request parameter.þþ#### vod_speed_param_nameþ* **syntax**: `vod_speed_param_name name`þ* **default**: `speed`þ* **context**: `http`, `server`, `location`þþThe name of the speed request parameter.þþ#### vod_lang_param_nameþ* **syntax**: `vod_lang_param_name name`þ* **default**: `lang`þ* **context**: `http`, `server`, `location`þþThe name of the language request parameter.þþ#### vod_force_sequence_indexþ* **syntax**: `vod_force_sequence_index on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþUse sequence index in segment uris even if there is only one sequenceþþ### Configuration directives - response headersþþ#### vod_expiresþ* **syntax**: `vod_expires time`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the value of the ""Expires"" and ""Cache-Control"" response headers for successful requests.þThis directive is similar to nginx's built-in `expires` directive, except that it only supports the expiration interval scenarioþ(epoch, max, off, day time are not supported)þMain motivation for using this directive instead of the built-in `expires` is to have different expiration for VOD and dynamic live content.þIf this directive is not specified, nginx-vod-module will not set the ""Expires"" / ""Cache-Control"" headers.þThis setting affects all types of requests in VOD playlists and segment requests in live playlists.þþ#### vod_expires_liveþ* **syntax**: `vod_expires_live time`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSame as `vod_expires` (above) for live requests that are not time dependent and not segments (e.g. HLS - master.m3u8, HDS - manifest.f4m).þþ#### vod_expires_live_time_dependentþ* **syntax**: `vod_expires_live_time_dependent time`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSame as `vod_expires` (above) for live requests that are time dependent (HLS - index.m3u8, HDS - bootstrap.abst, MSS - manifest, DASH - manifest.mpd).þþ#### vod_last_modifiedþ* **syntax**: `vod_last_modified time`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the value of the Last-Modified header returned on the response, by default the module does not return a Last-Modified header.þThe reason for having this parameter here is in order to support If-Modified-Since / If-Unmodified-Since.þSince nginx's builtin ngx_http_not_modified_filter_module runs before any other header filter module, it will not see any headers set by add_headers / more_set_headers.þThis makes nginx always reply as if the content changed (412 for If-Unmodified-Since / 200 for If-Modified-Since)þFor live requests that are not segments (e.g. live DASH MPD), Last-Modified is set to the current server time.þþ#### vod_last_modified_typesþ* **syntax**: `vod_last_modified_types mime-type1 mime-type2 ...`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the MIME types for which the Last-Modified header should be set.þThe special value ""*"" matches any MIME type.þþ### Configuration directives - ad stitching (mapped mode only)þþ#### vod_dynamic_mapping_cacheþ* **syntax**: `vod_dynamic_mapping_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the cache that stores the mapping of dynamic clips.þþ#### vod_dynamic_clip_map_uriþ* **syntax**: `vod_dynamic_clip_map_uri uri`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the uri that should be used to map dynamic clips. þThe parameter value can contain variables, specifically, `$vod_clip_id` contains the id of the clip that should be mapped.þThe expected response from this uri is a JSON containing a concat clip object.þþ#### vod_source_clip_map_uriþ* **syntax**: `vod_source_clip_map_uri uri`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the uri that should be used to map source clips defined using the clipIds property of concat. þThe parameter value can contain variables, specifically, `$vod_clip_id` contains the id of the clip that should be mapped.þThe expected response from this uri is a JSON containing a source clip object.þþ#### vod_redirect_segments_urlþ* **syntax**: `vod_redirect_segments_url url`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets a url to which requests for segments should be redirected.þThe parameter value can contain variables, specifically, `$vod_dynamic_mapping` contains a serialized representation of the mapping of dynamic clips.þþ#### vod_apply_dynamic_mappingþ* **syntax**: `vod_apply_dynamic_mapping mapping`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþMaps dynamic clips to concat clips using the given expression, previously generated by `$vod_dynamic_mapping`.þThe parameter value can contain variables.þþ#### vod_notification_uriþ* **syntax**: `vod_notification_uri uri`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the uri that should be used to issue notifications. þThe parameter value can contain variables, specifically, `$vod_notification_id` contains the id of the notification that is being fired.þThe response from this uri is ignored.þþ### Configuration directives - DRM / encryptionþþ#### vod_secret_keyþ* **syntax**: `vod_secret_key string`þ* **default**: `empty`þ* **context**: `http`, `server`, `location`þþSets the seed that is used to generate the TS encryption key and DASH/MSS encryption IVs.þThe parameter value can contain variables, and will usually have the structure ""secret-$vod_filepath"".þSee the list of nginx variables added by this module below.þþ#### vod_encryption_iv_seedþ* **syntax**: `vod_encryption_iv_seed string`þ* **default**: `empty`þ* **context**: `http`, `server`, `location`þþSets the seed that is used to generate the encryption IV, currently applies only to HLS/fMP4 with AES-128 encryption.þThe parameter value can contain variables.þþ#### vod_drm_enabledþ* **syntax**: `vod_drm_enabled on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, the module encrypts the media segments according to the response it gets from the drm upstream.þCurrently supported only for dash and mss (play ready).þþ#### vod_drm_single_keyþ* **syntax**: `vod_drm_single_key on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, the module requests the drm info only for the first sequence and applies it to all sequences.þWhen disabled, the drm info is requested for each sequence separately.þIn addition, in DASH, enabling this setting makes the module place the ContentProtection tag under AdaptationSet,þotherwise, it is placed under Representation.þþ#### vod_drm_clear_lead_segment_countþ* **syntax**: `vod_drm_clear_lead_segment_count count`þ* **default**: `1`þ* **context**: `http`, `server`, `location`þþSets the number of clear (unencrypted) segments in the beginning of the stream. A clear lead enables the player to start playing without having to wait for the license response.þþ#### vod_drm_max_info_lengthþ* **syntax**: `vod_drm_max_info_length length`þ* **default**: `4K`þ* **context**: `http`, `server`, `location`þþSets the maximum length of a drm info returned from upstream.þþ#### vod_drm_upstream_locationþ* **syntax**: `vod_drm_upstream_location location`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the nginx location that should be used for getting the DRM info for the file.þþ#### vod_drm_info_cacheþ* **syntax**: `vod_drm_info_cache zone_name zone_size [expiration]`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþConfigures the size and shared memory object name of the drm info cache.þþ#### vod_drm_request_uriþ* **syntax**: `vod_drm_request_uri uri`þ* **default**: `$vod_suburi`þ* **context**: `http`, `server`, `location`þþSets the uri of drm info requests, the parameter value can contain variables.þIn case of multi url, `$vod_suburi` will be the current sub uri (a separate drm info request is issued per sub URL)þþ#### vod_min_single_nalu_per_frame_segmentþ* **syntax**: `vod_min_single_nalu_per_frame_segment index`þ* **default**: `0`þ* **context**: `http`, `server`, `location`þþSets the minimum segment index (1-based) that should be assumed to have a single h264 nalu per frame.þIf the value is 0, no assumption is being made on the number of nal units per frame.þThis setting only affects DASH and MSS configurations that have DRM enabled.þþWhen transcoding videos using libx264, by default, all frames have a single nal unit, except the first frameþthat contains an additional nalu with the libx264 copyright information. Setting this parameter to a valueþgreater than 0 can provide a significant performance improvement, since the layout of the segment can beþcalculated in advance, allowing the module to:þ* Output segment buffers as they are generated (it doesn't have to wait for the whole segment to complete)þ* Avoid frame processing for requests that do not need the segment data (e.g. HEAD, range 0-0, etc.)þþ### Configuration directives - DASHþþ#### vod_dash_absolute_manifest_urlsþ* **syntax**: `vod_dash_absolute_manifest_urls on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled the server returns absolute URLs in MPD requestsþþ#### vod_dash_manifest_file_name_prefixþ* **syntax**: `vod_dash_manifest_file_name_prefix name`þ* **default**: `manifest`þ* **context**: `http`, `server`, `location`þþThe name of the MPD file (an mpd extension is implied).þþ#### vod_dash_profilesþ* **syntax**: `vod_dash_profiles profiles`þ* **default**: `urn:mpeg:dash:profile:isoff-main:2011`þ* **context**: `http`, `server`, `location`þþSets the profiles that are returned in the MPD tag in manifest responses.þþ#### vod_dash_init_file_name_prefixþ* **syntax**: `vod_dash_init_file_name_prefix name`þ* **default**: `init`þ* **context**: `http`, `server`, `location`þþThe name of the MP4 initialization file (an mp4 extension is implied).þþ#### vod_dash_fragment_file_name_prefixþ* **syntax**: `vod_dash_fragment_file_name_prefix name`þ* **default**: `frag`þ* **context**: `http`, `server`, `location`þþThe name of the fragment files (an m4s extension is implied).þþ#### vod_dash_manifest_formatþ* **syntax**: `vod_dash_manifest_format format`þ* **default**: `segmenttimeline`þ* **context**: `http`, `server`, `location`þþSets the MPD format, available options are:þ* `segmentlist` - uses SegmentList and SegmentURL tags, in this format the URL of each fragment is explicitly set in the MPDþ* `segmenttemplate` - uses SegmentTemplate, reporting a single duration for all fragmentsþ* `segmenttimeline` - uses SegmentTemplate and SegmentTimeline to explicitly set the duration of the fragmentsþþ#### vod_dash_subtitle_formatþ* **syntax**: `vod_dash_subtitle_format format`þ* **default**: `webvtt`þ* **context**: `http`, `server`, `location`þþSets the format of the subtitles returned in the MPD, available options are:þ* `webvtt` - WebVTTþ* `smpte-tt` - SMPTE Timed Textþþ#### vod_dash_init_mp4_psshþ* **syntax**: `vod_dash_init_mp4_pssh on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled, the DRM pssh boxes are returned in the DASH init segment and in the manifest.þWhen disabled, the pssh boxes are returned only in the manifest.þþ#### vod_dash_duplicate_bitrate_thresholdþ* **syntax**: `vod_dash_duplicate_bitrate_threshold threshold`þ* **default**: `4096`þ* **context**: `http`, `server`, `location`þþThe bitrate threshold for removing identical bitrates, streams whose bitrate differences are less thanþthis value will be considered identical.þþ#### vod_dash_use_base_url_tagþ* **syntax**: `vod_dash_use_base_url_tag on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, a BaseURL tag will be used to specify the fragments/init segment base url.þOtherwise, the media/initialization attributes under SegmentTemplate will contain absolute URLs. þþ### Configuration directives - HDSþþ#### vod_hds_absolute_manifest_urlsþ* **syntax**: `vod_hds_absolute_manifest_urls on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled the server returns the base URL in the F4M manifestþþ#### vod_hds_manifest_file_name_prefixþ* **syntax**: `vod_hds_manifest_file_name_prefix name`þ* **default**: `manifest`þ* **context**: `http`, `server`, `location`þþThe name of the HDS manifest file (an f4m extension is implied).þþ#### vod_hds_fragment_file_name_prefixþ* **syntax**: `vod_hds_fragment_file_name_prefix name`þ* **default**: `frag`þ* **context**: `http`, `server`, `location`þþThe prefix of fragment file names, the actual file name is `frag-f<file-index>-v<video-track-index>-a<audio-track-index>-Seg1-Frag<index>`.þþ#### vod_hds_generate_moof_atomþ* **syntax**: `vod_hds_generate_moof_atom on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled the module generates a moof atom in the HDS fragments, when disabled only an mdat atom is generated.þTurning this parameter off reduces the packaging overhead, however the default is on since Adobe tools are generating this atom.þþ### Configuration directives - HLSþþ#### vod_hls_encryption_methodþ* **syntax**: `vod_hls_encryption_method method`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the encryption method of HLS segments, allowed values are: none (default), aes-128, sample-aes, sample-aes-cenc.þþ#### vod_hls_force_unmuxed_segmentsþ* **syntax**: `vod_hls_force_unmuxed_segments on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled the server returns the audio stream in separate segments than the ones used by the video stream (using EXT-X-MEDIA)þþ#### vod_hls_container_formatþ* **syntax**: `vod_hls_container_format mpegts/fmp4/auto`þ* **default**: `auto`þ* **context**: `http`, `server`, `location`þþSets the container format of the HLS segments. þThe default behavior is to use fmp4 for HEVC, and mpegts otherwise (Apple does not support HEVC over MPEG TS).þþ#### vod_hls_absolute_master_urlsþ* **syntax**: `vod_hls_absolute_master_urls on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled the server returns absolute playlist URLs in master playlist requestsþþ#### vod_hls_absolute_index_urlsþ* **syntax**: `vod_hls_absolute_index_urls on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled the server returns absolute segment URLs in media playlist requestsþþ#### vod_hls_absolute_iframe_urlsþ* **syntax**: `vod_hls_absolute_iframe_urls on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled the server returns absolute segment URLs in iframe playlist requestsþþ#### vod_hls_output_iframes_playlistþ* **syntax**: `vod_hls_output_iframes_playlist on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen disabled iframe playlists are not returned as part of master playlistsþþ#### vod_hls_master_file_name_prefixþ* **syntax**: `vod_hls_master_file_name_prefix name`þ* **default**: `master`þ* **context**: `http`, `server`, `location`þþThe name of the HLS master playlist file (an m3u8 extension is implied).þþ#### vod_hls_index_file_name_prefixþ* **syntax**: `vod_hls_index_file_name_prefix name`þ* **default**: `index`þ* **context**: `http`, `server`, `location`þþThe name of the HLS media playlist file (an m3u8 extension is implied).þþ#### vod_hls_iframes_file_name_prefixþ* **syntax**: `vod_hls_iframes_file_name_prefix name`þ* **default**: `iframes`þ* **context**: `http`, `server`, `location`þþThe name of the HLS I-frames playlist file (an m3u8 extension is implied).þþ#### vod_hls_segment_file_name_prefixþ* **syntax**: `vod_hls_segment_file_name_prefix name`þ* **default**: `seg`þ* **context**: `http`, `server`, `location`þþThe prefix of segment file names, the actual file name is `seg-<index>-v<video-track-index>-a<audio-track-index>.ts`.þþ#### vod_hls_init_file_name_prefixþ* **syntax**: `vod_hls_init_file_name_prefix name`þ* **default**: `init`þ* **context**: `http`, `server`, `location`þþThe name of the init segment file name, only relevant when using fmp4 container.þþ#### vod_hls_encryption_key_file_nameþ* **syntax**: `vod_hls_encryption_key_file_name name`þ* **default**: `encryption.key`þ* **context**: `http`, `server`, `location`þþThe name of the encryption key file name, only relevant when encryption method is not `none`.þþ#### vod_hls_encryption_key_uriþ* **syntax**: `vod_hls_encryption_key_uri uri`þ* **default**: `a url pointing to encryption.key`þ* **context**: `http`, `server`, `location`þþSets the value of the URI attribute of EXT-X-KEY, only relevant when encryption method is not `none`.þThe parameter value can contain variables.þþ#### vod_hls_encryption_key_formatþ* **syntax**: `vod_hls_encryption_key_format format`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the value of the KEYFORMAT attribute of EXT-X-KEY, only relevant when encryption method is not `none`.þþ#### vod_hls_encryption_key_format_versionsþ* **syntax**: `vod_hls_encryption_key_format_versions versions`þ* **default**: `none`þ* **context**: `http`, `server`, `location`þþSets the value of the KEYFORMATVERSIONS attribute of EXT-X-KEY, only relevant when encryption method is not `none`.þþ#### vod_hls_mpegts_interleave_framesþ* **syntax**: `vod_hls_mpegts_interleave_frames on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, the HLS muxer interleaves frames of different streams (audio / video).þWhen disabled, on every switch between audio / video the muxer flushes the MPEG TS packet.þþ#### vod_hls_mpegts_align_framesþ* **syntax**: `vod_hls_mpegts_align_frames on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled, every video / audio frame is aligned to MPEG TS packet boundary,þpadding is added as needed.þþ#### vod_hls_mpegts_output_id3_timestampsþ* **syntax**: `vod_hls_mpegts_output_id3_timestamps on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, an ID3 TEXT frame will be outputted in each TS segment, containing a JSON with the absolute segment timestamp.þThe timestamp is measured in milliseconds since the epoch (unixtime x 1000), the JSON structure is: `{""timestamp"":1459779115000}`þþ### Configuration directives - MSSþþ#### vod_mss_manifest_file_name_prefixþ* **syntax**: `vod_mss_manifest_file_name_prefix name`þ* **default**: `manifest`þ* **context**: `http`, `server`, `location`þþThe name of the manifest file (has no extension).þþ#### vod_mss_duplicate_bitrate_thresholdþ* **syntax**: `vod_mss_duplicate_bitrate_threshold threshold`þ* **default**: `4096`þ* **context**: `http`, `server`, `location`þþThe bitrate threshold for removing identical bitrates, streams whose bitrate differences are less thanþthis value will be considered identical.þþ### Configuration directives - thumbnail captureþþ#### vod_thumb_file_name_prefixþ* **syntax**: `vod_thumb_file_name_prefix name`þ* **default**: `thumb`þ* **context**: `http`, `server`, `location`þþThe name of the thumbnail file (a jpg extension is implied).þþ#### vod_thumb_accurate_positioningþ* **syntax**: `vod_thumb_accurate_positioning on/off`þ* **default**: `on`þ* **context**: `http`, `server`, `location`þþWhen enabled, the module grabs the frame that is closest to the requested offset.þWhen disabled, the module uses the keyframe that is closest to the requested offset.þSetting this parameter to off can result in faster thumbnail capture, since the module þalways decodes a single video frame per request.þþ#### vod_gop_look_behindþ* **syntax**: `vod_gop_look_behind millis`þ* **default**: `10000`þ* **context**: `http`, `server`, `location`þþSets the interval (in milliseconds) before the thumbnail offset that should be loaded.þThis setting should be set to the maximum GOP size, setting it to a lower value may result in capture failure.þNote that the metadata of all frames between `offset - vod_gop_look_behind` and `offset + vod_gop_look_ahead`þis loaded, however only the frames of the minimum GOP containing `offset` will be read and decoded.þþ#### vod_gop_look_aheadþ* **syntax**: `vod_gop_look_ahead millis`þ* **default**: `1000`þ* **context**: `http`, `server`, `location`þþSets the interval (in milliseconds) after the thumbnail offset that should be loaded.þþ### Configuration directives - volume mapþþ#### vod_volume_map_file_name_prefixþ* **syntax**: `vod_volume_map_file_name_prefix name`þ* **default**: `volume_map`þ* **context**: `http`, `server`, `location`þþThe name of the volume map file (a csv extension is implied).þþ#### vod_volume_map_intervalþ* **syntax**: `vod_volume_map_interval millis`þ* **default**: `1000`þ* **context**: `http`, `server`, `location`þþSets the interval/resolution (in milliseconds) of the volume map.þþ### Configuration directives - miscþþ#### vod_ignore_edit_listþ* **syntax**: `vod_ignore_edit_list on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, the module ignores any edit lists (elst) in the MP4 file.þþ#### vod_parse_hdlr_nameþ* **syntax**: `vod_parse_hdlr_name on/off`þ* **default**: `off`þ* **context**: `http`, `server`, `location`þþWhen enabled, the module parses the name field of the hdlr MP4 atom, and uses it as the stream label.þþ### Nginx variablesþþThe module adds the following nginx variables:þ* `$vod_suburi` - the current sub uri. For example, if the url is:þ  `http://<domain>/<location>/<prefix>,<middle1>,<middle2>,<middle3>,<postfix>.urlset/<filename>`þ  `$vod_suburi` will have the value `http://<domain>/<location>/<prefix><middle1><postfix>/<filename>` þ  when processing the first uri.þ* `$vod_filepath` - in local / mapped modes, the file path of current sub uri. In remote mode, has the same value as `$vod_suburi`.þ* `$vod_set_id` - contains the id of the set.þ* `$vod_sequence_id` - contains the id of the current sequence, if no id was specified in the mapping json this variable will be the same as `$vod_suburi`.þ* `$vod_clip_id` - the id of the current clip, this variable has a value during these phases:þ  1. Mapping of dynamic clips to concat clipsþ  2. Mapping of source clip to pathsþ* `$vod_notification_id` - the id of the current notification, the value is non-empty only when referenced by `vod_notification_uri`þ* `$vod_dynamic_mapping` - a serialized representation of the mapping of dynamic clips to concat clips.þ* `$vod_request_params` - a serialized representation of the request params, e.g. 12-f2-v1-a1. The variable contains:þ  1. The segment index (for a segment request)þ  2. The sequence indexþ  3. A selection of audio/video tracksþ* `$vod_status` - the internal error code of the module, provides a more fine grained classification of errors than http status.þ the following values are defined:þ `BAD_REQUEST` - the request is invalid, for example, `clipFrom` is larger than the video durationþ `NO_STREAMS` - an invalid segment index was requestedþ `EMPTY_MAPPING` - the mapping response is emptyþ `BAD_MAPPING` - the mapping json is invalid, for example, the `sequences` element is missingþ `BAD_DATA` - the video file is corruptþ `EXPIRED` - the current server time is larger than `expirationTime`þ `ALLOC_FAILED` - the module failed to allocate memoryþ `UNEXPECTED` - a scenario that is not supposed to happen, most likely a bug in the moduleþ* `$vod_segment_duration` - for segment requests, contains the duration of the segment in millisecondsþ* `$vod_frames_bytes_read` - for segment requests, total number of bytes read while processing media framesþþNote: Configuration directives that can accept variables are explicitly marked as such.þþ### Sample configurationsþþ#### Local configurationþþ http {þ  upstream fallback {þ   server fallback.kaltura.com:80;þ  }þþ  server {þ   # vod settingsþ   vod_mode local;þ   vod_fallback_upstream_location /fallback;þ   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';þ   vod_last_modified_types *;þþ   # vod cachesþ   vod_metadata_cache metadata_cache 512m;þ   vod_response_cache response_cache 128m;þ   þ   # gzip manifestsþ   gzip on;þ   gzip_types application/vnd.apple.mpegurl;þþ   # file handle caching / aioþ   open_file_cache          max=1000 inactive=5m;þ   open_file_cache_valid    2m;þ   open_file_cache_min_uses 1;þ   open_file_cache_errors   on;þ   aio on;þ   þ   location ^~ /fallback/ {þ    internal;þ    proxy_pass http://fallback/;þ    proxy_set_header Host $http_host;þ   }þþ   location /content/ {þ    root /web/;þ    vod hls;þ    þ    add_header Access-Control-Allow-Headers '*';þ    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';þ    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';þ    add_header Access-Control-Allow-Origin '*';þ    expires 100d;þ   }þ  }þ }þþ#### Mapped configurationþþ http {þ  upstream kalapi {þ   server www.kaltura.com:80;þ  }þþ  upstream fallback {þ   server fallback.kaltura.com:80;þ  }þþ  server {þ   # vod settingsþ   vod_mode mapped;þ   vod_upstream_location /kalapi;þ   vod_upstream_extra_args ""pathOnly=1"";þ   vod_fallback_upstream_location /fallback;þ   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';þ   vod_last_modified_types *;þþ   # vod cachesþ   vod_metadata_cache metadata_cache 512m;þ   vod_response_cache response_cache 128m;þ   vod_mapping_cache mapping_cache 5m;þ   þ   # gzip manifestsþ   gzip on;þ   gzip_types application/vnd.apple.mpegurl;þþ   # file handle caching / aioþ   open_file_cache          max=1000 inactive=5m;þ   open_file_cache_valid    2m;þ   open_file_cache_min_uses 1;þ   open_file_cache_errors   on;þ   aio on;þ   þ   location ^~ /fallback/ {þ    internal;þ    proxy_pass http://fallback/;þ    proxy_set_header Host $http_host;þ   }þþ   location ^~ /kalapi/ {þ    internal;þ    proxy_pass http://kalapi/;þ    proxy_set_header Host $http_host;þ   }þþ   location ~ ^/p/\d+/(sp/\d+/)?serveFlavor/ {þ    # encrypted hlsþ    vod hls;þ    vod_secret_key ""mukkaukk$vod_filepath"";þ    vod_hls_encryption_method aes-128;þ    þ    add_header Access-Control-Allow-Headers '*';þ    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';þ    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';þ    add_header Access-Control-Allow-Origin '*';þ    expires 100d;þ   }þ  }þ }þþ#### Mapped + Remote configurationþþ http {þ  upstream jsonupstream {þ   server jsonserver:80;þ  }þþ  server {þ   # vod settingsþ   vod_mode mapped;þ   vod_upstream_location /json;þ   vod_remote_upstream_location /proxy;þ   vod_upstream_extra_args ""pathOnly=1"";þ   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';þ   vod_last_modified_types *;þþ   # vod cachesþ   vod_metadata_cache metadata_cache 512m;þ   vod_response_cache response_cache 128m;þ   vod_mapping_cache mapping_cache 5m;þþ   # gzip manifestsþ   gzip on;þ   gzip_types application/vnd.apple.mpegurl;þþ   # file handle caching / aioþ   open_file_cache   max=1000 inactive=5m;þ   open_file_cache_valid    2m;þ   open_file_cache_min_uses 1;þ   open_file_cache_errors   on;þ   aio on;þþ   location ^~ /json/hls/ {þ    internal;þ    proxy_pass http://jsonupstream/;þ    proxy_set_header Host $http_host;þ   }þþ   location ~ /proxy/([^/]+)/(.*) {þ    internal;þ    proxy_pass $1://$2;þ    resolver 8.8.8.8;þ   }þþ   location ~ ^/hls/ {þ    vod hls;þþ    add_header Access-Control-Allow-Headers '*';þ    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';þ    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';þ    add_header Access-Control-Allow-Origin '*';þ    expires 100d;þ   }þ  }þ }þþSet it up so that http://jsonserver:80/test.json returns the following JSON:þþ {þ  ""sequences"": [{þ   ""clips"": [{þ    ""type"": ""source"",þ    ""path"": ""/http/commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4""þ   }]þ  }]þ }þþAnd use this stream URL - http://nginx-vod-server/hls/test.json/master.m3u8þþ#### Remote configurationþþ http {þ  upstream kalapi {þ   server www.kaltura.com:80;þ  }þþ  server {þ   # vod settingsþ   vod_mode remote;þ   vod_upstream_location /kalapi;þ   vod_last_modified 'Sun, 19 Nov 2000 08:52:00 GMT';þ   vod_last_modified_types *;þþ   # vod cachesþ   vod_metadata_cache metadata_cache 512m;þ   vod_response_cache response_cache 128m;þ   þ   # gzip manifestsþ   gzip on;þ   gzip_types application/vnd.apple.mpegurl;þ   þ   location ^~ /kalapi/ {þ    internal;þ    proxy_pass http://kalapi/;þ    proxy_set_header Host $http_host;þ   }þþ   location ~ ^/p/\d+/(sp/\d+/)?serveFlavor/ {þ    vod hls;þ    þ    add_header Access-Control-Allow-Headers '*';þ    add_header Access-Control-Expose-Headers 'Server,range,Content-Length,Content-Range';þ    add_header Access-Control-Allow-Methods 'GET, HEAD, OPTIONS';þ    add_header Access-Control-Allow-Origin '*';þ    expires 100d;þ   }þ  }þ }þþ### Copyright & LicenseþþAll code in this project is released under the [AGPLv3 license](http://www.gnu.org/licenses/agpl-3.0.html) unless a different license for a particular library is specified in the applicable library path. þþCopyright © Kaltura Inc. All rights reserved."
strongswan/strongswan,93414,957,100,444,Organization,False,17484,84,218,75,False,strongSwan - IPsec-based VPN,https://www.strongswan.org,6,0,0,,,,,13,160,1,11,5339,5,172,11273,3334,0,0,8,1,,,"# strongSwan Configuration #þþ## Overview ##þþstrongSwan is an OpenSource IPsec-based VPN solution.þþThis document is just a short introduction of the strongSwan **swanctl** commandþwhich uses the modern [**vici**](src/libcharon/plugins/vici/README.md) *VersatileþIKE Configuration Interface*. The deprecated **ipsec** command using the legacyþ**stroke** configuration interface is described [**here**](README_LEGACY.md).þFor more detailed information consult the man pages andþ[**our wiki**](https://wiki.strongswan.org).þþþ## Quickstart ##þþCertificates for users, hosts and gateways are issued by a fictitiousþstrongSwan CA. In our example scenarios the CA certificate `strongswanCert.pem`þmust be present on all VPN endpoints in order to be able to authenticate theþpeers. For your particular VPN application you can either use certificates fromþany third-party CA or generate the needed private keys and certificates yourselfþwith the strongSwan **pki** tool, the use of which will be explained in one ofþthe sections following below.þþþ### Site-to-Site Case ###þþIn this scenario two security gateways _moon_ and _sun_ will connect theþtwo subnets _moon-net_ and _sun-net_ with each other through a VPN tunnelþset up between the two gateways:þþ    10.1.0.0/16 -- | 192.168.0.1 | === | 192.168.0.2 | -- 10.2.0.0/16þ      moon-net          moon                 sun           sun-netþþConfiguration on gateway _moon_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/moonCert.pemþ    /etc/swanctl/private/moonKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            net-net {þ                remote_addrs = 192.168.0.2þþ                local {þ                    auth = pubkeyþ                    certs = moonCert.pemþ                }þ                remote {þ                    auth = pubkeyþ                    id = ""C=CH, O=strongSwan, CN=sun.strongswan.org""þ                }þ                children {þ                    net-net {þ                        local_ts  = 10.1.0.0/16þ                        remote_ts = 10.2.0.0/16þ                        start_action = trapþ                    }þ                }þ            }þ        }þþConfiguration on gateway _sun_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/sunCert.pemþ    /etc/swanctl/private/sunKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            net-net {þ                remote_addrs = 192.168.0.1þþ                local {þ                    auth = pubkeyþ                    certs = sunCert.pemþ                }þ                remote {þ                    auth = pubkeyþ                    id = ""C=CH, O=strongSwan, CN=moon.strongswan.org""þ                }þ                children {þ                    net-net {þ                        local_ts  = 10.2.0.0/16þ                        remote_ts = 10.1.0.0/16þ                        start_action = trapþ                    }þ                }þ            }þ        }þþThe local and remote identities used in this scenario are theþ*subjectDistinguishedNames* contained in the end entity certificates.þThe certificates and private keys are loaded into the **charon** daemon withþthe commandþþ    swanctl --load-credsþþwhereasþþ    swanctl --load-connsþþloads the connections defined in `swanctl.conf`. With `start_action = trap` theþIPsec connection is automatically set up with the first plaintext payload IPþpacket wanting to go through the tunnel.þþ### Host-to-Host Case ###þþThis is a setup between two single hosts which don't have a subnet behindþthem.  Although IPsec transport mode would be sufficient for host-to-hostþconnections we will use the default IPsec tunnel mode.þþ    | 192.168.0.1 | === | 192.168.0.2 |þ         moon                sunþþConfiguration on host _moon_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/moonCert.pemþ    /etc/swanctl/private/moonKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            host-host {þ                remote_addrs = 192.168.0.2þþ                local {þ                    auth=pubkeyþ                    certs = moonCert.pemþ                }þ                remote {þ                    auth = pubkeyþ                    id = ""C=CH, O=strongSwan, CN=sun.strongswan.org""þ                }þ                children {þ                    net-net {þ                        start_action = trapþ                    }þ                }þ            }þ        }þþConfiguration on host _sun_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/sunCert.pemþ    /etc/swanctl/private/sunKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            host-host {þ                remote_addrs = 192.168.0.1þþ                local {þ                    auth = pubkeyþ                    certs = sunCert.pemþ                }þ                remote {þ                    auth = pubkeyþ                    id = ""C=CH, O=strongSwan, CN=moon.strongswan.org""þ                }þ                children {þ                    host-host {þ                        start_action = trapþ                    }þ                }þ            }þ        }þþþ### Roadwarrior Case ###þþThis is a very common case where a strongSwan gateway serves an arbitraryþnumber of remote VPN clients usually having dynamic IP addresses.þþ    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x |þ      moon-net          moon              carolþþConfiguration on gateway _moon_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/moonCert.pemþ    /etc/swanctl/private/moonKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            rw {þ                local {þ                    auth = pubkeyþ                    certs = moonCert.pemþ                    id = moon.strongswan.orgþ                }þ                remote {þ                    auth = pubkeyþ                }þ                children {þ                    net-net {þ                        local_ts  = 10.1.0.0/16þ                    }þ                }þ            }þ        }þþConfiguration on roadwarrior _carol_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/carolCert.pemþ    /etc/swanctl/private/carolKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            home {þ                remote_addrs = moon.strongswan.orgþþ                local {þ                    auth = pubkeyþ                    certs = carolCert.pemþ                    id = carol@strongswan.orgþ                }þ                remote {þ                    auth = pubkeyþ                    id = moon.strongswan.orgþ                }þ                children {þ                    home {þ                        local_ts  = 10.1.0.0/16þ                        start_action = startþ                    }þ                }þ            }þ        }þþFor `remote_addrs` the hostname `moon.strongswan.org` was chosen which will beþresolved by DNS at runtime into the corresponding IP destination address.þIn this scenario the identity of the roadwarrior `carol` is the email addressþ`carol@strongswan.org` which must be included as a *subjectAlternativeName* inþthe roadwarrior certificate `carolCert.pem`.þþþ### Roadwarrior Case with Virtual IP ###þþRoadwarriors usually have dynamic IP addresses assigned by the ISP they areþcurrently attached to.  In order to simplify the routing from _moon-net_ backþto the remote access client _carol_ it would be desirable if the roadwarrior hadþan inner IP address chosen from a pre-defined pool.þþ    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x | -- 10.3.0.1þ      moon-net          moon              carol       virtual IPþþIn our example the virtual IP address is chosen from the address poolþ`10.3.0.0/16` which can be configured by adding the sectionþþ    pools {þ        rw_pool {þ            addrs = 10.3.0.0/16þ        }þ    }þþto the gateway's `swanctl.conf` from where they are loaded into the **charon**þdaemon using the commandþþ    swanctl --load-poolsþþTo request an IP address from this pool a roadwarrior can use IKEv1 mode configþor IKEv2 configuration payloads. The configuration for both is the sameþþ    vips = 0.0.0.0þþConfiguration on gateway _moon_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/moonCert.pemþ    /etc/swanctl/private/moonKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            rw {þ                pools = rw_poolþþ                local {þ                    auth = pubkeyþ                    certs = moonCert.pemþ                    id = moon.strongswan.orgþ                }þ                remote {þ                    auth = pubkeyþ                }þ                children {þ                    net-net {þ                        local_ts  = 10.1.0.0/16þ                    }þ                }þ            }þ        }þþ        pools {þ            rw_pool {þ                addrs = 10.30.0.0/16þ            }þ        }þþConfiguration on roadwarrior _carol_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/carolCert.pemþ    /etc/swanctl/private/carolKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            home {þ                remote_addrs = moon.strongswan.orgþ                vips = 0.0.0.0þþ                local {þ                    auth = pubkeyþ                    certs = carolCert.pemþ                    id = carol@strongswan.orgþ                }þ                remote {þ                    auth = pubkeyþ                    id = moon.strongswan.orgþ                }þ                children {þ                    home {þ                        local_ts  = 10.1.0.0/16þ                        start_action = startþ                    }þ                }þ            }þ        }þþþ### Roadwarrior Case with EAP Authentication ###þþThis is a very common case where a strongSwan gateway serves an arbitraryþnumber of remote VPN clients which authenticate themselves via a passwordþbased *Extended Authentication Protocol* as e.g. *EAP-MD5* or *EAP-MSCHAPv2*.þþ    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x |þ      moon-net          moon              carolþþConfiguration on gateway _moon_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/moonCert.pemþ    /etc/swanctl/private/moonKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            rw {þ                local {þ                    auth = pubkeyþ                    certs = moonCert.pemþ                    id = moon.strongswan.orgþ                }þ                remote {þ                    auth = eap-md5þ                }þ                children {þ                    net-net {þ                        local_ts  = 10.1.0.0/16þ                    }þ                }þ                send_certreq = noþ            }þ        }þþThe  `swanctl.conf` file additionally contains a `secrets` section defining allþclient credentialsþþ        secrets {þ            eap-carol {þ                id = carol@strongswan.orgþ                secret = Ar3etTnpþ            }þ            eap-dave {þ                id = dave@strongswan.orgþ                secret = W7R0g3doþ            }þ        }þþConfiguration on roadwarrior _carol_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            home {þ                remote_addrs = moon.strongswan.orgþþ                local {þ                    auth = eapþ                    id = carol@strongswan.orgþ                }þ                remote {þ                    auth = pubkeyþ                    id = moon.strongswan.orgþ                }þ                children {þ                    home {þ                        local_ts  = 10.1.0.0/16þ                        start_action = startþ                    }þ                }þ            }þ        }þþ        secrets {þ            eap-carol {þ                id = carol@strongswan.orgþ                secret = Ar3etTnpþ            }þ        }þþþ### Roadwarrior Case with EAP Identity ###þþOften a client EAP identity is exchanged via EAP which differs from theþexternal IKEv2 identity. In this example the IKEv2 identity defaults toþthe IPv4 address of the client.þþ    10.1.0.0/16 -- | 192.168.0.1 | === | x.x.x.x |þ      moon-net          moon              carolþþConfiguration on gateway _moon_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþ    /etc/swanctl/x509/moonCert.pemþ    /etc/swanctl/private/moonKey.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            rw {þ                local {þ                    auth = pubkeyþ                    certs = moonCert.pemþ                    id = moon.strongswan.orgþ                }þ                remote {þ                    auth = eap-md5þ                    eap_id = %anyþ                }þ                children {þ                    net-net {þ                        local_ts  = 10.1.0.0/16þ                    }þ                }þ                send_certreq = noþ            }þ        }þþ        secrets {þ            eap-carol {þ                id = carolþ                secret = Ar3etTnpþ            }þ            eap-dave {þ                id = daveþ                secret = W7R0g3doþ            }þ        }þþConfiguration on roadwarrior _carol_:þþ    /etc/swanctl/x509ca/strongswanCert.pemþþ    /etc/swanctl/swanctl.conf:þþ        connections {þ            home {þ                remote_addrs = moon.strongswan.orgþþ                local {þ                    auth = eapþ                    eap_id = carolþ                }þ                remote {þ                    auth = pubkeyþ                    id = moon.strongswan.orgþ                }þ                children {þ                    home {þ                        local_ts  = 10.1.0.0/16þ                        start_action = startþ                    }þ                }þ            }þ        }þþ        secrets {þ            eap-carol {þ                id = carolþ                secret = Ar3etTnpþ            }þ        }þþþ## Generating Certificates and CRLs ##þþThis section is not a full-blown tutorial on how to use the strongSwan **pki**þtool. It just lists a few points that are relevant if you want to generate yourþown certificates and CRLs for use with strongSwan.þþþ### Generating a CA Certificate ###þþThe pki statementþþ    pki --gen --type ed25519 --outform pem > strongswanKey.pemþþgenerates an elliptic Edwards-Curve key with a cryptographic strength of 128þbits. The corresponding public key is packed into a self-signed CA certificateþwith a lifetime of 10 years (3652 days)þþ    pki --self --ca --lifetime 3652 --in strongswanKey.pem \þ               --dn ""C=CH, O=strongSwan, CN=strongSwan Root CA"" \þ               --outform pem > strongswanCert.pemþþwhich can be listed with the commandþþ    pki --print --in strongswanCert.pemþþ    subject:  ""C=CH, O=strongSwan, CN=strongSwan Root CA""þ    issuer:   ""C=CH, O=strongSwan, CN=strongSwan Root CA""þ    validity:  not before May 18 08:32:06 2017, okþ               not after  May 18 08:32:06 2027, ok (expires in 3651 days)þ    serial:    57:e0:6b:3a:9a:eb:c6:e0þ    flags:     CA CRLSign self-signedþ    subjkeyId: 2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5dþ    pubkey:    ED25519 256 bitsþ    keyid:     a7:e1:6a:3f:e7:6f:08:9d:89:ec:23:92:a9:a1:14:3c:78:a8:7a:f7þ    subjkey:   2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5dþþIf you prefer the CA private key and X.509 certificate to be in binary DER formatþthen just omit the `--outform pem` option. The directory `/etc/swanctl/x509ca`þcontains all required CA certificates either in binary DER or in Base64 PEMþformat. Irrespective of the file suffix the correct format will be determinedþby strongSwan automagically.þþþ### Generating a Host or User End Entity Certificate ###þþAgain we are using the commandþþ    pki --gen --type ed25519 --outform pem > moonKey.pemþþto generate an Ed25519 private key for the host `moon`. Alternatively you couldþtypeþþ    pki --gen --type rsa --size 3072 > moonKey.derþþto generate a traditional 3072 bit RSA key and store it in binary DER format.þAs an alternative a **TPM 2.0** *Trusted Platform Module* available on everyþrecent Intel platform could be used as a virtual smartcard to securely store anþRSA or ECDSA private key. For details, refer to the TPM 2.0þ[HOWTO](https://wiki.strongswan.org/projects/strongswan/wiki/TpmPlugin).þþIn a next step the commandþþ    pki --req --type priv --in moonKey.pem \þ              --dn ""C=CH, O=strongswan, CN=moon.strongswan.org \þ              --san moon.strongswan.org --outform pem > moonReq.pemþþcreates a PKCS#10 certificate request that has to be signed by the CA.þThrough the [multiple] use of the `--san` parameter any number of desiredþ*subjectAlternativeNames* can be added to the request. These can be of theþformþþ    --san sun.strongswan.org     # fully qualified host nameþ    --san carol@strongswan.org   # RFC822 user email addressþ    --san 192.168.0.1            # IPv4 addressþ    --san fec0::1                # IPv6 addressþþBased on the certificate request the CA issues a signed end entity certificateþwith the following commandþþ    pki --issue --cacert strongswanCert.pem --cakey strongswanKey.pem \þ                --type pkcs10 --in moonReq.pem --serial 01 --lifetime 1826 \þ                --outform pem > moonCert.pemþþIf the `--serial` parameter with a hexadecimal argument is omitted then a randomþserial number is generated. Some third party VPN clients require that a VPNþgateway certificate contains the *TLS Server Authentication* Extended Key Usageþ(EKU) flag which can be included with the following optionþþ    --flag serverAuthþþIf you want to use the dynamic CRL fetching feature described in one of theþfollowing sections then you may include one or several *crlDistributionPoints*þin your end entity certificates using the `--crl` parameterþþ    --crl  http://crl.strongswan.org/strongswan.crlþ    --crl ""ldap://ldap.strongswan.org/cn=strongSwan Root CA, o=strongSwan,c=CH?certificateRevocationList""þþThe issued host certificate can be listed withþþ    pki --print --in moonCert.pemþþ    subject:  ""C=CH, O=strongSwan, CN=moon.strongswan.org""þ    issuer:   ""C=CH, O=strongSwan, CN=strongSwan Root CA""þ    validity:  not before May 19 10:28:19 2017, okþ               not after  May 19 10:28:19 2022, ok (expires in 1825 days)þ    serial:    01þ    altNames:  moon.strongswan.orgþ    flags:     serverAuthþ    CRL URIs:  http://crl.strongswan.org/strongswan.crlþ    authkeyId: 2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5dþ    subjkeyId: 60:9d:de:30:a6:ca:b9:8e:87:bb:33:23:61:19:18:b8:c4:7e:23:8fþ    pubkey:    ED25519 256 bitsþ    keyid:     39:1b:b3:c2:34:72:1a:01:08:40:ce:97:75:b8:be:ce:24:30:26:29þ    subjkey:   60:9d:de:30:a6:ca:b9:8e:87:bb:33:23:61:19:18:b8:c4:7e:23:8fþþUsually, a Windows, OSX, Android or iOS based VPN client needs its private key,þits host or user certificate and the CA certificate.  The most convenient wayþto load this information is to put everything into a PKCS#12 container:þþ    openssl pkcs12 -export -inkey carolKey.pem \þ                   -in carolCert.pem -name ""carol"" \þ                   -certfile strongswanCert.pem -caname ""strongSwan Root CA"" \þ                   -out carolCert.p12þþThe strongSwan **pki** tool currently is not able to create PKCS#12 containersþso that **openssl** must be used.þþþ### Generating a CRL ###þþAn empty CRL that is signed by the CA can be generated with the commandþþ    pki --signcrl --cacert strongswanCert.pem --cakey strongswanKey.pem \þ                  --lifetime 30 > strongswan.crlþþIf you omit the `--lifetime` option then the default value of 15 days is used.þCRLs can either be uploaded to a HTTP or LDAP server or put in binary DER orþBase64 PEM format into the `/etc/swanctl/x509crl` directory from where they areþloaded into the **charon** daemon with the commandþþ    swanctl --load-credsþþþ### Revoking a Certificate ###þþA specific end entity certificate is revoked with the commandþþ    pki --signcrl --cacert strongswanCert.pem --cakey strongswanKey.pem \þ                  --lifetime 30 --lastcrl strongswan.crl \þ                  --reason key-compromise --cert moonCert.pem > new.crlþþInstead of the certificate file (in our example moonCert.pem), the serial numberþof the certificate to be revoked can be indicated using the `--serial`þparameter. The `pki --signcrl --help` command documents all possible revocationþreasons but the `--reason` parameter can also be omitted. The content of the newþCRL file can be listed with the commandþþ    pki --print --type crl --in new.crlþþ    issuer:   ""C=CH, O=strongSwan, CN=strongSwan Root CA""þ    update:    this on May 19 11:13:01 2017, okþ               next on Jun 18 11:13:01 2017, ok (expires in 29 days)þ    serial:    02þ    authKeyId: 2b:95:14:5b:c3:22:87:de:d1:42:91:88:63:b3:d5:c1:92:7a:0f:5dþ    1 revoked certificate:þ      01: May 19 11:13:01 2017, key compromiseþþþ### Local Caching of CRLs ###þþThe `strongswan.conf` optionþþ    charon {þ        cache_crls = yesþ    }þþactivates the local caching of CRLs that were dynamically fetched from anþHTTP or LDAP server.  Cached copies are stored in `/etc/swanctl/x509crl` using aþunique filename formed from the issuer's *subjectKeyIdentifier* and theþsuffix `.crl`.þþWith the cached copy the CRL is immediately available after startup.  When theþlocal copy has become stale, an updated CRL is automatically fetched from one ofþthe defined CRL distribution points during the next IKEv2 authentication."
openbsd/src,1111198,1512,133,431,Organization,False,206040,1,0,80,False,Public git conversion mirror of OpenBSD's official CVS src repository. Pull requests not accepted - send diffs to the tech@ mailing list.,https://www.openbsd.org,1,7,0,,,,,0,18,0,4,,0,0,0,0,0,0,4,2,,,
GNOME/glib,97385,866,75,364,Organization,False,21824,303,478,609,False,Low level core library,https://gitlab.gnome.org/GNOME/glib,0,6,0,,,,,0,32,0,1,8041,32,255,19426,14236,0,0,393,2,,,"# GLibþþGLib is the low-level core library that forms the basis for projects suchþas GTK and GNOME. It provides data structure handling for C, portabilityþwrappers, and interfaces for such runtime functionality as an event loop,þthreads, dynamic loading, and an object system.þþThe official download locations are:þ  <https://download.gnome.org/sources/glib>þþThe official web site is:þ  <https://www.gtk.org/>þþ## InstallationþþSee the file '[INSTALL.in](INSTALL.in)'þþ## How to report bugsþþBugs should be reported to the GNOME issue tracking system.þ(<https://gitlab.gnome.org/GNOME/glib/issues/new>). You will needþto create an account for yourself.þþIn the bug report please include:þþ* Information about your system. For instance:þ  * What operating system and versionþ  * For Linux, what version of the C libraryþ  * And anything else you think is relevant.þ* How to reproduce the bug.þ  * If you can reproduce it with one of the test programs that are builtþ  in the tests/ subdirectory, that will be most convenient.  Otherwise,þ  please include a short test program that exhibits the behavior.þ  As a last resort, you can also provide a pointer to a larger pieceþ  of software that can be downloaded.þ* If the bug was a crash, the exact text that was printed outþ  when the crash occured.þ* Further information such as stack traces may be useful, butþ  is not necessary.þþ## PatchesþþPatches should also be submitted as merge requests to gitlab.gnome.org. If theþpatch fixes an existing issue, please refer to the issue in your commit messageþwith the following notation (for issue 123):þCloses: #123þþOtherwise, create a new merge request that introduces the change, filing aþseparate issue is not required.þþ## Notesþþ### Notes about GLib 2.48þþ* The system copy of PCRE is now used by default to implement GRegex.þ  Configure with --with-pcre=internal if a system PCRE versionþ  is unavailable or undesired.þþ### Notes about GLib 2.46þþ* GTask no longer imposes a fixed limit on the number of tasks thatþ  can be run_in_thread() simultaneously, since doing this inevitablyþ  results in deadlocks in some use cases. Instead, it now has a baseþ  number of threads that can be used ""for free"", but will graduallyþ  add more threads to the pool if too much time passes without anyþ  tasks completing.þþ  The exact behavior may continue to change in the future, and it'sþ  possible that some future version of GLib may not do anyþ  rate-limiting at all. As a result, you should no longer assume thatþ  GTask will rate-limit tasks itself (or, by extension, that calls toþ  certain async gio methods will automatically be rate-limited forþ  you). If you have a very large number of tasks to run, and don'tþ  want them to all run at once, you should rate-limit them yourself.þþ### Notes about GLib 2.40þþ* g_test_run() no longer runs tests in exactly the order they areþ  registered; instead, it groups them according to test suites (ie,þ  path components) like the documentation always claimed it did. Inþ  some cases, this can result in a sub-optimal ordering of tests,þ  relative to the old behavior. The fix is to change the test paths toþ  properly group together the tests that should run together. (eg, ifþ  you want to run test_foo_simple(), test_bar_simple(), andþ  test_foo_using_bar() in that order, they should have test paths likeþ  ""/simple/foo"", ""/simple/bar"", ""/complex/foo-using-bar"", notþ  ""/foo/simple"", ""/bar/simple"", ""/foo/using-bar"" (which would resultþ  in test_foo_using_bar() running before test_bar_simple()).þþ  (The behavior actually changed in GLib 2.36, but it was notþ  documented at the time, since we didn't realize it mattered.)þþ### Notes about GLib 2.36þþ* It is no longer necessary to call g_type_init().  If you areþ  loading GLib as a dynamic module, you should be careful to avoidþ  unloading it, then subsequently loading it again.  This neverþ  really worked before, but it is now explicitly undefined behavior.þ  Note that if g_type_init() was the only explicit use of a GObjectþ  API and you are using linker flags such as --no-add-needed, thenþ  you may have to artificially use some GObject call to keep theþ  linker from optimizing away -lgobject. We recommend to useþ  g_type_ensure (G_TYPE_OBJECT) for this purpose.þþ* This release contains an incompatible change to the g_get_home_dir()þ  function.  Previously, this function would effectively ignore the HOMEþ  environment variable and always return the value from /etc/password.þ  As of this version, the HOME variable is used if it is set and theþ  value from /etc/passwd is only used as a fallback.þþ* The 'flowinfo' and 'scope_id' fields of GInetSocketAddressþ  (introduced in GLib 2.32) have been fixed to be in host byte orderþ  rather than network byte order. This is an incompatible change, butþ  the previous behavior was clearly broken, so it seems unlikely thatþ  anyone was using it.þþ### Notes about GLib 2.34þþ* GIO now looks for thumbnails in XDG_CACHE_HOME, following aþ  recent alignment of the thumbnail spec with the basedir spec.þþ* The default values for GThreadPools max_unused_threads andþ  max_idle_time settings have been changed to 2 and 15*1000,þ  respectively.þþ### Notes about GLib 2.32þþ* It is no longer necessary to use g_thread_init() or to link againstþ  libgthread.  libglib is now always thread-enabled. Custom threadþ  system implementations are no longer supported (including errorcheckþ  mutexes).þþ* The thread and synchronisation APIs have been updated.þ  GMutex and GCond can be statically allocated without explicitþ  initialisation, as can new types GRWLock and GRecMutex.  Theþ  GStatic_______ variants of these types have been deprecated.  GPrivateþ  can also be statically allocated and has a nicer API (deprecatingþ  GStaticPrivate).  Finally, g_thread_create() has been replaced with aþ  substantially simplified g_thread_new().þþ* The g_once_init_enter()/_leave() functions have been replaced withþ  macros that allow for a pointer to any gsize-sized object, not just aþ  gsize*.  The assertions to ensure that a pointer to a correctly-sizedþ  object is being used will not work with generic pointers (ie: (void*)þ  and (gpointer) casts) which would have worked with the old version.þþ* It is now mandatory to include glib.h instead of individual headers.þþ* The -uninstalled variants of the pkg-config files have been dropped.þþ* For a long time, gobject-2.0.pc mistakenly declared a publicþ  dependency on gthread-2.0.pc (when the dependency should have beenþ  private).  This means that programs got away with callingþ  g_thread_init() without explicitly listing gthread-2.0.pc among theirþ  dependencies.þþ  gthread has now been removed as a gobject dependency, which will causeþ  such programs to break.þþ  The fix for this problem is either to declare an explicit dependencyþ  on gthread-2.0.pc (if you care about compatibility with older GLibþ  versions) or to stop calling g_thread_init().þþ* g_debug() output is no longer enabled by default.  It can be enabledþ  on a per-domain basis with the G_MESSAGES_DEBUG environment variableþ  likeþ    G_MESSAGES_DEBUG=domain1,domain2þ  orþ    G_MESSAGES_DEBUG=allþþ### Notes about GLib 2.30þþ* GObject includes a generic marshaller, g_cclosure_marshal_generic.þ  To use it, simply specify NULL as the marshaller in g_signal_new().þ  The generic marshaller is implemented with libffi, and consequentlyþ  GObject depends on libffi now.þþ### Notes about GLib 2.28þþ* The GApplication API has changed compared to the version that wasþ  included in the 2.25 development snapshots. Existing users will needþ  adjustments.þþ### Notes about GLib 2.26þþ* Nothing noteworthy.þþ### Notes about GLib 2.24þþ* It is now allowed to call g_thread_init(NULL) multiple times, andþ  to call glib functions before g_thread_init(NULL) is calledþ  (although the later is mainly a change in docs as this worked beforeþ  too). See the GThread reference documentation for the details.þþ* GObject now links to GThread and threads are enabled automaticallyþ  when g_type_init() is called.þþ* GObject no longer allows to call g_object_set() on construct-only propertiesþ  while an object is being initialized. If this behavior is needed, setting aþ  custom constructor that just chains up will re-enable this functionality.þþ* GMappedFile on an empty file now returns NULL for the contents instead ofþ  returning an empty string. The documentation specifically states that codeþ  may not rely on nul-termination here so any breakage caused by this changeþ  is a bug in application code.þþ### Notes about GLib 2.22þþ* Repeated calls to g_simple_async_result_set_op_res_gpointer usedþ  to leak the data. This has been fixed to always call the providedþ  destroy notify.þþ### Notes about GLib 2.20þþ* The functions for launching applications (e.g. g_app_info_launch() +þ  friends) now passes a FUSE file:// URI if possible (requires gvfsþ  with the FUSE daemon to be running and operational). With gvfs 2.26,þ  FUSE file:// URIs will be mapped back to gio URIs in the GFileþ  constructors. The intent of this change is to better integrateþ  POSIX-only applications, see bug #528670 for the rationale.  Theþ  only user-visible change is when an application needs to examine anþ  URI passed to it (e.g. as a positional parameter). Instead ofþ  looking at the given URI, the application will now need to look atþ  the result of g_file_get_uri() after having constructed a GFileþ  object with the given URI.þþ### Notes about GLib 2.18þþ* The recommended way of using GLib has always been to only include theþ  toplevel headers glib.h, glib-object.h and gio.h. GLib enforces this byþ  generating an error when individual headers are directly included.þ  To help with the transition, the enforcement is not turned on byþ  default for GLib headers (it is turned on for GObject and GIO).þ  To turn it on, define the preprocessor symbol G_DISABLE_SINGLE_INCLUDES.þþ### Notes about GLib 2.16þþ* GLib now includes GIO, which adds optional dependencies against libattrþ  and libselinux for extended attribute and SELinux support. Useþ  --disable-xattr and --disable-selinux to build without these.þþ### Notes about GLib 2.10þþ* The functions g_snprintf() and g_vsnprintf() have been removed fromþ  the gprintf.h header, since they are already declared in glib.h. Thisþ  doesn't break documented use of gprintf.h, but people have been knownþ  to include gprintf.h without including glib.h.þþ* The Unicode support has been updated to Unicode 4.1. This adds severalþ  new members to the GUnicodeBreakType enumeration.þþ* The support for Solaris threads has been retired. Solaris has providedþ  POSIX threads for long enough now to have them available on everyþ  Solaris platform.þþ* 'make check' has been changed to validate translations by callingþ  msgfmt with the -c option. As a result, it may fail on systems withþ  older gettext implementations (GNU gettext < 0.14.1, or Solaris gettext).þ  'make check' will also fail on systems where the C compiler does notþ  support ELF visibility attributes.þþ* The GMemChunk API has been deprecated in favour of a new 'sliceþ  allocator'. See the g_slice documentation for more details.þþ* A new type, GInitiallyUnowned, has been introduced, which isþ  intended to serve as a common implementation of the 'floating reference'þ  concept that is e.g. used by GtkObject. Note that changing theþ  inheritance hierarchy of a type can cause problems for languageþ  bindings and other code which needs to work closely with the typeþ  system. Therefore, switching to GInitiallyUnowned should be doneþ  carefully. g_object_compat_control() has been added to GLib 2.8.5þ  to help with the transition.þþ### Notes about GLib 2.6.0þþ* GLib 2.6 introduces the concept of 'GLib filename encoding', which is theþ  on-disk encoding on Unix, but UTF-8 on Windows. All GLib functionsþ  returning or accepting pathnames have been changed to expectþ  filenames in this encoding, and the common POSIX functions dealingþ  with pathnames have been wrapped. These wrappers are declared in theþ  header <glib/gstdio.h> which must be included explicitly; it is notþ  included through <glib.h>.þþ  On current (NT-based) Windows versions, where the on-disk file namesþ  are Unicode, these wrappers use the wide-character API in the Cþ  library. Thus applications can handle file names containing anyþ  Unicode characters through GLib's own API and its POSIX wrappers,þ  not just file names restricted to characters in the system codepage.þþ  To keep binary compatibility with applications compiled againstþ  older versions of GLib, the Windows DLL still provides entry pointsþ  with the old semantics using the old names, and applicationsþ  compiled against GLib 2.6 will actually use new names for theþ  functions. This is transparent to the programmer.þþ  When compiling against GLib 2.6, applications intended to beþ  portable to Windows must take the UTF-8 file name encoding intoþ  consideration, and use the gstdio wrappers to access files whoseþ  names have been constructed from strings returned from GLib.þþ* Likewise, g_get_user_name() and g_get_real_name() have been changedþ  to return UTF-8 on Windows, while keeping the old semantics forþ  applications compiled against older versions of GLib.þþ* The GLib uses an '_' prefix to indicate private symbols thatþ  must not be used by applications. On some platforms, symbols beginningþ  with prefixes such as _g will be exported from the library, on others not.þ  In no case can applications use these private symbols. In addition to that,þ  GLib+ 2.6 makes several symbols private which were not in any installedþ  header files and were never intended to be exported.þþ* To reduce code size and improve efficiency, GLib, when compiledþ  with the GNU toolchain, has separate internal and external entryþ  points for exported functions. The internal names, which begin withþ  IA__, may be seen when debugging a GLib program.þþ* On Windows, GLib no longer opens a console window when printingþ  warning messages if stdout or stderr are invalid, as they are inþ  ""Windows subsystem"" (GUI) applications. Simply redirect stdout orþ  stderr if you need to see them.þþ* The child watch functionality tends to reveal a bug in manyþ  thread implementations (in particular the older LinuxThreadsþ  implementation on Linux) where it's not possible to call waitpid()þ  for a child created in a different thread. For this reason, forþ  maximum portability, you should structure your code to fork allþ  child processes that you want to wait for from the main thread.þþ* A problem was recently discovered with g_signal_connect_object();þ  it doesn't actually disconnect the signal handler once the object beingþ  connected to dies, just disables it. See the API docs for the functionþ  for further details and the correct workaround that will continue toþ  work with future versions of GLib."
buserror/simavr,4564,995,75,249,User,False,952,4,18,66,False,"simavr is a lean, mean and hackable AVR simulator for linux & OSX",,7,7,0,59,93,11,4,22,208,4,8,3855,6,7,46,20,0,0,26,,114,,"simavr - a lean and mean Atmel AVR simulator for linuxþ======þþ_simavr_ is a new AVR simulator for linux, or any platform that uses avr-gcc. It uses þavr-gcc's own register definition to simplify creating new targets for supported AVRþdevices. The core was made to be small and compact, and hackable so allow quick þprototyping of an AVR project. The AVR core is now stable for use with parts þwith <= 128KB flash, and with preliminary support for the bigger parts. The þsimulator loads ELF files directly, and there is even a way to specify simulation þparameters directly in the emulated code using an .elf section. You can also þload multipart HEX files.þþInstallationþ------------þOn OSX, we recommend using [homebrew](https://brew.sh):þþ    brew tap osx-cross/avrþ    brew install --HEAD simavrþþOn Ubuntu, SimAVR is available in the Bionic package source:þþ    apt-get install simavrþþ(Note that the command is made available under the name `simavr` not `run_avr`.)þþOtherwise, `make` is enough to just start using __bin/simavr__. To install the __simavr__ command system-wide, `make install RELEASE=1`.þþSupported IOsþ--------------þ* _eeprom_þ* _watchdog_þ* _IO ports_ (including pin interrupts)þ* _Timers_, 8 &16 (Normal, CTC and Fast PWM, the overflow interrupt too)þ* The _UART_, including tx & rx interrupts (there is a loopback/local echo test mode too)þ* _SPI_, master/slave including the interruptþ* _i2c_ Master & Slaveþ* External _Interrupts_, INT0 and so on.þ* _ADC_þ* Self-programming (ie bootloaders!)þþEmulated Cores (very easy to add new ones!)þ--------------þ+ ATMega2560þ+ AT90USB162 (with USB!)þ+ ATMega1281þ+ ATMega1280þ+ ATMega128þ+ ATMega128rf1þ+ ATMega16M1þ+ ATMega169þ+ ATMega162þ+ ATMega164/324/644þ+ ATMega48/88/168/328þ+ ATMega8/16/32þ+ ATTiny25/45/85þ+ ATTIny44/84þ+ ATTiny2313/2313vþ+ ATTiny13/13aþþExtras:þ-------þ* fully working _gdb_ support including some pretty cool “passive modes”.þ* There is also very easy support for “VCD” (Value Change Dump) that can be visualized þgraphically as “waveforms” with tools like _gtkwave_ (see below).þ* There are a few examples of real life firmwares running on simavr, including OpenGL rendering of the display…þ* There is support for _Arduino_, but no IDE integrationþþDocumentation And Further Informationþ-------------------------------------þþ* Manual / Developer Guide: https://github.com/buserror-uk/simavr/blob/master/doc/manual/manual.pdf?raw=trueþ* Examples: https://github.com/buserror-uk/simavr/tree/master/examplesþ* Mailing List: http://groups.google.com/group/simavrþ* IRC: _#simavr_ on FreenodeþþContributingþ------------þþPatches are always welcome! Please submit your changes via Github pull requests.þþVCD Support -- built in logic analyzer þ-----------þ_simavr_ can output most of its pins, firmware variables, interrupts and a few otherþthings as signals to be dumped into a file that can be plotted using gtkwave forþfurther, precise analysis.þA firmware can contain instructions for _simavr_ to know what to trace, and the file isþautomatically generated.þExample:þþ const struct avr_mmcu_vcd_trace_t _mytrace[]  _MMCU_ = {þ  { AVR_MCU_VCD_SYMBOL(""UDR0""), .what = (void*)&UDR0, },þ  { AVR_MCU_VCD_SYMBOL(""UDRE0""), .mask = (1 << UDRE0), .what = (void*)&UCSR0A, },þ };þþWill tell _simavr_ to generate a trace everytime the UDR0 register changes and everytimeþthe interrupt is raised (in UCSR0A). The *_MMCU_* tag tells gcc that it needs compiling,þbut it won't be linked in your program, so it takes literally zero bytes, this is a codeþsection that is private to _simavr_, it's free!þA program running with these instructions and writing to the serial port will generateþa file that will display:þþ $ ./simavr/run_avr tests/atmega88_example.axfþ AVR_MMCU_TAG_VCD_TRACE 00c6:00 - UDR0þ AVR_MMCU_TAG_VCD_TRACE 00c0:20 - UDRE0þ Loaded 1780 .textþ Loaded 114 .dataþ Loaded 4 .eepromþ Starting atmega88 - flashend 1fff ramend 04ff e2end 01ffþ atmega88 initþ avr_eeprom_ioctl: AVR_IOCTL_EEPROM_SET Loaded 4 at offset 0þ Creating VCD trace file 'gtkwave_trace.vcd'þ Read from eeprom 0xdeadbeef -- should be 0xdeadbeef..þ Read from eeprom 0xcafef00d -- should be 0xcafef00d..þ simavr: sleeping with interrupts off, quitting gracefullyþþAnd when the file is loaded in gtkwave, you see:þ![gtkwave](https://github.com/buserror-uk/simavr/raw/master/doc/img/gtkwave1.png)þþYou get a very precise timing breakdown of any change that you add to the trace, downþto the AVR cycle. þþExample:þ--------þ_simavr_ is really made to be the center for emulating your own AVR projects, not justþa debugger, but also the emulating the peripherals you will use in your firmware, so þyou can test and develop offline, and now and then try it on the hardware.þþYou can also use _simavr_ to do test units on your shipping firmware to validate itþbefore you ship a new version, to prevent regressions or mistakes.þþ_simavr_ has a few 'complete projects/ that demonstrate this, most of them were madeþusing real hardware at some point, and the firmware binary is _exactly_ the one thatþran on the hardware. The key here is to emulate the _parts_ or peripherals thatþare hooked to the AVR. Of course, you don't have to emulate the full hardware, you justþneed to generate the proper stimulus so that the AVR is fooled.þþHD44780 LCD Board Demoþ----------------------þþ![lcd](https://github.com/buserror-uk/simavr/raw/master/doc/img/hd44780.png)þþThis example board hooks up an Atmega48 to an emulated HD44780 LCD and display a runningþcounter in the 'lcd'. Everything is emulated, the firmware runs exactly like thisþon a real hardware.þþ![lcd-gtkwave](https://github.com/buserror-uk/simavr/raw/master/doc/img/hd44780-wave.png)þþAnd this is a gtkwave trace of what the firmware is doing. You can zoom in, measure, etcþin gtkwave, select trades to see etc.þþQuite a few other examples are available!"
gnab/rtl8812au,2059,1061,99,397,User,False,138,1,0,50,False,Realtek 802.11n WLAN Adapter Linux driver,,0,6,0,101,30,6,3,1,61,0,6,2399,5,5,12,6,0,0,29,,182,,"## Changesþ2019-07-11: Updated to compile against kernel 5.2þþ## Realtek 802.11ac (rtl8812au)þþThis is a fork of the Realtek 802.11ac (rtl8812au) v4.2.2 (7502.20130507)þdriver altered to build on Linux kernel version >= 3.10.þþ### PurposeþþMy D-Link DWA-171 wireless dual-band USB adapter needs the Realtek 8812auþdriver to work under Linux.þþThe current rtl8812au version (per nov. 20th 2013) doesn't compile on Linuxþkernels >= 3.10 due to a change in the proc entry API, specifically theþdeprecation of the `create_proc_entry()` and `create_proc_read_entry()`þfunctions in favor of the new `proc_create()` function.þþ### BuildingþþThe Makefile is preconfigured to handle most x86/PC versions.  If you are compiling for something other than an intel x86 architecture, you need to first select the platform, e.g. for the Raspberry Pi, you need to set the I386 to n and the ARM_RPI to y:þ```shþ...þCONFIG_PLATFORM_I386_PC = nþ...þCONFIG_PLATFORM_ARM_RPI = yþ```þþThere are many other platforms supported and some other advanced options, e.g. PCI instead of USB, but most won't be needed.þþThe driver is built by running `make`, and can be tested by loading theþbuilt module using `insmod`:þþ```shþ$ makeþ$ sudo insmod 8812au.koþ```þþAfter loading the module, a wireless network interface named __Realtek 802.11n WLAN Adapter__ should be available.þþ### InstallingþþInstalling the driver is simply a matter of copying the built moduleþinto the correct location and updating module dependencies using `depmod`:þþ```shþ$ sudo cp 8812au.ko /lib/modules/$(uname -r)/kernel/drivers/net/wirelessþ$ sudo depmodþ```þþThe driver module should now be loaded automatically.þþ### DKMSþþAutomatically rebuilds and installs on kernel updates. DKMS is in official sources of Ubuntu, for installation do:þþ```shþ$ sudo apt-get install build-essential dkms þ```þþThe driver source must be copied to /usr/src/8812au-4.2.3þþThen add it to DKMS:þþ```shþ$ sudo dkms add -m 8812au -v 4.2.3þ$ sudo dkms build -m 8812au -v 4.2.3þ$ sudo dkms install -m 8812au -v 4.2.3þ```þþCheck with:þ```shþ$ sudo dkms statusþ```þAutomatically load at boot:þ```shþ$ echo 8812au | sudo tee -a /etc/modulesþ```þEventually remove from DKMS with:þ```shþ$ sudo dkms remove -m 8812au -v 4.2.3 --allþ```þþ### Referencesþþ- D-Link DWA-171þ  - [D-Link page](http://www.dlink.com/no/nb/home-solutions/connect/adapters/dwa-171-wireless-ac-dual-band-usb-adapter)þ  - [wikidevi page](http://wikidevi.com/wiki/D-Link_DWA-171_rev_A1)"
coreutils/coreutils,37867,1633,90,432,Organization,False,29052,6,365,161,False,upstream mirror,http://git.savannah.gnu.org/gitweb/?p…,0,6,0,2,13,1,2,1,16,1,1,10092,11,58,2367,1454,0,0,3,2,,,
pgbouncer/pgbouncer,2789,1073,65,260,Organization,False,1218,3,55,37,False,lightweight connection pooler for PostgreSQL,https://www.pgbouncer.org/,1,9,0,110,223,22,10,39,119,7,5,4842,4,44,3201,2564,0,0,2,0,,,"PgBouncerþ=========þþLightweight connection pooler for PostgreSQL.þþHomepage: <https://www.pgbouncer.org/>þþSources, bug tracking: <https://github.com/pgbouncer/pgbouncer>þþBuildingþ---------þþPgBouncer depends on few things to get compiled:þþ* [GNU Make] 3.81+þ* [Libevent] 2.0+þ* [pkg-config]þ* [OpenSSL] 1.0.1+ for TLS supportþ* (optional) [c-ares] as alternative to Libevent's evdnsþ* (optional) PAM librariesþþ[GNU Make]: https://www.gnu.org/software/make/þ[Libevent]: http://libevent.org/þ[pkg-config]: https://www.freedesktop.org/wiki/Software/pkg-config/þ[OpenSSL]: https://www.openssl.org/þ[c-ares]: http://c-ares.haxx.se/þþWhen dependencies are installed just run:þþ    $ ./configure --prefix=/usr/localþ    $ makeþ    $ make installþþIf you are building from Git, or are building for Windows, please seeþseparate build instructions below.þþDNS lookup supportþ------------------þþPgBouncer does host name lookups at connect time instead of just onceþat configuration load time.  This requires an asynchronous DNSþimplementation.  The following table shows supported backends andþtheir probing order:þþ| backend                    | parallel | EDNS0 (1) | /etc/hosts | SOA lookup (2) | note                                  |þ|----------------------------|----------|-----------|------------|----------------|---------------------------------------|þ| c-ares                     | yes      | yes       | yes        | yes            | IPv6+CNAME buggy in <=1.10            |þ| udns                       | yes      | yes       | no         | yes            | IPv4 only                             |þ| evdns, libevent 2.x        | yes      | no        | yes        | no             | does not check /etc/hosts updates     |þ| getaddrinfo_a, glibc 2.9+  | yes      | yes (3)   | yes        | no             | N/A on non-glibc                      |þ| getaddrinfo, libc          | no       | yes (3)   | yes        | no             | N/A on Windows, requires pthreads     |þþ1. EDNS0 is required to have more than 8 addresses behind one host name.þ2. SOA lookup is needed to re-check host names on zone serial change.þ3. To enable EDNS0, add `options edns0` to `/etc/resolv.conf`.þþc-ares is the most fully-featured implementation and is recommendedþfor most uses and binary packaging (if a sufficiently new version isþavailable).  Libevent's built-in evdns is also suitable for many uses,þwith the listed restrictions.  The other backends are mostly legacyþoptions at this point and don't receive much testing anymore.þþBy default, c-ares is used if it can be found.  Its use can be forcedþwith `configure --with-cares` or disabled with `--without-cares`.  Ifþc-ares is not used (not found or disabled), then specify `--with-udns`þto pick udns, else Libevent is used.  Specify `--disable-evdns` toþdisable the use of Libevent's evdns and fall back to a libc-basedþimplementation.þþPAM authenticationþ------------------þþTo enable PAM authentication, `./configure` has a flag `--with-pam`þ(default value is no).  When compiled with PAM support, a new globalþauthentication type `pam` is available to validate users through PAM.þþsystemd integrationþ-------------------þþTo enable systemd integration, use the `configure` optionþ`--with-systemd`.  This allows using `Type=notify` service units asþwell as socket activation.  See `etc/pgbouncer.service` andþ`etc/pgbouncer.socket` for examples.þþBuilding from Gitþ-----------------þþBuilding PgBouncer from Git requires that you fetch the libusualþsubmodule and generate the header and configuration files beforeþyou can run `configure`:þþ $ git clone https://github.com/pgbouncer/pgbouncer.gitþ $ cd pgbouncerþ $ git submodule initþ $ git submodule updateþ $ ./autogen.shþ $ ./configure ...þ $ makeþ $ make installþþAdditional packages required: autoconf, automake, libtool, pandocþþBuilding on Windowsþ-------------------þþThe only supported build environment on Windows is MinGW.  Cygwin andþVisual $ANYTHING are not supported.þþTo build on MinGW, do the usual:þþ $ ./configure ...þ $ makeþþIf cross-compiling from Unix:þþ $ ./configure --host=i586-mingw32msvc ...þþRunning on Windowsþ------------------þþRunning from the command line goes as usual, except that the `-d` (daemonize),þ`-R` (reboot), and `-u` (switch user) switches will not work.þþTo run PgBouncer as a Windows service, you need to configure theþ`service_name` parameter to set name for service.  Then:þþ $ pgbouncer -regservice config.iniþþTo uninstall service:þþ $ pgbouncer -unregservice config.iniþþTo use the Windows event log, set `syslog = 1` in the configuration file.þBut before that you need to register `pgbevent.dll`:þþ $ regsvr32 pgbevent.dllþþTo unregister it, do:þþ $ regsvr32 /u pgbevent.dll"
yasm/yasm,27906,854,79,214,Organization,False,2159,23,21,21,False,Yasm Assembler mainline development tree,http://yasm.tortall.net/,0,7,0,85,18,1,1,17,25,4,1,6977,4,4,37,392,0,0,5,1,,,
OP-TEE/optee_os,22087,789,149,567,Organization,False,3797,10,41,125,False,Trusted side of the TEE,,0,10,1,22,1635,22,160,14,2259,14,260,2203,30,384,49217,27175,0,0,8,0,,,# OP-TEE Trusted OSþThis git contains source code for the secure side implementation of OP-TEEþproject.þþAll official OP-TEE documentation has moved to http://optee.readthedocs.io.þþ// OP-TEE core maintainers
FRRouting/frr,73876,1376,170,565,Organization,False,19989,20,50,223,False,The FRRouting Protocol Suite,https://frrouting.org/,17,65,7,372,1270,101,105,72,4859,61,770,4807,41,1092,88990,35809,0,0,6,25,,,"<p align=""center"">þ<img src=""http://docs.frrouting.org/en/latest/_static/frr-icon.svg"" alt=""Icon"" width=""20%""/>þ</p>þþFRRoutingþ=========þþFRR is free software that implements and manages various IPv4 and IPv6 routingþprotocols. It runs on nearly all distributions of Linux and BSD as well asþSolaris and supports all modern CPU architectures.þþFRR currently supports the following protocols:þþ* BGPþ* OSPFv2þ* OSPFv3þ* RIPv1þ* RIPv2þ* RIPngþ* IS-ISþ* PIM-SM/MSDPþ* LDPþ* BFDþ* Babelþ* PBRþ* OpenFabricþ* VRRPþ* EIGRP (alpha)þ* NHRP (alpha)þþInstallation & Useþ------------------þþFor source tarballs, see theþ[releases page](https://github.com/FRRouting/frr/releases).þþFor Debian and its derivatives, use the APT repository atþ[https://deb.frrouting.org/](https://deb.frrouting.org/).þþInstructions on building and installing from source for supported platforms mayþbe found in theþ[developer docs](http://docs.frrouting.org/projects/dev-guide/en/latest/building.html).þþOnce installed, please refer to the [user guide](http://docs.frrouting.org/)þfor instructions on use.þþCommunityþ---------þþThe FRRouting email list server is locatedþ[here](https://lists.frrouting.org/listinfo) and offers the following publicþlists:þþ| Topic             | List                         |þ|-------------------|------------------------------|þ| Development       | dev@lists.frrouting.org      |þ| Users & Operators | frog@lists.frrouting.org     |þ| Announcements     | announce@lists.frrouting.org |þþFor chat, we currently use [Slack](https://frrouting.slack.com). You can joinþby clicking the ""Slack"" link under theþ[Participate](https://frrouting.org/#participate) section of our website.þþþContributingþ------------þþFRR maintains [developer's documentation](http://docs.frrouting.org/projects/dev-guide/en/latest/index.html)þwhich contains the [project workflow](http://docs.frrouting.org/projects/dev-guide/en/latest/workflow.html)þand expectations for contributors. Some technical documentation on projectþinternals is also available.þþWe welcome and appreciate all contributions, no matter how small!þþþSecurityþ--------þþTo report security issues, please use our security mailing list:þþ```þsecurity [at] lists.frrouting.orgþ```"
emsec/ChameleonMini,9716,948,127,250,Organization,False,332,2,1,25,False,The ChameleonMini is a versatile contactless smartcard emulator compliant to NFC. The ChameleonMini was developed by https://kasper-oswald.de. The device is available at https://shop.kasper.it. For further information see the Getting Started Page https://rawgit.com/emsec/ChameleonMini/master/Doc/Doxygen/html/_page__getting_started.html or the Wi…,,10,7,0,48,148,10,4,11,56,2,4,2364,3,7,6871,2838,0,0,12,2,,,"Chameleon-Miniþ==============þThis is the official repository of ChameleonMini, a freely programmable, portable tool for NFC security analysis that can emulate and clone contactless cards, read RFID tags and sniff/log RF data. Thanks to over 1700 backers from our [Kickstarter project](https://www.kickstarter.com/projects/1980078555/chameleonmini-a-versatile-nfc-card-emulator-and-mo), the current Revision G has been realized by Kasper & Oswald GmbH.þþThe ChameleonMini RevG is now also available via the Kasper & Oswald [Webshop](https://shop.kasper.it/). Thank you for supporting the project!þþIMPORTANT: Third-party Clonesþ------------------þWe are aware of various third-party ChameleonMini clones or modified variants that are available on the Internet. Warning: We have evidence that some of these devices are defective or suffer from reading problems et cetera. Please understand that we cannot give support for these non-official devices, as we have no schematics / layout or other information, nor do we know the manufacturers. In case of problems, please contact the manufacturers of your device directly.þþNote to the manufacturers: Some of the third-party ChameleonMini are violating the ChameleonMini license, please obey the license (see LICENSE.txt)!þþFirst Stepsþ-----------þTo upgrade the firmware of your ChameleonMini, please visit the [Getting Started page](https://cdn.statically.io/gh/emsec/ChameleonMini/master/Doc/Doxygen/html/_page__getting_started.html) from the [doxygen documentation](https://cdn.statically.io/gh/emsec/ChameleonMini/master/Doc/Doxygen/html/index.html).þþSupported Cards and Codecsþ--------------------------þSee [here](https://github.com/emsec/ChameleonMini/wiki/Supported-Cards-and--Codecs).þþþQuestionsþ---------þIf you have any questions, please visit the [Issues page](https://github.com/emsec/ChameleonMini/issues) and ask your questions there so everyone benefits from the answer.þþ**Please note that we cannot give any support if you have issues with the RevE-rebooted hardware or any GUI. Please use this issues page only for problems with the RevG hardware as distributed by Kasper & Oswald and for problems with the firmware from this page.**þþExternal Contributionsþ----------------------þWe appreciate every contribution to the project. Have a look at the [External Contributions page](https://github.com/emsec/ChameleonMini/wiki/External-Contributions).þþRepository Structureþ--------------------þThe code repository containsþ* Doc: A doxygen documentationþ* Drivers: Chameleon drivers for Windows and Linuxþ* Dumps: Dumps of different smartcardsþ* Hardware: The layout and schematics of the PCBþ* Firmware: The complete firmware including a modified Atmel DFU bootloader and LUFAþ* Software: Contains a python tool for an easy configuration (and more) of the ChameleonMini, Note that this is currently under constructionþ* RevE: Contains the whole contents of the discontinued RevE repository.þ* RevE-light: Contains our development files for the RevE-light - **WARNING:** currently not supported / not functional"
lanoxx/tilda,3821,904,40,139,User,False,592,14,34,45,False,A Gtk based drop down terminal for Linux and Unix,,0,14,2,90,229,9,11,9,94,1,1,2917,2,37,6629,6322,0,0,26,,37,,"# What is Tilda?þþTilda is a terminal emulator and can be compared with other popular terminal emulators such asþgnome-terminal (Gnome), Konsole (KDE), xterm and many others. The specialities of Tildaþare that it does not behave like a normal window but instead it can be pulled up and down from the topþof the screen with a special hotkey. Additionally Tilda is highly configurable. It is possible to configure theþhotkeys for keybindings, change the appearance and many options that affect the behavior of Tilda. The screen shotsþbelow show some of the options that Tilda provides.þþ## Tilda Terminalþ![Tilda window with search bar](images/tilda_terminal_with_search_bar.png)þþStarting with version 1.3 Tilda comes with a search bar. Currently we support searching forwards and backwardsþas well as options to search case sensitive and to use regular expressions. The search bar can be activated fromþthe context menu or with a configurable hotkey that defaults to `<Ctrl><Shift>F`þ## General optionsþ![General](images/tilda_general-16-9.png)þþ## Title and Commandþ![Title And Command](images/tilda_title_and_command-16-9.png)þþ## Appearance optionsþ![Appearance](images/tilda_appearance-16-9.png)þþ## Colorsþ![Colors](images/tilda_colors-16-9.png)þþ## Keybindings optionsþþStarting with version 1.4 Tilda's keybindings page switches toþa new list based layout that is easier to use.þþ![Keybindings](images/tilda_keybindings-16-9.png)þþ## Supported PlatformsþTilda currently works only on Xorg-based desktops. Previously thatþmeant that virtually all Linux distributions and some BSD's would be supported.þRecently however, some Linux distributionsþ(such as Ubuntu 17.10) have started to use Wayland as theirþdefault display server. Tilda currently does not support Wayland and will notþwork on such desktops. As a result it will fail to start.þPatches that introduce wayland support for tilda are very welcome. Pleaseþlook into the issue section or write me a mail if you would like to contributeþto tilda.þþ# Installing TildaþþTilda should be packaged for your distribution if you are running Debian or any Debian derived distribution such asþUbuntu or Linux Mint. For other distributions please check your package manager if it provides Tilda. You can alsoþinstall Tilda from source. For instructions to compile, install and optionally package tilda please seeþ**[HACKING.md](HACKING.md)**.þþ# Running TildaþþOnce you have installed tilda, it should have automatically registered a menu entry in your desktops application menu.þAlternatively, you can run `tilda` from your command line.þþThe first time you run Tilda, it will create the default configuration file forþyou, and show the configuration wizard. If you do not want to change anyþsettings, just press the ""OK"" button to accept the defaults.þþThe default keybindings to show and hide Tilda are as follows:þþ * F1 - the first instanceþ * F2 - the second instanceþ * F3 - the third instanceþ * ...þþOther default keybindings are:þþ * Shift-Ctrl-T - Open new tabþ * Ctrl-PageUp - Next tabþ * Ctrl-PageDown - Previous tabþ * Shift-Ctrl-W - Close current tabþ * Shift-Ctrl-Q - Exit Tildaþþ# Specifying your own keybinding to hide / show TildaþþWe, the developers, have attempted to make the keybinding setting work with asþlittle trouble as possible. Some example keybindings follow:þþ| Keybinding String | Notes                                                             |þ|-------------------|-------------------------------------------------------------------|þ| `grave`           | This will use the tilde key. Many people want this.               |þ| `~`               | This is the same as `<Shift>grave`                                |þ| `space`           | This will use the spacebar to show / hide Tilda. NOT RECOMMENDED! |þ| `<Shift><Ctrl>A`  | Press Shift, Control, and the ""a"" key at the same time            |þ| `<Shift>space`    | Press Shift and the spacebar at the same time                     |þ| `<Ctrl>z`         | Press Control and the ""z"" key at the same time                    |þþThat should cover most of the cases. If you want to use something else, itþprobably follows the pattern, so give it a try. Alternatively, you can use theþconfiguration wizard, and press the ""Pull Down Terminal"" button on theþ""Keybindings"" tab, then type the combination you want to use. Hopefully, Tildaþwill be able to grab it for you. The `<Tab>` key cannot be grabbed, so at the momentþcombinations such as `<Ctrl><Tab>` are not possible.þþ# Files that Tilda createsþþSince approximately version 0.9.6 Tilda adheres to the XDG Base Directory Specification andþcreates its files in the $XDG_CONFIG_HOME and $XDG_CACHE_HOME folders which normally default toþ~/.config/ and ~/.cache/. Tilda will create a lock file in the cache directoryþeach time it starts, to keep track of how many instances are running:þþ    ~/.cache/tilda/locks/lock_$PID_$INSTANCEþþTilda will also create the config files in:þþ    ~/.config/tilda/config_$INSTANCEþþwhere `$INSTANCE` is the number of how many instances are running andþ`$PID` the process id. Tilda automatically migrates the files for you if it detects configuration fileþat the old location `~/.tilda`.þþ# Getting more help / Questions and CommentsþþTo get more help, you should first open a command prompt, and run `tildaþ--help`. You can also take a look at the **[Wiki](https://github.com/lanoxx/tilda/wiki)**, or emailþquestions and comments to anyone listed in the [AUTHORS](AUTHORS) file.þþ# Reporting BugsþþWe have done our best to make sure that Tilda is free from bugs, but it isþinevitable that we have missed some.þþYou may open bugs in the **[issue section](http://github.com/lanoxx/tilda/issues)** or email them to theþdevelopers directly.þþ# Contributing to TildaþþTilda is an open source project that lives by the help of volunteersþwho fix bugs and implement new features in their spare time. Everybody isþwelcome to join and help us to fix bugs or to implement new features.þPull requests and patches are always welcome.þþ## PrerequisitesþþTilda is written in C with the use of the librariesþ**glib**, **GTK+**, **libconfuse** and **X11**. You should have aþgood background in C and some experience with GTK and glib already. Someþareas of tilda will also require to know a little about X11 programming.þþ## What to work on?þþIf you already have the necessary background feel free to submit a patchþthat fixes an issue or implements a new feature. If you are unsure ifþyour patch will be accepted then open an issue first, describe your issueþand ask if its likely that the patch gets accepted.þþYou can also look into the [TODO.md](TODO.md) file and see if thereþis something there that you would like to do.þþ## Getting helpþþFeel free to mail the developers if you have questions about theþtilda source code or if you are unsure how something works."
DuyTrandeLion/BLE_Bike_mbed_os,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
TheOfficialFloW/VitaShell,52239,945,134,177,User,False,996,1,52,85,False,Multi-functional file manager for PS Vita,,0,14,0,66,337,8,3,0,163,0,12,1608,9,15,756,548,0,0,20,,1,,"# VitaShellþþVitaShell is an alternative replacement of the PS Vita's LiveArea. It offers you a file manager, package installer, built-in FTP and much more.þThis homebrew was an entry of the Revitalize PS Vita homebrew competition and won the first prize. HENkaku's molecularShell is also based on VitaShell.þþ## ChangelogþSee [CHANGELOG.md](CHANGELOG.md)þþ## How to use an USB flash drive as Memory Card on a PS TVþ- Format your USB flash drive as exFAT or FAT32.þ- Launch VitaShell and press `▲` in the `home` section.þ- Select `Mount uma0:` and attach your USB flash drive. You can now copy stuff from/to your USB stick.þ- Once `uma0:` is listed under the partitions, press `▲` again and choose `Mount USB ux0:`. This will copy important apps like VitaShell, molecularShell, and other files.þ- Your USB flash drive is now acting as a Memory Card.þ- To sync all your apps on your USB flash drive, press `▲` and choose `Refresh livearea`. This will NOT refresh PSP games.þ- If you wish to revert the patch, press `▲` and select `Umount USB ux0:`.þ- Note that this patch is only temporary and you need to redo the procedure everytime you launch your PS TV.þþ## CustomizationþYou can customize those files:þþ| File                   | Note                        |þ| ---------------------- | --------------------------- |þ| colors.txt             | All colors adjustable       |þ| archive_icon.png       | Archive icon                |þ| audio_icon.png         | Audio icon                  |þ| battery.png            | Battery border icon         |þ| battery_bar_charge.png | Charging battery bar        |þ| battery_bar_green.png  | Green battery bar           |þ| battery_bar_red.png    | Red battery bar             |þ| bg_audioplayer.png     | Background for audio player |þ| bg_browser.png         | Background for file browser |þ| bg_hexeditor.png       | Background for hex editor   |þ| bg_photoviewer.png     | Background for photo viewer |þ| bg_texteditor.png      | Background for text editor  |þ| context.png            | Context menu image (Can be any size. Suggestion: It will look great if you add alpha channel to your image)  |þ| context_more.png       | Context menu more image (Can be any size. Suggestion: It will look great if you add alpha channel to your image)  |þ| cover.png              | Default album cover         |þ| dialog.png             | Dialog menu image (Can be any size. This image file will be stretched by VitaShell to fit the dialog box. Suggestion: Don't use motives, as it will not look good with wrong proportion)  |þ| fastforward.png        | Fastforward icon            |þ| fastrewind.png         | Fastrewind icon             |þ| file_icon.png          | File icon                   |þ| folder_icon.png        | Folder icon                 |þ| ftp.png                | FTP icon                    |þ| image_icon.png         | Image icon                  |þ| pause.png              | Pause icon                  |þ| play.png               | Play icon                   |þ| settings.png           | Settings icon               |þ| sfo_icon.png           | SFO icon                    |þ| text_icon.png          | Text icon                   |þ| wallpaper.png          | Wallpaper                   |þþ**Theme setting:** VitaShell will load the theme that is set in `ux0:VitaShell/theme/theme.txt` (`THEME_NAME = ""YOUR_THEME_NAME""`)þþ**General info:** You don't need to have all these files in your custom theme, if one of them is missing, the default image file will be loaded instead.þþ**Dialog and context image:** If these files are not available, the colors `DIALOG_BG_COLOR` and `CONTEXT_MENU_COLOR` from `colors.txt` will be used instead.þþ## Multi-languageþPut your language file at `ux0:VitaShell/language/x.txt`, where the file must be UTF-8 encoded and `x` is one of the language listed below:þþ- japaneseþ- english_usþ- frenchþ- spanishþ- germanþ- italianþ- dutchþ- portugueseþ- russianþ- koreanþ- chinese_tþ- chinese_sþ- finnishþ- swedishþ- danishþ- norwegianþ- polishþ- portuguese_brþ- turkishþþVitaShell does automatically load the language that matches to the current system language.þIf your system language is for example french, it will load from `ux0:VitaShell/language/french.txt`.þþLanguages files are available in the `l10n` folder of this repository.þþ## BuildingþInstall [vitasdk](https://github.com/vitasdk) and build VitaShell using:þþ```þmkdir build && cd build && cmake .. && makeþ```þþ## Creditsþ* Team Molecule for HENkakuþ* xerpi for ftpvitalib and vita2dlibþ* wololo for the Revitalize contestþ* sakya for Lightmp3þ* Everybody who contributed on vitasdk"
netsniff-ng/netsniff-ng,3063,834,100,205,Organization,False,1328,2,22,40,False,A Swiss army knife for your daily Linux network plumbing.,http://netsniff-ng.org,7,7,0,40,92,2,1,4,85,0,5,2651,4,11,28,12,0,0,2,0,,,
corosync/corosync,21882,812,96,172,Organization,False,4144,37,84,66,False,The Corosync Cluster Engine,http://corosync.github.com/corosync,0,3,0,33,153,8,6,11,362,4,26,5465,5,27,1090,750,0,0,5,3,,,
LILIDD/FFmpeg,131268,0,1,0,User,False,67391,2,0,619,False,,,0,7,0,,,,,0,0,0,0,7103,0,0,0,0,0,0,130,,30,,"FFmpeg READMEþ=============þþFFmpeg is a collection of libraries and tools to process multimedia contentþsuch as audio, video, subtitles and related metadata.þþ## Librariesþþ* `libavcodec` provides implementation of a wider range of codecs.þ* `libavformat` implements streaming protocols, container formats and basic I/O access.þ* `libavutil` includes hashers, decompressors and miscellaneous utility functions.þ* `libavfilter` provides a mean to alter decoded Audio and Video through chain of filters.þ* `libavdevice` provides an abstraction to access capture and playback devices.þ* `libswresample` implements audio mixing and resampling routines.þ* `libswscale` implements color conversion and scaling routines.þþ## Toolsþþ* [ffmpeg](http://ffmpeg.org/ffmpeg.html) is a command line toolbox toþ  manipulate, convert and stream multimedia content.þ* [ffplay](http://ffmpeg.org/ffplay.html) is a minimalistic multimedia player.þ* [ffprobe](http://ffmpeg.org/ffprobe.html) is a simple analisys tool to inspectþ  multimedia content.þ* Additional small tools such as `aviocat`, `ismindex` and `qt-faststart`.þþ## DocumentationþþThe offline documentation is available in the **doc/** directory.þþThe online documentation is available in the main [website](http://ffmpeg.org)þand in the [wiki](http://trac.ffmpeg.org).þþ### ExamplesþþCoding examples are available in the **doc/examples** directory.þþ## LicenseþþFFmpeg codebase is mainly LGPL-licensed with optional components licensed underþGPL. Please refer to the LICENSE file for detailed information."
NurKeinNeid/dharma_kernel_nextbit_msm8992,647254,0,1,0,User,False,435328,3,4,4751,False,A Dysfunctional CAF-based custom kernel for ether @ReaperRoms,,0,8,0,,,,,0,0,0,0,6641,0,0,0,0,0,0,60,,23,,
sdegutis/mjolnir,11473,5081,86,136,Organization,False,1502,2,4,32,False,Lightweight automation and productivity app for OS X,,0,7,1,0,493,0,0,0,82,0,0,2357,1,1,19,0,0,0,10,0,,mjolnirapp/mjolnir,"# Mjolnirþþ<img src=""https://raw.githubusercontent.com/mjolnirapp/mjolnir/master/Mjolnir/Images.xcassets/AppIcon.appiconset/icon_128x128.png"" alt=""Mjolnir logo"" title=""Mjolnir logo"" align=""right"" width=""64"" height=""64""/>þþ*Lightweight automation and productivity power-tool for OS X*þþ* Current version:  Mjolnir 1.0.1þ* Requires:         OS X 10.12 or higherþþ## What is Mjolnir?þþMjolnir is an OS X app that lets you automate common tasks using theþlanguage Lua. At its core, it doesn't actually do anything besidesþload up a Lua environment; the real power lies in all the usefulþmodules that you can install.þþYou write a ""config"", which just means `~/.mjolnir/init.lua`. Thisþfile, along with whatever modules it requires, have full access to theþbuilt-in `mjolnir` module, and all Lua modules that you have installedþ(e.g. from LuaRocks or any way you want to install them).þþ## Try it outþþ1. Download [the latest release](https://github.com/mjolnirapp/mjolnir/releases/latest), unzip, right-click `Mjolnir.app`, choose ""Open"". Or, install it with [Homebrew](https://brew.sh/):þþ   ~~~bashþ   $ brew cask install mjolnirþ   ~~~þþ2. Install Lua into /usr/local e.g. from [Homebrew](http://brew.sh/), and then install LuaRocks:þþ   ~~~bashþ   $ brew updateþ   $ brew install luaþ   $ brew install luarocksþ   ~~~þþ3. Install some modules from this list: https://luarocks.org/search?q=mjolnirþþ   ~~~bashþ   $ luarocks install mjolnir.hotkeyþ   $ luarocks install mjolnir.applicationþ   ~~~þþ   Note: you don't need to install every module, since some of them have lower-level ones as dependencies, e.g. installing mjolnir-hotkey automatically installs mjolnir-keycodes, etc.þþ4. Create `~/.mjolnir/init.lua`, and at the top, require the modules you installed, e.g. like this:þþ   ~~~luaþ   local application = require ""mjolnir.application""þ   local hotkey = require ""mjolnir.hotkey""þ   local window = require ""mjolnir.window""þ   local fnutils = require ""mjolnir.fnutils""þ   ~~~þþ   NOTE: The `mjolnir.window` module comes with `mjolnir.application`,þ         so you don't need to (and can't) install it separately. Also,þ         `mjolnir.fnutils` is already installed as a dependency of theþ         other modules, so you don't need to explicitly install it.þþ5. Start writing some fun stuff!þþ   ~~~luaþ   hotkey.bind({""cmd"", ""alt"", ""ctrl""}, ""D"", function()þ      local win = window.focusedwindow()þ      local f = win:frame()þ      f.x = f.x + 10þ      win:setframe(f)þ   end)þ   ~~~þ   þ6. Reload config using `mjolnir.reload()` in the console.þþ## UninstallingþþIf for any reason you want to undo everything in the above steps, do:þþ~~~bashþ$ luarocks purge --tree=/usr/localþ$ brew uninstall lua luarocksþ$ rm ~/.luarocks/config.luaþ~~~þþ## Installing to $HOMEþþIf you run `luarocks --local install ...` instead of `luarocksþinstall ...`, it will install to `~/.luarocks/` instead ofþ`/usr/local`. Update your `package.path` and `package.cpath`þaccordingly, as noted in the FAQ.þþ## Finding modulesþþCheck out https://luarocks.org/search?q=mjolnir for a list ofþpublished Mjolnir modules.þþNotable modules:þþ- `mjolnir.hotkey` for creating global hotkeysþ- `mjolnir.application` for inspecting and manipulating running OS X applications and windowsþ- `mjolnir.alert` for showing on-screen messagesþþ## DocumentationþþMjolnir and mjolnir-modules use [Dash](http://kapeli.com/dash) forþdocumentation. You can install Mjolnir's docset from the UserþContributed section of the Downloads tab in Dash's Preferencesþwindow. It should generally update on its own.þþ## Publishing modulesþþWrote an awesome module, and want to share with the world? Check outþthe `sample-plugin` subdirectory.þþWhen it's published, please announce it on our mailing list :)þþAvoid the temptation to reformat the Repos page in the wiki. It usesþa strict format that's needed by the documentation generator.þþ## PrinciplesþþDevelopment of Mjolnir.app and the core Mjolnir modules follow theseþprinciples:þþ1. They must be stable. The app should never crash. You should onlyþ   ever have to launch it once, and it should stay running until youþ   quit. Period.þþ2. They must be lightweight. They should never do anything that drainsþ   your computer's battery. They should never poll for anything. Theyþ   should use as little RAM as possible. Everything they do shouldþ   feel instant and snappy, never sluggish or delayed.þþ3. They should be completely transparent. There should be no surprisesþ   in how it's behaving, or what's being executed and when. Everythingþ   should be fully predictable.þþ4. They must not be bloated. The app and core modules must alwaysþ   adhere to the minimalist philosophy, no excuses. Everything elseþ   can be a separate Lua module.þþ## FAQþþ1. **Add Spaces support!**þþ   Not a question. But anyway, there are no public *or* private APIsþ   to manage Spaces in OS X 10.9 or 10.10, there are only private APIsþ   that work in 10.8. No open source project has been able to crackþ   this problem yet, not just us.þþ2. **Does LuaRocks have a way to upgrade modules automatically?**þþ   Sadly no. But it can be done manually by removing a module andþ   re-installing it. I'm hoping maybe one day some enthusiasticþ   Mjolnir users can jump in and improve the tooling around this. ;)þþ3. **I'm getting an error like this: ""attempt to index field 'win' (a nil value)""**þþ   Disable and re-enable accessibility. It may look enabled, but do itþ   anyway. (This is an OS X bug, not a Mjolnir bug.)þþ4. **I don't have things in /usr/local, so I can't load modules!**þþ   Add the path to `package.path` and `package.cpath` in yourþ   init-file. For example, if you're using Boxen, add this:þþ   ~~~luaþ   package.path = package.path .. ';/opt/boxen/homebrew/share/lua/5.3/?.lua'þ   package.cpath = package.cpath .. ';/opt/boxen/homebrew/lib/lua/5.3/?.so'þ   ~~~þþ## Mjolnir vs. other appsþþ1. **Hydra, Phoenix, or Zephyros?**þþ   Those are my old apps. Mjolnir is their spiritual successor.þþ2. **Slate**þþ   They're both programmer-centric with somewhat similar goals butþ   different approaches. Mjolnir is more modularized, Slate is moreþ   all-in-one. Try them both and see which one suits you better.þþ3. **Spectacle, Moom, SizeUp, Divvy**þþ   Mjolnir is intended for programmers who want to write programs thatþ   customize their environment. It's not intended to be a drag-n-dropþ   solution; it's meant to allow you to write your own personalizedþ   productivity enhancement suite to keep and to use long-term.þþ4. **Hammerspoon**þþ   [Hammerspoon](https://github.com/hammerspoon) is a fork of Mjolnir (get it? a ""fork and/or spoon"" of Mjolnir aka. Thor's ""hammer""? :) ). It was created to turn Mjolnir back into an all-in-one application, for those who prefer that over a completely decentralized module system with a bare-bones core (kind of like the debate of monolithic kernel vs microkernel). It's actively maintained, like literally there's commit activit every week.þþ## CommunityþþOur [mailing list](https://groups.google.com/forum/#!forum/mjolnir-io)þis a fine place to share ideas, and follow releases and announcements.þþWe also have a shrinking IRC channel on freenode, #mjolnir.þþ## Credits and ThanksþþMjolnir is developed by Steven Degutis with the help ofþ[various contributors](https://github.com/sdegutis/mjolnir/graphs/contributors).þþSpecial thanks, in no special order:þþ- @Habbie for his constant help and support ever since the moment Iþ  first jumped into #lua and said ""anyone wanna try out an OS X windowþ  manager scriptable in Lua?"" and being the first person to join ourþ  IRC channel and for helping with nearly every Lua question I hadþþ- @cmsj, @Keithbsmiley, @BrianGilbert, @muescha, @chdiza, @asmagill,þ  @splintax, @arxanas, and @DomT4 for all their help and supportþþ- @jasonm23 for writing, not one, not two, but *three* app icons forþ  this projectþþ- @jhgg for contributing so many awesome modules to the projectþþ- @kapeli for his patience with my constant Dash questions and PRsþþ- Everyone else who has helped who I've probably forgotten: thanks forþ  all your help!þþ- Everyone who has donated: thank you so much for your support!þþSee the in-app About panel for the open source licenses for theþsoftware Mjolnir uses internally (basically just Lua's license).þþ## Changesþþ**NOTE:** When upgrading, System Preferences will *pretend* likeþ  Mjolnir's accessibility is enabled, showing a checked checkbox. Butþ  in fact, you'll still need to be disable and re-enable it. This is aþ  bug in OS X.þþ### 0.4.3þþ- Removed donation requestsþþ### 0.4.{0,1,2}þþ- Default implementation of `mjolnir.showerror(err)` now opens the console and focuses Mjolnirþ- There's a new variable, `mjolnir.configdir = ""~/.mjolnir/""` for users and modules to coordinateþ- New `mjolnir.focus()` function to make Mjolnir the focused appþ- The original `print` function is now stored in `mjolnir.rawprint` (rather than `mjolnir.print`, to disambiguate it)þ- New `mjolnir.openconsole()` function to open console (and bring Mjolnir to front)þþ### 0.3.1þþ- Renamed global `mj` to `mjolnir`þþ### 0.3þþ- The UI has changed drastically. Expect nothing to be in the sameþ  place or look the same. Pretend it's a brand new app.þ- Modules are now handled by LuaRocks instead of by the app itself.þ- The ""core"" namespace has been renamed to ""mj"".þ- The 'mj.window' module now ships with the 'mj.application' LuaRocksþ  package since they depend on each other.þ- `mj.screen:frame_without_dock_or_menu()` is now called `mj.screen:frame()`þ- `mj.screen:frame_including_dock_and_menu()` is now called `mj.screen:fullframe()`þþ## Licenseþþ> Released under MIT license.þ>þ> Copyright (c) 2014 Steven Degutisþ>þ> Permission is hereby granted, free of charge, to any person obtaining a copyþ> of this software and associated documentation files (the ""Software""), to dealþ> in the Software without restriction, including without limitation the rightsþ> to use, copy, modify, merge, publish, distribute, sublicense, and/or sellþ> copies of the Software, and to permit persons to whom the Software isþ> furnished to do so, subject to the following conditions:þ>þ> The above copyright notice and this permission notice shall be included inþ> all copies or substantial portions of the Software.þ>þ> THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS ORþ> IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,þ> FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THEþ> AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHERþ> LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,þ> OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS INþ> THE SOFTWARE."
antirez/disque,2493,7668,444,528,User,False,602,1,1,53,False,Disque is a distributed message broker,,0,11,1,42,64,0,0,18,92,1,0,2049,0,0,0,0,0,0,63,,14,,"[![Build Status](https://travis-ci.org/antirez/disque.svg)](https://travis-ci.org/antirez/disque)þþDisque, an in-memory, distributed job queueþ===þþDisque is an ongoing experiment to build a distributed, in-memory, messageþbroker.þIts goal is to capture the essence of the ""Redis as a jobs queue"" use case,þwhich is usually implemented using blocking list operations, and moveþit into an ad-hoc, self-contained, scalable, and fault tolerant design, withþsimple to understand properties and guarantees, but still resembling Redisþin terms of simplicity, performance, and implementation as a C non-blockingþnetworked server.þþCurrently (2 Jan 2016) the project is in release candidate state. People areþencouraged to start evaluating it and report bugs and experiences.þþ**WARNING: This is beta code and may not be suitable for production usage. The API is considered to be stable if not for details that may change in the next release candidates, however it's new code, so handle with care!**þþWhat is a message queue?þ---þþ*Hint: skip this section if you are familiar with message queues.*þþYou know how humans use text messages to communicate, right? I could writeþmy wife ""please get the milk at the store"", and she maybe will reply ""Ok messageþreceived, I'll get two bottles on my way home"".þþA message queue is the same as human text messages, but for computer programs.þFor example a web application, when an user subscribes, may send anotherþprocess, that handles sending emails, ""please send the confirmation emailþto tom@example.net"".þþMessage systems like Disque allow communication between processes usingþdifferent queues. So a process can send a message to a queue with a givenþname, and only processes which fetch messages from this queue will return thoseþmessages. Moreover, multiple processes can listen for messages in a givenþqueue, and multiple processes can send messages to the same queue.þþThe important part of a message queue is to be able to provide guarantees soþthat messages are eventually delivered even in the face of failures. So evenþif in theory implementing a message queue is very easy, to write a veryþrobust and scalable one is harder than it may appear.þþGive me the details!þ---þþDisque is a distributed and fault tolerant message broker, so it works as aþmiddle layer among processes that want to exchange messages.þþProducers add messages that are served to consumers.þSince message queues are often used in order to process delayedþjobs, Disque often uses the term ""job"" in the API and in the documentation,þhowever jobs are actually just messages in the form of strings, so Disqueþcan be used for other use cases. In this documentation ""jobs"" and ""messages""þare used in an interchangeable way.þþJob queues with a producer-consumer model are pretty common, so the devil isþin the details. A few details about Disque are:þþDisque is a **synchronously replicated job queue**. By default when a new job is added, it is replicated to W nodes before the client gets an acknowledgement about the job being added. W-1 nodes can fail and the message will still be delivered.þþDisque supports both **at-least-once and at-most-once** delivery semantics. At-least-once delivery semantics is where most effort was spent in the design and implementation, while at-most-once semantics is a trivial result of using a *retry time* set to 0 (which means, never re-queue the message again) and a replication factor of 1 for the message (not strictly needed, but it is useless to have multiple copies of a message around if it will be delivered at most one time). You can have, at the same time, both at-least-once and at-most-once jobs in the same queues and nodes, since this is a per message setting.þþDisque at-least-once delivery is designed to **approximate single delivery** when possible, even during certain kinds of failures. This means that while Disque can only guarantee a number of deliveries equal or greater to one, it will try hard to avoid multiple deliveries whenever possible.þþDisque is a distributed system where **all nodes have the same role** (aka, it is multi-master). Producers and consumers can attach to whatever node they like, and there is no need for producers and consumers of the same queue to stay connected to the same node. Nodes will automatically exchange messages based on load and client requests.þþDisque is Available (it is an eventually consistent AP system in CAP terms): producers and consumers can make progress as long as a single node is reachable.þþDisque supports **optional asynchronous commands** that are low latency for the client but provide less guarantees. For example a producer can add a job to a queue with a replication factor of 3, but may want to run away before knowing if the contacted node was really able to replicate it to the specified number of nodes or not. The node will replicate the message in the background in a best effort way.þþDisque **automatically re-queues messages that are not acknowledged** as already processed by consumers, after a message-specific retry time. There is no need for consumers to re-queue a message if it was not processed.þþDisque uses **explicit acknowledges** in order for a consumer to signal a message as delivered (or, using a different terminology, to signal a job as already processed).þþDisque queues only provides **best effort ordering**. Each queue sorts messages based on the job creation time, which is obtained using the *wall clock* of the local node where the message was created (plus an incremental counter for messages created in the same millisecond), so messages created in the same node are normally delivered in the same order they were created. This is not causal ordering since correct ordering is violated in different cases: when messages are re-issued because they are not acknowledged, because of nodes local clock drifts, and when messages are moved to other nodes for load balancing and federation (in this case you end with queues having jobs originated in different nodes with different wall clocks). However all this also means that normally messages are not delivered in random order and usually messages created first are delivered first.þþNote that since Disque does not provide strict FIFO semantics, technically speaking it should not be called a *message queue*, and it could better identified as a message broker. However I believe that at this point in the IT industry a *message queue* is often more lightly used to identify a generic broker that may or may not be able to guarantee order in all cases. Given that we document the semantics very clearly, I grant myself the right to call Disque a message queue anyway.þþDisque provides the user with fine-grained control for each job **using three time related parameters**, and one replication parameter. For each job, the user can control:þþ1. The replication factor (how many nodes have a copy).þ2. The delay time (the min time Disque will wait before putting the message in a queue, making the message deliverable).þ3. The retry time (how much time should elapse since the last time the job was queued and without an acknowledge about the job delivery, before the job is re-queued for delivery).þ4. The expire time (how much time should elapse for the job to be deleted regardless of whether it was successfully delivered, i.e. acknowledged, or not).þþFinally, Disque supports optional disk persistence, which is not enabled by default, but that can be handy in single data center setups and during restarts.þþOther minor features are:þþ* Ability to block queues.þ* A few statistics about queue activity.þ* Stateless iterators for queues and jobs.þ* Commands to control the visibility of single jobs.þ* Easy resize of the cluster (adding nodes is trivial).þ* Graceful removal of nodes without losing job replicas.þþACKs and retriesþ---þþDisque's implementation of *at-least-once* delivery semantics is designed inþorder to avoid multiple delivery during certain classes of failures. It is not able to guarantee that no multiple deliveries will occur. However there are many *at-least-once* workloads where duplicated deliveries are acceptable (or explicitly handled), but not desirable either. A trivial example is sending emails to users (it is not terrible if an user gets a duplicated email, but is important to avoid it when possible), or doing idempotent operations that are expensive (all the times where it is critical for performance to avoid multiple deliveries).þþIn order to avoid multiple deliveries when possible, Disque uses client ACKs. When a consumer processes a message correctly, it should acknowledge this fact to Disque. ACKs are replicated to multiple nodes, and are garbage collected as soon as the system believes it is unlikely that more nodes in the cluster have the job (the ACK refers to) still active. Under memory pressure or under certain failure scenarios, ACKs are eventually discarded.þþMore explicitly:þþ1. A job is replicated to multiple nodes, but usually only *queued* in a single node. There is a difference between having a job in memory, and queueing it for delivery.þ2. Nodes having a copy of a message, if a certain amount of time has elapsed without getting the ACK for the message, will re-queue it. Nodes will run a best-effort protocol to avoid re-queueing the message multiple times.þ3. ACKs are replicated and garbage collected across the cluster so that eventually processed messages are evicted (this happens ASAP if there are no failures nor network partitions).þþFor example, if a node having a copy of a job gets partitioned away during the time the job gets acknowledged by the consumer, it is likely that when it returns (in a reasonable amount of time, that is, before the retry time is reached) it will be informed about the ACK and will avoid to re-queue the message. Similarly, jobs can be acknowledged during a partition to just a single available node, and when the partition heals the ACK will be propagated to other nodes that may still have a copy of the message.þþSo an ACK is just a **proof of delivery** that is replicated and retained forþsome time in order to make multiple deliveries less likely to happen in practice.þþAs already mentioned, in order to control replication and retries, a Disque job has the following associated properties: number of replicas, delay, retry and expire.þþIf a job has a retry time set to 0, it will get queued exactly once (and in this case a replication factor greater than 1 is useless, and signaled as an error to the user), so it will get delivered either a single time or will never get delivered. While jobs can be persisted on disk for safety, queues aren't, so this behavior is guaranteed even when nodes restart after a crash, whatever the persistence configuration is. However when nodes are manually restarted by the sysadmin, for example for upgrades, queues are persisted correctly and reloaded at startup, since the store/load operation is atomic in this case, and there are no race conditions possible (it is not possible that a job was delivered to a client and is persisted on disk as queued at the same time).þþFast acknowledgesþ---þþDisque supports a faster way to acknowledge processed messages, via theþ`FASTACK` command. The normal acknowledge is very expensive from the point ofþview of messages exchanged between nodes, this is what happens during a normalþacknowledge:þþ1. The client sends ACKJOB to one node.þ2. The node sends a SETACK message to everybody it believes to have a copy.þ3. The receivers of SETACK reply with GOTACK to confirm.þ4. The node finally sends DELJOB to all the nodes.þþ*Note: actual garbage collection is more complex in case of failures and is explained in the state machine later. The above is what happens 99% of times.*þþIf a message is replicated to 3 nodes, acknowledging requires 1+2+2+2 messages,þfor the sake of retaining the ack if some nodes may not be reached when theþmessage is acknowledged. This makes the probability of multiple deliveries ofþthis message less likely.þþHowever the alternative **fast ack**, while less reliable, is much fasterþand invovles exchanging less messages. This is how a fast acknowledge works:þþ1. The client sends `FASTACK` to one node.þ2. The node evicts the job and sends a best effort DELJOB to all the nodes that may have a copy, or to all the cluster if the node was not aware of the job.þþIf during a fast acknowledge a node having a copy of the message is notþreachable, for example because of a network partition, the node will deliverþthe message again, since it has a non-acknowledged copy of the message andþthere is nobody able to inform it the message has been acknowledged when theþpartition heals.þþIf the network you are using is pretty reliable, and you are very concerned withþperformance, and multiple deliveries in the context of your applications areþa non issue, then `FASTACK` is probably the way to go.þþþDead letter queueþ---þþMany message queues implement a feature called *dead letter queue*. It isþa special queue used in order to accumulate messages that cannot be processedþfor some reason. Common causes could be:þþ1. The message was delivered too many times but never correctly processed.þ2. The message time-to-live reached zero before it was processed.þ3. Some worker explicitly asked the system to flag the message as having issues.þþThe idea is that the administrator of the system checks (usually via automaticþsystems) if there is something in the dead letter queue in order to understandþif there is some software error or other kind of error preventing messagesþfrom being processed as expected.þþSince Disque is an in-memory system, the message time-to-live is an importantþproperty. When it is reached, we want messages to go away, since the TTL shouldþbe chosen so that after such a time it is no longer meaningful to processþthe message. In such a system, to use memory and create a queue in responseþto an error or to messages timing out looks like a non optimal idea. Moreover,þdue to the distributed nature of Disque, dead letters could end up spawningþmultiple nodes and having duplicated entries in them.þþSo Disque uses a different approach. Each node message representation hasþtwo counters: a **nacks counter** and an **additional deliveries** counter.þThe counters are not consistent among nodes having a copy of the same message,þthey are just best effort counters that may not increment in some node duringþnetwork partitions.þþThe idea of these two counters is that one is incremented every time a workerþuses the `NACK` command to tell the queue the message was not processed correctlyþand should be put back on the queue ASAP. The other is incremented for every other condition (different than the `NACK` call) that requires a message to be put backþon the queue again. This includes messages that get lost and are enqueued againþor messages that are enqueued on one side of the partition since the messageþwas processed on the other side and so forth.þþUsing the `GETJOB` command with the `WITHCOUNTERS` option, or using theþ`SHOW` command to inspect a job, it is possible to retrieve these two countersþtogether with the other job information, so if a worker, before processingþa message, sees the counters have values over some application-defined limit, itþcan notify operations people in multiple ways:þþ1. It may send an email.þ2. Set a flag in a monitoring system.þ3. Put the message in a special queue (simulating the dead letter feature).þ4. Attempt to process the message and report the stack trace of the error if any.þþBasically the exact handling of the feature is up to the application usingþDisque. Note that the counters don't need to be consistent in the face ofþfailures or network partitions: the idea is that eventually if a message hasþissues the counters will get incremented enough times to reach the limitþselected by the application as a warning threshold.þþThe reason for having two distinct counters is that applications may wantþto handle the case of explicit negative acknowledges via `NACK` differentlyþthan multiple deliveries because of timeouts or messages getting lost.þþDisque and disk persistenceþ---þþDisque can be operated in-memory only, using synchronous replication as aþdurability guarantee, or can be operated using the Append Only File whereþjobs creations and evictions are logged on disk (with configurable fsyncþpolicies) and reloaded at restart.þþAOF is recommended especially if you run in a single availability zoneþwhere a mass reboot of all your nodes is possible.þþNormally Disque only reloads job data in memory, without populating queues,þsince unacknowledged jobs are requeued eventually. Moreover, reloadingþqueue data is not safe in the case of at-most-once jobs having the retry valueþset to 0. However a special option is provided in order to reload the fullþstate from the AOF. This is used together with an option that allows shuttingþdown the server just after the AOF is generated from scratch, in order toþmake it safe even to reload jobs with retry set to 0, since the AOF is generatedþwhile the server no longer accepts commands from clients, so no race conditionþis possible.þþEven when running memory-only, Disque is able to dump its memory on disk and reload from disk on controlled restarts, for example in order to upgrade the software.þþThis is how to perform a controlled restart, that works whether AOF is enabledþor not:þþ1. CONFIG SET aof-enqueue-jobs-once yesþ2. CONFIG REWRITEþ3. SHUTDOWN REWRITE-AOFþþAt this point we have a freshly generated AOF on disk, and the server isþconfigured in order to load the full state only at the next restartþ(`aof-enqueue-jobs-once` is automatically turned off after the restart).þþWe can just restart the server with the new software, or in a new server, andþit will restart with the full state. Note that `aof-enqueue-jobs-once`þimplies loading the AOF even if AOF support is switched off, so there isþno need to enable AOF just for the upgrade of an in-memory only server.þþJob IDsþ---þþDisque jobs are uniquely identified by an ID like the following:þþ    D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1þþJob IDs are composed of exactly 40 characters and start with the prefix `D-`.þþWe can split an ID into multiple parts:þþ    D- | dcb833cf | 8YL1NT17e9+wsA/09NqxscQI | 05a1þþ1. `D-` is the prefix.þ2. `dcb833cf` is the first 8 bytes of the node ID where the message was generated.þ3. `8YL1NT17e9+wsA/09NqxscQI` is the 144 bit ID pseudo-random part encoded in base64.þ4. `05a1` is the Job TTL in minutes. Because of it, message IDs can be expired safely even without having the job representation.þþIDs are returned by ADDJOB when a job is successfully created, are part ofþthe GETJOB output, and are used in order to acknowledge that a job wasþcorrectly processed by a worker.þþPart of the node ID is included in the message so that a worker processingþmessages for a given queue can easily guess what are the nodes where jobsþare created, and move directly to these nodes to increase efficiency insteadþof listening for messages in a node that will require to fetch messages fromþother nodes.þþOnly 32 bits of the original node ID is included in the message, howeverþin a cluster with 100 Disque nodes, the probability of two nodes havingþidentical 32 bit ID prefixes is given by the birthday paradox:þþ    P(100,2^32) = .000001164þþIn case of collisions, the workers may just make a non-efficient choice.þþCollisions in the 144 bits random part are believed to be impossible,þsince it is computed as follows.þþ    144 bit ID = HIGH_144_BITS_OF_SHA1(seed || counter)þþWhere:þþ* **seed** is a seed generated via `/dev/urandom` at startup.þ* **counter** is a 64 bit counter incremented at every ID generation.þþSo there are 22300745198530623141535718272648361505980416 possible IDs,þselected in a uniform way. While the probability of a collision is non-zeroþmathematically, in practice each ID can be regarded as unique.þþThe encoded TTL in minutes has a special property: it is always even forþat most once jobs (job retry value set to 0), and is always odd otherwise.þThis changes the encoded TTL precision to 2 minutes, but allows to tellþif a Job ID is about a job with deliveries guarantees or not.þNote that this fact does not mean that Disque jobs TTLs have a precision ofþtwo minutes. The TTL field is only used to expire job IDs of jobs a givenþnode does not actually have a copy, search ""dummy ACK"" in this documentationþfor more information.þþSetupþ===þþTo play with Disque please do the following:þþ1. Compile Disque - if you can compile Redis, you can compile Disque, it's the usual ""no external deps"" thing. Just type `make`. Binaries (`disque` and `disque-server`) will end up in the `src` directory.þ2. Run a few Disque nodes on different ports. Create different `disque.conf` files following the example `disque.conf` in the source distribution.þ3. After you have them running, you need to join the cluster. Just select a random node among the nodes you are running, and send the command `CLUSTER MEET <ip> <port>` for every other node in the cluster.þþ**Please note that you need to open two TCP ports on each node**, the base port of the Disque instance, for example 7711, plus the cluster bus port, which is always at a fixed offset, obtained summing 10000 to the base port, so in the above example, you need to open both 7711 and 17711. Disque uses the base port to communicate with clients and the cluster bus port to communicate with other Disque processes.þþTo run a node, just call `./disque-server`.þþFor example, if you are running three Disque servers in port 7711, 7712, 7713, in order to join the cluster you should use the `disque` command line tool and run the following commands:þþ    ./disque -p 7711 cluster meet 127.0.0.1 7712þ    ./disque -p 7711 cluster meet 127.0.0.1 7713þþYour cluster should now be ready. You can try to add a job and fetch it backþin order to test if everything is working:þþ    ./disque -p 7711þ    127.0.0.1:7711> ADDJOB queue body 0þ    D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1þ    127.0.0.1:7711> GETJOB FROM queueþ    1) 1) ""queue""þ       2) ""D-dcb833cf-8YL1NT17e9+wsA/09NqxscQI-05a1""þ       3) ""body""þþRemember that you can add and get jobs from different nodes as Disqueþis multi master. Also remember that you need to acknowledge jobs otherwiseþthey'll never go away from the server memory (unless the time-to-live isþreached).þþMain APIþ===þþThe Disque API is composed of a small set of commands, since the system solves aþsingle very specific problem. The three main commands are:þþ#### `ADDJOB queue_name job <ms-timeout> [REPLICATE <count>] [DELAY <sec>] [RETRY <sec>] [TTL <sec>] [MAXLEN <count>] [ASYNC]`þþAdds a job to the specified queue. Arguments are as follows:þþ* *queue_name* is the name of the queue, any string, basically. You don't need to create queues, if a queue does not exist, it gets created automatically. If one has no more jobs, it gets removed.þ* *job* is a string representing the job. Disque is job meaning agnostic, for it a job is just a message to deliver. Job max size is 4GB.þ* *ms-timeout* is the command timeout in milliseconds. If no ASYNC is specified, and the replication level specified is not reached in the specified number of milliseconds, the command returns with an error, and the node does a best-effort cleanup, that is, it will try to delete copies of the job across the cluster. However the job may still be delivered later. Note that the actual timeout resolution is 1/10 of second or worse with the default server hz.þ* *REPLICATE count* is the number of nodes the job should be replicated to.þ* *DELAY sec* is the number of seconds that should elapse before the job is queued by any server. By default there is no delay.þ* *RETRY sec* period after which, if no ACK is received, the job is put into the queue again for delivery. If RETRY is 0, the job has at-most-once delivery semantics. The default retry time is 5 minutes, with the exception of jobs having a TTL so small that 10% of TTL is less than 5 minutes. In this case the default RETRY is set to TTL/10 (with a minimum value of 1 second).þ* *TTL sec* is the max job life in seconds. After this time, the job is deleted even if it was not successfully delivered. If not specified, the default TTL is one day.þ* *MAXLEN count* specifies that if there are already *count* messages queued for the specified queue name, the message is refused and an error reported to the client.þ* *ASYNC* asks the server to let the command return ASAP and replicate the job to other nodes in the background. The job gets queued ASAP, while normally the job is put into the queue only when the client gets a positive reply.þþThe command returns the Job ID of the added job, assuming ASYNC is specified, or if the job was replicated correctly to the specified number of nodes. Otherwise an error is returned.þþ#### `GETJOB [NOHANG] [TIMEOUT <ms-timeout>] [COUNT <count>] [WITHCOUNTERS] FROM queue1 queue2 ... queueN`þþReturn jobs available in one of the specified queues, or return NULLþif the timeout is reached. A single job per call is returned unless a count greater than 1 is specified. Jobs are returned as a three-element array containing the queue name, the Job ID, and the job body itself. If jobs are available in multiple queues, queues are processed left to right.þþIf there are no jobs for the specified queues, the command blocks, and messages are exchanged with other nodes, in order to move messages about these queues to this node, so that the client can be served.þþOptions:þþ* **NOHANG**: Ask the command to not block even if there are no jobs in all the specified queues. This way the caller can just check if there are available jobs without blocking at all.þ* **WITHCOUNTERS**: Return the best-effort count of NACKs (negative acknowledges) received by this job, and the number of additional deliveries performed for this job. See the *Dead Letters* section for more information.þþ#### `ACKJOB jobid1 jobid2 ... jobidN`þþAcknowledges the execution of one or more jobs via job IDs. The node receiving the ACK will replicate it to multiple nodes and will try to garbage collect both the job and the ACKs from the cluster so that memory can be freed.þþA node receiving an ACKJOB command about a job ID it does not know will createþa special empty job, with the state set to ""acknowledged"", called a ""dummy ACK"".þThe dummy ACK is used in order to retain the acknolwedge during a netsplit ifþthe ACKJOB is sent to a node that does not have a copy of the job. When theþpartition heals, job garbage collection will be attempted.þþHowever, since the job ID encodes information about the job being an ""at-most-þonce"" or an ""at-least-once"" job, the dummy ACK is only created for at-least-þonce jobs.þþ#### `FASTACK jobid1 jobid2 ... jobidN`þþPerforms a best-effort cluster-wide deletion of the specified job IDs. When theþnetwork is well connected and there are no node failures, this is equivalent toþ`ACKJOB` but much faster (due to less messages being exchanged), however duringþfailures it is more likely that fast acknowledges will result in multipleþdeliveries of the same messages.þþ#### `WORKING jobid`þþClaims to be still working with the specified job, and asks Disque to postponeþthe next time it will deliver the job again. The next delivery is postponedþfor the job retry time, however the command works in a **best effort** wayþsince there is no way to guarantee during failures that another node in aþdifferent network partition won't perform a delivery of the same job.þþAnother limitation of the `WORKING` command is that it cannot be sent toþnodes not knowing about this particular job. In such a case the command repliesþwith a `NOJOB` error. Similarly, if the job is already acknowledged an errorþis returned.þþNote that the `WORKING` command is refused by Disque nodes if 50% of the jobþtime to live has already elapsed. This limitation makes Disque safer sinceþusually the *retry* time is much smaller than the time-to-live of a job, soþit can't happen that a set of broken workers monopolize a job with `WORKING`þand never process it. After 50% of the TTL has elapsed, the job will be deliveredþto other workers anyway.þþNote that `WORKING` returns the number of seconds you (likely) postponed theþmessage visibility for other workers (the command basically returns theþ*retry* time of the job), so the worker should make sure to send the nextþ`WORKING` command before this time elapses. Moreover, a worker that may wantþto use this interface may fetch the retry value with the `SHOW` commandþwhen starting to process a message, or may simply send a `WORKING` commandþASAP, like in the following example (in pseudo code):þþ    retry = WORKING(jobid)þ    RESET timerþ    WHILE ... work with the job still not finished ...þ        IF timer reached 80% of the retry timeþ            WORKING(jobid)þ            RESET timerþ        ENDþ    ENDþþ#### `NACK <job-id> ... <job-id>`þþThe `NACK` command tells Disque to put the job back in the queue ASAP. Itþis very similar to `ENQUEUE` but it increments the job `nacks` counterþinstead of the `additional-deliveries` counter. The command should be usedþwhen the worker was not able to process a message and wants the message toþbe put back into the queue in order to be processed again.þþOther commandsþ===þþ#### `INFO`þþGeneric server information / stats.þþ#### `HELLO`þþReturns hello format version, this node ID, all the nodes IDs, IP addresses,þports, and priority (lower is better, means a node is more available).þClients should use this as a handshake command when connecting with aþDisque node.þþ#### `QLEN <queue-name>`þþReturn the length of the queue.þþ#### `QSTAT <queue-name>`þþShow information about a queue as an array of key-value pairs.þBelow is an example of the output, however, implementations should not relyþon the order of the fields nor on the existence of the fields listed.þThey may be (unlikely) removed or more can be (likely) addedþin the future.þþIf a queue does not exist, NULL is returned. Note that queues areþautomatically evicted after some time if empty and without clients blockedþwaiting for jobs, even if there are active jobs for the queue. So theþnon existence of a queue does not mean there are not jobs in the nodeþor in the whole cluster about this queue. The queue will be immediatelyþcreated again when needed to serve requests.þþExample output:þþ```þQSTAT fooþ 1) ""name""þ 2) ""foo""þ 3) ""len""þ 4) (integer) 56520þ 5) ""age""þ 6) (integer) 601þ 7) ""idle""þ 8) (integer) 3þ 9) ""blocked""þ10) (integer) 50þ11) ""import-from""þ12) 1) ""dcb833cf8f42fbb7924d92335ff6d67d3cea6e3d""þ    2) ""4377bdf656040a18d8caf4d9f409746f1f9e6396""þ13) ""import-rate""þ14) (integer) 19243þ15) ""jobs-in""þ16) (integer) 3462847þ17) ""jobs-out""þ18) (integer) 3389522þ19) ""pause""þ20) ""none""þ```þþMost fields should be obvious. The `import-from` field shows a list of nodeþIDs this node is importing jobs from, for this queue, in order to serveþclients requests. The `import-rate` is the instantaneous amount of jos/secþwe import in order to handle our outgoing traffic (GETJOB commands).þ`blocked` is the number of clients blocked on this queue right now.þ`age` and `idle` are reported in seconds. The `jobs-in` and `-out` counters areþincremented every time a job is enqueued or dequeued for any reason.þþ#### `QPEEK <queue-name> <count>`þþReturn, without consuming from the queue, *count* jobs. If *count* is positiveþthe specified number of jobs are returned from the oldest to the newestþ(in the same best-effort FIFO order as GETJOB). If *count* is negative theþcommands changes behavior and shows the *count* newest jobs, from the newestþfrom the oldest.þþ#### `ENQUEUE <job-id> ... <job-id>`þþQueue jobs if not already queued.þþ#### `DEQUEUE <job-id> ... <job-id>`þþRemove the job from the queue.þþ#### `DELJOB <job-id> ... <job-id>`þþCompletely delete a job from a node.þNote that this is similar to `FASTACK`, but limited to a single node sinceþno `DELJOB` cluster bus message is sent to other nodes.þþ#### `SHOW <job-id>`þþDescribe the job.þþ#### `QSCAN [COUNT <count>] [BUSYLOOP] [MINLEN <len>] [MAXLEN <len>] [IMPORTRATE <rate>]`þþThe command provides an interface to iterate all the existing queues inþthe local node, providing a cursor in the form of an integer that is passedþto the next command invocation. During the first call, the cursor must be 0,þin the next calls the cursor returned in the previous call is used in theþnext. The iterator guarantees to return all the elements but may returnþduplicated elements.þþOptions:þþ* `COUNT <count>` A hint about how much work to do per iteration.þ* `BUSYLOOP` Block and return all the elements in a busy loop.þ* `MINLEN <count>` Don't return elements with less than `count` jobs queued.þ* `MAXLEN <count>` Don't return elements with more than `count` jobs queued.þ* `IMPORTRATE <rate>` Only return elements with a job import rate (from other nodes) `>=` `rate`.þþThe cursor argument can be in any place, the first non matching optionþthat has valid cursor form of an unsigned number will be sensed as a validþcursor.þþ#### `JSCAN [<cursor>] [COUNT <count>] [BUSYLOOP] [QUEUE <queue>] [STATE <state1> STATE <state2> ... STATE <stateN>] [REPLY all|id]`þþThe command provides an interface to iterate all the existing jobs inþthe local node, providing a cursor in the form of an integer that is passedþto the next command invocation. During the first call the cursor must be 0,þin the next calls the cursor returned in the previous call is used in theþnext. The iterator guarantees to return all the elements but may returnþduplicated elements.þþOptions:þþ* `COUNT <count>` A hint about how much work to do per iteration.þ* `BUSYLOOP` Block and return all the elements in a busy loop.þ* `QUEUE <queue>` Return only jobs in the specified queue.þ* `STATE <state>` Return jobs in the specified state. Can be used multiple times for a logical OR.þ* `REPLY <type>` Job reply type. Type can be `all` or `id`. Default is to report just the job ID. If `all` is specified the full job state is returned like for the SHOW command.þþThe cursor argument can be in any place, the first non matching optionþthat has valid cursor form of an unsigned number will be sensed as a validþcursor.þþ#### `PAUSE <queue-name> option1 [option2 ... optionN]`þþControl the paused state of a queue, possibly broadcasting the command toþother nodes in the cluster. Disque queues can be paused in both directions,þinput and output, or both. Pausing a queue makes it unavailable for inputþor output operations. Specifically:þþA queue paused in input will have changed behavior in the following ways:þþ1. ADDJOB returns a `-PAUSED` error for queues paused in input.þ2. The node where the queue is paused, no longer accepts to replicate jobs for this queue when requested by other nodes. Since ADDJOB by default uses synchronous replication, it means that if the queue is paused in enough nodes, adding jobs with a specified level of replication may fail. In general the node where the queue is paused will not create new jobs in the local node about this queue.þ3. The job no longer accepts ENQUEUE messages from other nodes. Those messages are usually used by nodes in out of memory conditions that replicate jobs externally (not holding a copy), in order to put the job in the queue of some random node, among the nodes having a copy of a job.þ4. Active jobs that reach their retry time, are not put back into the queue. Instead their retry timer is updated and the node will try again later.þþBasically a queue paused in input never creates new jobs for this queue, and never puts active jobs (jobs for which the node has a copy but are not currently queued) back in the queue, for all the time the queue is paused.þþA queue paused in output instead will behave in the following way:þþ1. GETJOB will block even if there are jobs available in the specified queue, instead of serving the jobs. But GETJOB will unblock if the queue output pause is cleared later.þ2. The node will not provide jobs to other nodes in the context of node federation, for paused queues.þþSo a queue paused in output will stop acting as a source of messages for bothþlocal and non local clients.þþThe paused state can be set for each queue using the PAUSE command followedþby options to specify how to change the paused state. Possible options are:þþ* **in**: pause the queue in input.þ* **out**: pause the queue in output.þ* **all**: pause the queue in input and output (same as specifying both the **in** and **out** options).þ* **none**: clear the paused state in input and output.þ* **state**: just report the current queue state.þ* **bcast**: send a PAUSE command to all the reachable nodes of the cluster to set the same queue in the other nodes to the same state.þþThe command always returns the state of the queue after the execution of the specified options, so the return value is one of **in**, **out**, **all**, **none**.þþQueues paused in input or output are never evicted to reclaim memory, even ifþthey are empty and inactive for a long time, since otherwise the paused stateþwould be forgotten.þþFor example, in order to block output for the queue `myqueue` in all theþcurrently reachable nodes, the following command should be send to a single node:þþ    PAUSE myqueue out bcastþþTo specify **all** is the same as to specify both **in** and **out**, so the two followingþforms are equivalent:þþ    PAUSE myqueue in outþ    PAUSE myqueue allþþTo just get the current state use:þþ    PAUSE myqueue stateþ    ""none""þþSpecial handling of messages with RETRY set to 0þ===þþIn order to provide a coherent API, messages with at-most-once deliveryþsemantics are still retained after being delivered a first time, and shouldþbe acknowledged like any other message. Of course, the acknowledge is notþmandatory, since the message may be lost and there is no way for the receiverþto get the same message again, since the message is associated with a retryþvalue of 0.þþIn order to avoid non acknowledged messages with retry set to 0 from leakingþinto Disque and eating all the memory, when the Disque server memory is fullþand starts to evict, it does not just evict acknowledged messages, but alsoþcan evict non acknowledged messages having, at the same time, the followingþtwo properties:þþ1. Their retry is set to 0.þ2. The job was already delivered.þþIn theory, acknowledging a job that will never be retried is a waste of timeþand resources, however this design has hidden advantages:þþ1. The API is exactly the same for all the kinds of jobs.þ2. After the job is delivered, it is still possible to examine it. Observability is a very good property of messaging systems.þþHowever, not acknowledging the job does not result in big issues since theyþare evicted eventually during memory pressure.þþAdding and removing nodes at runtimeþ===þþAdding nodes is trivial, and just consists in starting a new node and sending itþa `CLUSTER MEET` command. Assuming the node you just started is locatedþat address 192.168.1.10 port 7714, and a random (you can use any) node ofþthe existing cluster is located at 192.168.1.9 port 7711, all you need to doþis:þþ    ./disque -h 192.168.1.10 -p 7714 cluster meet 192.168.1.9 7711þþNote that you can invert the source and destination arguments and theþnew node will still join the cluster. It does not matter if it's the old node toþmeet the new one or the other way around.þþIn order to remove a node, it is possible to use the crude way of justþshutting it down, and then use `CLUSTER FORGET <old-node-id>` in all theþother nodes in order to remove references to it from the configuration ofþthe other nodes. However this means that, for example, messages that hadþa replication factor of 3, and one of the replicas was the node you areþshutting down, suddenly are left with just 2 replicas even if no *actual*þfailure happened. Moreover if the node you are removing had messages inþqueue, you'll need to wait the retry time before the messages will beþqueued again. For all these reasons, Disque has a better way to remove nodesþwhich is described in the next section.þþGracefully removal of nodesþ===þþIn order to empty a node of its content before removing it, it is possibleþto use a feature that puts a node in *leaving* state. To enable this featureþjust contact the node to remove, and use the following command:þþ    CLUSTER LEAVING yesþþThe node will start advertising itself as *leaving*, so in a matter of secondsþall the cluster will know (if there are partitions, when the partition healsþall the nodes will eventually be informed), and this is what happensþwhen the node is in this state:þþ1. When the node receives `ADDJOB` commands, it performs external replication, like when a node is near the memory limits. This means that it will make sureþto create the number of replicas of the message in the cluster **without using itself** as a replica. So no new messages are created in the context of a node which is leaving.þ2. The node starts to send `-LEAVING` messages to all clients that use `GETJOB` but would block waiting for jobs. The `-LEAVING` error means the clients should connect to another node. Clients that were already blocked waiting for messages will be unblocked and a `-LEAVING` error will be sent to them as well.þ3. The node no longer sends `NEEDJOBS` messages in the context of Disque federation, so it will never ask other nodes to transfer messages to it.þ4. The node and all the other nodes will advertise it with a bad priority in the `HELLO` command output, so that clients will select a different node.þ5. The node will no longer create *dummy acks* in response to an `ACKJOB` command about a job it does not know.þþAll these behavior changes result in the node participating only as a source of messages, so eventually its message count will drop to zero (it is possible to check for this condition using `INFO jobs`). When this happens the node can be stopped and removed from the other nodes tables using `CLUSTER FORGET` as described in the section above.þþClient librariesþ===þþDisque uses the same protocol as Redis itself. To adapt Redis clients, or to use them directly, should be pretty easy. However note that Disque's default port is 7711 and not 6379.þþWhile a vanilla Redis client may work well with Disque, clients should optionally use the following protocol in order to connect with a Disque cluster:þþ1. The client should be given a number of IP addresses and ports where nodes are located. The client should select random nodes and should try to connect until an available one is found.þ2. On a successful connection the `HELLO` command should be used in order to retrieve the Node ID and other potentially useful information (server version, number of nodes).þ3. If a consumer sees a high message rate received from foreign nodes, it may optionally have logic in order to retrieve messages directly from the nodes where producers are producing the messages for a given topic. The consumer can easily check the source of the messages by checking the Node ID prefix in the messages IDs.þ4. The `GETJOB` command, or other commands, may return a `-LEAVING` error instead of blocking. This error should be considered by the client library as a request to connect to a different node, since the node it is connected to is not able to serve the request since it is leaving the cluster. Nodes in this state have a very high *priority* number published via `HELLO`, so will be unlikely to be picked at the next connection attempt.þþThis way producers and consumers will eventually try to minimize node message exchanges whenever possible.þþSo basically you could perform basic usage using just a Redis client, howeverþthere are already specialized client libraries implementing a more specializedþAPI on top of Disque:þþ*C++*þþ- [disque C++ client](https://github.com/zhengshuxin/acl/tree/master/lib_acl_cpp/samples/disque)þþ*Common Lisp*þþ- [cl-disque](https://github.com/CodyReichert/cl-disque)þþ*Elixir*þþ- [exdisque](https://github.com/mosic/exdisque)þþ*Erlang*þþ- [edisque](https://github.com/nacmartin/edisque)þþ*Go*þþ- [disque-go](https://github.com/zencoder/disque-go)þ- [go-disque](https://github.com/EverythingMe/go-disque)þ- [disque](https://github.com/goware/disque)þþ*Java*þþ- [jedisque](https://github.com/xetorthio/jedisque)þ- [spinach](https://github.com/mp911de/spinach)þþ*Node.js*þþ- [disque.js](https://www.npmjs.com/package/disque.js)þ- [thunk-disque](https://github.com/thunks/thunk-disque)þ- [disqueue-node](https://www.npmjs.com/package/disqueue-node)þþ*Perl*þþ- [perl-disque](https://github.com/lovelle/perl-disque)þþ*PHP*þþ- [phpque](https://github.com/s12v/phpque) (PHP/HHVM)þ- [disque-php](https://github.com/mariano/disque-php) ([Composer/Packagist](https://packagist.org/packages/mariano/disque-php))þ- [disque-client-php](https://github.com/mavimo/disque-client-php) ([Composer/Packagist](https://packagist.org/packages/mavimo/disque-client))þ- [phloppy](https://github.com/0x20h/phloppy) ([Composer/Packagist](https://packagist.org/packages/0x20h/phloppy))þþ*Python*þþ- [disq](https://github.com/ryansb/disq) ([PyPi](https://pypi.python.org/pypi/disq))þ- [pydisque](https://github.com/ybrs/pydisque) ([PyPi](https://pypi.python.org/pypi/pydisque))þ- [django-q](https://github.com/koed00/django-q) ([PyPi](https://pypi.python.org/pypi/django-q))þþ*Ruby*þþ- [disque-rb](https://github.com/soveran/disque-rb)þ- [disque_jockey](https://github.com/DevinRiley/disque_jockey)þ- [Disc](https://github.com/pote/disc)þþ*Rust*þþ- [disque-rs](https://github.com/seppo0010/disque-rs)þþ*.NET*þþ- [Disque.Net](https://github.com/ziyasal/Disque.Net)þþImplementation detailsþ===þþJob replication strategyþ---þþ1. Disque tries to replicate to W-1 (or W during out of memory) reachable nodes, shuffled.þ2. The cluster REPLJOB message is used to replicate a job to multiple nodes, the job is sent together with the list of nodes that may have a copy.þ2. If the required replication is not reached promptly, the job is send to one additional node every 50 milliseconds. When this happens, a new REPLJOB message is also re-sent to each node that may already have a copy, in order to refresh the list of nodes that have a copy.þ3. If the specified synchronous replication timeout is reached, the node that originally received the ADDJOB command from the client gives up and returns an error to the client. When this happens the node performs a best-effort procedure to delete the job from nodes that may have already received a copy of the job.þþCluster topologyþ---þþDisque is a full mesh, with each node connected to each other. Disque performsþdistributed failure detection via gossip, only in order to adjust theþreplication strategy (try reachable nodes first when trying to replicateþa message), and in order to inform clients about non reachable nodes whenþthey want the list of nodes they can connect to.þþAs Disque is multi-master, the event of nodes failing is not handled in anyþspecial way.þþCluster messagesþ---þþNodes communicate via a set of messages, using the node-to-node message bus.þA few of the messages are used in order to check that other nodes areþreachable and to mark nodes as failing. Those messages are PING, PONG andþFAIL. Since failure detection is only used to adjust the replication strategyþ(talk with reachable nodes first in order to improve latency), the detailsþare yet not described. Other messages are more important since they are usedþin order to replicate jobs, re-issue jobs while trying to minimize multipleþdeliveries, and in order to auto-federate to serve consumers when messagesþare produced in different nodes compared to where consumers are.þþThe following is a list of messages and what they do, split by category.þNote that this is just an informal description, while in the next sectionsþdescribing the Disque state machine, there is a more detailed descriptionþof the behavior caused by message reception, and in what cases they areþgenerated.þþCluster messages related to jobs replication and queueingþ---þþ* REPLJOB: ask the receiver to replicate a job, that is, to add a copy of the job among the registered jobs in the target node. When a job is accepted, the receiver replies with GOTJOB to the sender. A job may not be accepted if the receiving node is near out of memory. In this case GOTJOB is not sent and the message discarded.þ* GOTJOB: The reply to REPLJOB to confirm the job was replicated.þ* ENQUEUE: Ask a node to put a given job into its queue. This message is used when a job is created by a node that does not want to take a copy, so it asks another node (among the ones that acknowledged the job replication) to queue it for the first time. If this message is lost, after the retry time some node will try to re-queue the message, unless retry is set to zero.þ* WILLQUEUE: This message is sent 500 milliseconds before a job is re-queued to all the nodes that may have a copy of the message, according to the sender table. If some of the receivers already have the job queued, they'll reply with QUEUED in order to prevent the sender to queue the job again (avoid multiple delivery when possible).þ* QUEUED: When a node re-queues a job, it sends QUEUED to all the nodes that may have a copy of the message, so that the other nodes will update the time at which they'll retry to queue the job. Moreover, every node that already has the same job in queue, but with a node ID which is lexicographically smaller than the sending node, will de-queue the message in order to best-effort de-dup messages that may be queued in multiple nodes at the same time.þþCluster messages related to ACK propagation and garbage collectionþ---þþ* SETACK: This message is sent to force a node to mark a job as successfully delivered (acknowledged by the worker): the job will no longer be considered active, and will never be re-queued by the receiving node. Also SETACK is send to the sender if the receiver of QUEUED or WILLQUEUE message has the same job marked as acknowledged (successfully delivered) already.þ* GOTACK: This message is sent in order to acknowledge a SETACK message. The receiver can mark a given node that may have a copy of a job, as informed about the fact that the job was acknowledged by the worker. Nodes delete (garbage collect) a message cluster wide when they believe all the nodes that may have a copy are informed about the fact the job was acknowledged.þ* DELJOB: Ask the receiver to remove a job. Is only sent in order to perform garbage collection of jobs by nodes that are sure the job was already delivered correctly. Usually the node sending DELJOB only does that when its sure that all the nodes that may have a copy of the message already marked the message ad delivered, however after some time the job GC may be performed anyway, in order to reclaim memory, and in that case, an otherwise avoidable multiple delivery of a job may happen. The DELJOB message is also used in order to implement *fast acknowledges*.þþCluster messages related to nodes federationþ---þþ* NEEDJOBS(queue,count): The sender asks the receiver to obtain messages for a given queue, possibly *count* messages, but this is only an hit for congestion control and messages optimization, the receiver is free to reply with whatever number of messages. NEEDJOBS messages are delivered in two ways: broadcasted to every node in the cluster from time to time, in order to discover new source nodes for a given queue, or more often, to a set of nodes that recently replies with jobs for a given queue. This latter mechanism is called an *ad hoc* delivery, and is possible since every node remembers for some time the set of nodes that were recent providers of messages for a given queue. In both cases, NEEDJOBS messages are delivered with exponential delays, with the exception of queues that drop to zero-messages and have a positive recent import rate, in this case an ad hoc NEEDJOBS delivery is performed regardless of the last time the message was delivered in order to allow a continuous stream of messages under load.þþ* YOURJOBS(array of messages): The reply to NEEDJOBS. An array of serialized jobs, usually all about the same queue (but future optimization may allow to send different jobs from different queues). Jobs into YOURJOBS replies are extracted from the local queue, and queued at the receiver node's queue with the same name. So even messages with a retry set to 0 (at most once delivery) still guarantee the safety rule since a given message may be in the source node, on the wire, or already received in the destination node. If a YOURJOBS message is lost, at least once delivery jobs will be re-queued later when the retry time is reached.þþDisque state machineþ---þþThis section shows the most interesting (as in less obvious) parts of the state machine each Disque node implements. While practically it is a single state machine, it is split in sections. The state machine description uses a convention that is not standard but should look familiar, since it is event driven, made of actions performed upon: message receptions in the form of commands received from clients, messages received from other cluster nodes, timers, and procedure calls.þþNote that: `job` is a job object with the following fields:þþ1. `job.delivered`: A list of nodes that may have this message. This list does not need to be complete, is used for best-effort algorithms.þ2. `job.confirmed`: A list of nodes that confirmed reception of ACK by replying with a GOTJOB message.þ3. `job.id`: The job 48 chars ID.þ4. `job.state`: The job state among: `wait-repl`, `active`, `queued`, `acked`.þ5. `job.replicate`: Replication factor for this job.þ5. `job.qtime`: Time at which we need to re-queue the job.þþList fields such as `.delivered` and `.confirmed` support methods like `.size` to get the number of elements.þþStates are as follows:þþ1. `wait-repl`: the job is waiting to be synchronously replicated.þ2. `active`: the job is active, either it reached the replication factor in the originating node, or it was created because the node received an `REPLJOB` message from another node.þ3. `queued`: the job is active and also is pending into a queue in this node.þ4. `acked`: the job is no longer active since a client confirmed the reception using the `ACKJOB` command or another Disque node sent a `SETACK` message for the job.þþGeneric functionsþ---þþPROCEDURE `LOOKUP-JOB(string job-id)`:þþ1. If job with the specified id is found, returns the corresponding job object.þ2. Otherwise returns NULL.þþPROCEDURE `UNREGISTER(object job)`:þþ1. Delete the job from memory, and if queued, from the queue.þþPROCEDURE `ENQUEUE(job)`:þþ1. If `job.state == queued` return ASAP.þ2. Add `job` into `job.queue`.þ3. Change `job.state` to `queued`.þþPROCEDURE `DEQUEUE(job)`:þþ1. If `job.state != queued` return ASAP.þ2. Remove `job` from `job.queue`.þ3. Change `job.state` to `active`.þþON RECV cluster message: `DELJOB(string job.id)`:þþ1. job = Call `LOOKUP-JOB(job-id)`.þ2. IF `job != NULL` THEN call `UNREGISTER(job)`.þþJob replication state machineþ---þþThis part of the state machine documents how clients add jobs to the clusterþand how the cluster replicates jobs across different Disque nodes.þþON RECV client command `ADDJOB(string queue-name, string body, integer replicate, integer retry, integer ttl, ...):þþ1. Create a job object in `wait-repl` state, having as body, ttl, retry, queue name, the specified values.þ2. Send REPLJOB(job.serialized) cluster message to `replicate-1` nodes.þ3. Block the client without replying.þþStep 3: We'll reply to the client in step 4 of `GOTJOB` message processing.þþON RECV cluster message `REPLJOB(object serialized-job)`:þþ1. job = Call `LOOKUP-JOB(serialized-job.id)`.þ2. IF `job != NULL` THEN: job.delivered = UNION(job.delivered,serialized-job.delivered). Return ASAP, since we have the job.þ3. Create a job from serialized-job information.þ4. job.state = `active`.þ5. Reply to the sender with `GOTJOB(job.id)`.þþStep 1: We may already have the job, since REPLJOB may be duplicated.þþStep 2: If we already have the same job, we update the list of jobs that may have a copy of this job, performing the union of the list of nodes we have with the list of nodes in the serialized job.þþON RECV cluster message `GOTJOB(object serialized-job)`:þþ1. job = Call `LOOKUP-JOB(serialized-job.id)`.þ2. IF `job == NULL` OR `job.state != wait-repl` Return ASAP.þ3. Add sender node to `job.confirmed`.þ4. IF `job.confirmed.size == job.replicate` THEN change `job.state` to `active`, call ENQUEUE(job), and reply to the blocked client with `job.id`.þþStep 4: As we receive enough confirmations via `GOTJOB` messages, we finally reach the replication factor required by the user and consider the message active.þþTIMER, firing every next 50 milliseconds while a job still did not reached the expected replication factor.þþ1. Select an additional node not already listed in `job.delivered`, call it `node`.þ2. Add `node` to `job.delivered`.þ3. Send REPLJOB(job.serialized) cluster message to each node in `job.delivered`.þþStep 3: We send the message to every node again, so that each node will have a chance to update `job.delivered` with the new nodes. It is not required for each node to know the full list of nodes that may have a copy, but doing so improves our approximation of single delivery whenever possible.þþJob re-queueing state machineþ---þþThis part of the state machine documents how Disque nodes put a given jobþback into the queue after the specified retry time elapsed without theþjob being acknowledged.þþTIMER, firing 500 milliseconds before the retry time elapses:þþ1. Send `WILLQUEUE(job.id)` to every node in `jobs.delivered`.þþTIMER, firing when `job.qtime` time is reached.þþ1. If `job.retry == 0` THEN return ASAP.þ2. Call ENQUEUE(job).þ3. Update `job.qtime` to NOW + job.retry.þ4. Send `QUEUED(job.id)` message to each node in `job.delivered`.þþStep 1: At most once jobs never get enqueued again.þþStep 3: We'll retry again after the retry period.þþON RECV cluster message `WILLQUEUE(string job-id)`:þþ1. job = Call `LOOKUP-JOB(job-id)`.þ2. IF `job == NULL` THEN return ASAP.þ3. IF `job.state == queued` SEND `QUEUED(job.id)` to `job.delivered`.þ4. IF `job.state == acked` SEND `SETACK(job.id)` to the sender.þþStep 3: We broadcast the message since likely the other nodes are going to retry as well.þþStep 4: SETACK processing is documented below in the acknowledges section of the state machine description.þþON RECV cluster message `QUEUED(string job-id)`:þþ1. job = Call `LOOKUP-JOB(job-id)`.þ2. IF `job == NULL` THEN return ASAP.þ3. IF `job.state == acked` THEN return ASAP.þ4. IF `job.state == queued` THEN if sender node ID is greater than my node ID call DEQUEUE(job).þ5. Update `job.qtime` setting it to NOW + job.retry.þþStep 4: If multiple nodes re-queue the job about at the same time because of race conditions or network partitions that make `WILLQUEUE` not effective, then `QUEUED` forces receiving nodes to dequeue the message if the sender has a greater node ID, lowering the probability of unwanted multiple delivery.þþStep 5: Now the message is already queued somewhere else, but the node will retry again after the retry time.þþAcknowledged jobs garbage collection state machineþ---þþThis part of the state machine is used in order to garbage collectþacknowledged jobs, when a job finally gets acknowledged by a client.þþPROCEDURE `ACK-JOB(job)`:þþ1. If job state is already `acked`, do nothing and return ASAP.þ2. Change job state to `acked`, dequeue the job if queued, schedule first call to TIMER.þþPROCEDURE `START-GC(job)`:þþ1. Send `SETACK(job.delivered.size)` to each node that is listed in `job.delivered` but is not listed in `job.confirmed`.þ2. IF `job.delivered.size == 0`, THEN send `SETACK(0)` to every node in the cluster.þþStep 2: this is an ACK about a job we don’t know. In that case, we can just broadcast the acknowledged hoping somebody knows about the job and replies.þþON RECV client command `ACKJOB(string job-id)`:þþ1. job = Call `LOOKUP-JOB(job-id)`.þ1. if job is `NULL`, ignore the message and return.þ2. Call `ACK-JOB(job)`.þ3. Call `START-GC(job)`.þþON RECV cluster message `SETACK(string job-id, integer may-have)`:þþ1. job = Call `LOOKUP-JOB(job-id)`.þ2. Call ACK-JOB(job) IF job is not `NULL`.þ3. Reply with GOTACK IF `job == NULL OR job.delivered.size <= may-have`.þ4. IF `job != NULL` and `jobs.delivered.size > may-have` THEN call `START-GC(job)`.þ5. IF `may-have == 0 AND job  != NULL`, reply with `GOTACK(1)` and call `START-GC(job)`.þþSteps 3 and 4 makes sure that among the reachable nodes that may have a message, garbage collection will be performed by the node that is aware of more nodes that may have a copy.þþStep 5 instead is used in order to start a GC attempt if we received a SETACK message from a node just hacking a dummy ACK (an acknowledge about a job it was not aware of).þþON RECV cluster message `GOTACK(string job-id, bool known)`:þþ1. job = Call `LOOKUP-JOB(job-id)`. Return ASAP IF `job == NULL`.þ2. Call `ACK-JOB(job)`.þ3. IF `known == true AND job.delivered.size > 0` THEN add the sender node to `job.delivered`.þ4. IF `(known == true) OR (known == false AND job.delivered.size > 0) OR (known == false AND sender is an element of job.delivered)` THEN add the sender node to `jobs.confirmed`.þ5. IF `job.delivered.size > 0 AND job.delivered.size == job.confirmed.size`, THEN send `DELJOB(job.id)` to every node in the `job.delivered` list and call `UNREGISTER(job)`.þ6. IF `job.delivered == 0 AND known == true`, THEN call `UNREGISTER(job)`.þ7. IF `job.delivered == 0 AND job.confirmed.size == cluster.size` THEN call `UNREGISTER(job)`.þþStep 3: If `job.delivered.size` is zero, it means that the node just holds a *dummy ack* for the job. It means the node has an acknowledged job it created on the fly because a client acknowledged (via ACKJOB command) a job it was not aware of.þþStep 6: we don't have to hold a dummy acknowledged jobs if there are nodes that have the job already acknowledged.þþStep 7: this happens when nobody knows about a job, like when a client acknowledged a wrong job ID.þþTIMER, from time to time (exponential backoff with random error), for every acknowledged job in memory:þþ1. call `START-GC(job)`.þþLimitationsþ===þþ* Disque is new code, not tested, and will require quite some time to reach production quality. It is likely very buggy and may contain wrong assumptions or tradeoffs.þ* As long as the software is non stable, the API may change in random ways without prior notification.þ* It is possible that Disque spends too much effort in approximating single delivery during failures. The **fast acknowledge** concept and command makes the user able to opt-out this efforts, but yet I may change the Disque implementation and internals in the future if I see the user base really not caring about multiple deliveries during partitions.þ* There is yet a lot of Redis dead code inside probably that could be removed.þ* Disque was designed a bit in *astronaut mode*, not triggered by an actual use case of mine, but more in response to what I was seeing people doing with Redis as a message queue and with other message queues. However I'm not an expert, if I succeeded to ship something useful for most users, this is kinda of an accomplishment. Otherwise it may just be that Disque is pretty useless.þ* As Redis, Disque is single threaded. While in Redis there are stronger reasons to do so, in Disque there is no manipulation of complex data structures, so maybe in the future it should be moved into a threaded server. We need to see what happens in real use cases in order to understand if it's worth it or not.þ* The number of jobs in a Disque process is limited to the amount of memory available. Again while this in Redis makes sense (IMHO), in Disque there are definitely simple ways in order to circumvent this limitation, like logging messages on disk when the server is out of memory and consuming back the messages when memory pressure is already acceptable. However in general, like in Redis, manipulating data structures in memory is a big advantage from the point of view of the implementation simplicity and the functionality we can provide to users.þ* Disque is completely not optimized for speed, was never profiled so far. I'm currently not aware of the fact it's slow, fast, or average, compared to other messaging solutions. For sure it is not going to have Redis-alike numbers because it does a lot more work at each command. For example when a job is added, it is serialized and transmitted to other `N` servers. There is a lot more message passing between nodes involved, and so forth. The good news is that being totally unoptimized, there is room for improvements.þ* Ability of federation to handle well low and high loads without incurring into congestion or high latency, was not tested well enough. The algorithm is reasonable but may fail short under many load patterns.þ* Amount of tested code path and possible states is not enough.þþFAQþ===þþIs Disque part of Redis?þ---þþNo, it is a standalone project, however a big part of the Redis networking source code, nodes message bus, libraries, and the client protocol, were reused in this new project. In theory it was possible to extract the common code and release it as a framework to write distributed systems in C. However this is not a perfect solution as well, since the projects are expected to diverge more and more in the future, and to rely on a common foundation was hard. Moreover the initial effort to turn Redis into two different layers: an abstract server, networking stack and cluster bus, and the actual Redis implementation, was a huge effort, ways bigger than writing Disque itself.þþHowever while it is a separated project, conceptually Disque is related to Redis, since it tries to solve a Redis use case in a vertical, ad-hoc way.þþWho created Disque?þ---þþDisque is a side project of Salvatore Sanfilippo, aka @antirez.þþThere are chances for this project to be actively developed?þ---þþCurrently I consider this just a public alpha: If I see people happy to use it for the right reasons (i.e. it is better in some use cases compared to other message queues) I'll continue the development. Otherwise it was anyway cool to develop it, I had much fun, and I definitely learned new things.þþWhat happens when a node runs out of memory?þ---þþ1. Maxmemory setting is mandatory in Disque, and defaults to 1GB.þ2. When 75% of maxmemory is reached, Disque starts to replicate the new jobs only to external nodes, without taking a local copy, so basically if there is free RAM into other nodes, adding still works.þ3. When 95% of maxmemory is reached, Disque starts to evict data that does not violates the safety guarantees: For instance acknowledged jobs and inactive queues.þ4. When 100% of maxmemory is reached, commands that may result into more memory used are not processed at all and the client is informed with an error.þþAre there plans to add the ability to hold more jobs than the physical memory of a single node can handle?þ---þþYes. In Disque it should be relatively simple to use the disk when memory is notþavailable, since jobs are immutable and don't need to necessarily exist inþmemory at a given time.þþThere are multiple strategies available. The current idea is thatþwhen an instance is out of memory, jobs are stored into a log file insteadþof memory. As more free memory is available in the instance, on disk jobsþare loaded.þþHowever in order to implement this, there is to observe strong evidence of itsþgeneral usefulness for the user base.þþWhen I consume and produce from different nodes, sometimes there is a delay in order for the jobs to reach the consumer, why?þ---þþDisque routing is not static, the cluster automatically tries to provideþmessages to nodes where consumers are attached. When there is an highþenough traffic (even one message per second is enough) nodes remember otherþnodes that recently were sources for jobs in a given queue, so it is possibleþto aggressively send messages asking for more jobs, every time there areþconsumers waiting for more messages and the local queue is empty.þþHowever when the traffic is very low, informations about recent sources ofþmessages are discarded, and nodes rely on a more generic mechanism in order toþdiscover other nodes that may have messages in the queues we need them (whichþis also used in high traffic conditions as well, in order to discover newþsources of messages for a given queue).þþFor example imagine a setup with two nodes, A and B.þþ1. A client attaches to node A and asks for jobs in the queue `myqueue`. Node A has no jobs enqueued, so the client is blocked.þ2. After a few seconds another client produces messages into `myqueue`, but sending them to node B.þþDuring step `1` if there was no recent traffic of imported messages for this queue, node A has no idea about who may have messages for the queue `myqueue`. Every other node may have, or none may have. So it starts to broadcast `NEEDJOBS` messages to the whole cluster. However we can't spam the cluster with messages, so if no reply is received after the first broadcast, the next will be sent with a larger delay, and so foth. The delay is exponential, with a maximum value of 30 seconds (this parameters will be configurable in the future, likely).þþWhen there is some traffic instead, nodes send `NEEDJOBS` messages ASAP to other nodes that were recent sources of messages. Even when no reply is received, the next `NEEDJOBS` messages will be sent more aggressively to the subset of nodes that had messages in the past, with a delay that starts at 25 milliseconds and has a maximum value of two seconds.þþIn order to minimize the latency, `NEEDJOBS` messages are not throttled at all when:þþ1. A client consumed the last message from a given queue. Source nodes are informed immediately in order to receive messages before the node asks for more.þ2. Blocked clients are served the last message available in the queue.þþFor more information, please refer to the file `queue.c`, especially the function `needJobsForQueue` and its callers.þþAre messages re-enqueued in the queue tail or head or what?þ---þþMessages are put into the queue according to their *creation time* attribute. This means that they are enqueued in a best effort order in the local node queue. Messages that need to be put back into the queue again because their delivery failed are usually (but not always) older than messages already in queue, so they'll likely be among the first to be delivered to workers.þþWhat Disque means?þ---þþDIStributed QUEue but is also a joke with ""dis"" as negation (like in *dis*order) of the strict concept of queue, since Disque is not able to guarantee the strict ordering you expect from something called *queue*. And because of this tradeof it gains many other interesting things.þþCommunity: how to get help and how to helpþ===þþGet in touch with us in one of the following ways:þþ1. Post on [Stack Overflow](http://stackoverflow.com) using the `disque` tag. This is the preferred method to get general help about Disque: other users will easily find previous questions so we can incrementally build a knowledge base.þ2. Join the `#disque` IRC channel at **irc.freenode.net**.þ3. Create an Issue or Pull request if your question or issue is about the Disque implementation itself.þþThanksþ===þþI would like to say thank you to the following persons and companies.þþ* Pivotal, for allowing me to work on Disque, most in my spare time, but sometimes during work hours. Moreover Pivotal agreed to leave the copyright of the code to me. This is very generous. Thanks Pivotal!þ* Michel Martens and Damian Janowski for providing early feedback about Disque while the project was still private.þ* Everybody who is already writing client libraries, sending pull requests, creating issues in order to move this forward from alpha to something actually usable."
nexjhealth/x264,4847,0,1,0,User,False,2762,1,0,93,False,,,0,7,0,0,0,0,0,0,0,0,0,5192,0,0,0,0,0,0,24,,3,,
MZO9400/android_device_oneplus_bacon-Minimal,8513,0,1,0,User,False,391,6,0,36,False,MinimalOS bacon device tree,,0,7,0,0,0,0,0,0,0,0,0,2294,0,0,0,0,0,0,100,,32,,
SamyPesse/How-to-Make-a-Computer-Operating-System,5986,18796,1671,3269,User,False,243,1,12,34,False,How to Make a Computer Operating System in C++,https://samypesse.gitbook.io/how-to-c…,0,6,1,38,25,1,0,37,54,1,0,2392,0,0,0,0,0,0,86,,2,,"How to Make a Computer Operating Systemþ=======================================þþOnline book about how to write a computer operating system in C/C++ from scratch.þþ**Caution**: This repository is a remake of my old course. It was written several years ago [as one of my first projects when I was in High School](https://github.com/SamyPesse/devos), I'm still refactoring some parts. The original course was in French and I'm not an English native. I'm going to continue and improve this course in my free-time.þþ**Book**: An online version is available at [http://samypesse.gitbooks.io/how-to-create-an-operating-system/](http://samypesse.gitbooks.io/how-to-create-an-operating-system/) (PDF, Mobi and ePub). It was generated using [GitBook](https://www.gitbook.com/).þþ**Source Code**: All the system source code will be stored in the [src](https://github.com/SamyPesse/How-to-Make-a-Computer-Operating-System/tree/master/src) directory. Each step will contain links to the different related files.þþ**Contributions**: This course is open to contributions, feel free to signal errors with issues or directly correct the errors with pull-requests.þþ**Questions**: Feel free to ask any questions by adding issues or commenting sections.þþYou can follow me on Twitter [@SamyPesse](https://twitter.com/SamyPesse) or [GitHub](https://github.com/SamyPesse).þþ### What kind of OS are we building?þþThe goal is to build a very simple UNIX-based operating system in C++, not just a ""proof-of-concept"". The OS should be able to boot, start a userland shell, and be extensible.þþ![Screen](./preview.png)"
tizenorg/platform.upstream.libva-intel-driver,4565,0,2,0,Organization,False,1308,32,126,22,False,,,0,7,0,0,0,0,0,0,0,0,0,3974,0,0,0,0,0,0,1,0,,,
rootmelo92118/SoftEtherVPN_Stable,356640,0,0,0,User,False,187,1,5,30,False,,,0,8,0,0,0,0,0,0,0,0,0,2357,0,0,0,0,0,0,75,,80,,
axxx007xxxz/android_hardware_qcom_audio,2169,0,1,0,User,False,1330,1,0,30,False,,,0,8,0,0,0,0,0,0,0,0,0,2903,0,0,0,0,0,0,107,,6,,
furthestworld/-pecl-php-memcache,298,0,1,0,Organization,False,221,1,0,10,False,pecl-php-memcache,,0,7,0,0,0,0,0,0,0,0,0,5969,0,0,0,0,0,0,14,0,,,"memcached module for PHPþ------------------------þThis module requires zlib library, used for on-the-fly data (de)compression.þAlso, you'll need memcached to use it =)þþThe memcached website is here:þ    http://www.danga.com/memcached/þþYou will probably need libevent to install memcached:þYou can download it here: http://www.monkey.org/~provos/libevent/þþHow to run tests:þ1. sh tests/memcache.shþ2. TEST_PHP_EXECUTABLE=/usr/local/bin/php php -dextension=modules/memcache.so run-tests.php -d extension=modules/memcache.soþþMaintainers:þHerman J. Radtke III hradtke at php dot netþþþ--------þ## 用官方[memcache扩展](https://git.php.net/repository/pecl/caching/memcache.git)代码为例子，研究下PHP扩展开发的一些常规套路þþ> 有兴趣的童鞋一起哈~þþ* 参考书籍þ    * [PHP扩展开发及内核应用](http://www.cunmou.com/phpbook/index.md)þ    请自行阅读一百遍~ :)"
ZdrowyGosciu/caf_device,994,0,0,0,User,False,1612,1,0,14,False,MSM8974 CAF,,0,7,0,,,,,0,0,0,0,3449,0,0,0,0,0,0,27,,19,,
fogleman/Craft,14647,7928,371,1015,User,False,751,16,1,9,False,A simple Minecraft clone written in C using modern OpenGL (shaders).,http://www.michaelfogleman.com/craft/,7,6,0,65,75,5,1,38,84,9,10,2623,1,1,2,2,0,0,131,,4,,"## CraftþþMinecraft clone for Windows, Mac OS X and Linux. Just a few thousand lines of C using modern OpenGL (shaders). Online multiplayer support is included using a Python-based server.þþhttp://www.michaelfogleman.com/craft/þþ![Screenshot](https://i.imgur.com/SH7wcas.png)þþ### Featuresþþ* Simple but nice looking terrain generation using perlin / simplex noise.þ* More than 10 types of blocks and more can be added easily.þ* Supports plants (grass, flowers, trees, etc.) and transparency (glass).þ* Simple clouds in the sky (they don't move).þ* Day / night cycles and a textured sky dome.þ* World changes persisted in a sqlite3 database.þ* Multiplayer support!þþ### DownloadþþMac and Windows binaries are available on the website.þþhttp://www.michaelfogleman.com/craft/þþSee below to run from source.þþ### Install Dependenciesþþ#### Mac OS XþþDownload and install [CMake](http://www.cmake.org/cmake/resources/software.html)þif you don't already have it. You may use [Homebrew](http://brew.sh) to simplifyþthe installation:þþ    brew install cmakeþþ#### Linux (Ubuntu)þþ    sudo apt-get install cmake libglew-dev xorg-dev libcurl4-openssl-devþ    sudo apt-get build-dep glfwþþ#### WindowsþþDownload and install [CMake](http://www.cmake.org/cmake/resources/software.html)þand [MinGW](http://www.mingw.org/). Add `C:\MinGW\bin` to your `PATH`.þþDownload and install [cURL](http://curl.haxx.se/download.html) so thatþCURL/lib and CURL/include are in your Program Files directory.þþUse the following commands in place of the ones described in the next section.þþ    cmake -G ""MinGW Makefiles""þ    mingw32-makeþþ### Compile and RunþþOnce you have the dependencies (see above), run the following commands in yourþterminal.þþ    git clone https://github.com/fogleman/Craft.gitþ    cd Craftþ    cmake .þ    makeþ    ./craftþþ### MultiplayerþþRegister for an account!þþhttps://craft.michaelfogleman.com/þþ#### ClientþþYou can connect to a server with command line arguments...þþ    ./craft craft.michaelfogleman.comþþOr, with the ""/online"" command in the game itself.þþ    /online craft.michaelfogleman.comþþ#### ServerþþYou can run your own server or connect to mine. The server is written in Pythonþbut requires a compiled DLL so it can perform the terrain generation just likeþthe client.þþ    gcc -std=c99 -O3 -fPIC -shared -o world -I src -I deps/noise deps/noise/noise.c src/world.cþ    python server.py [HOST [PORT]]þþ### Controlsþþ- WASD to move forward, left, backward, right.þ- Space to jump.þ- Left Click to destroy a block.þ- Right Click or Cmd + Left Click to create a block.þ- Ctrl + Right Click to toggle a block as a light source.þ- 1-9 to select the block type to create.þ- E to cycle through the block types.þ- Tab to toggle between walking and flying.þ- ZXCVBN to move in exact directions along the XYZ axes.þ- Left shift to zoom.þ- F to show the scene in orthographic mode.þ- O to observe players in the main view.þ- P to observe players in the picture-in-picture view.þ- T to type text into chat.þ- Forward slash (/) to enter a command.þ- Backquote (`) to write text on any block (signs).þ- Arrow keys emulate mouse movement.þ- Enter emulates mouse click.þþ### Chat Commandsþþ    /goto [NAME]þþTeleport to another user.þIf NAME is unspecified, a random user is chosen.þþ    /listþþDisplay a list of connected users.þþ    /login NAMEþþSwitch to another registered username.þThe login server will be re-contacted. The username is case-sensitive.þþ    /logoutþþUnauthenticate and become a guest user.þAutomatic logins will not occur again until the /login command is re-issued.þþ    /offline [FILE]þþSwitch to offline mode.þFILE specifies the save file to use and defaults to ""craft"".þþ    /online HOST [PORT]þþConnect to the specified server.þþ    /pq P QþþTeleport to the specified chunk.þþ    /spawnþþTeleport back to the spawn point.þþ### Screenshotþþ![Screenshot](https://i.imgur.com/foYz3aN.png)þþ### Implementation Detailsþþ#### Terrain GenerationþþThe terrain is generated using Simplex noise - a deterministic noise function seeded based on position. So the world will always be generated the same way in a given location.þþThe world is split up into 32x32 block chunks in the XZ plane (Y is up). This allows the world to be “infinite” (floating point precision is currently a problem at large X or Z values) and also makes it easier to manage the data. Only visible chunks need to be queried from the database.þþ#### RenderingþþOnly exposed faces are rendered. This is an important optimization as the vast majority of blocks are either completely hidden or are only exposing one or two faces. Each chunk records a one-block width overlap for each neighboring chunk so it knows which blocks along its perimeter are exposed.þþOnly visible chunks are rendered. A naive frustum-culling approach is used to test if a chunk is in the camera’s view. If it is not, it is not rendered. This results in a pretty decent performance improvement as well.þþChunk buffers are completely regenerated when a block is changed in that chunk, instead of trying to update the VBO.þþText is rendered using a bitmap atlas. Each character is rendered onto two triangles forming a 2D rectangle.þþ“Modern” OpenGL is used - no deprecated, fixed-function pipeline functions are used. Vertex buffer objects are used for position, normal and texture coordinates. Vertex and fragment shaders are used for rendering. Matrix manipulation functions are in matrix.c for translation, rotation, perspective, orthographic, etc. matrices. The 3D models are made up of very simple primitives - mostly cubes and rectangles. These models are generated in code in cube.c.þþTransparency in glass blocks and plants (plants don’t take up the full rectangular shape of their triangle primitives) is implemented by discarding magenta-colored pixels in the fragment shader.þþ#### DatabaseþþUser changes to the world are stored in a sqlite database. Only the delta is stored, so the default world is generated and then the user changes are applied on top when loading.þþThe main database table is named “block” and has columns p, q, x, y, z, w. (p, q) identifies the chunk, (x, y, z) identifies the block position and (w) identifies the block type. 0 represents an empty block (air).þþIn game, the chunks store their blocks in a hash map. An (x, y, z) key maps to a (w) value.þþThe y-position of blocks are limited to 0 <= y < 256. The upper limit is mainly an artificial limitation to prevent users from building unnecessarily tall structures. Users are not allowed to destroy blocks at y = 0 to avoid falling underneath the world.þþ#### MultiplayerþþMultiplayer mode is implemented using plain-old sockets. A simple, ASCII, line-based protocol is used. Each line is made up of a command code and zero or more comma-separated arguments. The client requests chunks from the server with a simple command: C,p,q,key. “C” means “Chunk” and (p, q) identifies the chunk. The key is used for caching - the server will only send block updates that have been performed since the client last asked for that chunk. Block updates (in realtime or as part of a chunk request) are sent to the client in the format: B,p,q,x,y,z,w. After sending all of the blocks for a requested chunk, the server will send an updated cache key in the format: K,p,q,key. The client will store this key and use it the next time it needs to ask for that chunk. Player positions are sent in the format: P,pid,x,y,z,rx,ry. The pid is the player ID and the rx and ry values indicate the player’s rotation in two different axes. The client interpolates player positions from the past two position updates for smoother animation. The client sends its position to the server at most every 0.1 seconds (less if not moving).þþClient-side caching to the sqlite database can be performance intensive when connecting to a server for the first time. For this reason, sqlite writes are performed on a background thread. All writes occur in a transaction for performance. The transaction is committed every 5 seconds as opposed to some logical amount of work completed. A ring / circular buffer is used as a queue for what data is to be written to the database.þþIn multiplayer mode, players can observe one another in the main view or in a picture-in-picture view. Implementation of the PnP was surprisingly simple - just change the viewport and render the scene again from the other player’s point of view.þþ#### Collision TestingþþHit testing (what block the user is pointing at) is implemented by scanning a ray from the player’s position outward, following their sight vector. This is not a precise method, so the step rate can be made smaller to be more accurate.þþCollision testing simply adjusts the player’s position to remain a certain distance away from any adjacent blocks that are obstacles. (Clouds and plants are not marked as obstacles, so you pass right through them.)þþ#### Sky DomeþþA textured sky dome is used for the sky. The X-coordinate of the texture represents time of day. The Y-values map from the bottom of the sky sphere to the top of the sky sphere. The player is always in the center of the sphere. The fragment shaders for the blocks also sample the sky texture to determine the appropriate fog color to blend with based on the block’s position relative to the backing sky.þþ#### Ambient OcclusionþþAmbient occlusion is implemented as described on this page:þþhttp://0fps.wordpress.com/2013/07/03/ambient-occlusion-for-minecraft-like-worlds/þþ#### Dependenciesþþ* GLEW is used for managing OpenGL extensions across platforms.þ* GLFW is used for cross-platform window management.þ* CURL is used for HTTPS / SSL POST for the authentication process.þ* lodepng is used for loading PNG textures.þ* sqlite3 is used for saving the blocks added / removed by the user.þ* tinycthread is used for cross-platform threading."
lyzsh/apv,64458,0,0,0,User,False,300,2,1,5,False,Automatically exported from code.google.com/p/apv,,0,16,0,116,55,0,0,0,0,0,0,4415,0,0,0,0,0,0,2,,0,,
macks22/comdetect,480,1,3,1,User,False,97,2,0,2,False,Community detection algorithm implementations.,,0,7,0,4,8,0,0,0,0,0,0,1958,0,0,0,0,0,0,41,,28,,# comdetectþCommunity detection algorithm implementations.
michaelarmstrong/mongoose,692,0,0,0,User,False,279,1,0,3,False,Automatically exported from code.google.com/p/mongoose,,0,17,0,32,383,0,0,0,0,0,0,3694,0,0,0,0,0,0,30,,28,,
bluelinkgaming/nintendon-t,58570,0,0,0,User,False,329,2,0,3,False,Automatically exported from code.google.com/p/nintendon-t,,0,12,0,133,64,0,0,0,0,0,0,2238,0,0,0,0,0,0,1,,0,,
TurokMods/T4Editor,3449,2,2,0,Organization,False,113,5,0,2,False,,https://www.turokforums.com/index.php…,0,8,0,27,1,0,0,0,5,0,0,985,0,0,0,0,0,0,2,0,,,
dasfoo/rpi-gpio,6,1,2,0,Organization,False,16,1,1,2,False,Very simple GPIO library in CGo using /dev/gpiomem (bonus: DHT11 sensor),,0,7,0,0,0,0,0,0,7,0,0,1629,0,0,0,0,0,0,17,2,,,# rpi-gpioþVery simple GPIO library in CGo using /dev/gpiomemþþ[![GoDoc](https://godoc.org/github.com/dasfoo/rpi-gpio?status.svg)](http://godoc.org/github.com/dasfoo/rpi-gpio)þ[![License](http://img.shields.io/:license-mit-blue.svg)](http://doge.mit-license.org)þ[![Build Status](https://travis-ci.org/dasfoo/rpi-gpio.svg?branch=master)](https://travis-ci.org/dasfoo/rpi-gpio)þ[![Coverage Status](https://coveralls.io/repos/dasfoo/rpi-gpio/badge.svg?branch=master&service=github)](https://coveralls.io/github/dasfoo/rpi-gpio?branch=master)
ahmadfahmip/syntax-validator-tbfo,56,3,1,3,User,False,31,1,0,2,False,Project for 2nd Task in Formal Language and Automata Theory class Institut Teknologi Bandung. CYK algorithms are used in this project.,,0,8,0,0,0,0,0,0,2,0,0,936,0,0,0,0,0,0,35,,27,,# syntax-validator-tbfoþProject for 2nd Task in Formal Language and Automata Theory class Institut Teknologi Bandung. CYK algorithms are used in this project.
michaelsvit/FIAR,48,0,2,0,User,False,82,1,0,2,False,Four In a Row CLI game,,0,7,0,0,0,0,0,0,7,0,0,1118,0,0,0,0,0,0,13,,1,,
zishuiym/vim,43812,0,0,0,User,False,6781,4,3435,1,False,Automatically exported from code.google.com/p/vim,,0,15,0,77,188,0,0,0,0,0,0,3687,0,0,0,0,0,0,22,,0,,
JamesH65/GertMatrix,120,0,2,2,User,False,5,1,0,2,False,Source code to drive the GertMatrix LED add on for the Raspberry PI.,,0,6,0,0,0,0,0,0,1,0,0,2378,0,0,0,0,0,0,21,,39,,GertMatrix README.md
tluyben/php-for-android,1672,0,0,0,User,False,8,1,1,1,False,Automatically exported from code.google.com/p/php-for-android,,0,12,0,14,4,0,0,0,0,0,0,3610,0,0,0,0,0,0,148,,26,,
Cisteaux/gimp-dds,1662,0,0,0,User,False,190,5,16,1,False,Automatically exported from code.google.com/p/gimp-dds,,0,13,0,19,24,0,0,0,0,0,0,3694,0,0,0,0,0,0,2,,0,,
dlopez890/skipfish,352,0,0,0,User,False,9,2,0,1,False,Automatically exported from code.google.com/p/skipfish,,0,18,0,39,178,0,0,0,0,0,0,2952,0,0,0,0,0,0,2,,0,,
id-Software/Quake-III-Arena,3576,4846,307,1393,Organization,False,1,1,0,1,False,Quake III Arena GPL Source Release,,0,0,0,,,,,1,1,0,0,3057,0,0,0,0,0,0,18,0,,,
DeXP/xkb-switch-win,196,15,1,1,User,False,16,1,1,1,False,xkb-switch-lib API port to Win32/Win64,,0,6,0,0,1,0,0,0,0,0,0,2630,0,0,0,0,0,0,30,,26,,"xkb-switch-winþ==============þþxkb-switch-lib API port to Win32/Win64. Needed for [vim-xkbswitch plugin](http://www.vim.org/scripts/script.php?script_id=4503) (GitHub: https://github.com/lyokha/vim-xkbswitch ) .þþþInstallation and configuringþ----------------------------þþIn windows you need not only the plugin, but DLL-files from [latest release](https://github.com/DeXP/xkb-switch-win/releases). If you have 64-bit Vim, you need libxkbswitch64.dll. For 32-bit version use libxkbswitch32.dll. If you do not know - you can get both, plugin detects version automatically.þþBy default you need to put DLL-file into top directory of Vim (where vim.exe is located). Or you need to set g:XkbSwitchLib variable:þþ```vimþlet g:XkbSwitchLib = 'c:\path\to\dll\libxkbswitch32.dll' þ```þþþCharacter mapsþ--------------þþCurrently only Russian winkeys layout translation map ('ru') is supported out of the box for vim-xkbswitch. But you can create your own layout-file, and 'charmapgen' can help you.þþCharmap generator outputs current installed in system charmaps to console. The easiest way to do that - download 'charmapgen32.exe' and 'charmapgen.bat' from [/charmap](https://github.com/DeXP/xkb-switch-win/tree/master/charmap) directory. Than double click on bat-file - a new file will be appeared in current directory.þþThan put 'charmap.txt' to your top directory of Vim (where vim.exe is located). You can see languages in this file. Than add needed languages to vimrc :þþ```vimþlet g:XkbSwitchIMappingsTrData = 'charmap.txt'þlet g:XkbSwitchIMappings = ['ru', 'by', 'ua']þ```þþYou can see samples of charmap files in [/charmap](https://github.com/DeXP/xkb-switch-win/tree/master/charmap) directory.þþþAuthorsþ-------þþAuthor: Dmitry Hrabrov a.k.a. DeXPeriXþþþOriginal xkb-switch for Linux: https://github.com/ierton/xkb-switch "
j4cobgarby/raytracer,12521,0,1,1,User,False,29,1,0,1,False,"My second renderer - works similar to my last one but can render anything (as opposed to AABBs), and the camera can rotate better.",,0,9,0,4,0,0,0,0,0,0,0,824,0,0,0,0,0,0,86,,13,,"# RaytracerþþI'm making a raytracer which is cooler than my previous ray _caster_, because the camera can rotateþalong a pitch and yaw, and also can render anything! As opposed to the last one just being able toþrender AABBs. To be fair, the last one would be reasonable to use for a game, whereas this - like anyþother raytracter - wouldn't be.þþ## Contributorsþþ... would be very useful. Read `CONTRIBUTING.md` for an overview of the different files, and also someþguidelines to contributing.þþ## Examplesþþ![Example 1](examples/suzanne.bmp)þ![Example 2](examples/out.bmp)"
XPila/HW,80,0,1,0,User,False,44,1,0,1,False,Hardware Library,,0,7,0,3,1,0,0,0,0,0,0,1664,0,0,0,0,0,0,20,,1,,# HWþHardware Libraryþtestþtest2
stefanesser/opensource_taig,168,129,44,33,User,False,20,1,0,1,False,Lets create an open source version of the latest TaiG jailbreak.,,0,7,0,1,0,0,0,0,2,0,0,1818,0,0,0,0,0,0,13,,979,,# opensource_taigþLets create an open source version of the latest TaiG jailbreak.þþ## StatusþþCurrently decompilation of the taig untether binary in progress.þBinary in question has the following properties:þþ* Filename: taigþ* Size: 312672 bytesþ* SHA1: 065783b31dd2016cc3d48e6b174fcc12e2dff337þ* MD5: 38dd260a09690465adaf872268def218þ* SHA256: df9c72d2f7be90847affdeec6e18483e985e093ead3264c962883d6ae103758bþþþtaig binary contains a number of obfuscated strings:þþ* IOPMrootDomainþ* IOHIDResourceþ* IOHIDLibUserClientþ* IOHIDEventServiceþ* IOUserClientClassþ* ReportDescriptorþ* ReportIntervalþþComponents found in the untether so far:þþ* planetbeing's patchfinder - https://github.com/planetbeing/ios-jailbreak-patchfinder/blob/master/patchfinder.cþ* libtarþ* google-toolbox-for-mac - https://code.google.com/p/google-toolbox-for-mac/
janiex/TheRQMProject,127,0,1,0,User,False,2,1,0,1,False,,,0,7,0,1,0,0,0,0,0,0,0,1132,0,0,0,0,0,0,9,,6,,"# TheRQMProjectþþþThe project contains the complete set of attributes like OSEK, ACC ADXL345 drivers, to achieve the required functionality for Ride Quality Measurement.þþCreate workspace, repository, project from scratchþ1) Create new workspace: File--> Switch worksapce --> Other . Type in or browse the directory name of the new workspace.þ2) Create new git: File-->New-->Other-->Git-->Git Repository. Browse to the same directory as above workspace.þ3) Create a new project: File --> New--> CCS project:þ3.1) Select the target (I use Tiva TM4C123GH6PM)þ3.2) Select the connection (I use Stellaris In-Circuit Debug Interface).þ3.3) Select a project template and project name (default will be Empty Project with main.c) --> finishþ4) Create and push to github: þ4.1) open github for windows --> click on ""+"" --> Add tab --> the directory path of the workspace aboveþ4.2) Commitþ4.3) Publish Repository: if you're already log-in before. Then just type name and description. Check private repository if you want to make it private ($7/per month).þ5) Restart CCS"
enchev/ios-ng2-tns,24996,5,0,0,User,False,2,1,0,1,False,,,0,7,0,1,0,0,0,0,0,0,0,1293,0,0,0,0,0,0,10,,59,,Read more about this app here: https://medium.com/@enchev/extend-your-existing-ios-app-with-angular-2-and-nativescript-c2225c9bf616#.tj7r7lez4þþRead more about NativeScript and Angular2 here: https://github.com/NativeScript/NativeScript https://github.com/NativeScript/nativescript-angular
Estwald/PSDK3v2,333582,27,7,20,User,False,17,1,0,1,False,"Homebrew PS3 SDK under MinGW/Win32 based in the marioga compilation, psl1ght, Tiny3D, and others libs",,0,6,0,1,0,1,0,0,0,0,0,2679,0,0,0,0,0,0,5,,30,,"PSDK3 v2þ========þþBasado en el trabajo de Estwald, Marioga y otros sceners y en librer�as como PSL1GHT, Tiny3D y PS3 Soundlib, PSDK3 pretende ser un entornoþestable de programaci�n de homebrew en PS3 bajo Windows, sin influencias externas que amenacen la integridad de las librer�as, ni de las þaplicaciones que se construyen con ellas, cuando alguien decide cambiarlo todo, rompiendo la compatibilidad con las aplicaciones creadas þcon esas librer�as y provocando un claro perjuicio. Aqu� el lema es ""si algo funciona, no lo toques"" y se pueden a�adir cosas, pero noþrestar, ni toquetear pijoteramente el c�digo.þþQue hacer:þþ- Baja el ZIP (en el boton).þþ- Crea en raiz de C: la carpeta ""PSDK3v2"" y descomprime dentro el contenido (puede instalarse en otras unidades o directorios cambiando þlos ficheros Make*.bat, si se prefiere, a posteriori)þþ- Extrae el fichero MinGW.7z (yo uso IZArc) y tendr�s la carpeta ""MinGW"" con el entorno para hacer Make, etc.þþ- En PSDK3v2\MinGW\msys\1.0\etc edita el fichero ""profile"" y al final, donde pone ""export PS3SDK=""/c/PSDK3v2""""þcambia la ruta por la que vayas a utilizar, si quieres lanzar la consola (msys.bat) con las variables de entornoþnecesarias.þþ- Extrae ps3dev.7z y tendr�s la carpeta ""ps3dev"" con los compiladores de PS3, las librer�as necesarias ya compiladasþy las utilidades þþLo que contiene el proyecto:þþ- MinGW: entorno MinGW/MSYS montado espec�ficamente para trabajar con las herramientas de PS3.þþ- ps3dev: contiene los compiladores, las librer�as externas compiladas y otras herramientas de apoyo, como la scetool þpara firmar aplicaciones (en sustitucion de las de geohot), crear paquetes, as� como algunas DLL necesarias. þþ- psl1ght: contiene la libreria psl1ght ya compiladaþþ- libraries-src: contiene los c�digos fuentes de PSL1GHT, Tiny3D y PS3 Soundlibþþ- project: contiene los c�digos fuentes de los ejemplos y los ficheros .bat para crear los ejecutablesþþCambiando la ruta de instalaci�n desde C: a otraþ-------------------------------------------------þþAparte de los cambios mencionados antes en /etc/profile (solopara la consola), si editas los Make*.bat y cambias:þþset PS3SDK=/F/PSDK3v2þset WIN_PS3SDK=F:/PSDK3v2þþpor la ruta correspondiente, bastar�.þþCompilandoþ----------þþMake_clean.bat -> Borra todos los ficheros compilados, excepto los .pkgþþMake_SELF.bat -> Crea un .self firmado con las keys 3.40 (mediante scetool)þþMake_EBOOT.BIN.bat -> Crea el self NPDRM EBOOT.BIN de la aplicaci�n. Esto es interesante, si se actualiza v�a FTP un .pkg þ                      ya instaladoþþMake_PKG.bat -> Crea el .pkg para instalar la aplicaci�n.þþLa ""scetool"" requiere un fichero con keys que usa una ruta relativa. Por ese motivo se ha incluido la utilidad ""fake_scetool""þpara apoyarla.þþppu_rules ha sido modificado convenientemente para usar esa aplicaci�n. Por defecto, contiene las keys en ""SCETOOL_FLAGS"". Se puedeþsobrecargar definiendo con SCETOOL_FLAGS := y a�adir m�s par�metros con SCETOOL_FLAGS += . Las keys 3.40 funcionan en los CFW que usamosþy por eso las he escogido.þþLos Makefile de referencia los tienes en los ejemplos de project/sample. En concreto, ""fireworks3D"" es un ejemplo de utilizaci�n conjuntaþde Tiny3D y la PS3 Soundlib y las librer�as necesariasþþEspero que estas herramientas os sean �tiles en vuestros proyectos.þþSaludos"
c0untd0wn/pintos,492,0,1,0,User,False,2,2,0,1,False,Pintos Project,,0,7,0,1,0,0,0,0,0,0,0,2147,0,0,0,0,0,0,25,,19,,pintosþ======þþPintos Project
satya1993/jenkins,2,0,0,0,User,False,6,2,0,1,False,,,0,8,0,0,0,0,0,0,1,0,0,,0,0,0,0,0,0,4,,0,,
mareku322/diosmios,208,0,0,0,User,False,22,2,0,0,False,Automatically exported from code.google.com/p/diosmios,,0,10,0,1,14,0,0,0,0,0,0,,0,0,0,0,0,0,1,,0,,
neutrak/accirc,393,2,3,0,User,False,225,2,2,0,False,"accidental irc, a multi-server ncurses irc client",,0,7,0,0,1,0,0,0,0,0,0,,0,0,0,0,0,0,12,,3,,
ColinGilbert/stringencoders,1456,0,0,0,User,False,414,2,0,0,False,Automatically exported from code.google.com/p/stringencoders,,0,12,0,22,20,0,0,0,0,0,0,,0,0,0,0,0,0,139,,7,,
Jesse-V/RLAGS-USU,343053,2,3,0,User,False,104,1,0,2,False,RLAG Sensor - Space Dynamics Laboratory,,0,7,0,0,0,0,0,0,0,0,0,2210,0,0,0,0,0,0,35,,56,,
LSayhi/book-paper-note,591903,50,6,19,User,False,27,1,0,1,False,books papers notes,,0,8,0,0,0,0,0,0,0,0,0,866,0,0,0,0,0,0,18,,28,,
wenshuai-xi/Document,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
stijnmeul/thesis,358681,1,2,0,User,False,157,1,0,1,False,Identity-based encryption for online social networks. Thesis text + implementation for applying identity-based encryption on Facebook by relying on the existing Scramble interface.,,0,6,0,0,0,0,0,0,0,0,0,2308,0,0,0,0,0,0,4,,1,,"Practical Identity-based encryption for Online Social Networksþ======þþThis is my github working directory containing all my thesis work on identity-based encryption for Online Social Networks. My thesis builds on Scramble ( https://www.cosic.esat.kuleuven.be/scramble/ ) a tool developed at COSIC, KU Leuven to broadcast encrypted status updates on Online Social Networks (OSNs).þþScramble originally relies on OpenPGP for key management such that any user with a keypair uploaded to the OpenPGP infrastructure can start receiving encrypted messages. However, since an average user is not familiar with asymmetric cryptography and all the additional steps required for uploading and generating keys to the OpenPGP infrastructure, the existing Scramble architecture is often considered complex by average social network users.þþInstead of relying on OpenPGP, my thesis designs a new system relying on identity-based encryption for key management. To show the feasibility of our solution, Scramble is further extended to apply our architecture to Facebook. Facebook URLs are being used as a public key. Private keys are initialsed based on Distributed Key Generation (DKG) in order to keep them secure as long as a threshold number of Public Key Generators (PKGs) are not colluding. For more information please check out the ""thesistext"" folder containing my full thesis text or the ""pets"" folder containing an article that was submitted to HotPETS 2014.þþThis thesis was promoted by prof. dr. ir. Vincent Rijmen and prof. dr. ir. Bart Preneel and supervised by Filipe Beato. Thesis results were evaluated by assessors prof. dr. ir. Claudia Diaz and prof. dr. ir. Frank Piessens. The thesis was graded with 18/20 (90%) and received the Vasco Data Security thesis prize ( http://eng.kuleuven.be/evenementen/masterproefprijzen2013-2014/criteriamasterproefprijzen2014 ).þþDirectory structureþ=====þanotherScramble:          Firefox files required for the Scramble Firefox extension.þþcode:                     C and C++ files required for the identity-based encryption mechanismþþ code/client:             C and C++ files for clientside encryption and decryptionþ þ code/cppmiracl:          Old C and C++ MIRACL files that can not be used for multithreadingþ þ code/dkg:                C and C++ files to support the DKG mechanismþ þ code/htdocs-thesis:      PHP files and C++ binaries for the server side of the DKG mechanismþ þ code/miraclthread:       C and C++ MIRACL files used to support the identity-based encryption mechanismþ þ code/setupPkg:           C and C++ files to build, install and copy the correct serverside binaries to the PHP server pathþ þ code/socketDemo:         A simple socket demoþ þfinalpresentation:        Keynote of my final thesis presentation in June 2014þþguidelines:               files describing the goals of this thesis and how the text should look likeþþintermediatepresentation: Keynote of my intermediate thesis presentation in December 2013þþpets:                     short article that was submitted to HotPETS 2014 (although not accepted)þþplanning:                 Ghant chart with deadlinesþþscramble:                 old Firefox files for an outdated Scramble extensionþþsecuritymodel:            files describing how the attacker model looks like. See thesistext for updated version."
yubaoliu/Library,1623614,0,1,0,User,False,3,1,0,1,False,"book, ppt, pdf resources for Software Development",,0,8,0,0,0,0,0,0,0,0,0,873,0,0,0,0,0,0,102,,10,,"# Libraryþbook, ppt, pdf resources for Software Development"
lisider/my_book,536042,22,5,21,User,False,8,1,0,1,False,喜欢的技术书籍,,0,8,0,0,0,0,0,0,0,0,0,775,0,0,0,0,0,0,50,,4,,
AdnanHaiderAD/MSc-Dissertation,2070000,0,1,0,User,False,258,1,0,1,False,,,0,6,0,0,0,0,0,0,0,0,0,2630,0,0,0,0,0,0,19,,2,,MSc-Dissertationþ================
yash98/Semester3-StudyMaterial,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
clhne/mybook,696858,1,1,0,User,False,22,1,0,2,False,,,0,8,0,0,0,0,0,0,0,0,0,943,0,0,0,0,0,0,805,,40,,# mybook
yjjing/ebooks,1567469,0,1,0,User,False,40,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1629,0,0,0,0,0,0,4,,0,,
lianhongHou/books,1521857,0,1,0,User,False,26,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1335,0,0,0,0,0,0,12,,0,,
olinrobotics/irl,1896713,8,15,1,Organization,False,2,9,0,2,False,"ST-R17 and UR5 Interactive Robotics Laboratory, formerly known as Edwin",,5,7,0,0,0,0,0,0,88,0,0,621,0,0,0,0,0,0,57,6,,,"### This repository is deprecated! Please check out the current repository, [hiro](https://github.com/olinrobotics/hiro).þ---þ# Interactive Robotics LaboratoryþST-R17 and UR5 interactive co-working dinosaur (and dragon) robotic armsþWebsite: https://olinrobotics.github.io/irl/þArm Manual: http://strobotics.com/manuals/R17%20manual.pdfþþ## TroubleshootingþþGrouped by error messageþþ*Encoder-stepper mismatch*þ* Is the area around the K11R control box and robot arm clear?þ* Turn the controller on/off. (Power-cycle the robot)þ* The arm generally makes a pseudo circle using its waist at the beginning of the calibration, done counterclockwise. Try starting it closer to its end point for that rotation, and then execute the startup code. The same thing could be tried for any of the joints (or axes, which is what Roboforth refers to them by).þþ---þþ*rosrun irl arm_node.py is stuck at ""in block_on_result""*þ* Is the turn-key at the front of the controller set to warm?þ* Is the light on the Tripp-Lite serial-usb converter blinking?þ* Open a new terminal window and run the command againþþ---þþ*Controller refuses to turn on*þ* Check the fuses. Two are located in the back of the controller, one is located on the power supply itself. (Where the power cord is plugged in)"
Gloader/Laboratory,455256,0,1,0,User,False,18,1,0,1,False,,,0,6,0,0,0,0,0,0,0,0,0,2343,0,0,0,0,0,0,4,,0,,Laboratoryþ==========
piaoshuai/DVD_BOOK,1917856,2,1,1,User,False,2,1,0,2,False,基于Xilinx ZYNQ 嵌入式软件硬件协同设计奖实战指南 光盘资源,,0,6,0,,,,,0,0,0,0,2448,0,0,0,0,0,0,2,,1,,DVD_BOOKþ========þþ基于Xilinx ZYNQ 嵌入式软件硬件协同设计奖实战指南 光盘资源þ</>þgit clone https://github.com/Zrobot/DVD_BOOK.git
VhJoren/CompPhys-Exercise-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
vania-mendonca/BookFinder,758552,0,3,1,User,False,70,2,0,3,False,Project in the context of 3D Simulation and Game Programming,,0,7,0,0,0,0,0,0,0,0,0,2224,0,0,0,0,0,0,11,,16,,PSJ_Unityþ=========þþINSTITUTO SUPERIOR TECNICOþMaster Degree in Information Systems and Computer EngineeringþþCourse: Three-Dimensional Simulation and Game Programmingþ57690 - Soraia Meneses Alarcaoþ64052 - Catia Diasþ68202 - Vania Mendonca
tgut/books,586483,1,1,2,User,False,11,1,0,1,False,useful book on programming,,0,8,0,0,0,0,0,0,0,0,0,467,1,3,0,0,0,0,10,,0,,# booksþsome books on computer programmingþthe file below are deleted for +100MB size.þþ./android/深入理解Android内核设计思想（带书签完整高清版）.林学森.pdfþ./android/Android：卷II 邓凡平著 PDF扫描版.pdfþ./android/Android系统源代码情景分析 [罗升阳著][电子工业出版社][2012.10][840页].pdf
isislovecruft/library--,1886220,391,37,57,User,False,1974,1,0,2,False,The papers and books I've read or am about to read.,,0,7,0,0,0,0,0,0,1,0,0,2308,0,0,0,0,0,0,160,,1,,
Leon555/Books-Programming,761708,5,2,5,User,False,1,1,0,1,False,"repo for OS-related books like linux, network, system",,0,7,0,0,0,0,0,0,0,0,0,1881,0,0,0,0,0,0,40,,9,,
klmr92/uguu,0,0,0,1,User,False,778,11,0,1,False,Automatically exported from code.google.com/p/uguu,,0,31,0,27,56,0,0,0,0,0,0,3813,0,0,0,0,0,0,2,,0,,
K0-0K/book,3508136,8,3,68,Organization,False,25,1,0,1,False,book,,0,7,0,,,,,0,0,0,0,2553,0,0,0,0,0,0,9,0,,,bookþ====þþbook
yenng/RNG,0,0,1,0,User,False,3,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1699,0,0,0,0,0,0,31,,7,,Project {#index}þ===================þþþThis is an example project.
Larzdk/Learning,0,0,1,0,User,False,4,1,0,1,False,Example Code from Learning C,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,2,,0,,
HanaAsakura/Juan_Aguilar_hw6,0,0,1,0,User,False,2,1,0,1,False,:),,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,12,,1,asakura09/Juan_Aguilar_hw6,# Juan_Aguilar_hw6þ:)
prachi-shahi/HelloWorld,0,0,1,0,User,False,2,1,0,1,False,First Repo,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,11,,2,,
chengyihe/kernel-module-debugfs_hello,0,1,0,0,User,False,1,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,22,,5,,
cbyu/hello-program,0,0,1,0,User,False,3,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,4,,1,,# hello-programþSending my first file
maxmiru/HelloWorld,0,0,1,0,User,False,8,1,0,1,False,This is my first github repository,,0,7,0,0,1,0,0,0,3,0,0,1678,0,0,0,0,0,0,3,,0,,"# HelloWorldþThis is my first github repositoryþþThe purpose of this repository is to introduce me to the world!þþHello WorldþHi OceansþþOk i hope now everybody knows what this project is for.þþThis is where Master comes in, let's see if it causes any conflictsþþBBþþP/S: i'll add more stuffs into this project later ."
pietrobarbiero/hello-world,0,0,0,0,User,False,22,2,0,0,False,My first repository,,0,7,0,0,1,0,0,0,1,0,0,,0,0,0,0,0,0,17,,3,,"hello-worldþþThis is my first repository.þI'm new on GitHub, but it's very interesting and I think I'll use it for long."
wdzhu82/npapi-chrome-plugin-helloworld-example,0,0,0,0,User,False,3,2,0,0,False,Automatically exported from code.google.com/p/npapi-chrome-plugin-helloworld-example,,0,11,0,2,0,0,0,0,0,0,0,,0,0,0,0,0,0,1,,0,,
nchronas/Hello-C,0,0,0,2,User,False,4,1,0,1,False,A hello world project using C with Resin.io,,0,7,0,0,0,0,0,1,0,0,0,1678,0,0,0,0,0,0,94,,11,,# Hello C in Resin.ioþþThis is a very simple project that is an example of how to run C code on a device that is supported by [Resin.io](http://resin.io).þþ### Note for Raspberry pi 1þIf the device you are planning to use is a raspberry pi 1 you will have to modify Dockerfile.template in order to use the application.þ```þFROM resin/%%RESIN_MACHINE_NAME%%-debianþ```þToþ```þFROM resin/%%RESIN_MACHINE_NAME%%-raspbianþ```
reksi0/helloworld,0,0,1,0,User,False,2,1,0,1,False,program that turns other programs into helloworld apps,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,3,,0,,# helloworldþprogram that turns other programs into helloworld apps
LimboLee/HelloWorld,0,0,1,0,User,False,2,1,0,1,False,Hello Github,,0,7,0,0,0,0,0,0,0,0,0,1671,0,0,0,0,0,0,1,,0,,# HelloWorldþHello Github
wayzyaw/first_hello,0,0,1,0,User,False,1,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,1,,0,,this is the first time to use git.
Martyn-MSys/Hello-git,0,0,1,1,User,False,6,1,0,1,False,example project,,0,7,0,0,1,0,0,0,2,0,0,1678,0,0,0,0,0,0,2,,0,,# Hello-gitþexample þþplease always do the following when pushing code:þþpull commit push
VictaminC/helloworld,0,0,1,0,User,False,15,2,0,1,False,我的第一个git repository,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,3,,0,,
yuqi1129/helloworld,0,0,1,0,User,False,2,1,0,1,False,My first project,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,126,,15,,# helloworldþMy first project
takano32/CodeIQ-Challenge-2518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
yujacha/command_hello,0,0,1,0,User,False,12,2,0,1,False,test_for_learning,,0,7,0,0,0,0,0,0,0,0,0,1678,0,0,0,0,0,0,4,,0,,remote repository of test
MoMoWan/hello1,0,0,2,0,User,False,3,4,0,0,False,第一个项目,,0,7,0,1,0,0,0,1,0,0,0,,0,0,0,0,0,0,128,,0,,
yuzhigangcn/helloworld-test,0,0,1,0,User,False,2,1,2,1,False,a helloworld demo test,,0,7,0,0,0,0,0,0,0,0,0,1671,0,0,0,0,0,0,5,,0,,
anontruck/Mytest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
chienhua7243/TestFile,0,0,1,0,User,False,2,1,0,1,False,test,,0,7,0,0,0,0,0,0,0,0,0,1664,0,0,0,0,0,0,1,,0,,
tsubone/hello-world,0,0,1,0,User,False,2,1,0,1,False,test repository,,0,7,0,0,0,0,0,0,0,0,0,1664,0,0,0,0,0,0,12,,0,,# hello-worldþtest repository
huangjiehua/xiaohua,0,0,1,0,User,False,3,1,0,1,False,我的第一个repository,,0,7,0,0,0,0,0,0,0,0,0,1790,0,0,0,0,0,0,23,,1,,# xiaohuaþ我的第一个repository，吼吼
akshit-sharma/ODM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
xaviedga/Mytest,0,0,1,0,Organization,False,2,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1657,0,0,0,0,0,0,1,0,,,
TEAM-RAZOR-DEVICES/device_oneplus_oneplus2_old,0,0,6,0,Organization,False,202,1,0,16,False,,,0,7,0,0,0,0,0,0,0,0,0,2196,0,0,0,0,0,0,54,1,,,
HyOsori/git_example,0,0,0,0,Organization,False,3,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1538,0,0,0,0,0,0,33,0,,,# example
HuyGemini/Array,0,1,1,2,User,False,2,1,0,1,False,G2-1. Array project in program technique class,,0,7,0,0,0,0,0,2,0,0,0,1573,0,0,0,0,0,0,5,,0,,# ArrayþG2-1. Array project in program technique class
michaelfeinberg/ListHW,0,0,1,0,User,False,2,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1342,0,0,0,0,0,0,22,,0,,# List
wanksta/FirstCodeOnGitHub,0,0,1,0,User,False,2,1,0,1,False,A test project,,0,7,0,0,0,0,0,0,0,0,0,1587,0,0,0,0,0,0,3,,0,,# FirstCodeOnGitHubþA test project
mohanasundari/sample_repo_1,0,0,1,0,User,False,1,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1545,0,0,0,0,0,0,1,,0,,
yoichiyaguchi/miru-wakate,0,0,1,0,User,False,2,2,0,1,False,Test repository for MIRU 2016 wakate-no-kai.,,0,7,0,0,0,0,0,0,0,0,0,1419,0,0,0,0,0,0,1,,0,,miru-wakateþ===þþTest repository for MIRU wakate.
yaejin91/the-c-programming-language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
mju-oss-class/sample_proj1,2,2,1,50,User,False,6,1,0,1,False,,,0,9,0,0,0,0,0,1,0,0,0,201,0,0,0,0,0,0,18,,15,,# sample_proj1
MidweekGames/gmBASS,0,0,2,0,Organization,False,1,1,0,0,False,BASS for GameMaker,,0,7,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,4,0,,,
CPTS224/HW4,0,0,0,3,Organization,False,1,1,0,1,False,,,0,8,0,0,0,0,0,0,0,0,0,957,0,0,0,0,0,0,9,0,,,
adarsh3010/example.c,0,0,0,0,User,False,1,1,0,1,False,example.c,,0,8,0,0,0,0,0,0,0,0,0,922,0,0,0,0,0,0,4,,0,,
nandujkishor/open-web-build,1,0,1,10,User,False,2,2,0,1,False,,,0,7,0,0,0,0,0,16,0,0,0,1804,0,0,0,0,0,0,24,,9,,# open-web-build
gandarela/LLRedBlack,0,0,0,0,User,False,1,1,0,1,False,,,0,8,0,0,0,0,0,0,0,0,0,936,0,0,0,0,0,0,7,,0,JoaoPedroD/LLRedBlack,
DevTLBB/Launch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
concretecloud/chirp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
Unsonghero/home-exercise,0,0,0,0,User,False,1,1,0,0,False,c language,,0,8,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,2,,0,,
Git-Math/malloc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
brunomatheusc/Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
GMECE/PROGRAMS,0,0,0,0,User,False,1,1,0,1,False,C PROGRAMS,,0,8,0,0,0,0,0,0,0,0,0,894,0,0,0,0,0,0,2,,0,,
ArenMarkBoghozian/Mathematics,0,0,0,0,User,False,1,1,0,1,False,,,0,8,0,0,0,0,0,0,0,0,0,789,0,0,0,0,0,0,11,,3,ArenBoghozian8/Mathematics,
iNavFlight/inav,249989,1291,149,742,Organization,False,9689,60,66,261,False,INAV: Navigation-enabled flight control software,https://inavflight.github.io,6,22,2,301,2766,104,221,69,2697,31,215,2630,20,291,10825,7978,0,0,14,3,,,"# INAV - navigation capable flight controllerþþ## F3 based flight controllersþþ> STM32 F3 flight controllers like Omnibus F3 or SP Racing F3 are deprecated and soon they will reach the end of support in INAV. If you are still using F3 boards, please migrate to F4 or F7.þþ![INAV](http://static.rcgroups.net/forums/attachments/6/1/0/3/7/6/a9088858-102-inav.png)þ![Travis CI status](https://travis-ci.org/iNavFlight/inav.svg?branch=master)þþ## Featuresþþ* Runs on the most popular F4 and F7 flight controllersþ* Outstanding performance out of the boxþ* Position Hold, Altitude Hold, Return To Home and Missionsþ* Excellent support for fixed wing UAVs: airplanes, flying wings þ* Fully configurable mixer that allows to run any hardware you want: multirotor, fixed wing, rovers, boats and other experimental devicesþ* Multiple sensor support: GPS, Pitot tube, sonar, lidar, temperature, ESC with BlHeli_32 telemetryþ* SmartAudio and IRC Tramp VTX supportþ* DSHOT and Multishot ESCsþ* Blackbox flight recorder loggingþ* On Screen Display (OSD) - both character and pixel styleþ* Telemetry: SmartPort, FPort, MAVlink, LTMþ* Multi-color RGB LED Strip supportþ* Advanced gyro filtering: Matrix Filter and RPM filterþ* Logic Conditions, Global Functions and Global Variables: you can program INAV with a GUIþ* And many more!þþFor a list of features, changes and some discussion please review consult the releases [page](https://github.com/iNavFlight/inav/releases) and the documentation.þþ## Toolsþþ### INAV ConfiguratorþþOfficial tool for INAV can be downloaded [here](https://github.com/iNavFlight/inav-configurator/releases). It can be run on Windows, MacOS and Linux machines and standalone application.  þþ### INAV Blackbox ExplorerþþTool for Blackbox logs analysis is available [here](https://github.com/iNavFlight/blackbox-log-viewer/releases)þþ### Telemetry screen for OpenTXþþUsers of FrSky Taranis X9 and Q X7 can use INAV Lua Telemetry screen created by @teckel12 . Software and installation instruction are available here: [https://github.com/iNavFlight/LuaTelemetry](https://github.com/iNavFlight/LuaTelemetry)þþ## InstallationþþSee: https://github.com/iNavFlight/inav/blob/master/docs/Installation.mdþþ## Documentation, support and learning resourcesþ* [Fixed Wing Guide](docs/INAV_Fixed_Wing_Setup_Guide.pdf)þ* [Autolaunch Guide](docs/INAV_Autolaunch.pdf)þ* [Modes Guide](docs/INAV_Modes.pdf)þ* [Wing Tuning Masterclass](docs/INAV_Wing_Tuning_Masterclass.pdf)þ* [Official documentation](https://github.com/iNavFlight/inav/tree/master/docs)þ* [Official Wiki](https://github.com/iNavFlight/inav/wiki)þ* [INAV Official on Telegram](https://t.me/INAVFlight)þ* [INAV Official on Facebook](https://www.facebook.com/groups/INAVOfficial)þ* [RC Groups Support](https://www.rcgroups.com/forums/showthread.php?2495732-Cleanflight-iNav-(navigation-rewrite)-project)þ* [Video series by Painless360](https://www.youtube.com/playlist?list=PLYsWjANuAm4qdXEGFSeUhOZ10-H8YTSnH)þ* [Video series by Paweł Spychalski](https://www.youtube.com/playlist?list=PLOUQ8o2_nCLloACrA6f1_daCjhqY2x0fB)þþ## ContributingþþContributions are welcome and encouraged.  You can contribute in many ways:þþ* Documentation updates and corrections.þ* How-To guides - received help?  help others!þ* Bug fixes.þ* New features.þ* Telling us your ideas and suggestions.þ* Buying your hardware from this [link](https://inavflight.com/shop/u/bg/)þþA good place to start is Telegram channel or Facebook group. Drop in, say hi.þþGithub issue tracker is a good place to search for existing issues or report a new bug/feature request:þþhttps://github.com/iNavFlight/inav/issuesþþhttps://github.com/iNavFlight/inav-configurator/issuesþþBefore creating new issues please check to see if there is an existing one, search first otherwise you waste peoples time when they could be coding instead!þþ## DevelopersþþPlease refer to the development section in the [docs/development](https://github.com/iNavFlight/inav/tree/master/docs/development) folder.þþþ## INAV Releasesþhttps://github.com/iNavFlight/inav/releases"
zpfd3d/MyNote2,13,0,1,0,User,False,15,2,0,1,False,Just some note.,,0,8,0,,,,,0,0,0,0,1125,0,0,0,0,0,0,29,,0,,111 2017-05-15þþgit remote add origin https://github.com/a379039233/MyNote2.gitþgit push -u origin master
TadaNoButa/GTK_Glade_C_Treeview_Test,4,0,0,0,User,False,2,1,0,1,False,A test of Treeview that should update EntryBoxes (GTK/Glade/C),,0,8,0,0,0,0,0,0,0,0,0,908,0,0,0,0,0,0,2,,0,,# GTK_Glade_C_Treeview_TestþA test of Treeview that should update EntryBoxes (GTK/Glade/C)
ArchSirius/inf2610,1417,0,1,2,User,False,24,1,0,1,False,Noyau d'un système d'exploitation,http://www.polymtl.ca/etudes/cours/de…,0,7,0,0,0,0,0,0,0,0,0,1713,0,0,0,0,0,0,22,,12,,
openspaceaarhus/ZABLET,22648,5,8,0,Organization,False,33,1,0,3,False,OSAAs TV-B-Gone + miniPOV clone,http://zablet.osaa.dk,0,0,0,0,0,0,0,0,0,0,0,3414,0,0,0,0,0,0,24,11,,,
lecram/gifenc,14,143,9,16,User,False,18,1,0,1,False,small C GIF encoder,,4,7,0,1,4,0,0,1,0,0,0,1776,0,0,0,0,0,0,27,,45,,
argyi2378/hello-world,2,0,0,0,User,False,4,1,0,1,False,argyi's first repository,,0,8,0,0,0,0,0,0,1,0,0,810,0,0,0,0,0,0,1,,0,,# hello-worldþargyi's first repositoryþyou have to make changes.
nodemcu/nodemcu-firmware,111218,6083,563,2760,Organization,False,2251,5,27,152,False,"Lua based interactive firmware for ESP8266, ESP8285 and ESP32",https://nodemcu.readthedocs.io,8,20,1,142,1744,57,51,34,1239,22,50,2049,18,61,75195,13360,0,0,5,6,,,"# NodeMCU 3.0.0þþ[![Join the chat at https://gitter.im/nodemcu/nodemcu-firmware](https://img.shields.io/gitter/room/badges/shields.svg)](https://gitter.im/nodemcu/nodemcu-firmware?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)þ[![Build Status](https://travis-ci.org/nodemcu/nodemcu-firmware.svg)](https://travis-ci.org/nodemcu/nodemcu-firmware)þ[![Documentation Status](https://img.shields.io/badge/docs-master-yellow.svg?style=flat)](http://nodemcu.readthedocs.io/en/master/)þ[![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE)þþ### A Lua based firmware for ESP8266 WiFi SOCþþNodeMCU is an open source [Lua](https://www.lua.org/) based firmware for the [ESP8266 WiFi SOC from Espressif](http://espressif.com/en/products/esp8266/) and uses an on-module flash-based [SPIFFS](https://github.com/pellepl/spiffs) file system. NodeMCU is implemented in C and is layered on the [Espressif NON-OS SDK](https://github.com/espressif/ESP8266_NONOS_SDK).þþThe firmware was initially developed as is a companion project to the popular ESP8266-based [NodeMCU development modules]((https://github.com/nodemcu/nodemcu-devkit-v1.0)), but the project is now community-supported, and the firmware can now be run on _any_ ESP module.þþ# Summaryþþ- Easy to program wireless node and/or access pointþ- Based on Lua 5.1.4 but without `debug`, `io`, `os` and (most of the) `math` modulesþ- Asynchronous event-driven programming modelþ- more than **65 built-in modules**þ- Firmware available with or without floating point support (integer-only uses less memory)þ- Up-to-date documentation at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io)þþ### LFS supportþIn July 2018 support for a Lua Flash Store (LFS) was introduced. LFS  allows Lua code and its associated constant data to be executed directly out of flash-memory; just as the firmware itself is executed. This now enables NodeMCU developers to create **Lua applications with up to 256Kb** Lua code and read-only constants executing out of flash. All of the RAM is available for read-write data!þþ# Programming ModelþþThe NodeMCU programming model is similar to that of [Node.js](https://en.wikipedia.org/wiki/Node.js), only in Lua. It is asynchronous and event-driven. Many functions, therefore, have parameters for callback functions. To give you an idea what a NodeMCU program looks like study the short snippets below. For more extensive examples have a look at the [`/lua_examples`](lua_examples) folder in the repository on GitHub.þþ```luaþ-- a simple HTTP serverþsrv = net.createServer(net.TCP)þsrv:listen(80, function(conn)þ conn:on(""receive"", function(sck, payload)þ  print(payload)þ  sck:send(""HTTP/1.0 200 OK\r\nContent-Type: text/html\r\n\r\n<h1> Hello, NodeMCU.</h1>"")þ end)þ conn:on(""sent"", function(sck) sck:close() end)þend)þ```þ```luaþ-- connect to WiFi access pointþwifi.setmode(wifi.STATION)þwifi.sta.config{ssid=""SSID"", pwd=""password""}þ```þþ# DocumentationþþThe entire [NodeMCU documentation](https://nodemcu.readthedocs.io) is maintained right in this repository at [/docs](docs). The fact that the API documentation is maintained in the same repository as the code that *provides* the API ensures consistency between the two. With every commit the documentation is rebuilt by Read the Docs and thus transformed from terse Markdown into a nicely browsable HTML site at [https://nodemcu.readthedocs.io](https://nodemcu.readthedocs.io).þþ- How to [build the firmware](https://nodemcu.readthedocs.io/en/master/en/build/)þ- How to [flash the firmware](https://nodemcu.readthedocs.io/en/master/en/flash/)þ- How to [upload code and NodeMCU IDEs](https://nodemcu.readthedocs.io/en/master/en/upload/)þ- API documentation for every moduleþþ# ReleasesþþDue to the ever-growing number of modules available within NodeMCU, pre-built binaries are no longer made available. Use the automated [custom firmware build service](http://nodemcu-build.com/) to get the specific firmware configuration you need, or consult the [documentation](http://nodemcu.readthedocs.io/en/master/en/build/) for other options to build your own firmware.þþThis project uses two main branches, `master` and `dev`. `dev` is actively worked on and it's also where PRs should be created against. `master` thus can be considered ""stable"" even though there are no automated regression tests. The goal is to merge back to `master` roughly every 2 months. Depending on the current ""heat"" (issues, PRs) we accept changes to `dev` for 5-6 weeks and then hold back for 2-3 weeks before the next snap is completed.þþA new tag is created every time `dev` is merged back to `master`. They are listed in the [releases section here on GitHub](https://github.com/nodemcu/nodemcu-firmware/releases). Tag names follow the \<SDK-version\>-master_yyyymmdd pattern.þþ# SupportþþSee [https://nodemcu.readthedocs.io/en/master/en/support/](https://nodemcu.readthedocs.io/en/master/en/support/).þþ# Licenseþþ[MIT](https://github.com/nodemcu/nodemcu-firmware/blob/master/LICENSE) © [zeroday](https://github.com/NodeMCU)/[nodemcu.com](http://nodemcu.com/index_en.html)"
alzomor/Test-Project,2,0,0,0,User,False,4,2,0,1,False,Just a test Project to pratice GitHub,,0,8,0,0,0,0,0,0,0,0,0,901,0,0,0,0,0,0,1,,0,,# Test-ProjectþJust a test Project to pratice GitHub
vasisht/fastq_splitter,5,2,1,1,User,False,5,1,0,1,False,Fast split of fastq files,,0,7,0,0,0,0,0,0,0,0,0,1363,0,0,0,0,0,0,4,,0,,
renkun/u8glib,13393,0,0,1,User,False,951,2,78,1,False,Automatically exported from code.google.com/p/u8glib,,0,38,0,79,271,0,0,0,0,0,0,2091,0,0,0,0,0,0,15,,5,,
HybridAlpaca/Raptor,140439,1,2,0,Organization,False,192,2,0,1,False,Scary-Efficient Game Engine for PC,https://github.com/HybridAlpaca/Raptor,2,8,0,0,0,0,0,1,2,0,0,789,0,0,0,0,0,0,10,4,,,"![Raptor Logo](./Engine/Assets/Icons/raptor-logo.png ""Raptor Game Engine"")þþ# RaptorþþScary-efficient game engine for VR platforms, targeting PC and console.þþ## Getting StartedþþThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.þþ### PrerequisitesþþIn order to begin hacking on the engine, you'll first need to set up your development enviroment with all the shiny new features of the modern digital world.  On linux systems, some quick `apt` magic should do the trick:þþ```þ$ sudo apt-get updateþ$ sudo apt-get install build-essential software-properties-common -yþ$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test -yþ$ sudo apt-get updateþ$ sudo apt-get install g++-8 cmakeþ```þþ### InstallingþþOnce you have your build environment set up, all the rest of the engine software and dependencies will be automagically installed once the engine is built. Just run:þþ```þ$ ./Engine/Build/build.shþ```þþAnd that's it!  If you find that you've messed something up, or your binaries have turned into doc-eating monsters of doom, don't panic.  Removing all bins, libs and temporaries are as simple as cleaning for a fresh build, like so:þþ```þ$ ./Engine/Build/clean.shþ```þþ**Be warned**: this will clear all your build caches (for the engine source *and* all dependancies!), so running `clean` will require a full rebuild.þþ## DeploymentþþTo get this working on your system, just navigate to Raptor's root directory and enter:þþ```þ$ ./Engine/Binaries/Raptorþ```þþThis will launch the engine in its entirety (i.e. editor, build tools, project management, etc).þþ## Built Withþþ* [CMake](https://cmake.org/) - Automated build toolþ* [GLFW](http://www.glfw.org/) - Dependency Managementþ* [Love](http://spongebob.wikia.com/wiki/Neptune%27s_Spatula) - Perfect patties are made with love, not magicþþ## ContributingþþThere currently is no code of conduct, and there isn't an official contributing markdown page either :P If you still consider your soul worthy of helping out on the project, don't hesitate to email us at `hybridalpaca[at]gmail[dot]com`.  Please, use common sense when copying that email address.þþ## VersioningþþWe use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/HybridAlpaca/Raptor/tags).þþ## Authorsþþ* **Seth Traman** - *Lead Developer* - [cellman123](https://github.com/cellman123)þþSee also the list of [contributors](https://github.com/your/project/contributors) who participated in this project.þþ## LicenseþþThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for detailsþþ## Acknowledgmentsþþ* Thanks to all the folks over at GitHub who made open-source code sharing possibleþþ* Special thanks to [PurpleBooth](https://github.com/PurpleBooth) for this snazzy Markdown template"
Integreight/1Sheeld-Firmware,403,44,18,19,Organization,False,346,4,7,4,False,This is the source code of the firmware shipped with your 1Sheeld board. It is an implementation of a custom version of the Firmata protocol ported to ATmega162.,,0,7,0,0,0,0,0,0,2,0,0,2378,0,0,0,0,0,0,7,6,,,"# 1Sheeld Firmware [![Build Status](https://travis-ci.org/Integreight/1Sheeld-Firmware.svg?branch=master)](https://travis-ci.org/Integreight/1Sheeld-Firmware)#þþ## Overview ##þþThis is the source code of the firmware shipped with your 1Sheeld board. It is an implementation of a custom version of the Firmata protocal ported to ATmega162. It supports some of Firmata's messages like digital messages but lacks capability query, analog, I2C and servos messages.þþ## Building ##þþThe project is an generic C project, and can be built using any port of *avr-gcc* for either Microsoft Windows, Linux, or Mac OSX. Just make sure you have both *GNU Make* and *avr-gcc* tool chain installed on your platform then run ``` make ``` on the repo's root directory and both the classic and plus boards variations will be built in a new subdirectory called *build*.þþIf you are not going to use the *make* command, make sure you that either *PLUS_BOARD* or *CLASSIC_BOARD* is defined. (You can use the config.h file for that)þþ## Fuse Bits ##þþ- Low Value: 0xFDþ- High Value: 0xD8þ- Extended Value: 0xFBþþClick [here](http://eleccelerator.com/fusecalc/fusecalc.php?chip=atmega162&LOW=FD&HIGH=D8&EXTENDED=FB&LOCKBIT=CC) for a description of the enabled fuse bits.þþ## Uploading ##þþThe ICSP pins are exposed with a 6-pin header on the bottom of your 1Sheeld board. You can easily connect any ATmega programmer and upload your own version of the firmware.þþIf you are using any [USBasp programmer](http://www.fischl.de/usbasp/) and have *avrdude* installed, after building using ``` make ``` you can flash your board by running either ``` make flashdebug ``` or ``` make flashrelease ``` on the repo's root directory. Make sure you erase the chip first by running ``` make erase ```.þþNote: 1Sheeld is equipped with a bootloader that enables us to push updates to the firmware from the app. If you attempt to upload this firmware and erased the chip, you will lose this functionality.þþ## Documentation ##þþYou can generate the documentation by installing and running ``` doxygen ``` on the repo's root directory.þþ## Contribution ##þþContributions are welcomed, please follow this pattern:þ- Fork the repo.þ- Open an issue with your proposed feature or bug fix.þ- Commit and push code to a new branch in your forked repo.þ- Submit a pull request to our *development* branch.þþDon't forget to drop us an email, post on our forum, or mention us on Twitter or Facebook about what you have built using 1Sheeld, we would love to hear about it.þþ## Changelog ##þþTo see what has changed in recent versions of 1Sheeld Firmware, see the [Change Log](CHANGELOG.md).þþ## License and Copyright ##þþ```þThis code is free software; you can redistribute it and/or modify itþunder the terms of the GNU General Public License version 3 only, asþpublished by the Free Software Foundation.þþThis code is distributed in the hope that it will be useful, but WITHOUTþANY WARRANTY; without even the implied warranty of MERCHANTABILITY orþFITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public Licenseþversion 3 for more details (a copy is included in the LICENSE file thatþaccompanied this code).þþPlease contact Integreight, Inc. at info@integreight.com or post on ourþsupport forums www.1sheeld.com/forum if you need additional informationþor have any questions.þ```"
fcr--/lua-matrix,130,0,1,0,User,False,34,1,0,1,False,yet another matrix library for lua without external dependencies,,0,8,0,7,10,0,0,0,0,0,0,726,0,0,0,0,0,0,45,,8,,"# lua-matrixþYet another matrix library for lua without external dependenciesþþ## UsageþþImporting the library:þ```luaþmatrix = require 'matrix'þ```þþ#### Creationþþ| method | description |þ|--------|-------------|þ| `m = matrix.new(h, w)` or<br> `m = matrix.new{h, w}` | creates a new h\*w matrix with zeros |þ| `m = matrix.new{h, w, value=v}` | creates a new h\*w matrix with value v in every cell |þ| `m = matrix.random(h, w)` | creates a random h\*w matrix (values between 0 and 1) |þ| `m = matrix.id(n)` | creates an identity n\*n matrix |þ| `m = matrix.fromtable{v1, ..., vn, rows=h, cols=w}` | creates a matrix from a table |þþ#### ConversionþþTo convert a matrix to a table: `m:totable()` which returns a lua table with indexes 1 to m.rows\*m.colsþcontaining the values for each cell plus the dimensions with the keys rows and cols.þMatrices are stored in column major order, which means that any h\*w matrix will be stored linearlyþwith the first *1* .. *h* entries containing the first column; *h+1* to *2\*h* the second one, and so on.þþFor instance, the following table, will be converted to `{1,2, 3,4, 5,6, cols=3, rows=2}`:þþ| 1 | 3 | 5 |þ|---|---|---|þ| 2 | 4 | 6 |þþA simple way to initialize a matrix (when the values are known) is to convert a table back to a matrixþwith `matrix.fromtable(t)`, where the argument must have the same format as returned by `m:totable()`.þEven though the `cols` and `rows` keys may be optional (default 1), the number of numeric items in theþtable `#t` must be equal to `(t.cols or 1)*(t.rows or 1)`.þþ#### SlicingþþIn matrix jargon, slicing refers to the process of extracting a subset made of contiguous rows andþcontiguous columns.  In this library both reading and writing slices is supported.þþLet m be a h\*w matrix:þ* `m[{}]` ← returns a copy of the array.þ* `m[{1}]` ← returns a 1\*w matrix with the contents of m's first rowþ* `m[{{2,4}}]` ← returns a 3\*w matrix with the contents of m's second to fourth rowsþ* `m[{nil,2}]` ← returns a h\*1 matrix with a copy of m's second column.þ* `m[{{10,19},{20,39}]` ← returns a 10\*20 matrix with the rows 10 to 19 of the columns 20 to 39 of m.þþNote that returned slices are not ""views"" but full fledged matrices, copying the content of theþslice instead of sharing memory.þþSlices can also be used for writing operations using a similar syntax. Let m be a h\*w matrix:þ* `m[{1}] = 42` ← fills the matrix first row with fourty-twosþ* `m[{nil,{2,3}}] = p` ← copies p into m's second and third columns, p must be a h\*2 matrix.þþ#### Operationsþþ| element to element operation | description |þ|--------|-------------|þ| `m+N`, `N+m`, `m+m`, `rv+m`, `m+rw`, `cv+m`, `m+cv` | add matrix with number, matrix, row vector or column vector |þ| `m-N`, `N-m`, `m-m`, `rv-m`, `m-rw`, `cv+m`, `m-cv` | substract matrix to/from number, matrix, row vector or column vector |þ| `m*N`, `N*m`, `m*m`, `rv*m`, `m*rw`, `cv*m`, `m*cv` | element to element multiplication |þ| `m/N`, `N/m`, `m/m`, `rv/m`, `m/rw`, `cv/m`, `m/cv` | element to element division |þ| `m%N`, `N%m`, `m%m`, `rv%m`, `m%rw`, `cv%m`, `m%cv` | element to element remainder of division |þ| `m^N`, `N^m`, `m^m`, `rv^m`, `m^rw`, `cv^m`, `m^cv` | element to element exponentiation |þ| `-m` | negation (equivalent to `0-m`) |þþ| matrix operation | description |þ|------------------|-------------|þ| `m:t()`          | transposed matrix |þ| `m:lup()`        | LU decomposition (with permutation) and determinant |þ| `m:rref()`       | reduced row echelon form |þ| `m:inv()`        | matrix inversion using rref |þþTranspose: `m:t()`þ> Returns the transposed matrix.þþLUp decomposition: `lu = m:lup()`þ> Returns a table containing the arrays for the LU decomposition, where:þ> * `lu.L` is a lower triangular matrix with ones on its diagonal,þ> * `lu.U` is an upper triangular matrix, andþ> * `lu.P` is a permutation matrix (also saved as a table in lu.p, such that all lu.P's non-zero elements are located at `{i, lu.p[i]}`).þ> * `lu.det` m's determinant.þ>þ> Note that lu.P:t():dot(lu.L:dot(lu.U)) should be approximately equal to lu ± computation errors.þþReduced Row Echelon Form: `rref, inv = m:rref()`þ> This function returns the RREF of m. If m is square it also returns m's inverse. At each column this implementation chooses the row with biggest absolute value as the pivot (swapping the rows if needed) for best stability.þþMatrix inversion: `inv = m:inv()`þ> The m matrix must be square or an error is returned. Since the implementation uses `select(2, m:rref())` a result is returned even if it's not invertible, so (in that case) don't expect m:inv():inv() ≈ m.þ> To check for invertibility use `m.cols == m.rows and m:rref()[m.cols*m.rows] == 1`.þþ#### Mutable OperationsþþThe operations documented in this section change in some or other way the content of the matricesþused, and are only provided for performance reasons. Be careful if you choose to use them.þþRow swap: `m:rswap(i1, i2)`þ> Interchanges the contents of rows *i1* and *i2*. The arguments must be integers between 1 and `m.rows`.þþColumn swap: `m:cswap(j1, j2)`þ> Similar to row swap, but this time the contents of the columns *j1* and *j2* get swapped. The arguments must be integers between 1 and `m.cols`.þþReshape: `m:reshape(h, w)`þ> This changes the `rows` and `cols` attribute of the underlaying matrix without changing the content.þ> However, the total size of the matrix must be kept the same, thus h\*w must be equal to m.rows\*m.cols.þ> This can be used as a fast way to transpose a row vector to a column vector and vice versa."
JamesCrook/audacity-from-svn-5GB,162042,1,0,2,User,False,13946,2,0,1,False,Automatically exported from code.google.com/p/audacity,,0,20,0,45,3,0,0,0,0,0,0,1909,1,1,3,3,0,0,12,,40,,"# AudacityþAudacity(R): A Free, Cross-Platform Digital Audio Editor. http://audacity.sourceforge.net/þþThis git repository is the FULL version of https://code.google.com/p/audacity/ SVN converted to git by the standard conversion tools as of 24th March 2015.  It's a snapshot of SVN and all its history as used for release of Audacity 2.1.0.  If you clone this repository you will end up with over 5GB of content.  All the branches and all the tags from SVN will be expanded.  It is  much more likely that you will want the subtree created by following this stackoverflow answer http://stackoverflow.com/questions/359424/detach-subdirectory-into-separate-git-repository/17864475#17864475 just for audacity-src/trunk which is at https://github.com/audacity/audacity.þþThis repository is kept here for people who (for example) want to see the old website, or mezzo, or who want to view detailed contributor history.  þþWe welcome feedback on Audacity, suggestions for new or improved features, þbug reports and patches at:þ  feedback@audacityteam.org .þþPersonal support with Audacity is not provided by e-mail, but on our Forum:þ  http://audacityteam.org/forum/ .þþAudacity is copyright (c) 1999-2020 by Audacity Team. This copyright noticeþapplies to all documents in the Audacity source code archive, except asþotherwise noted (mostly in the lib-src subdirectories).þþThe documentation for Audacity is licensed under the Creative CommonsþAttribution 3.0 license:þhttp://creativecommons.org/licenses/by/3.0/legalcode .þþþ""Audacity"" is a registered trademark of Dominic Mazzoni.þþVersion 2.1.0 þþ--------------------------------------------------------------------------------þþ## LicensingþþThis program is free software; you can redistribute it and/or modify itþunder the terms of the GNU General Public License as published by theþFree Software Foundation; either version 2 of the License, or (at yourþoption) any later version. The program source code is also freelyþavailable as per Section 4 of this README.þþThis program is distributed in the hope that it will be useful, but WITHOUTþANY WARRANTY; without even the implied warranty of MERCHANTABILITYþor FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General PublicþLicense for more details.þþYou should have received a copy of the GNU General Public Licenseþalong with this program (in a file called LICENSE.txt); if not, goþto http://www.gnu.org/licenses/old-licenses/gpl-2.0.html or write to:þþ```þFree Software Foundation, Inc.þ59 Temple Place - Suite 330þBoston, MA 02111-1307 USAþ```þþ-------------------------------------------------------------------------------"
medivhna/template_adaptation,2252,3,3,1,User,False,187,1,0,12,False,"N. Crosswhite, J. Byrne, O. M. Parkhi, et al. Template adaptation for face verification and identification. arXiv preprint arXiv:1603.03958, 2016.",https://arxiv.org/abs/1603.03958,0,7,0,1,0,0,0,0,0,0,0,2679,0,0,0,0,0,0,49,,19,,
neftons-poker-lab/Poker_FSM,3332,0,1,1,Organization,False,2,2,1,1,False,Universal poker FSM C++,,0,6,0,5,0,0,0,0,0,0,0,2385,0,0,0,0,0,0,3,1,,,
christophbeatty/gnome-mplayer,9445,0,0,1,User,False,2418,3,38,1,False,Automatically exported from code.google.com/p/gnome-mplayer,,0,17,0,87,648,0,0,0,0,0,0,2917,0,0,0,0,0,0,1,,0,,
Helioform/d3dRTCW,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
PierreBougon/RayTracer,84459,1,4,0,User,False,237,1,0,4,False,RayTracer implementation in c.,http://raytracer.strikingly.com/,0,7,0,0,0,0,0,0,0,0,0,1524,0,0,0,0,0,0,24,,14,,"# RayTracerþ#### 3D Engine with RayTracing techniqueþ![Logo](assets/screenshots/logo2_0.png?raw=true ""Logo"")þþRayTracing implementation in c. Use our own ini format for the scenes.þþThis is a school project we realized in 1 month.þþ100% HandMade using LibLapin 1.8 which is minimal school grafical library.þþCheck out the [site](http://raytracer.strikingly.com/) to see more informations and images.þþþ## Build & Usageþþ* Run `make install` to install dependecies and the program.þ* Run `make` if you want to compile the progrman alone.þþ* To run the program :þ   - `./raytracer [scene.ini]`þþþ## Engine implementationþþ### Featuresþþ* Objectsþ  - Sphereþ  - Planþ  - Cubeþ  - Coneþ  - Cylinderþ  - Hyperboloidþ  - Paraboloidþ  - Helicoidþ  - Tore (Coming soon)þ  - Hole Cube (Coming soon)þ* Constructive Solid Geometryþ* Soft Shadowsþ* Anti-Aliasingþ* Texture Mappingþ* Reflectionþ* Phongþ* Perlin Noiseþ* Brightnessþ* Limited Objectsþþ### Reflectionþ![Engine](assets/screenshots/reflect.png?raw=true ""Reflection view"")þþ### Perlin noiseþ![Engine](assets/screenshots/perlin_noise.png?raw=true ""Perlin view"")þþ### Soft Shadowsþ![Engine](assets/screenshots/soft_shadows.png?raw=true ""Perlin view"")þþþ## GUI Interfaceþþ* Check out the [Documentation on the site](http://raytracer.strikingly.com/) if you want to learn how to use the interface. þþ### Use the interface to create your own scenesþ![Interface](assets/screenshots/itfc.png?raw=true ""Interface view"")þþþ## Rendered final scenesþ![scene](assets/screenshots/glass1.png?raw=true ""Basic view"")þ![scene](assets/screenshots/csg.png?raw=true ""Csg view"")þþþþ##### Project realized with Marc Brout, Benjamin Duhieu and Romain Samuel."
panaC/libft,156,0,1,0,User,False,98,1,0,1,False,libft 42,,0,8,0,1,0,0,0,0,0,0,0,950,0,0,0,0,0,0,59,,12,,"# 42 - LibFTþþFull C library from scratch for coding the Ansi C original library.þThis is the first big project at 42 school.þþThe aims of this project is the construction of own's C library as i could seeþþthe libft is sourounded by many .h header files like :þþ- libft.h : the main header contains about 60-70 fct.þ- get_next_line : an another 42 project : only one fct.þ- arg.h : my personnal arg cli handle, a fewer fct.þ- matrix.h : my personal matrix manipulation fct.þ- vector.h : my personal vector manipulation fct.þþ## Utilisationþþ```cþ#include ""libft.h""þ```þþ> Compile with libft.aþþ## Documentationþþ> Cf each header files for whenever i have doesn't create documentation for eachþþAll of this repository contains more than 100 fct."
radiganm/sandbox,13020,1,1,0,User,False,104,1,0,1,False,"Short, Self Contained, Correct (Compilable), Examples",http://sscce.org,0,7,0,0,0,0,0,0,0,0,0,1097,0,0,0,0,0,0,161,,2,,
barryo/attila-libss7,220,2,1,1,User,False,57,1,0,1,False,A patched libss7 stack supporting SS7 ISUP SAM messages from Domjan Attila. Added to GitHub for a howto I wrote and for prosperity! The original SVN repository is linked in the homepage URL. See http://www.barryodonovan.com/index.php/2012/01/12/asterisk-ss7-sam-support for my tutorial.,https://observer.router.hu/repos_pub/…,0,0,0,0,0,0,0,0,0,0,0,3078,0,0,0,0,0,0,30,,32,,
JvanKatwijk/swradio-8,35074,13,6,8,User,False,177,1,3,1,False,"shortwave receiver for use with sdrplay, hackrf, dabsticks and pmsdr",,0,8,0,0,1,0,0,0,0,0,0,845,1,2,34424,15067,0,0,29,,90,,"# swradio-8 [![Build Status](https://travis-ci.org/JvanKatwijk/swradio-8.svg?branch=master)](https://travis-ci.org/JvanKatwijk/swradio-8)þþ------------------------------------------------------------------þIntroductionþ------------------------------------------------------------------þþswradio is  Software for Linux and windows for listening to short wave radio.þIt is a rewrite and simplification of sdr-j-sw.þThe software supports hackrf devices, rtlsdr devicesþ(using the rt820 tuner chip) and the SDRplay devices.þ(For the rtlsdr based devices, use was made of a special version of theþlibrary, the one by Oliver Jowett. Sources are included in the source tree ofþthis program.)þþ![swradio-8](/swradio-picture-1.png?raw=true)þþOne of the less common decoders is a *drm-decoder*, the picture showsþthe reception of Nigeria. There are not that many drm transmissions,þbut Kuwait, Tiganesti (i.e. Romenia), and Nigeria are receoved very well here.þThe current version is limited to drm transmissions with a spectrumþequal to or less than 10 KHz.þþClassical decoders are - obviously - available as well. Early evening thereþis always - at least here - the ""Nederlandstalig amateurnet"" is well received.þþ![swradio-8](/swradio-picture-2.png?raw=true)þþ------------------------------------------------------------------------------þImplemented decodersþ-----------------------------------------------------------------------------þþ*Decoders** are:þ* amþ* ssb, with selection for usb or lsb;þ* psk, with a wide selection of modes and settings and with a visual tuning aid,þ* mfsk, with a visual tuning aid,þ* rtty, with a wide selection of modes and settings;þ* cw, with (almost) automatic selection of speed and a visual tuning aid,þ* drm, limited to 10 k bandwidth;þ* amtor, with a wide selection of options;þ* weatherfax decoder, with selection of a variety of settings.þþ![swradio-8](/swradio-picture-3.png?raw=true)þþThe ""main screen"" shows - next to the spectrum (or, if the widgetþis touched with the right mouse button a waterfall) -the selectedþfrequency (in Hz), the assumed signal strength at the selectedþfrequency, and - if installed - a brief description of the band toþwhich the selected frequency belongs.þþTouching a screen with the right mouse button will change the view onþthe spectrum from ""classical"" spectrum view to ""waterfall"" (and back).þþ![swradio-8](/swradio-picture-4.png?raw=true)þþThis feature applies to both the display showing the full spectrum asþthe display showing the ""decoder"" spectrum.þþ--------------------------------------------------------------------------þUsing the swradioþ---------------------------------------------------------------------------þþIf a configured device is connected to the computer where the program runs,þthe device will be connected and opened. If no device is detected,þit is assumed that file input is requested and a file selectionþmenu will appear (input files in PCM format, with a 2 channel,þ96000 samples/second configuration will be accepted.)þþMost controls are on the main widget. Touching the frequency selectþbutton will cause a keypad to be shown where the frequency can beþtypes in (in KHz or MHz).þþOne may select among a number of different filterings:þ* wide, used for e.g. DRM decoding, uses the full 12 k bandwidth;þ* am, used - as the name suggests - for am decoding, uses 9 k;þ* usb, used for smallband decoding in the upper side band, has a width of 2500 Hz;þ* lsb, used for ssb decoding is the lower sideband, has a width of 2500 HzþþThe input can be written to a file, that file can be processed later on.þþFrequency presetsþcan be stored, together with a user defined label (a program name).þA table of preferred frequencies (programs) isþmaintained between program invocations.þA selected frequency can be stored by pressing the save frequency button.þIf touched, one is asked to specify a name to be used to label that frequency.þThe pair (name, frequency) then is added to the list.þþSelecting such a ""preferred program"" is just by clicking the mouse on þthe programname or the associated field with the frequency.þþButtons and slider are equipped with a *tooltip*, touching the button orþslider will show a brief description of the function.þþ----------------------------------------------------------------------------þA bandplanþ----------------------------------------------------------------------------þþOn program startup the program reads in a file "".sw-bandplan.xml""þwith an xml encoded bandplan from the home directory (folder).þAn example bandplan file ""sw-bandplan.xml"" is part of the sourceþdistribution.þþ-------------------------------------------------------------------------------þWindowsþ--------------------------------------------------------------------------------þþFor Windows there is an installer, to be found in the releases section, setup-swradio.exe, for the swradio.þThe installer will install an executable as well as the required dll's. The installerþwill call the official installer for the dll implementing the api to get access to theþSDRplay device.þþ-------------------------------------------------------------------------------þLinuxþ-------------------------------------------------------------------------------þFor Linux there is a description of how to create an executable, it is writtenþfor Ubuntu, it is, however, simply to translate to scripts to be used with otherþdistros. þþFurthermore there is an ""appImage"", to be found in the releases sectionþ(while the indicator at the top mentions ""failure"", the appImage is correct).þThe appImage was created - using the travis service - in an Ubuntu 14 environment,þit contains the required libraries and should run on any more or less recent Linuxþenvironment. (Note that a passwd is asked form since the software tries to installþthe udev rules for the devices).þþþ-------------------------------------------------------------------------þUsing a pmSDR device and a soundcardþ-------------------------------------------------------------------------þþI dusted off my old pmSDR, dating from the era that the data was entering theþcomputer through the soundcard. Now, in that time I bought an HP Pavilionþsince - according to the specs - it could deliver samples with a rateþof 192K, sufficient to do experiments with FM demodulation, which isþwhat I wanted to do at that time.þþAnd indeed, samples could be read with a speed of 192k, however,þsome internal filtering in the stupid computer halved the signal in bandwidth,þso receiving a signal sampled at 192k gave me a signal with a bandwidth ofþless than 96k, completely useless for FM decoding (it is a time agoþbut still pretty frustrated about it).þþ(*frustration section:*)þEven further, when sampling on 96k, the band was halved as well,þso the effective bandwidth then would only be 48k. þOf course a solution to get 96K was to decimate (in software)þa signal sampled at 192k with a factor 2, but it was not why I bought theþstupid thing.þþI'll not report on the discussion I had with the HP service deskþ(bad for my health), I cannot remember to have met such unwilling (ignorant?)þpeople (and I can assure you that I met lots of them).þThey, the HP people, (c/w)ould not confirm or falsify that the band was halved. Theyþclaimed that they did not have a program to verify my claim, so my claimþwas (""by definition"") false. Seems a little silly for a largeþorganization like HP. þOf course, the programs that I wrote to show the bandwidth/samplerates were mine, not from HP, so resultsþI sent them (nice spectrum pictures) were ""not admissable as evidence"", so afterþsome talking they decided that there was no problem whatsoever, so no need to communicateþfurther. Needless to say that I'll never buy an HP laptop again. (*end of frustration section*)þþAnyway, for using a soundcard, I had to buy an external card, an EMU-202 withþwhich I did all kinds of FM decoding at the time in combination with the pmSDR.þHere we need ""only"" 96k, it works well under Linux and at the time it worked on W7.þHowever, it does not like Windows-10, using it under W10 leads to a crash.þþ![swradio-8](/swradio-pmsdr-drm.png?raw=true)þþ------------------------------------------------------------------þUbuntu Linuxþ------------------------------------------------------------------þþFor generating an executable under Ubuntu (16.04 or newer) one may take theþfollowing steps.þþ1. Download the source tree (it is assumed that you have a git client and cmake installed.þ   ```þ   git clone https://github.com/JvanKatwijk/swradio-8þ   ```þþ2. Fetch needed componentsþ   ```þ   sudo apt-get updateþ   sudo apt-get install qt5-qmake build-essential g++þ   sudo apt-get install libsndfile1-dev qt5-default libfftw3-dev portaudio19-dev þ   sudo apt-get install zlib1g-dev libusb-1.0-0-dev mesa-common-devþ   sudo apt-get install libgl1-mesa-dev libqt5opengl5-dev libsamplerate0-dev libqwt-qt5-devþ   sudo apt-get install qtbase5-devþþ   ```þþ3. Create the faad_drm library if you want to use the drm decoder.þ   To make life easy, the sources for the faad library are includedþ   in the source tree (packed).þþ   ```þ   cd ./swradio-8þ   tar zxvf faad2-2.8.8.tar.gzþ   cd faad2-2.8.8þ   ./configureþ   makeþ   sudo make installþ   cd ..þ   rm -rf faad2-2.8.8þ   ```þþ4. Device supportþþ  a) if you have an SDRplay device, I assume you already have installedþthe library, otherwise visit ""www.sdrplay.com""  and follow the instructions.þMake sure to uncomment in swradio-8.pro the lineþþ CONFIG += sdrplayþþ  b) the sources for using the pmSDR device are part of the sourcetree. Noteþthat for pmSDR the cardread functions are installed. The idea is to useþeither the pmSDR or the ""fast"" devices, reflected in the name. A configurationþwith (only) pmSDR will be named ""swradio-pmsdr"", a configuration with (only)þfast input devices will be named ""swradio-8"".þþFor selecting the pmSDR, make sure to uncomment in swradio-8.pro the lineþþ CONFIG += pmsdrþþand to comment out the linesþþ #CONFIG += sdrplayþ #CONFIG += rtlsdrþ #CONFIG += hackrfþþ   Create a file /etc/udev/rules.d/96-pmsdr.rules with as contentþþ #þ # udev rules file for Microchip 18F4455 USB Micro (PMSDR)þ #þ SUBSYSTEM==""usb"", ATTRS{idVendor}==""04d8"", ATTRS{idProduct}==""000c"", MODE:=""0666""þþ   to ensure non-root access to the device through usb.þþ   c) To make life easy, the sources for the required -non-standard - rtlsdr library used are included in the source tree,þ   again as a packed file.þþ  ```þ   tar zxvf rtl-sdr.tgzþ   cd rtl-sdr/þ   mkdir buildþ   cd buildþ   cmake ../ -DINSTALL_UDEV_RULES=ON -DDETACH_KERNEL_DRIVER=ONþ   makeþ   sudo make installþ   rm -rf rtl-sdrþ   sudo ldconfigþ   cd ..þ   rm -rf buildþ   cd ..þ  ```þþ   Make sure that a file exists in the `/etc/udev/rules.d` directoryþ   describing the device, allowing ""ordinary"" users to access the device.þþ   d) Create a library for the hackrf device by downloading the sources and compilingþþ   ```þ   git clone https://github.com/mossmann/hackrfþ   cd hackrfþ   cd hostþ   mkdir buildþ   cd buildþ   cmake .. -DINSTALL_UDEV_RULES=ONþ   sudo make installþ  ```þþ   Make sure that a file exists in the `/etc/udev/rules.d` directoryþ   describing the device, allowing ""ordinary"" users to access the device. I.e.þ   add yourself to the ""plugdev"" group.þ   þ5. Edit the `swradio-8.pro` file for configuring the supported devices and decoders.þ   For the devicesþþ * CONFIG += sdrplayþ * CONFIG += hackrfþ * CONFIG += rtlsdrþ   orþ * CONFIG += pmsdrþþ   Select - or deselect - decoders:þþ * CONFIG          += am-decoderþ * CONFIG          += ssb-decoderþ * CONFIG          += cw-decoderþ * CONFIG          += amtor-decoderþ * CONFIG          += psk-decoderþ * CONFIG          += rtty-decoderþ * CONFIG          += fax-decoderþ * CONFIG          += drm-decoderþ * CONFIG          += mfsk-decoderþþNote that the ""faad_drm"" library is (only) needed for the drm decoder.þþThe ""DESTDIR"" parameter in the unix section in the "".pro"" file tells where the result is to be put.þþ6. Check the installation path to qwt. If you were downloading it from http://qwt.sourceforge.net/qwtinstall.html please mention the correct path in `qt-dab.pro` file (for other installation change it accordingly): þ  ```þ  INCLUDEPATH += /usr/local/include  /usr/local/qwt-6.1.3þ  ````þþ7. Build and makeþ  ```þ  qmake qt-dab.proþ  makeþ  ```þAlternatively, you could use the ""cmake"" route. The file CMakeLists.txt-qt5 can be used for qt-5,þthe file CMakeLists.txt-qt4 is merely used for the construction of the appImage.þThe configurations here include the three mentioned ""fast"" devices.þþ-------------------------------------------------------------------------þ--------------------------------------------------------------------------þþ# Copyrightþþ Copyright (C)  2013, 2014, 2015, 2016, 2017, 2018þ Jan van Katwijk (J.vanKatwijk@gmail.com)þ Lazy Chair Computingþþ The swradio software is made available under the GPL-2.0.þ The SDR-J software, of which the sw-radio software is a part, þ is distributed in the hope that it will be useful,þ but WITHOUT ANY WARRANTY; without even the implied warranty ofþ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theþ GNU General Public License for more details.þþ# swradio-8"
ananjaser1211/RefinedNougat,222839,8,9,4,User,False,312,6,0,18,False,Samsung Galaxy Note 4 Exynos - Nougat Kernel,,0,8,0,0,0,0,0,0,1,0,0,2441,0,0,0,0,0,0,35,,115,,
facchinm/Arduino_avrusb_firmware,1867,3,6,4,User,False,13,2,0,2,False,firmwares for Arduino boards based on LUFA151115,,0,7,0,3,0,0,0,0,0,0,0,1622,0,0,0,0,0,0,133,,200,,
msikma/liballeg.4.4.2-osx,1997,2,1,0,User,False,5,1,1,1,False,Mac OS X build for Allegro (liballeg.4.4.2.dylib) plus tools and other binaries,,0,7,0,1,0,0,0,0,0,0,0,1356,0,0,0,0,0,0,124,,48,,
meepingsnesroms/libretro-palmm515emu,28895,93,13,13,User,False,1014,9,1,4,False,"A new Palm OS emulator targeting compatibility, speed and accuracy in that order.",https://meepingsnesroms.github.io/,3,8,0,6,24,0,1,0,68,0,0,817,0,0,0,0,0,0,61,,31,meepingsnesroms/Mu,
f0rb1dd3n/cve-2010-4221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
escalade/LEDE,150815,14,4,2,User,False,47558,3,8,594,False,Fork of the LEDE Project (“Linux Embedded Development Environment”),,0,8,0,0,0,0,0,0,1,0,0,5920,58,1040,1318771,1235469,0,0,12,,11,,
sylcrq/libevent,348,4,2,2,User,False,24,2,2,1,False,Libevent源码学习,,0,7,0,0,0,0,0,0,0,0,0,2098,0,0,0,0,0,0,64,,37,,libeventþ========þþ*[libevent](http://libevent.org/)源码学习*þþ* [libevent 0.1](http://libevent.org/old-releases.html#0.x)þþ* [libevent 1.0](http://libevent.org/old-releases.html#1.0)þþ*开发环境*þþ* Ubuntu 14.04 LTSþþ* gcc version 4.8.2þþBy syl
rryqszq4/php7-ext-jsonrpc,75,7,2,4,User,False,13,1,0,1,False,JsonRPC 2.0 client/server for PHP 7 extension,,0,7,0,0,0,0,0,0,0,0,0,1657,0,0,0,0,0,0,48,,119,,"php7-ext-jsonrpcþ================þþ[![Build Status](https://travis-ci.org/rryqszq4/php7-ext-jsonrpc.svg?branch=master)](https://travis-ci.org/rryqszq4/php7-ext-jsonrpc)þþJson-RPC 2.0 client/server for php-7 extension.þþ[php5.3~php5.6](https://github.com/rryqszq4/JsonRPC)þþFeaturesþ--------þ* JSON-RPC 2.0 protocolþ* Base on curl and epoll of the multi clientþ* Persistent epoll in php-fpmþ* Persistent curl_multi queue in php-fpmþ* Support message and notifi notificationþ* Linux only(need to epoll)þþRequirementþ-----------þ- PHP 7.0þþInstallþ-------þ```þ$/path/to/phpizeþ$./configure --with-php-config=/path/to/php-configþ$make && make installþ```þþServerþ-----------þ**Interface**þ- Jsonrpc_Server::__constructþ- Jsonrpc_Server::registerþ- Jsonrpc_Server::bindþ- Jsonrpc_Server::jsonformatþ- Jsonrpc_Server::rpcformatþ- Jsonrpc_Server::executeprocedureþ- Jsonrpc_Server::executecallbackþ- Jsonrpc_Server::executemethodþ- Jsonrpc_Server::getresponseþ- Jsonrpc_Server::executeþþ**Register Function**þ```phpþ<?phpþþ$server = new Jsonrpc_Server();þþ// style one function variableþ$add1 = function($a, $b){þ  return $a + $b;þ};þ$server->register('addition1', $add1);þþ// style two function stringþfunction add2($a, $b){þ  return $a + $b;þ}þ$server->register('addition2', 'add2');þþ// style three function closureþ$server->register('addition3', function ($a, $b) {þ    return $a + $b;þ});þþ//style four class method stringþclass Api þ{þ  static public function add($a, $b)þ  {þ    return $a + $b;þ  }þ}þ$server->register('addition4', 'Api::add');þþecho $server->execute();þþ//output >>>þ//{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32700,""message"":""Parse error""}}þþ?>þ```þþ**Bind Method**þ```phpþ<?phpþþ$server = new Jsonrpc_Server();þþclass Apiþ{þ  static public function add($a, $b)þ  {þ    return $a + $b;þ  }þþ  public function newadd($a,$b){þ    return $a + $b;þ  }þ}þþ$server->bind('addition5', 'Api', 'add');þþ$server->bind('addition6', $a=new Api, 'newadd');þþecho $server->execute();þþ//output >>>þ//{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32700,""message"":""Parse error""}}þþ?>þþ```þþClientþ------------þ**Interface**þ- Jsonrpc_Client::__constructþ- Jsonrpc_Client::callþ- Jsonrpc_Client::executeþþ**Persistent**þ> Jsonrpc_client(1) þ> When two resource epoll and curl_multi queue persist, the parame is 1. The default use of non-persistent.þþ**Multi Call**þ```phpþ<?phpþþ$client = new Jsonrpc_Client(1);þ$client->call('http://localhost/server.php', 'addition1', array(3,5));þ$client->call('http://localhost/server.php', 'addition2', array(10,20));þþ/* ... */þ$result = $client->execute();þþvar_dump($result);þþ//output >>>þ/*þarray(2) {þ  [0]=>þ  array(3) {þ    [""jsonrpc""]=>þ    string(3) ""2.0""þ    [""id""]=>þ    int(110507766)þ    [""result""]=>þ    int(8)þ  }þ  [1]=>þ  array(3) {þ    [""jsonrpc""]=>þ    string(3) ""2.0""þ    [""id""]=>þ    int(1559316299)þ    [""result""]=>þ    int(30)þ  }þ  ...þ}þ*/þ?>þ```þ**Custom ID**þ```phpþ<?phpþþ$client = new Jsonrpc_client(1);þ$client->call('http://localhost/server.php', 'addition', array(3,5),""custom_id_001"");þ$result = $client->execute();þvar_dump($result);þþ//output >>>þ/*þarray(1) {þ  [0]=>þ  array(3) {þ    [""jsonrpc""]=>þ    string(3) ""2.0""þ    [""id""]=>þ    string(13) ""custom_id_001""þ    [""result""]=>þ    int(8)þ  }þ}þ*/þ?>þ```þþError Infoþ--------------þ**jsonrpc 2.0 Error**þ```javascriptþ// Parse errorþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32700,""message"":""Parse error""}}þþ// Invalid Requestþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32600,""message"":""Invalid Request""}}þþ// Method not foundþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32601,""message"":""Method not found""}}þþ// Invalid paramsþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32602,""message"":""Invalid params""}}þþ//þ```þþ**HTTP Error**þ```javascriptþ// 400þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32400,""message"":""Bad Request""}}þ// 401þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32401,""message"":""Unauthorized""}}þ// 403þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32403,""message"":""Forbidden""}}þ// 404þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32404,""message"":""Not Found""}}þþ// 500þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32500,""message"":""Internal Server Error""}}þ// 502þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32502,""message"":""Bad Gateway""}}þ...þþ// unknowþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32599,""message"":""HTTP Unknow""}}þ```þþ**Curl Error**þ```javascriptþ// 1 CURLE_UNSUPPORTED_PROTOCOLþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32001,""message"":""Curl Unsupported Protocol""}}þþ// 2 CURLE_FAILED_INITþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32002,""message"":""Curl Failed Init""}}þþ// 3 CURLE_URL_MALFORMATþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32003,""message"":""Curl Url Malformat""}}þþ// 4þ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32004,""message"":""Curl Not Built In""}}þþ// 5 CURLE_COULDNT_RESOLVE_PROXYþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32005,""message"":""Curl Couldnt Resolve Proxy""}}þþ// 6 CURLE_COULDNT_RESOLVE_HOSTþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32006,""message"":""Curl Couldnt Resolve Host""}}þþ// 7 CURLE_COULDNT_CONNECTþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32007,""message"":""Curl Couldnt Connect""}}þ...þþ// CURL ERROR UNKNOWþ{""jsonrpc"":""2.0"",""id"":null,""error"":{""code"":-32099,""message"":""Curl Error Unknow""}}þ```"
essteban/ofxImageTS,10172,2,1,0,User,False,8,1,0,1,False,openFrameworks image/pixel fx library,,0,7,0,0,0,0,0,0,0,0,0,1468,0,0,0,0,0,0,37,,33,,# ofxImageTSþ![photo](ideas.png)
svn2github/ccl,6984,2,1,0,User,False,283,1,0,0,False,"This is a clone of an SVN repository at http://ccl.googlecode.com/svn. It had been cloned by http://svn2github.com/ , but the service was since closed. Please read a closing note on my blog post: http://piotr.gabryjeluk.pl/blog:closing-svn2github . If you want to continue synchronizing this repo, look at https://github.com/gabrys/svn2github",,0,7,0,2,0,0,0,0,0,0,0,,0,0,0,0,0,0,1,,302,,
keith-epidev/deltabot,65276,3,2,1,User,False,34,1,0,1,False,Open CNC / 3D Printer Deltabot,http://printer.epidev.com,0,6,0,0,0,0,0,0,0,0,0,2518,0,0,0,0,0,0,3,,5,,"Deltabot Projectþ================þþThis 'DeltaBot' is the product of Keith Brown's final year electronics project at [La Trobe University](http://www.latrobe.edu.au/). It falls under the umbrella project called [RepRap](http://reprap.org/). þþThis repository serves as a place to store and track progress at a source level. Forcing software, hardware and designs into a single repository might not be the most practical use of git but it does allow me to easily share updates. Once the components come close to a stable release I will break this up into individual repos.þþIf you are interested, you can follow a brief blog at [http://printer.epidev.com](http://printer.epidev.com).þþ##Goalsþ- Cheapþ- Modularþ- CNC etching + 3D printingþ- Produce alternative hardware, electonics and software. (Simply to learn)þ- Automated Calibrationþþ##VersionsþThere are three variants being produced by this project:  þ1. Large robot  þ2. Small, slow but strong platform  þ3. Small, fast robot  þþ##InspirationþAlthough I have not forked any current designs, this project is still based upon exiting work.þThis project is primarily a derivative of the RepRap [Kossel](https://github.com/jcrocholl/kossel) by [jcrocholl](https://github.com/jcrocholl)þOf course I have also been inspired by other insane delta robots:þ- [http://www.youtube.com/watch?v=Gv5B63HeF1E](http://www.youtube.com/watch?v=Gv5B63HeF1E)þ- [http://www.youtube.com/watch?v=0-Kpv-ZOcKY](http://www.youtube.com/watch?v=0-Kpv-ZOcKY)þ- [http://www.youtube.com/watch?v=NDzUiZsbQtw](http://www.youtube.com/watch?v=NDzUiZsbQtw)þþAnd every other RepRap printer. But mostly The RepRap Project founder, Adrian Bowyer.þ- [http://en.wikipedia.org/wiki/File:Adrian_Bowyer_-_PopTech_2007.webm](http://en.wikipedia.org/wiki/File:Adrian_Bowyer_-_PopTech_2007.webm)"
eggfly/LinkItProjects,2222,4,1,6,User,False,59,1,0,1,False,my linkit one and linkit assist 2502 projects,,0,7,0,0,0,0,0,0,0,0,0,1685,0,0,0,0,0,0,682,,55,,# LinkItProjectsþmy linkit one and linkit assist 2502 projectsþTODO: a good gitignore file.
lldavuull/PWM,3433,1,0,2,User,False,15,2,0,1,False,DMX512 for PIC16F1574,,0,7,0,1,0,0,0,0,0,0,0,1125,0,0,0,0,0,0,7,,6,,"# RDMþDMX512 for PIC16F1574þþ1. See video : https://www.youtube.com/watch?v=7dGFsjU8-uQ  (0:02~1:54)þSorry for Chinese speech, this video used for self introduction to professor.þþ2. 0:27~0:43 is MPLAB X IDE, 'xc.h' is in the IDE. I don't use makefile, because I compile on this IDE.þMPLAB X is free, but the compiler 'xc8' isn't free. You would find xc8 crack software to install it.þþ3. The code extended from PIC demonstration code (DMX512 only), code is at bottom.þhttps://www.microchip.com/design-centers/intelligent-lighting-control/tools/lighting-communications-development-platformþþ4. The code used for PIC16F1575, the space is twice to pic16f1574.þcircuit diagram:þ![](circuit%20diagram.png)"
david-wang-1/ghost-mouse,6895,1,2,0,User,False,8,1,0,1,False,Final Project for Microcontrollers & Embedded Systems: Bluetooth Connected Gesture Recognition mouse,,0,7,0,0,0,0,0,0,0,0,0,1146,0,0,0,0,0,0,10,,4,,# ghost-mouseþFinger gesture operated Bluetooth mouse. Devpost link with more details can be found here: https://devpost.com/software/ghost-mouse. 
ghosoft/FlashLedSW,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
RockySong/micropython-rocky,82287,142,16,70,User,False,201,5,8,5,False,micropython and OpenMV port to NXP MCUs,,0,8,0,10,1,0,0,0,12,0,5,880,2,42,2521844,3975872,0,0,14,,63,,"[![Build Status](https://travis-ci.org/micropython/micropython.png?branch=master)](https://travis-ci.org/micropython/micropython) [![Coverage Status](https://coveralls.io/repos/micropython/micropython/badge.png?branch=master)](https://coveralls.io/r/micropython/micropython?branch=master)þþThe MicroPython and OpenMV port to NXP MCUsþ=======================þ<p align=""center"">þ  <img src=""https://raw.githubusercontent.com/micropython/micropython/master/logo/upython-with-micro.jpg"" alt=""MicroPython Logo""/>þ  <img src=""https://github.com/RockySong/micropython-rocky/blob/master/omvrt_banner.jpg"" alt=""project banner"" />þ</p>þþThis is project is cloned from MicroPython project, which aims to put an implementationþof Python 3.x on microcontrollers and small embedded systems.þYou can find the official website at [micropython.org](http://www.micropython.org).þþWARNING: this project is in beta stage and is subject to changes of theþcode-base, including project-wide name changes and API changes.þþMicroPython implements the entire Python 3.4 syntax (including exceptions,þ`with`, `yield from`, etc., and additionally `async`/`await` keywords fromþPython 3.5). The following core datatypes are provided: `str` (includingþbasic Unicode support), `bytes`, `bytearray`, `tuple`, `list`, `dict`, `set`,þ`frozenset`, `array.array`, `collections.namedtuple`, classes and instances.þBuiltin modules include `sys`, `time`, and `struct`, etc. Select ports haveþsupport for `_thread` module (multithreading). Note that only a subset ofþPython 3 functionality is implemented for the data types and modules.þþMicroPython can execute scripts in textual source form or from precompiledþbytecode, in both cases either from an on-device filesystem or ""frozen"" intoþthe MicroPython executable.þþSee the repository http://github.com/micropython/pyboard for the MicroPythonþboard (PyBoard), the officially supported reference electronic circuit board.þþMajor components in this repository:þ- py/ -- the core Python implementation, including compiler, runtime, andþ  core library.þ- mpy-cross/ -- the MicroPython cross-compiler which is used to turn scriptsþ  into precompiled bytecode.þ- ports/unix/ -- a version of MicroPython that runs on Unix.þ- ports/nxp_rt1050_60/ -- a version of MicroPython that runs on the imxrtevk105x and similarþ  i.mx rt1050/60 boards (using NXP's MCUXpresso SDK drivers). OpenMV is also ported to i.MX RT1050/60 devices.þ- ports/nxp_lpc546/ -- a version of MicroPython that runs on the LPCXPresso54608 board and similarþ  lpc54608 boards (using NXP's MCUXpresso SDK drivers).þ- tests/ -- test framework and test scripts.þ- docs/ -- user documentation in Sphinx reStructuredText format. Renderedþ  HTML documentation is available at http://docs.micropython.org (be sureþ  to select needed board/port at the bottom left corner).þþAdditional components:þ- ports/bare-arm/ -- a bare minimum version of MicroPython for ARM MCUs. Usedþ  mostly to control code size.þ- ports/teensy/ -- a version of MicroPython that runs on the Teensy 3.1þ  (preliminary but functional).þ- ports/pic16bit/ -- a version of MicroPython for 16-bit PIC microcontrollers.þ- ports/cc3200/ -- a version of MicroPython that runs on the CC3200 from TI.þ- ports/esp8266/ -- an experimental port for ESP8266 WiFi modules.þ- extmod/ -- additional (non-core) modules implemented in C.þ- tools/ -- various tools, including the pyboard.py module.þ- examples/ -- a few example Python scripts.þþThe subdirectories above may include READMEs with additional info.þþ""make"" is used to build the components, or ""gmake"" on BSD-based systems.þYou will also need bash, gcc, and Python (at least 2.7 or 3.3).þþThe Unix versionþ----------------þþThe ""unix"" port requires a standard Unix environment with gcc and GNU make.þx86 and x64 architectures are supported (i.e. x86 32- and 64-bit), as wellþas ARM and MIPS. Making full-featured port to another architecture requiresþwriting some assembly code for the exception handling and garbage collection.þAlternatively, fallback implementation based on setjmp/longjmp can be used.þþTo build (see section below for required dependencies):þþ    $ git submodule update --initþ    $ cd ports/unixþ    $ make axtlsþ    $ makeþþThen to give it a try:þþ    $ ./micropythonþ    >>> list(5 * x + y for x in range(10) for y in [4, 2, 1])þþUse `CTRL-D` (i.e. EOF) to exit the shell.þLearn about command-line options (in particular, how to increase heap sizeþwhich may be needed for larger applications):þþ    $ ./micropython --helpþþRun complete testsuite:þþ    $ make testþþUnix version comes with a builtin package manager called upip, e.g.:þþ    $ ./micropython -m upip install micropython-pystoneþ    $ ./micropython -m pystoneþþBrowse available modules onþ[PyPI](https://pypi.python.org/pypi?%3Aaction=search&term=micropython).þStandard library modules come fromþ[micropython-lib](https://github.com/micropython/micropython-lib) project.þþExternal dependenciesþ---------------------þþBuilding MicroPython ports may require some dependencies installed.þþFor Unix port, `libffi` library and `pkg-config` tool are required. OnþDebian/Ubuntu/Mint derivative Linux distros, install `build-essential`þ(includes toolchain and make), `libffi-dev`, and `pkg-config` packages.þþOther dependencies can be built together with MicroPython. This mayþbe required to enable extra features or capabilities, and in recentþversions of MicroPython, these may be enabled by default. To buildþthese additional dependencies, first fetch git submodules for them:þþ    $ git submodule update --initþþUse the same command to get the latest versions of dependencies, asþthey are updated from time to time. After that, in the port directoryþ(e.g. `ports/unix/`), execute:þþ    $ make deplibsþþThis will build all available dependencies (regardless whether theyþare used or not). If you intend to build MicroPython with additionalþoptions (like cross-compiling), the same set of options should be passedþto `make deplibs`. To actually enable/disable use of dependencies, editþ`ports/unix/mpconfigport.mk` file, which has inline descriptions of the options.þFor example, to build SSL module (required for `upip` tool described above,þand so enabled by dfeault), `MICROPY_PY_USSL` should be set to 1.þþFor some ports, building required dependences is transparent, and happensþautomatically. They still need to be fetched with the git submodule commandþabove.þþThe i.mx RT105x and LPC54608 versionþ-----------------þþThe ""rt1050_60"" port supports GCC toolchain (as mpy official) and KEIL.þTo build under keil, just open KEIL project in ""prj_keil_<MCU series>"" folder.þTo build under GCC, requires an ARM compiler, arm-none-eabi-gcc, and associatedþbin-utils.  For those using Arch Linux, you need arm-none-eabi-binutils,þarm-none-eabi-gcc and arm-none-eabi-newlib packages.  Otherwise, try here:þhttps://launchpad.net/gcc-arm-embeddedþþTo build:þþ    $ git submodule update --initþ    $ cd ports/nxp_rt105þ    $ makeþþþþContributingþ------------þþMicroPython and OpenMV are open-source projects and welcome contributions. To beþproductive, please be sure to follow theþ[Contributors' Guidelines](https://github.com/micropython/micropython/wiki/ContributorGuidelines)þand the [Code Conventions](https://github.com/micropython/micropython/blob/master/CODECONVENTIONS.md).þNote that MicroPython is licenced under the MIT license, and all contributionsþshould follow this license."
bcomnes/fpga-lab,43404,3,1,0,User,False,27,1,1,1,False,Repo for educational labs using a Digilent Nexys 3 FPGA card,,0,6,0,0,0,0,0,0,0,0,0,2749,0,0,0,0,0,0,407,,450,,"fpga-labþ========þþRepo for educational labs using a Digilent Nexys 3 FPGA card.þþI'll write a real readme soon, but here is some useful information to work with in the meantime.þþ## Some useful Linksþþ*   [Cosmiac Website](http://www.cosmiac.org/development.html)þ    *   [Example Labs](http://www.cosmiac.org/Projects_FPGA.html#Lab1) - A bunch of labs having to do with a Nexys 2 board, but many of the labs have been updated for the Nexys 3.þ*   [Xilinx University Program](http://www.xilinx.com/university/index.htm) - A program associated with the Nexys 3 board, providing educational licenses to the IDE used to program their FPGAs.þ*   [Digilent Tutorials](http://www.digilentinc.com/classroom/Tutorials/) - Tutorials provided to accompany the Nexys 3 board."
Nikesh001/android_kernel_xiaomi_msm8937,809431,4,0,8,User,False,532432,3,0,5947,False,,,0,8,0,0,0,0,0,0,0,0,0,6641,0,0,0,0,0,0,37,,53,,
smurfjack/MJOY16-CN,10153,5,3,3,User,False,19,1,0,1,False,Joystick With Mega16,,0,7,0,0,0,0,0,0,0,0,0,1573,0,0,0,0,0,0,4,,0,,# MJOY16-CNþ����DIY�������վҡ�ˣ��Ӵ���MJOY16þMJOY16��Mindaugas Milasauskas������Ӳ����Դ����������Դ�����񻹺����о���ֵ�����߿�����MJOY����Դ����mega8Ϊ���أ����Լ������Mega16Ϊ���ص�MJOY16����Ƶúܹ淶��֧��8�����������64��������16���������أ�4������ñ�����ʺ�DIYþ���������Ѿ��Ҳ����ټ��ˣ�����˹���ڴ˻����Ͽ�����MMJOY������arduino��Դ��arduino��Ȼ�����ã���������mega32�����ʺ��ֹ�DIY����������ԭ����C�������Ľ�MJOY16��������˸�������MJOY16-CN��֧��ԭ��̼�������ҵ��ʱ������Դ������Դ�������ħ���о�������������ԭ���ߵ�ԭ���·���Լ�ʹ��40pin IDE�ӿ���չ������ӵ��뷨�� ��mega16��4��IO����ȫ������������JTAG���Խӿڣ�Ӧ����˵�����Ժ�ǿ�ˣ���ȫ������Ϊavr�Ŀ���������������;��
lalalaring/AdbWide,4460,3,3,3,User,False,6,1,0,1,False,Unicode Support Adb ;Chinese path support;中文支持的adb,http://yanyuhongchen.net,0,7,0,0,0,0,0,0,0,0,0,2203,0,0,0,0,0,0,446,,24,,AdbWideþ=======þþUnicode Support Adb ;Chinese path supportþ中文路径支持的ADBþshell也支持中文þþ[![Build status](https://ci.appveyor.com/api/projects/status/dpdsmak9r7q9se5y?svg=true)](https://ci.appveyor.com/project/lalalaring/adbwide)
pdroaugust/waytickets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
arii/FollowTheYellowBrickRoad,1134,0,1,2,User,False,20,1,0,2,False,16.31 class project :-),,0,7,0,0,1,0,0,0,1,0,0,1671,0,0,0,0,0,0,35,,54,,# FollowTheYellowBrickRoadþ16.31 class project :-)
satanthedoge/Arduinomodule1,6794,0,0,0,User,False,27,1,0,3,False,Premier module projet,,0,8,0,0,0,0,0,0,0,0,0,943,0,0,0,0,0,0,7,,0,,
atrisovic/yadageexamples,2,0,0,0,User,False,4,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1377,0,0,0,0,0,0,42,,22,,# yadageexamples
SpicedEggs/decimal_turn,2,0,1,0,User,False,4,1,0,1,False,decimal turn to hex octal,,0,7,0,0,0,0,0,0,0,0,0,1328,0,0,0,0,0,0,3,,0,,
mingzhou-yang/LeetCode,196,0,1,0,User,False,5,1,0,1,False,LeetCode programming problems,,0,7,0,0,0,0,0,0,0,0,0,2035,0,0,0,0,0,0,4,,1,,
5icode/mydpp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Found,
sgfowler98/TCPServer,4,0,0,0,User,False,3,1,0,1,False,TCP Server and Client for Networks & Network Programming class,,0,8,0,0,0,0,0,0,0,0,0,831,0,0,0,0,0,0,7,,0,,"# TCPServerþTCP Server and Client for Networks &amp; Network Programming classþsgfowle - Sam FowlerþþThis program is a TCP server and client. The client creates a socket, connects,þand then sends data line-by-line from an input file, then closes the connection.þThe server creates a socket, binds the address, listens, then when it accepts aþclient it receives the data, reverses the strings, and prints the reversed stringsþto stdout, then it closes the connection.þþFiles:þtcpserver.cþThis file provides the implementation of the TCP serverþþtcpclient.cþThis file provides the implementation of the TCP clientþþMakefileþThis file provides compilation rules for the programþþOther information:þWhen the total file length is greater than the server's buffer, ntohl(len) was returningþhuge numbers which would make the program exit on the first line length check even thoughþthe length was well below the max length.þWhen trying to investigate that, sometimes my program was appending 'time' to my firstþline of output, and then later it was adding vdso_get_ to my output. I couldn't findþany relevant information about this problem and my only guess is something to do withþthe OS."
gitpan/Net-Sharktools,220,0,4,0,Organization,False,7,1,23,0,False,Read-only release history for Net-Sharktools,http://metacpan.org/release/Net-Shark…,0,7,0,,,,,0,0,0,0,,0,0,0,0,0,0,36,2,,,
Swainstha/GPIB_To_DRAW,16009,0,0,0,User,False,4,1,0,1,False,The coordinates of waveform can be taken from oscilloscope 5400B and it was transferred to computer via a GPIB to UART converter which we built ourselves and using opengl we plotted the points in the computer screen.,,0,8,0,0,0,0,0,0,0,0,0,978,0,0,0,0,0,0,43,,9,,
kitsune-denshi/sysmon,449,1,1,0,User,False,5,1,0,1,False,USB system load monitor,,0,7,0,0,0,0,0,0,0,0,0,1601,0,0,0,0,0,0,2,,0,,# sysmonþUSB system load monitor
az333/31-10-17-systems,2,0,1,0,User,False,3,1,0,0,False,,,0,8,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,40,,0,,# 31-10-17-systems
shun-yo/cethping,160,0,0,0,User,False,14,1,0,1,False,L2 packet sender,,0,8,0,0,0,0,0,0,0,0,0,719,0,0,0,0,0,0,6,,3,,"# cethpingþþRaw socketで通信。[ethping](https://github.com/y-sira/pyng)を参考。パケットを作って文字列""Hi""を送信する。þþþEnvironmentþ-------þþ- Ubuntu14.04þþþCompileþ-------þmakeでコンパイルþ```bashþmakeþ```þþþUsageþ-------þþ### Serverþreceiver側のインターフェース名を指定þþ```bashþsudo cethpingd $INTERFACE_NAMEþ```þþ### Clientþ接続先のMACアドレスとsender側のインターフェースを指定þ```bashþsudo cethping $DESTINATION_MAC_ADDRESS $SOURCE_INTERFACE_NAMEþ```þþþScreen shotþ-------þ![screen shot](images/image1.png)"
noitojp/nutil,92,1,1,1,User,False,5,1,0,1,False,,,0,6,0,0,0,0,0,0,0,0,0,2987,0,0,0,0,0,0,9,,5,,
onkar37/ROD,1422,0,0,0,User,False,10,1,0,1,False,,,0,7,0,0,0,0,0,0,0,0,0,1356,0,0,0,0,0,0,4,,0,,
uccs/librte,952,2,9,0,Organization,False,156,3,0,3,False,The RTE project,,0,6,0,10,8,0,0,0,2,0,0,2497,0,0,0,0,0,0,2,0,,,librteþ======þþThe RTE project
JIAQIA/static_power,360,0,1,0,User,False,6,1,0,1,False,static power control,,0,6,0,0,0,0,0,0,0,0,0,2630,0,0,0,0,0,0,9,,0,,static_powerþ============þþstatic power control
sdu-hpcl/BGSA,1192,1,9,3,Organization,False,20,2,1,2,False,A Bit-Parallel Global Sequence Alignment Toolkit for Multi-core and Many-core Architectures.,https://sdu-hpcl.github.io/BGSA/,0,7,0,0,0,0,0,0,1,0,0,1048,0,0,0,0,0,0,6,1,,,"# BGSAþBGSA is a bit-parallel global sequence alignment toolkit for multi-core and many-core architectures. BGSAþsupports both Myers algorithm (including the banded version) and BitPAl in order to gain fast speed of calculating unit-cost global alignments as well as global alignments with general integer scoring schemes.þþ## Contentsþþ<!-- toc -->þþ- [Features](#features)þ- [Usage](#usage)þ- [Generator](#generator)þ- [Performance](#performance)þ- [Use BGSA in your project](#use-bgsa-in-your-project)þ- [Support or Contact](#support-or-contact)þþ<!-- tocstop -->þþ## Featuresþ* Supports both **unit-cost global aligment([Myers algorithm](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.9395&rep=rep1&type=pdf))** and **general integer scoring global aligment([BitPAl algorithm](https://www.ncbi.nlm.nih.gov/pubmed/25075119))**.þ* Supports both **global and semi-global aligment**.þ* It also implements the **banded Myers algorithm** for fast verification under a given error threshold _e_;þ* **Supports multiple SIMD intrinsics**: SSE, AVX2, KNC and AVX512. It also supports **heterogeneous platform** that contains KNC and CPU.þ* The parallel framework can **extend to other similar algorithm easily** with a little change.þ* **Super faster** than other similar softwares.þþ## Usageþ* Step 1: Use the _generator_ program to  generate the kernel source with your specified score and architecture. And you need the Java runtime 1.7 or higher.þ ```þ cd generatorþ java -jar generator.jar -M 2 -I -3 -G -5 -a sseþ ```þþ* Step 2: Move the generated file to the BGSA source folder accroding to your selected architecture, and then run `make` command. þ ```þ mv generated/align_core.c ../original/BGSA_SSE/þ cd ../original/BGSA_SSE/þ makeþ ```þ The default compiler is icc and CPU platform. You can pass `arch=KNL` for KNL platform and `cc=gcc` for other compilers.þ ```þ # use gcc compiler on KNL platformþ make arch=KNL cc=gccþ ```þþ* Step 3: Run a test.þ ```þ ./aligner -q sample-data/query.txt -d sample-data/subject.txt -f data/result.txtþ ```þþ* Step 4: Convert the result file to readable format.þ ```þ ./convert -r data/result.txtþ ```þþ## GeneratorþWe provide a code generator written in Java to quickly generate algorithm kernel source for a wide variety of devices as well as different scoring schemes. The following are the parameters of the generator module:þþ```þ-M,--match <arg>:             Specify the match score which should be positive or zero. Default is 2.þþ-I,--mismatch <arg>:          Specify the mismatch score which should be negative. Default is -3.þþ-G,--gap <arg>:               Specify the gap score which should be negative. Default is -5.þþ-d,--directory <arg>:         Specify the directory where to place generated source files.þþ-n,--name <arg>:              Set the name of generated source file.þþ-t,--type <arg>:              Set BitPAl algorithm type. Valid values are: non-packed, packed. Default is packed.þþ-m,--myers:                   Using Myers' algorithm. Valid values are: 0, 1. 0 presents weights(0, -1, -1) and 1 presents weights(0, 1, 1). Default is 0.þþ-a,--arch:                    Specify the SIMD architecture. Valid values are: none, sse, avx2, knc, avx512. Default is sse. If you want generate kernel source for multiple architectures, you can use '-' to join them as none-sse-avx2þþ-e,--element:                 Specify vector element size. Valid value is 64, 32, 16, 8. Default is 32.þþ-s,--semi:                    Using semi-global algorithm.þþ-b,--banded:                  Using banded myers algorithm.þþ-h,--help:                    Display help Information.þ```þþ## PerformanceþþThe following figures show the performance comparison of BGSA, Parasail and SeqAn on CPU and Xeon Phi platforms for unit scoring scheme. From the figures, we can see that the performance of BGSA is much faster than that of the other two algorithms. For more performance evluations, you can see our paper(under submission). And you can download test data from [this link](https://pan.baidu.com/s/1JFmfIYzOBH_TK9V_4IFFiw).þþ![](images/cpu.png)þ> Comparison on E5-2620 and W-2123þþ![](images/knl.png)þ> Comparison on Phi-7110 and Phi-7210þþ## Use BGSA in your projectþYou can use BGSA in your project by directly copying header and source files. For simplicity, you can first save the sequences to be compared into temporary files, and then call BGSA to read sequences from the temporary files, calculate alignment scores and save them to a result file, and finally your program can read the scores from the result file. If you want to use sequences already stored in an array, you can modify the logic in calling `get_read_from_file` and `get_ref_from_file` and change the pointer to point the existing sequence array. þþIf you just want to use the kernel alignment method, you need to process your data into the format required by this method. The following is a complete demo using SSE intrinsics:þþ> demo.cþþ```cþ#include <stdio.h>þ#include <string.h>þ#include ""align_core.h""þþint cpu_threads = 1;þþvoid align(char *query, char **subjects, int subject_count) {þ    int query_len = strlen(query);þ    int subject_len = strlen(subjects[0]);þ    int real_subject_count = subject_count;þ    int total_subject_count = real_subject_count;þ    if (real_subject_count % SSE_V_NUM != 0) {þ        total_subject_count = (real_subject_count / SSE_V_NUM + 1) * SSE_V_NUM;þ    }þ    seq_t subject_seq;þ    subject_seq.content = malloc_mem(sizeof(char) * total_subject_count * (subject_len + 1));þ    subject_seq.count = real_subject_count;þ    subject_seq.len = subject_len;þ    for (int i = 0; i < total_subject_count; i++) {þ        for (int j = 0; j < subject_len + 1; j++) {þ            subject_seq.content[i * (subject_len + 1) + j] = subjects[i][j];þ        }þ    }þþ    int word_num;þ    if (full_bits) {þ        word_num = (subject_len + SSE_WORD_SIZE) / (SSE_WORD_SIZE - 1);þ    } else {þ        word_num = (subject_len + SSE_WORD_SIZE - 1) / (SSE_WORD_SIZE - 2);þ    }þ    int pre_process_size = sizeof(sse_read_t) * word_num * CHAR_NUM * real_subject_count;þ    sse_read_t *pre_precess_subjects = malloc_mem(pre_process_size);þ    init_mapping_table();þ    sse_handle_reads(&subject_seq, pre_precess_subjects, word_num, 0, total_subject_count);þþ    char *pre_precess_query = malloc_mem(sizeof(char) * query_len);þ    for (int i = 0; i < query_len; i++) {þ        pre_precess_query[i] = mapping_table[query[i]];þ    }þþ    sse_write_t *results = malloc_mem(sizeof(sse_write_t) * total_subject_count);þ    __m128i *bit_mem = malloc_mem(sizeof(__m128i) * word_num * dvdh_len);þ    align_sse(pre_precess_query, pre_precess_subjects, query_len, subject_len, word_num, 1, 0, results, bit_mem);þ    for (int i = 0; i < real_subject_count; i++) {þ        printf(""%d\n"", results[i]);þ    }þ    free_mem(subject_seq.content);þ    free_mem(pre_precess_subjects);þ    free_mem(pre_precess_query);þ    free_mem(results);þ    free_mem(bit_mem);þ}þþint main() {þ    char *query = ""AAAA"";þ    char *subjects[4] = {""AAAA"", ""AACA"", ""CAAC"", ""AGGG""};þ    align(query, subjects, 4);þ    return 0;þ}þ```þYou can use the following command to compile demo.c .þ```bashþicc -o demo demo.c global.c align_core.c -qopenmpþ```þþ## Support or ContactþþIf you have any questions, please contact: Weiguo,Liu ( weiguo.liu@sdu.edu.cn)."
x42/convoLV2,182,16,8,4,User,False,159,3,25,3,False,LV2 convolution plugin,http://x42-plugins.com/x42/x42-convolver,0,6,0,0,5,0,1,0,2,0,0,2854,1,3,23,14,0,0,109,,277,,"convoLV2þ========þþconvoLV2 is a [LV2](http://lv2plug.in) plugin to convolve audio signals withþzero latency.þþIt is a very basic plugin: Currently the only parameter is the Impulse-Response fileþand hence it is robust and efficient convolver.þþþThe plugin comes in three variants:þ*   Mono:  1 channel in, 1 channel out. Mono IR fileþ*   Mono To Stereo:  1 channel in, 2 channel out. Stereo IR file. (L, R)þ*   True Stereo: 2 in, 2 out.  4 channel IR file (L -> L, R -> R, L -> R, R -> L)þþExcess channels in an IR file are ignored. If an IR file has insufficient channelsþfor the required configuration, channel-assignment wraps around (modulo file channel count).þþconvoLV2's main use-case is cabinet-emulation and generic signal processing where latency matters.þþFor fancy reverb applications, see also [IR.lv2](https://tomszilagyi.github.io/plugins/ir.lv2/)þþInstallþ-------þþ```bashþmakeþsudo make install PREFIX=/usrþþ# Test in jalv LV2 hostþjalv.gtk http://gareus.org/oss/lv2/convoLV2#Monoþ# orþjalv.gtk http://gareus.org/oss/lv2/convoLV2#MonoToStereoþ# orþjalv.gtk http://gareus.org/oss/lv2/convoLV2#Stereoþ```þþþNote to packagers: The Makefile honors `PREFIX` and `DESTDIR` variables as wellþas `CFLAGS`, `LDFLAGS` and `OPTIMIZATIONS` (additions to `CFLAGS`), alsoþsee the first 10 lines of the Makefile.þYou really want to package the superset of [x42-plugins](https://github.com/x42/x42-plugins).þþþUnder the hoodþ--------------þþ[libzita-convolver](http://kokkinizita.linuxaudio.org/linuxaudio/downloads/) is used toþperform the convolution, [libsndfile](http://www.mega-nerd.com/libsndfile/) to readþthe impulse-response and [libsamplerate](http://www.mega-nerd.com/SRC/) to resampleþthe IR if necessary.þþconvoLV2 was written to demonstrate new features of LV2 1.2.0 (back in 2012):þþ*   http://lv2plug.in/ns/ext/buf-size/#powerOf2BlockLength - the plugin requires a blocksize that is a power of two.þ*   http://lv2plug.in/ns/ext/buf-size/#maxBlockLength - the plugin only works with blocksizes between 64 and 8192 samples per period.þ*   http://lv2plug.in/ns/ext/patch/ - allow a host to pass filenames to a plugin.þ*   http://lv2plug.in/ns/ext/worker/ - on/offline instances. Re-loading an IR file is performed in the background, making the plugin realtime safe.þþIt since serves as example code for those LV2 extensions.þþþWhile the convolution engine supports pre-delay, channel-mapping and per-channel gain settings, these parametersþare currently not exposed in the LV2 interface (hack tip: they are supported in the LV2 DSP and saved asþtext in the plugin-state which can be directly edited)."
aignacio/esp_rgb,620,0,0,0,User,False,1,1,0,0,False,esp code for rgb lights,,0,7,0,0,0,0,0,0,0,0,0,,0,0,0,0,0,0,77,,25,,"**esp_mqtt**þ==========þ![](https://travis-ci.org/tuanpmt/esp_mqtt.svg?branch=master)þþThis is MQTT client library for ESP8266, port from: [MQTT client library for Contiki](https://github.com/esar/contiki-mqtt) (thanks)þþþþ**Features:**þþ * Support subscribing, publishing, authentication, will messages, keep alive pings and all 3 QoS levels (it should be a fully functional client).þ * Support multiple connection (to multiple hosts).þ * Support SSL connection (sdk 1.3 with path)þ * Easy to setup and useþ * Update support SDK 1.3þþ**Compile:**þþMake sure to add PYTHON PATH and compile PATH to Eclipse environment variable if using Eclipseþþfor Windows:þþ```bashþgit clone --recursive https://github.com/tuanpmt/esp_mqttþcd esp_mqttþ#cleanþmingw32-make cleanþ#makeþmingw32-make SDK_BASE=""c:/Espressif/ESP8266_SDK"" FLAVOR=""release"" allþ#flashþmingw32-make ESPPORT=""COM1"" flashþ```þþfor Mac or Linux:þþ```bashþgit clone --recursive https://github.com/tuanpmt/esp_mqttþcd esp_mqttþ#cleanþmake cleanþ#makeþmake SDK_BASE=""/opt/Espressif/ESP8266_SDK"" FLAVOR=""release"" allþ#flashþmake ESPPORT=""/dev/ttyUSB0"" flashþ```þþ**Usage**þ```cþ#include ""ets_sys.h""þ#include ""driver/uart.h""þ#include ""osapi.h""þ#include ""mqtt.h""þ#include ""wifi.h""þ#include ""config.h""þ#include ""debug.h""þ#include ""gpio.h""þ#include ""user_interface.h""þ#include ""mem.h""þþMQTT_Client mqttClient;þþvoid wifiConnectCb(uint8_t status)þ{þ if(status == STATION_GOT_IP){þ  MQTT_Connect(&mqttClient);þ } else {þ  MQTT_Disconnect(&mqttClient);þ }þ}þvoid mqttConnectedCb(uint32_t *args)þ{þ MQTT_Client* client = (MQTT_Client*)args;þ INFO(""MQTT: Connected\r\n"");þ MQTT_Subscribe(client, ""/mqtt/topic/0"", 0);þ MQTT_Subscribe(client, ""/mqtt/topic/1"", 1);þ MQTT_Subscribe(client, ""/mqtt/topic/2"", 2);þþ MQTT_Publish(client, ""/mqtt/topic/0"", ""hello0"", 6, 0, 0);þ MQTT_Publish(client, ""/mqtt/topic/1"", ""hello1"", 6, 1, 0);þ MQTT_Publish(client, ""/mqtt/topic/2"", ""hello2"", 6, 2, 0);þþ}þþvoid mqttDisconnectedCb(uint32_t *args)þ{þ MQTT_Client* client = (MQTT_Client*)args;þ INFO(""MQTT: Disconnected\r\n"");þ}þþvoid mqttPublishedCb(uint32_t *args)þ{þ MQTT_Client* client = (MQTT_Client*)args;þ INFO(""MQTT: Published\r\n"");þ}þþvoid mqttDataCb(uint32_t *args, const char* topic, uint32_t topic_len, const char *data, uint32_t data_len)þ{þ char *topicBuf = (char*)os_zalloc(topic_len+1),þ   *dataBuf = (char*)os_zalloc(data_len+1);þþ MQTT_Client* client = (MQTT_Client*)args;þþ os_memcpy(topicBuf, topic, topic_len);þ topicBuf[topic_len] = 0;þþ os_memcpy(dataBuf, data, data_len);þ dataBuf[data_len] = 0;þþ INFO(""Receive topic: %s, data: %s \r\n"", topicBuf, dataBuf);þ os_free(topicBuf);þ os_free(dataBuf);þ}þþþvoid user_init(void)þ{þ uart_init(BIT_RATE_115200, BIT_RATE_115200);þ os_delay_us(1000000);þþ CFG_Load();þþ MQTT_InitConnection(&mqttClient, sysCfg.mqtt_host, sysCfg.mqtt_port, sysCfg.security);þ //MQTT_InitConnection(&mqttClient, ""192.168.11.122"", 1880, 0);þþ MQTT_InitClient(&mqttClient, sysCfg.device_id, sysCfg.mqtt_user, sysCfg.mqtt_pass, sysCfg.mqtt_keepalive, 1);þ //MQTT_InitClient(&mqttClient, ""client_id"", ""user"", ""pass"", 120, 1);þþ MQTT_InitLWT(&mqttClient, ""/lwt"", ""offline"", 0, 0);þ MQTT_OnConnected(&mqttClient, mqttConnectedCb);þ MQTT_OnDisconnected(&mqttClient, mqttDisconnectedCb);þ MQTT_OnPublished(&mqttClient, mqttPublishedCb);þ MQTT_OnData(&mqttClient, mqttDataCb);þþ WIFI_Connect(sysCfg.sta_ssid, sysCfg.sta_pwd, wifiConnectCb);þþ INFO(""\r\nSystem started ...\r\n"");þ}þþ```þþ**Publish message and Subscribe**þþ```cþ/* TRUE if success */þBOOL MQTT_Subscribe(MQTT_Client *client, char* topic, uint8_t qos);þþBOOL MQTT_Publish(MQTT_Client *client, const char* topic, const char* data, int data_length, int qos, int retain);þþ```þþ**Already support LWT: (Last Will and Testament)**þþ```cþþ/* Broker will publish a message with qos = 0, retain = 0, data = ""offline"" to topic ""/lwt"" if client don't send keepalive packet */þMQTT_InitLWT(&mqttClient, ""/lwt"", ""offline"", 0, 0);þþ```þþ#Default configurationþþSee: **include/user_config.h**þþIf you want to load new default configurations, just change the value of CFG_HOLDER in **include/user_config.h**þþ**Define protocol name in include/user_config.h**þþ```cþ#define PROTOCOL_NAMEv31 /*MQTT version 3.1 compatible with Mosquitto v0.15*/þ//PROTOCOL_NAMEv311   /*MQTT version 3.11 compatible with https://eclipse.org/paho/clients/testing/*/þ```þþIn the Makefile, it will erase section hold the user configuration at 0x3C000þþ```bashþflash: firmware/0x00000.bin firmware/0x40000.binþ $(PYTHON) $(ESPTOOL) -p $(ESPPORT) write_flash 0x00000 firmware/0x00000.bin 0x3C000 $(BLANKER) 0x40000 firmware/0x40000.bin þ```þThe BLANKER is the blank.bin file you find in your SDKs bin folder.þþ**Create SSL Self sign**þþ```þopenssl req -x509 -newkey rsa:1024 -keyout key.pem -out cert.pem -days XXXþ```þþ**SSL Mqtt broker for test**þþ```javascriptþvar mosca = require('mosca')þvar SECURE_KEY = __dirname + '/key.pem';þvar SECURE_CERT = __dirname + '/cert.pem';þvar ascoltatore = {þ  //using ascoltatoreþ  type: 'mongo',þ  url: 'mongodb://localhost:27017/mqtt',þ  pubsubCollection: 'ascoltatori',þ  mongo: {}þ};þþvar moscaSettings = {þ  port: 1880,þ  stats: false,þ  backend: ascoltatore,þ  persistence: {þ    factory: mosca.persistence.Mongo,þ    url: 'mongodb://localhost:27017/mqtt'þ  },þ  secure : {þ    keyPath: SECURE_KEY,þ    certPath: SECURE_CERT,þ    port: 1883þ  }þ};þþvar server = new mosca.Server(moscaSettings);þserver.on('ready', setup);þþserver.on('clientConnected', function(client) {þ    console.log('client connected', client.id);þ});þþ// fired when a message is receivedþserver.on('published', function(packet, client) {þ  console.log('Published', packet.payload);þ});þþ// fired when the mqtt server is readyþfunction setup() {þ  console.log('Mosca server is up and running')þ}þ```þþ**Example projects using esp_mqtt:**<br/>þ- [https://github.com/eadf/esp_mqtt_lcd](https://github.com/eadf/esp_mqtt_lcd)þþ**Limited:**<br/>þ- Not fully supported retransmit for QoS1 and QoS2þþ**Status:** *Pre release.*þþ[https://github.com/tuanpmt/esp_mqtt/releases](https://github.com/tuanpmt/esp_mqtt/releases)þþ[MQTT Broker for test](https://github.com/mcollina/mosca)þþ[MQTT Client for test](https://chrome.google.com/webstore/detail/mqttlens/hemojaaeigabkbcookmlgmdigohjobjm?hl=en)þþ**Contributing:**þþ***Feel free to contribute to the project in any way you like!***þþ**Requried:**þþSDK esp_iot_sdk_v0.9.4_14_12_19 or higherþþ**Authors:**þ[Tuan PM](https://twitter.com/TuanPMT)þþþþ**LICENSE - ""MIT License""**þþCopyright (c) 2014-2015 Tuan PM, https://twitter.com/TuanPMTþþPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:þþThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.þþTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
FluidSynth/fluidsynth,6890,606,40,111,Organization,False,2108,8,29,38,False,Software synthesizer based on the SoundFont 2 specifications,http://www.fluidsynth.org,12,10,2,16,337,5,23,1,299,1,25,6305,7,82,2378,1868,0,0,1,2,,,"# FluidSynthþþ| | Build Status |þ|---|---|þ| <img src=""https://www.kernel.org/theme/images/logos/tux.png"" height=""30"" alt=""""> **Linux** | [![Build Status Travis](https://travis-ci.org/FluidSynth/fluidsynth.svg?branch=master)](https://travis-ci.org/FluidSynth/fluidsynth/branches) |þ| <img src=""https://cdn.pling.com/img//hive/content-pre1/112422-1.png"" height=""25"" alt=""""> **FreeBSD** | [![Build Status](https://api.cirrus-ci.com/github/FluidSynth/fluidsynth.svg?branch=master)](https://cirrus-ci.com/github/FluidSynth/fluidsynth) |þ| <img src=""https://www.microsoft.com/windows/favicon.ico"" height=""25"" alt=""""> **Windows** | [![Build Status](https://dev.azure.com/tommbrt/tommbrt/_apis/build/status/FluidSynth.fluidsynth.Win?branchName=master)](https://dev.azure.com/tommbrt/tommbrt/_build/latest?definitionId=3&branchName=master) |þ| <img src=""https://www.microsoft.com/windows/favicon.ico"" height=""25"" alt=""""> **Windows (vcpkg)** | [![Build status](https://ci.appveyor.com/api/projects/status/anbmtebt5uk4q1it/branch/master?svg=true)](https://ci.appveyor.com/project/derselbst/fluidsynth-g2ouw/branch/master) |þ| <img src=""https://www.apple.com/favicon.ico"" height=""30"" alt=""""> **MacOSX** | [![Build Status](https://dev.azure.com/tommbrt/tommbrt/_apis/build/status/FluidSynth.fluidsynth.macOS?branchName=master)](https://dev.azure.com/tommbrt/tommbrt/_build/latest?definitionId=5&branchName=master) |þ| <img src=""https://www.android.com/favicon.ico"" height=""30"" alt=""""> **Android** | [![CircleCI](https://circleci.com/gh/FluidSynth/fluidsynth/tree/master.svg?style=shield)](https://circleci.com/gh/FluidSynth/fluidsynth) |þþþþ#### FluidSynth is a cross-platform, real-time software synthesizer based on the Soundfont 2 specification.þþFluidSynth generates audio by reading and handling MIDI events from MIDI input devices by using a [SoundFont](https://github.com/FluidSynth/fluidsynth/wiki/SoundFont). It is the software analogue of a MIDI synthesizer. FluidSynth can also play MIDI files.þþ[![OHLOH Project Stats](https://www.openhub.net/p/fluidsynth/widgets/project_thin_badge?format=gif)](https://www.openhub.net/p/fluidsynth)þþ## DocumentationþþThe central place for documentation and further links is our **wiki** here at GitHub:þþ#### https://github.com/FluidSynth/fluidsynth/wikiþþIf you are missing parts of the documentation, let us know by writing to our mailing list.þOf course, you are welcome to edit and improve the wiki yourself. All you need is an account at GitHub. Alternatively, you may send an EMail to our mailing list along with your suggested changes. Further information about the mailing list is available in the wiki as well.þþLatest information about FluidSynth is also available on the web site at http://www.fluidsynth.org/.þþ## LicenseþþThe source code for FluidSynth is distributed under the terms of the [GNU Lesser General Public License](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html), see the [LICENSE](https://github.com/FluidSynth/fluidsynth/blob/master/LICENSE) file. To better understand the conditions how FluidSynth can be used in e.g. commercial or closed-source projects, please refer to the [LicensingFAQ in our wiki](https://github.com/FluidSynth/fluidsynth/wiki/LicensingFAQ).þþ## Building from sourceþþFor information on how to build FluidSynth from source, please [refer to our wiki](https://github.com/FluidSynth/fluidsynth/wiki/BuildingWithCMake).þþ## Linksþþ- FluidSynth's Home Page, http://www.fluidsynth.orgþþ- FluidSynth's wiki, https://github.com/FluidSynth/fluidsynth/wikiþþ- FluidSynth's API documentation, http://www.fluidsynth.org/api/þþ---þþ## Historical backgroundþþ### Why did we do itþþThe synthesizer grew out of a project, started by Samuel Bianchini andþPeter Hanappe, and later joined by Johnathan Lee, that aimed atþdeveloping a networked multi-user game.þþSound (and music) was considered a very important part of the game. Inþaddition, users had to be able to extend the game with their ownþsounds and images. Johnathan Lee proposed to use the Soundfontþstandard combined with intelligent use of midifiles. The argumentsþwere:þþ- Wavetable synthesis is low on CPU usage, it is intuitive and it canþ  produce rich soundsþþ- Hardware acceleration is possible if the user owns a Soundfontþ  compatible soundcard (important for games!)þþ- MIDI files are small and Soundfont2 files can be made small thru theþ  intelligent use of loops and wavetables. Together, they are easier toþ  downloaded than MP3 or audio files.þþ- Graphical editors are available for both file format: variousþ  Soundfont editors are available on PC and on Linux (Smurf!), andþ  MIDI sequencers are available on all platforms.þþIt seemed like a good combination to use for an (online) game. þþIn order to make Soundfonts available on all platforms (Linux, Mac,þand Windows) and for all sound cards, we needed a software Soundfontþsynthesizer. That is why we developed FluidSynth.þþ### Design decisionsþþThe synthesizer was designed to be as self-contained as possible forþseveral reasons:þþ- It had to be multi-platform (Linux, macOS, Win32). It was thereforeþ  important that the code didn't rely on any platform-specificþ  library.þþ- It had to be easy to integrate the synthesizer modules in variousþ  environments, as a plugin or as a dynamically loadable object. Iþ  wanted to make the synthesizer available as a plugin (jMax, LADSPA,þ  Xmms, WinAmp, Director, ...); develop language bindings (Python,þ  Java, Perl, ...); and integrate it into (game) frameworks (Crystalþ  Space, SDL, ...). For these reasons I've decided it would be easiestþ  if the project stayed very focused on its goal (a Soundfontþ  synthesizer), stayed small (ideally one file) and didn't dependentþ  on external code."
vitasdk/samples,535,137,22,50,Organization,False,75,3,0,23,False,Sample code.,,0,7,0,2,10,1,1,2,41,0,1,1727,1,1,3,0,0,0,13,3,,,"# vitasdk code samplesþþ## PrerequisitesþþIn order to build a vita sample, you need to add the toolchain `bin/` directory to your `$PATH`.þþ## BuildingþþEvery samples directory should include a CMake list.þTo build a sample, place yourself into this directory and use the `cmake . && make` command to build it.þþ## RunningþþTo run a sample:þ- Send the generated `.vpk` to your vita:þ - Start an FTP server on your vita (for example, with VitaShell - by pressing the select button).þ - Upload the `.vpk` to the Vita using your FTP client (for example, with Curl - with `curl -T *.vpk ftp://YOUR_VITA_IP:1337/ux0:/`)þ - If Curl returns `fatal: No names found, cannot describe anything` it mean that you are trying to overwrite a folder with a file. Add a `/` to the end of your url to explain that you want to upload IN this folder.þ- Install the `.vpk` on your vita using a vpk installer (for example, with VitaShell - by pressing the X button on the `.vpk`)þ- This will create a new folder in the `ux0:/app/` for the sample.þþ## Building everythingþþUse the following command to build every samples:þþ```þmkdir build && cd buildþcmake ..þmakeþ```þþ## List of samplesþþ* `audio`: Simple audio wave generator.þ* `camera`: Demonstration of camera features.þ* `common`: Common functions for samples.þ* `ctrl`: A minimal controller (button) sample.þ* `debug_print`: A minimal debug print sample.þ* `debugscreen`: Debug text printing sample.þ* `hello_cpp_world`: A minimal hello world sample in C++.þ* `hello_world`: A minimal hello world sample.þ* `ime`: Graphical dialog sample.þ* `microphone`: Demonstration of microphone features.þ* `motion`: Prints accelerometer data.þ* `net_http`: A minimal HTTP download sample.þ* `net_http_bsd`: A minimal HTTP download sample using BSD sockets.þ* `net_libcurl`: A libcurl download sample.þ* `power`: A minimal power sample.þ* `pretty_livearea`: A minimal hello world sample with example livearea styling and features.þ* `prx_loader`: Load/list prx modules.þ* `prx_simple`: Minimal sample prx module.þ* `redrectangle`: Example SDL rendering.þ* `rtc`: A minimal RTC sample.þ* `socket_ping`: ICMP ping using raw sockets.þ* `soloud`: Plays an audio file and Text To Speech.þ* `touch`: A minimal touch sample.þþ## Notes on imagesþ- Images shall use indexed palettes (PNG-8 128 Dithered).þ- The size of an image shall not exceed 420KB.þ- For some reasons, some PNG files created by GIMP makes the .vpk installation crash.þ- You can further minimize overhead by running your images through [pngquant](https://pngquant.org/).þþ## Notes on supporting files and foldersþ- File names shall not exceed 32B.þ- Directory names shall not exceed 16B.þ- Folder creation shall not exceed one level.þþ## Notes on XMLþ- UTF-8 character encoding, CRLF line termination.þ- File size shall not exceed 32KB.þ- Different visual styles are available, check the sample `pretty_livearea` for an example.þþ## LicenseþþAll code and build scripts in this repo is licensed under the terms of [CC0 1.0 Universal](https://creativecommons.org/publicdomain/zero/1.0/)."
drhelius/Gearboy,56093,437,34,101,User,False,1399,2,24,21,False,"Game Boy / Gameboy Color emulator for iOS, macOS, Raspberry Pi, Windows, Linux and RetroArch.",http://twitter.com/drhelius,11,5,0,6,72,4,16,0,39,0,2,2875,1,234,243506,176853,0,0,42,,107,,"# Gearboyþþ[![Build Status](https://travis-ci.org/drhelius/Gearboy.svg?branch=master)](https://travis-ci.org/drhelius/Gearboy)þþGearboy is a cross-platform Game Boy / GameBoy Color emulator written in C++ that runs on Windows, macOS, Linux, iOS, Raspberry Pi and RetroArch.þþThis is an open source project with its ongoing development made possible thanks to the support by these awesome [backers](backers.md).þþPlease, consider [sponsoring](https://github.com/sponsors/drhelius) and following me on [Twitter](https://twitter.com/drhelius) for updates.þþ----------þþ## Downloadsþþ- **Windows**: [Gearboy-3.1.1-Windows.zip](https://github.com/drhelius/Gearboy/releases/download/gearboy-3.1.1/Gearboy-3.1.1-Windows.zip)þ  + NOTE: You may need to install the [Microsoft Visual C++ Redistributable](https://go.microsoft.com/fwlink/?LinkId=746572)þ- **macOS**:þ  + `brew cask install gearboy`þ  + Or install manually: [Gearboy-3.1.1-macOS.zip](https://github.com/drhelius/Gearboy/releases/download/gearboy-3.1.1/Gearboy-3.1.1-macOS.zip)þ- **Linux**: [Gearboy-3.1.1-Linux.tar.xz](https://github.com/drhelius/Gearboy/releases/download/gearboy-3.1.1/Gearboy-3.1.1-Linux.tar.xz)þ  + NOTE: You may need to install `libsdl2` and `libglew`þ- **iOS**: Build Gearboy with Xcode and transfer it to your device. You can open rom files from other apps like Safari or Dropbox, or use your iCloud Drive.þ- **RetroArch**: [Libretro core documentation](https://libretro.readthedocs.io/en/latest/library/gearboy/).þ- **Raspberry Pi**: Build Gearboy from sources. Optimized projects are provided for Raspberry Pi 1, 2, 3 and 4.þþ## Featuresþþ- Accurate CPU emulation, passes cpu_instrs.gb from blargg's tests.þ- Accurate instruction and memory timing, passes instr_timing.gb and mem_timing.gb from blargg's tests.þ- Supported cartridges: ROM, ROM + RAM, MBC1, MBC2, MBC3 + RTC, MBC5, HuC-1 and MBC1M (multicart).þ- Accurate LCD controller emulation with correct timings and priorities including mid-scanline effects.þ- Game Boy Color support.þ- LCD screen ghosting effect as seen in the original Game Boy.þ- LCD dot matrix effect.þ- Sound emulation using SDL Audio and [Gb_Snd_Emu library](http://blargg.8bitalley.com/libs/audio.html#Gb_Snd_Emu).þ- Save battery powered RAM cartridges to file.þ- Save states.þ- Compressed rom support (ZIP).þ- Game Genie and GameShark cheat support.þ- Supported platforms: Windows, Linux, macOS, Raspberry Pi, iOS and RetroArch (libretro).þ- Full debugger with disassembler, breakpoints, debug symbols, memory editor, IO inspector and and VRAM viewer including tiles, sprites, backgrounds and palettes.þþ<img src=""http://www.geardome.com/files/gearboy/gearboy_debug_01.png"" width=""880"" height=""455"">þþ## Build Instructionsþþ### Windowsþþ- Install Microsoft Visual Studio Community 2019 or later.þ- Open the Gearboy Visual Studio solution `platforms/windows/Gearboy.sln` and build.þ- You may want to use the `platforms/windows/Makefile` to build the application using MinGW.þþ### macOSþþ- Install Xcode and run `xcode-select --install` in the terminal for the compiler to be available on the command line.þ- Run this commands to generate a Mac *app* bundle:þþ``` shellþbrew install sdl2þcd platforms/macosþmake distþ```þþ### Linuxþþ- Ubuntu / Debian:þþ``` shellþsudo apt-get install build-essential libsdl2-dev libglew-devþcd platforms/linuxþmakeþ```þþ- Fedora:þþ``` shellþsudo dnf install @development-tools gcc-c++ SDL2-devel glew-develþcd platforms/linuxþmakeþ```þþ### iOSþþ- Install Xcode for macOS. You need iOS 13 SDK or later.þ- Build the project `platforms/ios/Gearboy.xcodeproj`þ- Run it on real hardware using your iOS developer certificate. Make sure it builds on *Release* for better performance.þþ### Libretroþþ- Ubuntu / Debian:þþ``` shellþsudo apt-get install build-essentialþcd platforms/libretroþmakeþ```þþ- Fedora:þþ``` shellþsudo dnf install @development-tools gcc-c++þcd platforms/libretroþmakeþ```þþ### Raspberry Pi 4 - Raspbian (Desktop)þþ``` shellþsudo apt install build-essential libsdl2-dev libglew-devþcd platforms/raspberrypi4þmakeþ```þþ### Raspberry Pi 2 & 3 - Raspbian (CLI)þþ- Install and configure [SDL 2](http://www.libsdl.org/download-2.0.php) for development:þþ``` shellþsudo apt-get updateþsudo apt-get upgradeþsudo apt-get install build-essential libfreeimage-dev libopenal-dev libpango1.0-dev libsndfile-dev libudev-dev libasound2-dev libjpeg-dev libtiff5-dev libwebp-dev automakeþcd ~þwget https://www.libsdl.org/release/SDL2-2.0.12.tar.gzþtar zxvf SDL2-2.0.12.tar.gzþcd SDL2-2.0.12 && mkdir build && cd buildþ../configure --disable-pulseaudio --disable-esd --disable-video-mir --disable-video-wayland --disable-video-x11 --disable-video-opengl --host=armv7l-raspberry-linux-gnueabihfþmake -j 4þsudo make installþ```þþ- Install libconfig library dependencies for development: `sudo apt-get install libconfig++-dev`þ- Use `make -j 4` in the `platforms/raspberrypi3/x64/` folder to build the project.þ- Use `export SDL_AUDIODRIVER=ALSA` before running the emulator for the best performance.þ- Gearboy generates a `gearboy.cfg` configuration file where you can customize keyboard and gamepads. Key codes are from [SDL](https://wiki.libsdl.org/SDL_Keycode).þþ## Accuracy TestsþþCompared to other emulators: [see here](http://tasvideos.org/EmulatorResources/GBAccuracyTests.html).þþTests from [blargg's test roms](https://github.com/retrio/gb-test-roms):þþ![cpu_instrs.gb](http://www.geardome.com/files/gearboy/gearboy_001.png)![insrt_timing.gb](http://www.geardome.com/files/gearboy/gearboy_002.png)![lcd_sync.gb](http://www.geardome.com/files/gearboy/gearboy_003.png)![dmg_sound.gb](http://www.geardome.com/files/gearboy/gearboy_032.png)![cgb_sound.gb](http://www.geardome.com/files/gearboy/gearboy_033.png)![mem_timing.gb](http://www.geardome.com/files/gearboy/gearboy_memtiming2.png)þþ## Screenshotsþþ![Screenshot](http://www.geardome.com/files/gearboy/gearboy_004.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_006.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_008.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_022.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_013.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_023.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_015.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_029.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_011.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_024.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_017.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_016.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_034.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_026.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_018.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_025.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_021.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_027.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_019.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_020.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_031.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_028.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_007.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_009.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_010.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_005.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_012.png)![Screenshot](http://www.geardome.com/files/gearboy/gearboy_014.png)þþ## LicenseþþGearboy is licensed under the GNU General Public License v3.0 License, see [LICENSE](LICENSE) for more information."
kuro68k/Retro-Adapter,16381,0,1,0,User,False,3,1,0,1,False,Retro Adapter V2,,0,7,0,0,0,0,0,0,0,0,0,1230,0,0,0,0,0,0,49,,3,,# Retro-AdapterþRetro Adapter V2þþConnect various old computer and game console controllers to USB.þþhttp://denki.world3.net/retro_v2.html
hachque-Emscripten/libpng-1.2.49,528,3,1,1,Organization,False,1,1,0,1,False,libpng port for Emscripten,,0,6,0,2,0,0,0,0,0,0,0,2924,0,0,0,0,0,0,4,1,,,
